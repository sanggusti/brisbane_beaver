{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Louis Owen (https://louisowen6.github.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfparser import pdf_to_text\n",
    "from preprocessor import preprocessing\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "import pylcs\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_1_doc = pdf_to_text('../data/essay_1.pdf')\n",
    "essay_1_similar_doc = pdf_to_text('../data/essay_1_similar.pdf')\n",
    "essay_2_doc = pdf_to_text('../data/essay_2.pdf')\n",
    "essay_2_similar_doc = pdf_to_text('../data/essay_2_similar.pdf')\n",
    "essay_3_doc = pdf_to_text('../data/essay_3.pdf')\n",
    "\n",
    "# The key will be filled with Author's name\n",
    "doc_dict = {'essay_1':essay_1_doc,\n",
    "            'essay_1_similar':essay_1_similar_doc,\n",
    "            'essay_2':essay_2_doc,\n",
    "            'essay_2_similar':essay_2_similar_doc,\n",
    "            'essay_3':essay_3_doc,\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Document Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngram_similarity(sentence_1,sentence_2,n=4):\n",
    "    ngrams_set_1 = set()\n",
    "    ngrams_set_2 = set()\n",
    "    \n",
    "    ngram1 = ngrams(sentence_1.split(), n)\n",
    "    for grams in ngram1:\n",
    "        ngrams_set_1.add(grams)\n",
    "        \n",
    "    ngram2 = ngrams(sentence_2.split(), n)\n",
    "    for grams in ngram2:\n",
    "        ngrams_set_2.add(grams)\n",
    "    \n",
    "    try:\n",
    "        jaccard_sim = len(ngrams_set_1.intersection(ngrams_set_2)) / len(ngrams_set_1.union(ngrams_set_2))\n",
    "\n",
    "        return jaccard_sim\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_pair_combinations = list(combinations(list(doc_dict.keys()), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essay_1 essay_1_similar\n",
      "----------- 0.6457399103139013\n",
      "essay_1 essay_2\n",
      "----------- 0.08991228070175439\n",
      "essay_1 essay_2_similar\n",
      "----------- 0.09544787077826726\n",
      "essay_1 essay_3\n",
      "----------- 0.11764705882352941\n",
      "essay_1_similar essay_2\n",
      "----------- 0.088139281828074\n",
      "essay_1_similar essay_2_similar\n",
      "----------- 0.09143686502177069\n",
      "essay_1_similar essay_3\n",
      "----------- 0.12583892617449666\n",
      "essay_2 essay_2_similar\n",
      "----------- 0.554531490015361\n",
      "essay_2 essay_3\n",
      "----------- 0.05795454545454545\n",
      "essay_2_similar essay_3\n",
      "----------- 0.06220839813374806\n"
     ]
    }
   ],
   "source": [
    "candidate_pairs = []\n",
    "\n",
    "for pair1,pair2 in doc_pair_combinations:\n",
    "    sim = extract_ngram_similarity(' '.join(doc_dict[pair1]),' '.join(doc_dict[pair2]),1)\n",
    "    print(pair1,pair2)\n",
    "    print('-----------',sim)\n",
    "    \n",
    "    if sim > 0.25:\n",
    "        candidate_pairs.append((pair1,pair2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('essay_1', 'essay_1_similar'), ('essay_2', 'essay_2_similar')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_doc_dict = {}\n",
    "for pair1,pair2 in candidate_pairs:\n",
    "    candidate_doc_dict[pair1] = [x for x in doc_dict[pair1] if len(x.split('.'))>2]\n",
    "    candidate_doc_dict[pair2] = [x for x in doc_dict[pair2] if len(x.split('.'))>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['essay_1', 'essay_1_similar', 'essay_2', 'essay_2_similar'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_doc_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Musim kemarau di Indonesia menyebabkan angka produksi padi menurun, penurunan sebesar 4,6 juta ton terjadi di tahun 2019 (1). Gambar 1 menunjukan luas fase persiapan lahan terjadi penurunan pada bulan April — Agustus yang sesuai dengan musim kemarau di Indonesia. Provinsi yang memiliki luas fase persiapan terbesar diantaranya Jawa Barat,  Jawa Tengah, Jawa Timur, Sulawesi Selatan, dan Sumatera Selatan.',\n",
       " 'Selain itu, provinsi tersebut memiliki tingkat petani skala kecil yang hanya memiliki luas sawah 0,16 hektar dengan persentase tertinggi di Indonesia. Salah satu indikator tujuan pembangunan berkelanjutan pada sektor pertanian yaitu nilai produksi per hektar, 90Y6 lahan pertanian di Jawa Barat, Jawa Timur, dan Nusa Tenggara Barat (NTB) dikategorikan lahan pertanian tidak berkelanjutan (21. Hal ini disebabkan karena tidak ada sumber air saat musim kemarau, bantuan dari pemerintah berupa pompa air tetapi sumber listrik di lokasi belum memadai. Mahalnya harga dan jauhnya akses untuk membeli bahan bakar minyak (BBM) membuat petani yang memiliki sawah di desa tidak sanggup untuk menggunakan BBM. Energi terbarukan bisa menjadi salah satu pembangkit listrik untuk menyuplai pompa air. Dengan potensi radiasi matahari di Indonesia, panel surya menjadi opsi yang tepat sebagai pembangkit listrik dan memiliki harga yang sudah ekonomis. Namun, penggunaan panel surya bisa mengurangi luas lahan pertanian dan mengakibatkan penurunan jumlah produksi. Salah satu solusi meningkatkan persentase keberlanjutan yaitu menggunakan  system of rice intensification (SRI) dengan sumber listrik pompa yang berasal dari panel',\n",
       " 'Metode pengairan sesuai jumlah dan waktu yang dibutuhkan oleh tanaman secara terputus- putus (intermittent). Dengan SRI dan pengelolaan tanaman yang baik dapat meningkatkan produktivitas tanaman sebesar 30-100 Y4 (31. Air selama satu masa tanam (100 hari) untuk sawah dengan sistem pemberian air secara SRI yaitu 467 mm atau 4,67 mm/hari dengan konversi 0,116 liter/detik/ha. Sementara, untuk produktivitas air (water productivity) atau rasio antara gabah kering giling yang dihasilkan (kg) dengan konsumsi air (m?) memiliki nilai 1412 kaim MI 2.2 Potensi penggunaan panel surya',\n",
       " 'Indonesia memiliki potensi radiasi matahari yang dapat dimanfaatkan oleh panel surya  sebesar 3,4 — 4,4 kWh/m?/hari termasuk pertanian di Jawa Barat, Jawa Timur, dan NTB. adalah efisiensi pompa, dan », adalah efisiensi motor.',\n",
       " 'Pada penelitian ini, sistem APV dibuat untuk mengatasi lahan pertanian yang tidak berkelanjutan akibat kekeringan. Sistem APV dibuat modular agar bisa dipindahkan dari satu petak ke petak lainnya. Penyangga APV menggunakan galvanized untuk mencegah  karat akihat hirian',\n",
       " 'Dirancang sebuah panel surya dengan kapasitas sebesar 1,5 KW untuk mengairi sawah sebesar 10 hektar. Sistem APV ini akan menggunakan modul surya berjenis monoerystalline sebesar 450 Wp akan ada 3 - 4 modul yang digunakan dengan kapasitas inverter 1,5 kW. Energy monitoring dipasang untuk mengetahui besar listrik yang  dibanakitkan oleh panel surva. hal tersebut',\n",
       " 'Diperlukan reservoir besar dengan volume air 22.500 liter untuk menyuplai air pada sawah tersebut yang didapat dari reservoir yang terisi air dengan elevasi lokasi reservoir lebih tinggi dari sawah. Dua buah pompa berukuran 4” digunakan untuk menghisap air dari tanah dan mengalirkannya ke sawah. Sensor akan bekerja sesuai dengan instruksi yang didesain. Biaya investasi yang dibutuhkan untuk membangun sistem ini direntang harga 100 juta. Dengan penghematan jika dibandingkan penggunakan diesel yang memerlukan 21-42 liter/jam, energi matahari mampu mendapatkan energi sepanjang hari. Satu sistem APV ini akan menolong 6 petani kecil agar mampu berproduksi sepanjang tahun dengan menambah  satu kali panen sebesar 16.360 kg/ha. Sistem ini mampu balik modal kurang dari dua tahun.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_doc_dict['essay_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in candidate_doc_dict:\n",
    "    candidate_doc_dict[doc] = [preprocessing(x,stemmer) for x in candidate_doc_dict[doc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['musim kemarau angka produksi padi turun turun 4 6 juta ton 2019 1 gambar 1 luas fase lahan turun april agustus musim kemarau provinsi milik luas fase jawa barat jawa jawa timur sulawesi selatan sumatera selatan',\n",
       " 'provinsi milik tingkat tani skala milik luas sawah 0 16 hektar persentase salah indikator tuju bangun sektor tani nilai produksi hektar 90y6 lahan tani jawa barat jawa timur nusa tenggara barat ntb kategori lahan tani 21 sumber musim kemarau bantu perintah pompa sumber lokasi mahal harga akses beli bahan bakar minyak bbm tani milik sawah desa sanggup bbm salah bangkit suplai pompa potensi radiasi matahari panel surya opsi bangkit milik harga ekonomis panel surya luas lahan tani akibat turun produksi salah solusi tingkat persentase system of rice intensification sri sumber pompa panel',\n",
       " 'metode butuh tanam terputus- putus intermittent sri kelola tanam tingkat produktivitas tanam 30-100 y4 31 tanam 100 sawah sistem sri 467 mm 4 67 mm konversi 0 116 liter detik ha produktivitas water productivity rasio gabah kering giling hasil kg konsumsi m milik nilai 1412 kaim mi 2 2 potensi panel surya',\n",
       " 'milik potensi radiasi matahari manfaat panel surya 3 4 4 4 kwh m tani jawa barat jawa timur ntb efisiensi pompa efisiensi motor',\n",
       " 'teliti sistem apv lahan tani akibat kering sistem apv modular pindah petak petak sangga apv galvanized cegah karat akihat hirian',\n",
       " 'rancang panel surya kapasitas 1 5 kw sawah 10 hektar sistem apv modul surya jenis monoerystalline 450 wp 3 - 4 modul kapasitas inverter 1 5 kw energy monitoring pasang dibanakitkan panel surva ',\n",
       " 'reservoir volume 22 500 liter suplai sawah reservoir isi elevasi lokasi reservoir sawah buah pompa ukur 4 menghisap tanah alir sawah sensor instruksi desain biaya investasi butuh bangun sistem rentang harga 100 juta hemat banding diesel 21-42 liter jam matahari sistem apv tolong 6 tani produksi kali panen 16 360 kg ha sistem modal']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_doc_dict['essay_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph Pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigram(arr):\n",
    "    return [arr[i]+' '+arr[i+1] for i in range(len(arr)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_dict = {'cleaned_paragraph_1':[],'cleaned_paragraph_2':[],'type':[],'pairs':[]}\n",
    "\n",
    "for pair1,pair2 in candidate_pairs:\n",
    "    for paragraph1 in candidate_doc_dict[pair1]:\n",
    "        for paragraph2 in candidate_doc_dict[pair2]:\n",
    "            paired_dict['cleaned_paragraph_1'].append(paragraph1)\n",
    "            paired_dict['cleaned_paragraph_2'].append(paragraph2)\n",
    "            paired_dict['type'].append('uni-paragraph')\n",
    "            paired_dict['pairs'].append((pair1,pair2))\n",
    "            \n",
    "    bi_paragraph1_list = generate_bigram(candidate_doc_dict[pair1])\n",
    "    bi_paragraph2_list = generate_bigram(candidate_doc_dict[pair2])\n",
    "    \n",
    "    for bi_paragraph1 in bi_paragraph1_list:\n",
    "        for bi_paragraph2 in bi_paragraph2_list:\n",
    "            paired_dict['cleaned_paragraph_1'].append(bi_paragraph1)\n",
    "            paired_dict['cleaned_paragraph_2'].append(bi_paragraph2)\n",
    "            paired_dict['type'].append('bi-paragraph')\n",
    "            paired_dict['pairs'].append((pair1,pair2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_paragraph_1</th>\n",
       "      <th>cleaned_paragraph_2</th>\n",
       "      <th>type</th>\n",
       "      <th>pairs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>musim kemarau angka produksi padi turun turun ...</td>\n",
       "      <td>musim kemarau angka produksi padi turun penuru...</td>\n",
       "      <td>uni-paragraph</td>\n",
       "      <td>(essay_1, essay_1_similar)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>musim kemarau angka produksi padi turun turun ...</td>\n",
       "      <td>provinsi milik tingkat tani skala milik luas s...</td>\n",
       "      <td>uni-paragraph</td>\n",
       "      <td>(essay_1, essay_1_similar)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>musim kemarau angka produksi padi turun turun ...</td>\n",
       "      <td>metode butuh tanam terputus- putus intermitten...</td>\n",
       "      <td>uni-paragraph</td>\n",
       "      <td>(essay_1, essay_1_similar)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>musim kemarau angka produksi padi turun turun ...</td>\n",
       "      <td>sistem kontrol atur arah arus pompa baterai si...</td>\n",
       "      <td>uni-paragraph</td>\n",
       "      <td>(essay_1, essay_1_similar)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>musim kemarau angka produksi padi turun turun ...</td>\n",
       "      <td>rancang panel surya kapasitas 4 kvv sawar 1 he...</td>\n",
       "      <td>uni-paragraph</td>\n",
       "      <td>(essay_1, essay_1_similar)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>to actualization their knowledge it needs hand...</td>\n",
       "      <td>transisi hadap tantang bijak aspek kunci cepat...</td>\n",
       "      <td>bi-paragraph</td>\n",
       "      <td>(essay_2, essay_2_similar)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>to actualization their knowledge it needs hand...</td>\n",
       "      <td>transisi hadap tantang bijak aspek kunci cepat...</td>\n",
       "      <td>bi-paragraph</td>\n",
       "      <td>(essay_2, essay_2_similar)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>to actualization their knowledge it needs hand...</td>\n",
       "      <td>tingkat tajam sumber utama kait emisi dioksida...</td>\n",
       "      <td>bi-paragraph</td>\n",
       "      <td>(essay_2, essay_2_similar)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>to actualization their knowledge it needs hand...</td>\n",
       "      <td>platform kolaborasi anak libat multi sektor bu...</td>\n",
       "      <td>bi-paragraph</td>\n",
       "      <td>(essay_2, essay_2_similar)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>to actualization their knowledge it needs hand...</td>\n",
       "      <td>survei nasional susenas 2019 pemuda usia 16-30...</td>\n",
       "      <td>bi-paragraph</td>\n",
       "      <td>(essay_2, essay_2_similar)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   cleaned_paragraph_1  \\\n",
       "0    musim kemarau angka produksi padi turun turun ...   \n",
       "1    musim kemarau angka produksi padi turun turun ...   \n",
       "2    musim kemarau angka produksi padi turun turun ...   \n",
       "3    musim kemarau angka produksi padi turun turun ...   \n",
       "4    musim kemarau angka produksi padi turun turun ...   \n",
       "..                                                 ...   \n",
       "203  to actualization their knowledge it needs hand...   \n",
       "204  to actualization their knowledge it needs hand...   \n",
       "205  to actualization their knowledge it needs hand...   \n",
       "206  to actualization their knowledge it needs hand...   \n",
       "207  to actualization their knowledge it needs hand...   \n",
       "\n",
       "                                   cleaned_paragraph_2           type  \\\n",
       "0    musim kemarau angka produksi padi turun penuru...  uni-paragraph   \n",
       "1    provinsi milik tingkat tani skala milik luas s...  uni-paragraph   \n",
       "2    metode butuh tanam terputus- putus intermitten...  uni-paragraph   \n",
       "3    sistem kontrol atur arah arus pompa baterai si...  uni-paragraph   \n",
       "4    rancang panel surya kapasitas 4 kvv sawar 1 he...  uni-paragraph   \n",
       "..                                                 ...            ...   \n",
       "203  transisi hadap tantang bijak aspek kunci cepat...   bi-paragraph   \n",
       "204  transisi hadap tantang bijak aspek kunci cepat...   bi-paragraph   \n",
       "205  tingkat tajam sumber utama kait emisi dioksida...   bi-paragraph   \n",
       "206  platform kolaborasi anak libat multi sektor bu...   bi-paragraph   \n",
       "207  survei nasional susenas 2019 pemuda usia 16-30...   bi-paragraph   \n",
       "\n",
       "                          pairs  \n",
       "0    (essay_1, essay_1_similar)  \n",
       "1    (essay_1, essay_1_similar)  \n",
       "2    (essay_1, essay_1_similar)  \n",
       "3    (essay_1, essay_1_similar)  \n",
       "4    (essay_1, essay_1_similar)  \n",
       "..                          ...  \n",
       "203  (essay_2, essay_2_similar)  \n",
       "204  (essay_2, essay_2_similar)  \n",
       "205  (essay_2, essay_2_similar)  \n",
       "206  (essay_2, essay_2_similar)  \n",
       "207  (essay_2, essay_2_similar)  \n",
       "\n",
       "[208 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(paired_dict)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(columns=['category','paragraph_1','paragraph_2','len_paragraph_1','is_plagiarism','plagiarism_type','cleaned_paragraph_1','cleaned_paragraph_2'])\n",
    "\n",
    "with open('../data/plagiarism_data_train.tsv','r') as f_in:\n",
    "    for i,line in enumerate(f_in):\n",
    "        if i>0:\n",
    "            columns = line.split('\\t')\n",
    "            columns[-1] = re.sub('\\n','',columns[-1])\n",
    "            df_train.loc[i] = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df_train,df_test):\n",
    "    corpus = df_train['cleaned_paragraph_1'].to_list() + df_train['cleaned_paragraph_2'].to_list()\n",
    "    \n",
    "    # Word Pairs\n",
    "    bigram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2), min_df=0.002)\n",
    "    bigram_vectorizer.fit(corpus)\n",
    "    \n",
    "    transformed_array_test = bigram_vectorizer.transform(df_test['cleaned_paragraph_1'] + ' '+ df_test['cleaned_paragraph_2']).toarray()\n",
    "    num_feats = transformed_array_test.shape[1]\n",
    "\n",
    "    df_preprocessed_test = pd.DataFrame(transformed_array_test,columns=[f'word_pairs_{i}' for i in range(num_feats)])\n",
    "    \n",
    "    # Words Similarity\n",
    "    df_preprocessed_test['words_similarity'] = df_test.apply(lambda x: extract_ngram_similarity(x['cleaned_paragraph_1'],x['cleaned_paragraph_2'],n=1), axis=1)\n",
    "    \n",
    "    # Fingerprints Similarity\n",
    "    df_preprocessed_test['fingerprint_similarity'] = df_test.apply(lambda x: extract_ngram_similarity(x['cleaned_paragraph_1'],x['cleaned_paragraph_2'],n=4), axis=1)\n",
    "    \n",
    "    # Longest Common Subsequence\n",
    "    df_preprocessed_test['lcs'] = df_test.apply(lambda x: pylcs.lcs2(x['cleaned_paragraph_1'],x['cleaned_paragraph_2'])/max(len(x['cleaned_paragraph_1']),len(x['cleaned_paragraph_2'])), axis=1)\n",
    "    \n",
    "    # LSA Similarity\n",
    "    num_component = 200\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True,smooth_idf=True)\n",
    "    lsa = TruncatedSVD(num_component, algorithm = 'randomized',random_state=0)\n",
    "\n",
    "    tfidf_vectorizer.fit(corpus)\n",
    "    dtm = tfidf_vectorizer.transform(corpus)\n",
    "    lsa.fit(dtm)\n",
    "        \n",
    "    dtm_lsa_test_1 = lsa.transform(tfidf_vectorizer.transform(df_test['cleaned_paragraph_1'].to_list()))\n",
    "    dtm_lsa_test_2 = lsa.transform(tfidf_vectorizer.transform(df_test['cleaned_paragraph_2'].to_list()))\n",
    "    \n",
    "    cosine_sim_test = []\n",
    "    for i in range(dtm_lsa_test_1.shape[0]):\n",
    "        cosine_sim_test.append(cosine_similarity([dtm_lsa_test_1[i]],[dtm_lsa_test_2[i]])[0][0])\n",
    "    \n",
    "    df_preprocessed_test['lsa_similarity'] = cosine_sim_test\n",
    "    \n",
    "    # Add Supporting Features\n",
    "    df_preprocessed_test[['type','pairs']] = df_test[['type','pairs']]\n",
    "    \n",
    "    return df_preprocessed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed_test = feature_engineering(df_train,df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df_preprocessed_test,model):\n",
    "\n",
    "    X_val = df_preprocessed_test.drop(columns=['type','pairs'])\n",
    "\n",
    "    y_pred = model.predict_proba(X_val)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_ml_model = pickle.load(open('../model/classic_ml_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_pairs_0</th>\n",
       "      <th>word_pairs_1</th>\n",
       "      <th>word_pairs_2</th>\n",
       "      <th>word_pairs_3</th>\n",
       "      <th>word_pairs_4</th>\n",
       "      <th>word_pairs_5</th>\n",
       "      <th>word_pairs_6</th>\n",
       "      <th>word_pairs_7</th>\n",
       "      <th>word_pairs_8</th>\n",
       "      <th>word_pairs_9</th>\n",
       "      <th>...</th>\n",
       "      <th>word_pairs_203</th>\n",
       "      <th>words_similarity</th>\n",
       "      <th>fingerprint_similarity</th>\n",
       "      <th>lcs</th>\n",
       "      <th>lsa_similarity</th>\n",
       "      <th>type</th>\n",
       "      <th>pairs</th>\n",
       "      <th>negative_prob</th>\n",
       "      <th>positive_prob</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.380531</td>\n",
       "      <td>0.986783</td>\n",
       "      <td>uni-paragraph</td>\n",
       "      <td>(essay_1, essay_1_similar)</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025994</td>\n",
       "      <td>0.340639</td>\n",
       "      <td>uni-paragraph</td>\n",
       "      <td>(essay_1, essay_1_similar)</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024221</td>\n",
       "      <td>0.096824</td>\n",
       "      <td>uni-paragraph</td>\n",
       "      <td>(essay_1, essay_1_similar)</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032710</td>\n",
       "      <td>0.022834</td>\n",
       "      <td>uni-paragraph</td>\n",
       "      <td>(essay_1, essay_1_similar)</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032397</td>\n",
       "      <td>0.142074</td>\n",
       "      <td>uni-paragraph</td>\n",
       "      <td>(essay_1, essay_1_similar)</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008593</td>\n",
       "      <td>0.092465</td>\n",
       "      <td>bi-paragraph</td>\n",
       "      <td>(essay_2, essay_2_similar)</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008895</td>\n",
       "      <td>0.091897</td>\n",
       "      <td>bi-paragraph</td>\n",
       "      <td>(essay_2, essay_2_similar)</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012121</td>\n",
       "      <td>0.066489</td>\n",
       "      <td>bi-paragraph</td>\n",
       "      <td>(essay_2, essay_2_similar)</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012706</td>\n",
       "      <td>0.061532</td>\n",
       "      <td>bi-paragraph</td>\n",
       "      <td>(essay_2, essay_2_similar)</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013977</td>\n",
       "      <td>0.088323</td>\n",
       "      <td>bi-paragraph</td>\n",
       "      <td>(essay_2, essay_2_similar)</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     word_pairs_0  word_pairs_1  word_pairs_2  word_pairs_3  word_pairs_4  \\\n",
       "0               0             0             0             0             0   \n",
       "1               0             0             0             0             0   \n",
       "2               0             0             0             0             0   \n",
       "3               0             0             0             0             0   \n",
       "4               0             0             0             0             0   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "203             0             0             0             0             0   \n",
       "204             0             0             0             0             0   \n",
       "205             0             0             0             0             0   \n",
       "206             0             0             0             0             0   \n",
       "207             0             0             0             0             0   \n",
       "\n",
       "     word_pairs_5  word_pairs_6  word_pairs_7  word_pairs_8  word_pairs_9  \\\n",
       "0               0             0             0             0             0   \n",
       "1               0             0             0             0             0   \n",
       "2               0             0             0             0             0   \n",
       "3               0             0             0             0             0   \n",
       "4               0             0             0             0             0   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "203             0             0             0             0             0   \n",
       "204             0             0             0             0             0   \n",
       "205             0             0             0             0             0   \n",
       "206             0             0             0             0             0   \n",
       "207             0             0             0             0             0   \n",
       "\n",
       "     ...  word_pairs_203  words_similarity  fingerprint_similarity       lcs  \\\n",
       "0    ...               0          0.896552                0.511111  0.380531   \n",
       "1    ...               0          0.132530                0.000000  0.025994   \n",
       "2    ...               0          0.046154                0.000000  0.024221   \n",
       "3    ...               0          0.041667                0.000000  0.032710   \n",
       "4    ...               0          0.079545                0.000000  0.032397   \n",
       "..   ...             ...               ...                     ...       ...   \n",
       "203  ...               0          0.018072                0.000000  0.008593   \n",
       "204  ...               0          0.011364                0.000000  0.008895   \n",
       "205  ...               0          0.022099                0.000000  0.012121   \n",
       "206  ...               0          0.026144                0.000000  0.012706   \n",
       "207  ...               0          0.029940                0.000000  0.013977   \n",
       "\n",
       "     lsa_similarity           type                       pairs  negative_prob  \\\n",
       "0          0.986783  uni-paragraph  (essay_1, essay_1_similar)           0.00   \n",
       "1          0.340639  uni-paragraph  (essay_1, essay_1_similar)           0.71   \n",
       "2          0.096824  uni-paragraph  (essay_1, essay_1_similar)           1.00   \n",
       "3          0.022834  uni-paragraph  (essay_1, essay_1_similar)           1.00   \n",
       "4          0.142074  uni-paragraph  (essay_1, essay_1_similar)           0.99   \n",
       "..              ...            ...                         ...            ...   \n",
       "203        0.092465   bi-paragraph  (essay_2, essay_2_similar)           0.98   \n",
       "204        0.091897   bi-paragraph  (essay_2, essay_2_similar)           1.00   \n",
       "205        0.066489   bi-paragraph  (essay_2, essay_2_similar)           1.00   \n",
       "206        0.061532   bi-paragraph  (essay_2, essay_2_similar)           1.00   \n",
       "207        0.088323   bi-paragraph  (essay_2, essay_2_similar)           1.00   \n",
       "\n",
       "     positive_prob  prediction  \n",
       "0             1.00           1  \n",
       "1             0.29           0  \n",
       "2             0.00           0  \n",
       "3             0.00           0  \n",
       "4             0.01           0  \n",
       "..             ...         ...  \n",
       "203           0.02           0  \n",
       "204           0.00           0  \n",
       "205           0.00           0  \n",
       "206           0.00           0  \n",
       "207           0.00           0  \n",
       "\n",
       "[208 rows x 213 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed_test[['negative_prob','positive_prob']] = predict(df_preprocessed_test,classic_ml_model)\n",
    "df_preprocessed_test['prediction'] = np.argmax(df_preprocessed_test[['negative_prob','positive_prob']].values,axis=1)\n",
    "df_preprocessed_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_plagiarism_score(df_preprocessed_pairs_i,candidate_doc_dict,pairs):\n",
    "    avg_pos_prob = df_preprocessed_pairs_i[df_preprocessed_pairs_i['prediction']==1]['positive_prob'].mean()\n",
    "    \n",
    "    uni_par_length = (len(candidate_doc_dict[pairs[0]]) + len(candidate_doc_dict[pairs[1]]))\n",
    "    bi_par_length = (len(candidate_doc_dict[pairs[0]]) - 1 + len(candidate_doc_dict[pairs[1]]) - 1) \n",
    "    freq = min(1,df_preprocessed_pairs_i['prediction'].value_counts().loc[1] / (uni_par_length+bi_par_length))\n",
    "    \n",
    "    return avg_pos_prob * freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('essay_1', 'essay_1_similar') 0.8031818181818182\n",
      "('essay_2', 'essay_2_similar') 0.7652631578947368\n"
     ]
    }
   ],
   "source": [
    "score_dict = {}\n",
    "for pairs in df_preprocessed_test['pairs'].unique():\n",
    "    df_preprocessed_pairs_i = df_preprocessed_test[df_preprocessed_test['pairs']==pairs]\n",
    "    \n",
    "    score = calculate_plagiarism_score(df_preprocessed_pairs_i,candidate_doc_dict,pairs)\n",
    "    \n",
    "    print(pairs,score)\n",
    "    score_dict[pairs] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('essay_1', 'essay_1_similar'): 0.8031818181818182,\n",
       " ('essay_2', 'essay_2_similar'): 0.7652631578947368}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from scripts.docxparser import docx_to_pdf\n",
    "from scripts.pdfparser import pdf_to_text\n",
    "from scripts.pipeline import plagiarism_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source: PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_1_doc = pdf_to_text('../data/essay_1.pdf')\n",
    "essay_1_similar_doc = pdf_to_text('../data/essay_1_similar.pdf')\n",
    "essay_2_doc = pdf_to_text('../data/essay_2.pdf')\n",
    "essay_2_similar_doc = pdf_to_text('../data/essay_2_similar.pdf')\n",
    "essay_3_doc = pdf_to_text('../data/essay_3.pdf')\n",
    "\n",
    "# The key will be filled with Author's name\n",
    "doc_dict = {'essay_1':essay_1_doc,\n",
    "            'essay_1_similar':essay_1_similar_doc,\n",
    "            'essay_2':essay_2_doc,\n",
    "            'essay_2_similar':essay_2_similar_doc,\n",
    "            'essay_3':essay_3_doc,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_ml_model = pickle.load(open('../model/classic_ml_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Candidate Document Retrieval: 100%|██████████| 10/10 [00:00<00:00, 1181.66it/s]\n",
      "Paragraph Filtering: 100%|██████████| 2/2 [00:00<00:00, 1840.82it/s]\n",
      "Paragraph Preprocessing: 100%|██████████| 4/4 [00:32<00:00,  8.15s/it]\n",
      "Paragraph Pairing: 100%|██████████| 2/2 [00:00<00:00, 7612.17it/s]\n",
      "Feature Engineering: 2013it [00:07, 269.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Model Inference\n",
      "============= Score Aggregation\n",
      "('essay_1', 'essay_1_similar') 0.8031818181818182\n",
      "('essay_2', 'essay_2_similar') 0.7652631578947368\n"
     ]
    }
   ],
   "source": [
    "score_dict = plagiarism_pipeline(doc_dict,classic_ml_model,\n",
    "                                candidate_doc_thres=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Candidate Document Retrieval: 100%|██████████| 10/10 [00:00<00:00, 1165.80it/s]\n",
      "Paragraph Filtering: 100%|██████████| 10/10 [00:00<00:00, 18859.28it/s]\n",
      "Paragraph Preprocessing: 100%|██████████| 5/5 [00:03<00:00,  1.29it/s]\n",
      "Paragraph Pairing: 100%|██████████| 10/10 [00:00<00:00, 16396.81it/s]\n",
      "Feature Engineering: 2013it [00:07, 276.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Model Inference\n",
      "============= Score Aggregation\n",
      "('essay_1', 'essay_1_similar') 0.8031818181818182\n",
      "('essay_1', 'essay_2') 0\n",
      "('essay_1', 'essay_2_similar') 0\n",
      "('essay_1', 'essay_3') 0.04576923076923077\n",
      "('essay_1_similar', 'essay_2') 0\n",
      "('essay_1_similar', 'essay_2_similar') 0\n",
      "('essay_1_similar', 'essay_3') 0.024090909090909093\n",
      "('essay_2', 'essay_2_similar') 0.7652631578947368\n",
      "('essay_2', 'essay_3') 0\n",
      "('essay_2_similar', 'essay_3') 0\n"
     ]
    }
   ],
   "source": [
    "# Check All Document Pairs\n",
    "score_dict = plagiarism_pipeline(doc_dict,classic_ml_model,\n",
    "                                 candidate_doc_thres=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source: DOCX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/users/User/Desktop/practisee_plagiarism/data/docx_to_pdf/essay_3.pdf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docx_to_pdf('../data/docx_to_pdf','../data/docx_files/essay_1.docx')\n",
    "docx_to_pdf('../data/docx_to_pdf','../data/docx_files/essay_1_similar.docx')\n",
    "docx_to_pdf('../data/docx_to_pdf','../data/docx_files/essay_2.docx')\n",
    "docx_to_pdf('../data/docx_to_pdf','../data/docx_files/essay_2_similar.docx')\n",
    "docx_to_pdf('../data/docx_to_pdf','../data/docx_files/essay_3.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_1_doc = pdf_to_text('../data/docx_to_pdf/essay_1.pdf')\n",
    "essay_1_similar_doc = pdf_to_text('../data/docx_to_pdf/essay_1_similar.pdf')\n",
    "essay_2_doc = pdf_to_text('../data/docx_to_pdf/essay_2.pdf')\n",
    "essay_2_similar_doc = pdf_to_text('../data/docx_to_pdf/essay_2_similar.pdf')\n",
    "essay_3_doc = pdf_to_text('../data/docx_to_pdf/essay_3.pdf')\n",
    "\n",
    "# The key will be filled with Author's name\n",
    "doc_dict = {'essay_1':essay_1_doc,\n",
    "            'essay_1_similar':essay_1_similar_doc,\n",
    "            'essay_2':essay_2_doc,\n",
    "            'essay_2_similar':essay_2_similar_doc,\n",
    "            'essay_3':essay_3_doc,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_ml_model = pickle.load(open('../model/classic_ml_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Candidate Document Retrieval: 100%|██████████| 10/10 [00:00<00:00, 912.78it/s]\n",
      "Paragraph Filtering: 100%|██████████| 2/2 [00:00<00:00, 4662.93it/s]\n",
      "Paragraph Preprocessing: 100%|██████████| 4/4 [00:32<00:00,  8.22s/it]\n",
      "Paragraph Pairing: 100%|██████████| 2/2 [00:00<00:00, 5870.26it/s]\n",
      "Feature Engineering: 2013it [00:08, 242.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Model Inference\n",
      "============= Score Aggregation\n",
      "('essay_1', 'essay_1_similar') 0.7445\n",
      "('essay_2', 'essay_2_similar') 0.8047222222222222\n"
     ]
    }
   ],
   "source": [
    "score_dict = plagiarism_pipeline(doc_dict,classic_ml_model,\n",
    "                                candidate_doc_thres=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Candidate Document Retrieval: 100%|██████████| 10/10 [00:00<00:00, 420.51it/s]\n",
      "Paragraph Filtering: 100%|██████████| 10/10 [00:00<00:00, 13285.73it/s]\n",
      "Paragraph Preprocessing: 100%|██████████| 5/5 [00:04<00:00,  1.19it/s]\n",
      "Paragraph Pairing: 100%|██████████| 10/10 [00:00<00:00, 12850.20it/s]\n",
      "Feature Engineering: 2013it [00:08, 246.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Model Inference\n",
      "============= Score Aggregation\n",
      "('essay_1', 'essay_1_similar') 0.7445\n",
      "('essay_1', 'essay_2') 0\n",
      "('essay_1', 'essay_2_similar') 0\n",
      "('essay_1', 'essay_3') 0.07050000000000001\n",
      "('essay_1_similar', 'essay_2') 0\n",
      "('essay_1_similar', 'essay_2_similar') 0\n",
      "('essay_1_similar', 'essay_3') 0.027307692307692307\n",
      "('essay_2', 'essay_2_similar') 0.8047222222222222\n",
      "('essay_2', 'essay_3') 0\n",
      "('essay_2_similar', 'essay_3') 0\n"
     ]
    }
   ],
   "source": [
    "# Check All Document Pairs\n",
    "score_dict = plagiarism_pipeline(doc_dict,classic_ml_model,\n",
    "                                 candidate_doc_thres=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source: DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/users/User/Desktop/practisee_plagiarism/data/doc_to_pdf/essay_3.pdf'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docx_to_pdf('../data/doc_to_pdf','../data/doc_files/essay_1.doc')\n",
    "docx_to_pdf('../data/doc_to_pdf','../data/doc_files/essay_1_similar.doc')\n",
    "docx_to_pdf('../data/doc_to_pdf','../data/doc_files/essay_2.doc')\n",
    "docx_to_pdf('../data/doc_to_pdf','../data/doc_files/essay_2_similar.doc')\n",
    "docx_to_pdf('../data/doc_to_pdf','../data/doc_files/essay_3.doc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_1_doc = pdf_to_text('../data/doc_to_pdf/essay_1.pdf')\n",
    "essay_1_similar_doc = pdf_to_text('../data/doc_to_pdf/essay_1_similar.pdf')\n",
    "essay_2_doc = pdf_to_text('../data/doc_to_pdf/essay_2.pdf')\n",
    "essay_2_similar_doc = pdf_to_text('../data/doc_to_pdf/essay_2_similar.pdf')\n",
    "essay_3_doc = pdf_to_text('../data/doc_to_pdf/essay_3.pdf')\n",
    "\n",
    "# The key will be filled with Author's name\n",
    "doc_dict = {'essay_1':essay_1_doc,\n",
    "            'essay_1_similar':essay_1_similar_doc,\n",
    "            'essay_2':essay_2_doc,\n",
    "            'essay_2_similar':essay_2_similar_doc,\n",
    "            'essay_3':essay_3_doc,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_ml_model = pickle.load(open('../model/classic_ml_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Candidate Document Retrieval: 100%|██████████| 10/10 [00:00<00:00, 773.40it/s]\n",
      "Paragraph Filtering: 100%|██████████| 2/2 [00:00<00:00, 5133.79it/s]\n",
      "Paragraph Preprocessing: 100%|██████████| 4/4 [00:01<00:00,  2.04it/s]\n",
      "Paragraph Pairing: 100%|██████████| 2/2 [00:00<00:00, 6186.29it/s]\n",
      "Feature Engineering: 2013it [00:11, 181.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Model Inference\n",
      "============= Score Aggregation\n",
      "('essay_1', 'essay_1_similar') 0.785\n",
      "('essay_2', 'essay_2_similar') 0.8038888888888889\n"
     ]
    }
   ],
   "source": [
    "score_dict = plagiarism_pipeline(doc_dict,classic_ml_model,\n",
    "                                candidate_doc_thres=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Candidate Document Retrieval: 100%|██████████| 10/10 [00:00<00:00, 480.87it/s]\n",
      "Paragraph Filtering: 100%|██████████| 10/10 [00:00<00:00, 9086.45it/s]\n",
      "Paragraph Preprocessing: 100%|██████████| 5/5 [00:00<00:00, 51.53it/s]\n",
      "Paragraph Pairing: 100%|██████████| 10/10 [00:00<00:00, 10866.07it/s]\n",
      "Feature Engineering: 2013it [00:09, 208.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Model Inference\n",
      "============= Score Aggregation\n",
      "('essay_1', 'essay_1_similar') 0.785\n",
      "('essay_1', 'essay_2') 0\n",
      "('essay_1', 'essay_2_similar') 0\n",
      "('essay_1', 'essay_3') 0.0695\n",
      "('essay_1_similar', 'essay_2') 0\n",
      "('essay_1_similar', 'essay_2_similar') 0\n",
      "('essay_1_similar', 'essay_3') 0.027307692307692307\n",
      "('essay_2', 'essay_2_similar') 0.8038888888888889\n",
      "('essay_2', 'essay_3') 0\n",
      "('essay_2_similar', 'essay_3') 0\n"
     ]
    }
   ],
   "source": [
    "# Check All Document Pairs\n",
    "score_dict = plagiarism_pipeline(doc_dict,classic_ml_model,\n",
    "                                 candidate_doc_thres=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test_env]",
   "language": "python",
   "name": "conda-env-test_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
