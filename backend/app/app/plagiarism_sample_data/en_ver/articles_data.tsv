category	author	publish_date	text	url	clean_text	len_chars
history	['View All Posts Followinghadrian', 'I Came', 'I Saw', 'I Photographed... Follow Me In The Footsteps Of Hadrian']	2021-05-18 00:00:00	"Shortly after celebrating Rome’s birthday (see here), Hadrian departed on his journey to the northern provinces and began his first extensive voyage through the empire. As he intended to be absent for a considerable time with little idea of the date in return, it was necessary to leave the control of Rome in trustworthy hands. He left Annius Verus, the grandfather of Marcus Aurelius, who was just born (see here), and his friend Turbo to look after the people and the senate.

The exact date of his departure and precise itinerary is a little elusive, and the Historia Augusta dismisses his journey with a few words:

After this he travelled​ to the provinces of Gaul,​ and came to the relief of all the communities with various acts of generosity, and from there he went over into Germany. HA Hadr. 10.1-2

So it seems that Hadrian headed to Gallia Narbonensis (southern France), then moved through Gallia Lugdunensis and from there went to Germany. He probably sailed from Ostia to Massilia (Marseille), southern Gaul’s main port, and proceeded up the Rhône River valley in the direction of Lugdunum (Lyon). His entourage included the Guard Prefect Septicius Clarus and his chief secretary, Tranquillus Suetonius, the historian and author of The Twelve Caesars. Sabina was most likely with him as the Historia Augusta places the incident of Clarus and Suetonius’ excessive familiarity with the empress in Britain, which Hadrian was to visit in 122 after he inspected the German frontier (Hadr. 11.3). Hadrian would abruptly fire both men.

—

By the Hadrianic era, Gaul was already a rich and flourishing county with well-established urbanisation and well-developed civic centres. It consisted at this time of four provinces; Narbonensis, which was governed by the senate, and Lugdunensis, Aquitania and Belgica, which were ruled by a proconsul of praetorian rank. Lugdunum was the capital of these last three provinces. Hadrian may have used this important administrative centre as a base for a few months. Unfortunately, his precise movements inside Gaul are highly elusive. His journey possibly began at the port of Massilia (Birley, 1997). This famous Greek colony of the Phocaeans founded around 600 BC was still a free city, a reward for its invaluable assistance and loyalty to Rome.

—

Massilia played a major role in distributing Mediterranean goods along the coast of Gaul and Iberia and into inland Gaul on the rivers Durance and Rhône, relying on a series of trading posts. As Carthage’s rival and Rome longstanding ally, the city thrived by acting as a link between Gaul and Rome’s insatiable need for new products and slaves. It retained its Greek character for a considerable length of time despite the gradual process of romanisation. Its Greek legacy lived on in the urban fabric – complete with a theatre, agora and temples – in its constitution and culture.

The Phocaean city also shone by its schools of science and was home to many renowned sailors and explorers. In the 6th century BC, the Massaliote Euthymenes left the city to explore the coast of West Africa beyond the columns of Hercules. In the 4th century BC, Phytheas made a voyage of exploration to north-western Europe, visited a considerable part of modern-day Great Britain and Ireland, and went as far north as Iceland and the Arctic Circle.

From Massilia, Hadrian probably sailed north up the Rhône River (Rhodanus) in the direction of Lugdunum. Two years earlier, in 119, the river boatmen of this river, the nautae Rhodanici, had made an offering to Hadrian (see here). They erected a statue of the emperor in the town of Tournus (Tournon-sur-Rhône) between Valencia (Valence) and Vienna (Vienne), at the confluence of the Rhône and the Doux, praising his generosity.

—

The Rhodanus provided a significant channel for communication and trade between the Mediterranean and central Gaul, and its chief tributary, the Arar (modern Saône), gave access to the Rhineland. The Rhodanus dominated the province of Narbonensis, flowing through Lugdunum (Lyon), Vienna (Vienne), Valentia (Valence), Acunum (Montélimar), Arausio (Orange), Avennio (Avignon), and Arelate (Arles), where the river divides into two large branches. These towns were established on the eastern side of the river’s bank and the via Agrippa.

—

Some of these cities, and others, may have received new rights from Hadrian, and at least one new Gallic colony was created at Avennio, perhaps even during this tour (Fraser, 2006). An inscription (now lost), found in Apt in 1786 and wrongly suspected for some time as fake (CIL XII 1120), mentions the Colonia Iulia Hadriana Avenniensis, showing that Hadrian elevated Avignon to a Roman colony. It was the highest status that a city could achieve. The medieval town of Avignon has left very few architectural remains from the Roman city, and none of its vestiges seems to have been dated to Hadrian’s time.

—

It is rather interesting to note that the funerary inscription from Apt mentions a priest (sacerdos) called Lucius Volusius Severianus (from the Voltinia tribe). They cared for the veneration of the goddess Urbs Roma Aeterna (the personification of the city of Rome). Her cult had become official by establishing the recent festival Romaia (Natalis Urbis Romae) by Hadrian on the foundation day of the town (21 April 121) and the inauguration of the magnificent temple on the Velian Hill, which he dedicated to Roma Aeterna and Venus Felix. The eternity of the city thereby became an integral part of imperial propaganda and a subject of public worship. Sacerdotes of this cult were to be found in Italy (outside Rome) and in the provinces in several towns (Ticinum CIL V 6991, Brixia CIL V 4484, Flavia Solva CIL III 5443).

Although no inscription confirms the presence of Hadrian at Lugdunum, his itinerary along the Rhône Valley can hardly have omitted the capital of the Three Gauls (Tres Galliae). A stay there would have been unavoidable, with some modern scholars even suggesting that he wintered there in 121/122. However, the German frontier was probably his absolute goal. If that was the case, as argued by Birley (1997), there is no reason to believe that he stayed in Lugdunum over such a long period and that he continued to journey north towards the Rhine only in the spring of 122. That would have left him very little time to inspect the limes before sailing to Britain in the summer of 122.

—

Lugdunum was founded as a Roman colony in 43 BC by Lucius Munatius Plancus, nine years after Caesar had completed his conquest of Gaul. Thanks to its prime location at the confluence of the Rhône and Saône, the city became a great commercial centre for Gaul and the nodal point of Agrippa’s Gallic road network, at the centre of four converging roads. These radiated west to Aquitania, north to the Rhine and the northern coast, south to Arelate, Narbonne and the Mediterranean coast, and east to Lake Léman. Several decades after its foundations, Lugdunum received the favours of Augustus. He decided to create a great sanctuary dedicated to the Imperial cult and establish an imperial mint that supplied gold and silver coins for half a century. As the capital and religious centre of the Three Gauls, Lugdunum was among the cities where the imperial authority was most present after Rome. It was the residence of the governor of the province and many high imperial civil servants from the Equestrian order.

—

Lugdunum was at the height of its prosperity at the time of Hadrian’s visit. It offered all the attractions of a great city and must have provided suitable accommodation for an emperor. The city boasted a forum, a temple of Roma and Augustus, a sanctuary for Cybele, a theatre, an odeon, an amphitheatre and a circus for chariot races.

The majority of Lugdunum’s public buildings were built during the Augustan period. Hadrian is said to have embellished the city and commissioned the restorations of a number of these buildings that required repair. However, the dating of these restorations is mainly due to tradition rather than scientific research. Even stratigraphic excavations and architectural decoration study have not precisely attributed these constructions or embellishments to Hadrian (Fellague, 2016). For sure, Hadrian had a team of architects, surveyors and builders, and the Historia Augusta refers to imperial favours given during a trip. Still, the dating to Hadrian’s reign remains hypothetical.

—

Tradition has it that Hadrian renovated and embellished the forum, repaired and decorated the theatre. The latter building, which originally had a seating capacity of 5000 spectators, was extended to seat 10,000. The cavea was made bigger with the addition of a third row. The stage building was embellished with columns and statues, and the orchestra floor was renovated with grey granite paving bordered with pink and green marble. A cuirassed statue of Hadrian (of which only fragments remain) was placed in the scaenae frons. An Odeon was built to the south of the theatre at the end of the 1st or early 2nd century AD. Its construction is sometimes attributed to Hadrian. Used for music and recitations, the Odeon was partly roofed and could hold around 3000 people seated in two rows of seats.

—

The amphitheatre was enlarged and restored later in Hadrian’s reign, c. 130/136, to increase its capacity to 20,000 spectators. Thanks to a building inscription, we know that the original amphitheatre was built during the reign of Tiberius in AD 19 by Gaius Julius Rufus, a priest of Rome and Augustus and a native of Mediolanum Santonum (modern-day Saintes in western France). It was relatively small in size with a single tier of seating and a capacity of only 1,800 and was located at the sacred precinct at Condate (the Confluence) above the rivers on a hillside. During the reign of Hadrian, two tiers of seats were added around the old amphitheatre, bringing its dimensions to 143 x 117 m, comparable to the arenas of Nîmes and Arles.

—

Near the amphitheatre (probably) stood the imperial Sanctuary of the Three Gauls. Inaugurated in 12 BC by Drusus, this religious complex dedicated to “Rome and Augustus” was the earliest and most important institution of its kind in the western part of the empire. It was here that every year in August, delegates from sixty cities from the three Gallic provinces met together to celebrate religious ceremonies and re-affirm their allegiance to the emperor (Concilium Galliarum). The connection between the sanctuary with the amphitheatre is clear from the seat inscriptions marking places for cult officials and council representatives.

The central element of the sanctuary was an altar that can be reconstructed from ancient written sources and representations on coins. The structure is shown to be rectangular, decorated in front with crowns and laurel branches in bas-reliefs and topped with tripods. It is flanked by two columns supporting statues of winged Victories, each holding a palm branch and garland.

The altar appears to have been rebuilt (or adapted) as a covered temple by Hadrian, while the original columns supporting the statues of the Victories were replace by columns of Egyptian Syenite (Fishwick, 1972). There is indeed an inscription (CIL XIII 1685) that mentions a donation from Hadrian to the altar of the Three Gauls, but the gaps in the text do not allow us to specify the nature of this favour (Fellague, 2016). Unfortunately, no trace of the monument itself has been found.

—

Another public building presumably built in the beginning of the 2nd century AD was the circus, the existence of which is confirmed by the magnificent mosaic of a chariot race discovered in 1806 in the Ainay district of Lyon, but also by a number of inscriptions. Its location is still debated, but it most probably stood on the heights of Fourvière behind the great theatre. The edifice would have measured 370 metres long and around 100 metres wide.

—

The urban development of Lugdunum was intimately connected with the growth of its water supply. Four large aqueducts provided water to the city with a total length of 200 kilometres supplying 39,000 cubic metres per day. This was the second-largest network of Roman aqueducts after Rome itself. During the first decades of its foundation, two aqueducts were built that fed the hill town of Fourvière and Mont-d’Or. A third, built under Claudius, brought water into the low-lying quarter of Les Minimes. A fourth, the Gier Aqueduct, was the longest (86 km) of the four and the most monumental. To protect the water channel, the aqueduct was underground for 95% of its course, and the difference in altitude between the two ends was only 150 metres. Its piers and arcades have been preserved in many places, as have several large reservoirs linked by siphons: the most impressive remains are at Chaponost and Beaunant.

—

Roman aqueducts required a comprehensive system of maintenance, cleaning and repair. They were access points at regular interval (77 metres or 240 Roman feet) where the vault of the channel was cut through by a sort of chimney with small cavities inside to help with the climbing down. About 100 of these access-points have been identified along the Gier aqueduct but there were originally around 1,000. The aqueduct was protected by a series of marker stones that served to delimit the areas where farming and other activities were not permitted. Two Hadrianic stone inscriptions (CIL XIII, 1623) found on the Gier Aqueduct southwest of Lyon refer to a ban on cultivating land near the aqueduct. Hadrian prohibited ploughing, sowing and planting in the grounds reserved for the aqueduct’s protection to preserve the channel from any damage and avoid the water being polluted. These marker stones must have been posted at regular intervals along the aqueduct’s course as both inscriptions were found 6 kilometres apart.

—

These two inscriptions have long been advanced as an argument for attributing the construction of the Gier aqueduct to Hadrian. Still, recent observations have led scholars and archaeologists to date the works to the beginning of the 1st century AD. However, these boundary stones teach us that the imperial administration intervened in protecting aqueducts in the capital of Gaul, like those of Rome.

Hadrian’s imperial presence in Gaul was to be commemorated by coins with the legends Adventi Galliae and Restitutor Galliae

—

Hadrian would then continue to the northern frontiers of Germania where he was to inspect the legions of the Rhineland and the system of fixed frontier defences ordered by him in 120 (see here). He may have have travelled to Germania from Lugdunum via Augusta Treverorum (Trier) to Mogontiacum (Mainz), the capital of Germania Superior. Milestones attest to the restoration of this street by Hadrian. Hadrian would return to Gaul in the autumn of 122, passing through this time on his way to Tarraco (Tarragona).

Sources & references:

Birley, Anthony R. (1997) Hadrian: The Restless Emperor, London and New York: Routledge, p. 113

Fellague, Djamila. (2016). La difficulté de datation des monuments: À propos des monuments de Lugudunum, en particulier ceux considérés comme hadrianiques. Revue Archeologique de l’Est. 65. 187-214.

Christol, Michel and Marc Heijmans. (1992). Les colonies latines de Narbonnaise: un nouveau document d’Arles mentionnant la Colonia Julia Augusta Avennio. Gallia, Vol. 49, 1992, pp. 37-44.

Fishwick, Duncan (2002). The Imperial Cult in the Latin West. Studies in the Ruler Cult of the Western Provinces of the Roman Empire, III, part 1, Leiden- New York. p. 142

Like this: Like Loading..."	https://followinghadrian.com/2021/05/18/spring-ad-121-hadrian-departs-for-the-northern-provinces-hadrian1900/	"Shortly after celebrating Rome’s birthday (see here), Hadrian departed on his journey to the northern provinces and began his first extensive voyage through the empire. As he intended to be absent for a considerable time with little idea of the date in return, it was necessary to leave the control of Rome in trustworthy hands. He left Annius Verus, the grandfather of Marcus Aurelius, who was just born (see here), and his friend Turbo to look after the people and the senate.
The exact date of his departure and precise itinerary is a little elusive, and the Historia Augusta dismisses his journey with a few words:
After this he travelled  to the provinces of Gaul,  and came to the relief of all the communities with various acts of generosity, and from there he went over into Germany. HA Hadr. 10.1-2
So it seems that Hadrian headed to Gallia Narbonensis (southern France), then moved through Gallia Lugdunensis and from there went to Germany. He probably sailed from Ostia to Massilia (Marseille), southern Gaul’s main port, and proceeded up the Rhône River valley in the direction of Lugdunum (Lyon). His entourage included the Guard Prefect Septicius Clarus and his chief secretary, Tranquillus Suetonius, the historian and author of The Twelve Caesars. Sabina was most likely with him as the Historia Augusta places the incident of Clarus and Suetonius’ excessive familiarity with the empress in Britain, which Hadrian was to visit in 122 after he inspected the German frontier (Hadr. 11.3). Hadrian would abruptly fire both men.
—
By the Hadrianic era, Gaul was already a rich and flourishing county with well-established urbanisation and well-developed civic centres. It consisted at this time of four provinces; Narbonensis, which was governed by the senate, and Lugdunensis, Aquitania and Belgica, which were ruled by a proconsul of praetorian rank. Lugdunum was the capital of these last three provinces. Hadrian may have used this important administrative centre as a base for a few months. Unfortunately, his precise movements inside Gaul are highly elusive. His journey possibly began at the port of Massilia (Birley, 1997). This famous Greek colony of the Phocaeans founded around 600 BC was still a free city, a reward for its invaluable assistance and loyalty to Rome.
—
Massilia played a major role in distributing Mediterranean goods along the coast of Gaul and Iberia and into inland Gaul on the rivers Durance and Rhône, relying on a series of trading posts. As Carthage’s rival and Rome longstanding ally, the city thrived by acting as a link between Gaul and Rome’s insatiable need for new products and slaves. It retained its Greek character for a considerable length of time despite the gradual process of romanisation. Its Greek legacy lived on in the urban fabric – complete with a theatre, agora and temples – in its constitution and culture.
The Phocaean city also shone by its schools of science and was home to many renowned sailors and explorers. In the 6th century BC, the Massaliote Euthymenes left the city to explore the coast of West Africa beyond the columns of Hercules. In the 4th century BC, Phytheas made a voyage of exploration to north-western Europe, visited a considerable part of modern-day Great Britain and Ireland, and went as far north as Iceland and the Arctic Circle.
From Massilia, Hadrian probably sailed north up the Rhône River (Rhodanus) in the direction of Lugdunum. Two years earlier, in 119, the river boatmen of this river, the nautae Rhodanici, had made an offering to Hadrian (see here). They erected a statue of the emperor in the town of Tournus (Tournon-sur-Rhône) between Valencia (Valence) and Vienna (Vienne), at the confluence of the Rhône and the Doux, praising his generosity.
—
The Rhodanus provided a significant channel for communication and trade between the Mediterranean and central Gaul, and its chief tributary, the Arar (modern Saône), gave access to the Rhineland. The Rhodanus dominated the province of Narbonensis, flowing through Lugdunum (Lyon), Vienna (Vienne), Valentia (Valence), Acunum (Montélimar), Arausio (Orange), Avennio (Avignon), and Arelate (Arles), where the river divides into two large branches. These towns were established on the eastern side of the river’s bank and the via Agrippa.
—
Some of these cities, and others, may have received new rights from Hadrian, and at least one new Gallic colony was created at Avennio, perhaps even during this tour (Fraser, 2006). An inscription (now lost), found in Apt in 1786 and wrongly suspected for some time as fake (CIL XII 1120), mentions the Colonia Iulia Hadriana Avenniensis, showing that Hadrian elevated Avignon to a Roman colony. It was the highest status that a city could achieve. The medieval town of Avignon has left very few architectural remains from the Roman city, and none of its vestiges seems to have been dated to Hadrian’s time.
—
It is rather interesting to note that the funerary inscription from Apt mentions a priest (sacerdos) called Lucius Volusius Severianus (from the Voltinia tribe). They cared for the veneration of the goddess Urbs Roma Aeterna (the personification of the city of Rome). Her cult had become official by establishing the recent festival Romaia (Natalis Urbis Romae) by Hadrian on the foundation day of the town (21 April 121) and the inauguration of the magnificent temple on the Velian Hill, which he dedicated to Roma Aeterna and Venus Felix. The eternity of the city thereby became an integral part of imperial propaganda and a subject of public worship. Sacerdotes of this cult were to be found in Italy (outside Rome) and in the provinces in several towns (Ticinum CIL V 6991, Brixia CIL V 4484, Flavia Solva CIL III 5443).
Although no inscription confirms the presence of Hadrian at Lugdunum, his itinerary along the Rhône Valley can hardly have omitted the capital of the Three Gauls (Tres Galliae). A stay there would have been unavoidable, with some modern scholars even suggesting that he wintered there in 121/122. However, the German frontier was probably his absolute goal. If that was the case, as argued by Birley (1997), there is no reason to believe that he stayed in Lugdunum over such a long period and that he continued to journey north towards the Rhine only in the spring of 122. That would have left him very little time to inspect the limes before sailing to Britain in the summer of 122.
—
Lugdunum was founded as a Roman colony in 43 BC by Lucius Munatius Plancus, nine years after Caesar had completed his conquest of Gaul. Thanks to its prime location at the confluence of the Rhône and Saône, the city became a great commercial centre for Gaul and the nodal point of Agrippa’s Gallic road network, at the centre of four converging roads. These radiated west to Aquitania, north to the Rhine and the northern coast, south to Arelate, Narbonne and the Mediterranean coast, and east to Lake Léman. Several decades after its foundations, Lugdunum received the favours of Augustus. He decided to create a great sanctuary dedicated to the Imperial cult and establish an imperial mint that supplied gold and silver coins for half a century. As the capital and religious centre of the Three Gauls, Lugdunum was among the cities where the imperial authority was most present after Rome. It was the residence of the governor of the province and many high imperial civil servants from the Equestrian order.
—
Lugdunum was at the height of its prosperity at the time of Hadrian’s visit. It offered all the attractions of a great city and must have provided suitable accommodation for an emperor. The city boasted a forum, a temple of Roma and Augustus, a sanctuary for Cybele, a theatre, an odeon, an amphitheatre and a circus for chariot races.
The majority of Lugdunum’s public buildings were built during the Augustan period. Hadrian is said to have embellished the city and commissioned the restorations of a number of these buildings that required repair. However, the dating of these restorations is mainly due to tradition rather than scientific research. Even stratigraphic excavations and architectural decoration study have not precisely attributed these constructions or embellishments to Hadrian (Fellague, 2016). For sure, Hadrian had a team of architects, surveyors and builders, and the Historia Augusta refers to imperial favours given during a trip. Still, the dating to Hadrian’s reign remains hypothetical.
—
Tradition has it that Hadrian renovated and embellished the forum, repaired and decorated the theatre. The latter building, which originally had a seating capacity of 5000 spectators, was extended to seat 10,000. The cavea was made bigger with the addition of a third row. The stage building was embellished with columns and statues, and the orchestra floor was renovated with grey granite paving bordered with pink and green marble. A cuirassed statue of Hadrian (of which only fragments remain) was placed in the scaenae frons. An Odeon was built to the south of the theatre at the end of the 1st or early 2nd century AD. Its construction is sometimes attributed to Hadrian. Used for music and recitations, the Odeon was partly roofed and could hold around 3000 people seated in two rows of seats.
—
The amphitheatre was enlarged and restored later in Hadrian’s reign, c. 130/136, to increase its capacity to 20,000 spectators. Thanks to a building inscription, we know that the original amphitheatre was built during the reign of Tiberius in AD 19 by Gaius Julius Rufus, a priest of Rome and Augustus and a native of Mediolanum Santonum (modern-day Saintes in western France). It was relatively small in size with a single tier of seating and a capacity of only 1,800 and was located at the sacred precinct at Condate (the Confluence) above the rivers on a hillside. During the reign of Hadrian, two tiers of seats were added around the old amphitheatre, bringing its dimensions to 143 x 117 m, comparable to the arenas of Nîmes and Arles.
—
Near the amphitheatre (probably) stood the imperial Sanctuary of the Three Gauls. Inaugurated in 12 BC by Drusus, this religious complex dedicated to “Rome and Augustus” was the earliest and most important institution of its kind in the western part of the empire. It was here that every year in August, delegates from sixty cities from the three Gallic provinces met together to celebrate religious ceremonies and re-affirm their allegiance to the emperor (Concilium Galliarum). The connection between the sanctuary with the amphitheatre is clear from the seat inscriptions marking places for cult officials and council representatives.
The central element of the sanctuary was an altar that can be reconstructed from ancient written sources and representations on coins. The structure is shown to be rectangular, decorated in front with crowns and laurel branches in bas-reliefs and topped with tripods. It is flanked by two columns supporting statues of winged Victories, each holding a palm branch and garland.
The altar appears to have been rebuilt (or adapted) as a covered temple by Hadrian, while the original columns supporting the statues of the Victories were replace by columns of Egyptian Syenite (Fishwick, 1972). There is indeed an inscription (CIL XIII 1685) that mentions a donation from Hadrian to the altar of the Three Gauls, but the gaps in the text do not allow us to specify the nature of this favour (Fellague, 2016). Unfortunately, no trace of the monument itself has been found.
—
Another public building presumably built in the beginning of the 2nd century AD was the circus, the existence of which is confirmed by the magnificent mosaic of a chariot race discovered in 1806 in the Ainay district of Lyon, but also by a number of inscriptions. Its location is still debated, but it most probably stood on the heights of Fourvière behind the great theatre. The edifice would have measured 370 metres long and around 100 metres wide.
—
The urban development of Lugdunum was intimately connected with the growth of its water supply. Four large aqueducts provided water to the city with a total length of 200 kilometres supplying 39,000 cubic metres per day. This was the second-largest network of Roman aqueducts after Rome itself. During the first decades of its foundation, two aqueducts were built that fed the hill town of Fourvière and Mont-d’Or. A third, built under Claudius, brought water into the low-lying quarter of Les Minimes. A fourth, the Gier Aqueduct, was the longest (86 km) of the four and the most monumental. To protect the water channel, the aqueduct was underground for 95% of its course, and the difference in altitude between the two ends was only 150 metres. Its piers and arcades have been preserved in many places, as have several large reservoirs linked by siphons: the most impressive remains are at Chaponost and Beaunant.
—
Roman aqueducts required a comprehensive system of maintenance, cleaning and repair. They were access points at regular interval (77 metres or 240 Roman feet) where the vault of the channel was cut through by a sort of chimney with small cavities inside to help with the climbing down. About 100 of these access-points have been identified along the Gier aqueduct but there were originally around 1,000. The aqueduct was protected by a series of marker stones that served to delimit the areas where farming and other activities were not permitted. Two Hadrianic stone inscriptions (CIL XIII, 1623) found on the Gier Aqueduct southwest of Lyon refer to a ban on cultivating land near the aqueduct. Hadrian prohibited ploughing, sowing and planting in the grounds reserved for the aqueduct’s protection to preserve the channel from any damage and avoid the water being polluted. These marker stones must have been posted at regular intervals along the aqueduct’s course as both inscriptions were found 6 kilometres apart.
—
These two inscriptions have long been advanced as an argument for attributing the construction of the Gier aqueduct to Hadrian. Still, recent observations have led scholars and archaeologists to date the works to the beginning of the 1st century AD. However, these boundary stones teach us that the imperial administration intervened in protecting aqueducts in the capital of Gaul, like those of Rome.
Hadrian’s imperial presence in Gaul was to be commemorated by coins with the legends Adventi Galliae and Restitutor Galliae
—
Hadrian would then continue to the northern frontiers of Germania where he was to inspect the legions of the Rhineland and the system of fixed frontier defences ordered by him in 120 (see here). He may have have travelled to Germania from Lugdunum via Augusta Treverorum (Trier) to Mogontiacum (Mainz), the capital of Germania Superior. Milestones attest to the restoration of this street by Hadrian. Hadrian would return to Gaul in the autumn of 122, passing through this time on his way to Tarraco (Tarragona).
Sources & references:
Birley, Anthony R. (1997) Hadrian: The Restless Emperor, London and New York: Routledge, p. 113
Fellague, Djamila. (2016). La difficulté de datation des monuments: À propos des monuments de Lugudunum, en particulier ceux considérés comme hadrianiques. Revue Archeologique de l’Est. 65. 187-214.
Christol, Michel and Marc Heijmans. (1992). Les colonies latines de Narbonnaise: un nouveau document d’Arles mentionnant la Colonia Julia Augusta Avennio. Gallia, Vol. 49, 1992, pp. 37-44.
Fishwick, Duncan (2002). The Imperial Cult in the Latin West. Studies in the Ruler Cult of the Western Provinces of the Roman Empire, III, part 1, Leiden- New York. p. 142
Like this: Like Loading..."	15638
history	['View All Posts Followinghadrian', 'I Came', 'I Saw', 'I Photographed... Follow Me In The Footsteps Of Hadrian']	2021-04-26 00:00:00	"Happy 1900th birthday, Marcus Aurelius! 🎉

Marcus Aurelius Antoninus was born on 26 April 121 in Rome during the reign of Hadrian to an aristocratic family of Italo-Hispanic origin, the gens Annia. The family had settled in the southern Spanish province of Baetica, in the small town of Ucubi (modern-day Espejo), a few miles southeast of Corduba (modern-day Córdoba). It came to prominence and became wealthy through olive oil production in Spain.

His father was the praetor Marcus Annius Verus, who came from a wealthy senatorial family. His mother was Domitia Lucilla, the heiress of a wealthy family that owned a tile factory near Rome. She was well educated and could read Greek and Latin.

—

Marcus’ year of birth, 121, was known in the Roman records as the year when his grandfather, Marcus Annius Verus, was consul for the second time. Verus was a close friend of Hadrian and would assume a third consulship in 126, an enormous mark of honour. Marcus’ grandfather married Rupilia Faustina, a daughter of the niece of Trajan, Salonia Matidia, Hadrian’s beloved mother-in-law (see here), and her elder half-sisters were Matidia Minor and Vibia Sabina, the wife of Hadrian. So Marcus’ paternal grandmother was the sister-in-law and third cousin of Hadrian. The couple had three children, Annia Galeria Faustina (Faustina the Elder), a future Empress who would marry Antoninus Pius (Hadrian’s successor), Marcus Annius Libo, a future consul (AD 128), and Marcus Annius Verus, the future father of Marcus Aurelius. Marcus’ grandfather Verus died in 138, nearly aged ninety. Marcus would later say of him in his Meditations:

From my grandfather Verus [I learned] good morals and the government of my temper.

121 was also known as the year 874 Ab urbe condita (‘from the founding of the City’, the dating system of classical Rome). Just a few days before Marcus’ birth, Hadrian celebrated Rome’s 874th birthday with chariot races in the Circus Maximus and the establishment of a new festival, the Romaia (see here).

—

Marcus was originally named Marcus Annius Catilius Severus, after his maternal great-grandfather, Lucius Catilius Severus. He would later be called Marcus Annius Verus after his father. He was raised as a boy in the family’s house on the Caelian, one of the hills of Rome with many aristocratic villas, a district Marcus would affectionately refer to as “my Caelian”.

Marcus was only three years old when his father died in 124. Though he can hardly have known him, Marcus wrote in his Meditations that he had learned “modesty and manliness” from his father’s memories and the man’s posthumous reputation.

From the reputation and remembrance of my father [I learned] modesty and a manly character.

—

At his father’s death, Marcus was adopted by his paternal grandfather Verus, but his maternal great-grandfather also participated in his upbringing. He was brought up in his mother’s home (the Horti Domitiae Lucillae) on the Caelian Hill, although she probably did not spend much time with her son. As was the custom in aristocratic households, Marcus Aurelius was in the care of nurses. However, he was to credit her with teaching him religious piety and how to avoid the manners of the rich.

From my mother, [I learned] piety and beneficence, and abstinence, not only from evil deeds, but even from evil thoughts; and further, simplicity in my way of living, far removed from the habits of the rich.

He was a solemn child from the very beginning; and as soon as he passed beyond the age when children are brought up under the care of nurses, he was handed over to advanced instructors and attained to a knowledge of philosophy. HA Marcus 2.1

Hadrian seems to have favoured the young Marcus by the frankness of his character as he nicknamed him Verissimus, meaning “most truthful”. In 127, at the age of six, Marcus was enrolled in the order of the equites on the recommendation of Hadrian, and the following year he was made salius Palatinus, the highly respected priesthood dedicated to Mars that dates back to the early days of the settlement of Rome on the Palatine Hill. While Marcus was a Salii an auspicious omen heralding Marcus Aurelius’ future rule occurred. One day, when the members of the college were throwing their crowns onto the banqueting couch of the gods, as was customary, Marcus Aurelius’ crown fell on the brow of Mars (HA Marcus 4.1-4)

His elementary education began in 128 when Marcus reached the age of seven. His first teachers were Euphoric, Geminus, and an unnamed tutor. Euphoric probably taught him Greek and Geminus Latin, while his tutor was probably charged with his general development. At the age of twelve, Marcus would have been ready for secondary education under the grammatici. He had a series of tutors in oratory and rhetoric, including Diognetus, who was to introduce him to philosophical texts. The most eminent of his tutors was Marcus Cornelius Fronto, best known as Fronto, the greatest Latin orator and rhetorician of his time. The pair would become life-long friends and keep in close correspondence for many years afterwards. Their surviving letters throw an attractive light on Marcus’ daily routine as a young man and as Caesar.

1) A day in the life of a young heir to the throne. In this delightful letter, a twenty-something Marcus Aurelius describes his day: ""Today I worked at my studies from 3am to 8am with some snacks. Then for an hour I cheerfully paced around my bedroom wearing only my slippers.. pic.twitter.com/LBHas4Cm4a — Gareth Harney (@OptimoPrincipi) April 24, 2021

In 136, in his fifteenth year, Marcus took on the toga virilis, symbolizing his passage into manhood. Shortly after, Hadrian arranged for his engagement to Ceionia Fabia, the daughter of the respected politician Lucius Ceionius Commodus (soon to become Lucius Aelius Caesar). Marcus was then made prefect of the city during the feriae Latinae. In the same year, Marcus met Apollonius of Chalcedon, a Stoic philosopher who taught Commodus. Apollonius would have a significant impact on Marcus and would study regularly with him. Like Hadrian, Marcus admired philosophy, a subject that he prized above all and would have the greatest influence on the young man. Fronto wanted him to become a rhetorician, but Marcus to become a student of philosophy instead.

He studied philosophy intensely, even when he was still a boy. When he was twelve years old he embraced the dress of a philosopher, and later, the endurance – studying in a Greek cloak and sleeping on the ground. However, (with some difficulty) his mother persuaded him to sleep on a couch spread with skins. HA Marcus 2.6

—

In late 136, Hadrian fell ill and almost died from a haemorrhage at his villa at Tivoli. With such uncertainty about the future, Hadrian selected Ceionius Commodus (who was married to Marcus Aurelius’ aunt Faustina) as his successor and adopted him as his son. Hadrian spent 300 million sesterces on publicly celebrating the adoption with lavish games in the Circus Maximus and distributing gifts to the public and military. As part of his adoption, Commodus took the name Lucius Aelius Caesar and was designated consul for the second time for the year 137. Although Lucius lacked military experience, he was sent to the Danube frontier and returned to Rome a year later. He was to deliverer a speech to the Senate on the first day of 138 but grew ill the night before and died of a haemorrhage.

On 24 January 138, while celebrating his sixty-second birthday, Hadrian selected Aurelius Antoninus as his new successor, formally adopting him the following month. As part of Hadrian’s wishes, Antoninus adopted his own nephew, the 17-year-old Marcus, and 7-year-old Lucius Commodus (his future co-emperor), the young son of Lucius Aelius Caesar, thus securing the succession for another generation. Marcus became M. Aelius Aurelius Verus and Lucius became L. Aelius Aurelius Commodus. Further, as stipulated by Hadrian, Antoninus’ daughter Faustina was betrothed to Lucius (his betrothal to Ceionia Fabia would be annulled).

The adoption was later depicted on the so-called Parthian Monument from Ephesus, now in the Ephesos Museum in Vienna.

—

The Historia Augusta records that on the night of his adoption Marcus Aurelius had a dream:

And it was on the day that Verus​ was adopted that he dreamed that he had shoulders of ivory, and when he asked if they were capable of bearing a burden, he found them much stronger than before. When he discovered, moreover, that Hadrian had adopted him, he was appalled rather than overjoyed, and when told to move to the private home of Hadrian, reluctantly departed from his mother’s villa. And when the members of his household asked him why he was sorry to receive royal adoption, he enumerated to them the evil things that sovereignty involved. HA Marcus 5.2

In 138, as his health had deteriorated steadily, Hadrian left for Baiae, a seaside resort on the Campanian coast. He died in the presence of his adopted son on 10 July. Antoninus succeeded to the throne, finalised Hadrian’s burial arrangements, and Marcus Aurelius held gladiatorial games at Rome. Antoninus had Aurelius’ betrothal to Ceionia Fabia annulled and arranged a marriage between him and Antoninus’ daughter Anna Galeria Faustina (the future Faustina the Younger).

—

Marcus held the consulship jointly with Antoninus in 140, then he and Antoninus were consuls again for the year 145. In late spring 145, Marcus Aurelius married Faustina Antoninus’ daughter, as planned since 138. Marcus took an increasingly important role in his adoptive father’s government for the next 23 years. His apprenticeship under Antoninus is illuminated by the correspondence between him and his teacher Fronto. Aurelius also devotes a long passage of praise to his adopted father in his Meditations, in which he lists the emperor’s impressive qualities (Book I.16).

In my father I observed mildness of temper, and unchangeable resolution in the things which he had determined after due deliberation; and no vainglory in those things which men call honours; and a love of labour and perseverance; and a readiness to listen to those who had anything to propose for the common weal; and undeviating firmness in giving to every man according to his deserts; and a knowledge derived from experience of the occasions for vigorous action and for remission.

—

On the death of Antoninus Pius in March 161, Marcus Aurelius became emperor and made Lucius Verus his colleague in government, keeping with Hadrian’s original designs. They ruled jointly until Lucius’ death in January 169. During their reign, the Empire entered a period troubled by natural disasters, plague and floods, and by invasions of barbarians. To console himself, Marcus Aurelius recorded his private notes to himself and ideas on Stoic philosophy. These are now known as his Meditations, and they reveal a mind of great humanity and natural humility.

He would go on to become on the last of the “Five Good Emperors” of Rome and a prominent Stoic philosopher. These successions of adoptions became known as the Antonine Dynasty. This era of more than 80 years was described by the 18th-century historian Edward Gibbon as the height of Roman power and glory and ‘the happiest times of humanity’.

—

“What we do now echoes in eternity.” Marcus Aurelius, Meditations

—

“All is ephemeral — fame and the famous as well.”

—

“When you arise in the morning think of what a privilege it is to be alive, to think, to enjoy, to love…”

—

“Look back over the past, with its changing empires that rose and fell, and you foresee the future too.”

—

“Very little is needed to make a happy life; it is all within yourself, in your way of thinking.”

—

“Do every act of your life as though it were the very last act of your life.”

“Often injustice lies in what you aren’t doing, not only in what you are doing.”

—

‘Never let the future disturb you. You will meet it, if you have to, with the same weapons of reason which today arm you against the present.’

—

“It is not death that a man should fear, but he should fear never beginning to live.”

—

Let’s celebrate! 🎈

Happy 1,900th birthday anniversary to Marcus Aurelius pic.twitter.com/lZX4iM6Rpn — Ripesh (@ripesh_bista) April 26, 2021

Happy 1900th Birthday to one of the 5 good #Roman Emperors, Marcus Aurelius. He was born in Rome on 26th April AD121 to Marcus Annius Verus & Domitia Lucilla . His great grandmother was Salonia Matidia, niece of Emperor Trajan & his great aunt was Sabina, wife of Emperor Hadrian pic.twitter.com/T3HJPymp9z — Trimontium Trust (@TrimontiumTrust) April 26, 2021

Born on this day 1900 years ago, Marcus Aurelius. Happy Birthday to The Philosopher King!@StoicWeek pic.twitter.com/dnxHnc2tdD — What Is Stoicism? 🌷 (@WhatIsStoicism) April 26, 2021

Happy Birthday, dear Marcus Aurelius! “If it is not ceasing to live that you are afraid of but never beginning to properly live then you are worthy of the world that made you.”

(Meditations 12.1) Worry not to die, rather worry never beginning to live. pic.twitter.com/D6rY0DvAG0 — A Stoic Student (@a_stoic_student) April 26, 2021

""What we do now echoes in eternity."" 1900 years of Marcus Aurelius' echoes. pic.twitter.com/q2Zeiy7n9r — Pranav Sharma (@Pranavv_Sharma) April 25, 2021

Links and further reading:

Like this: Like Loading..."	https://followinghadrian.com/2021/04/26/26-april-ad-121-future-philosopher-emperor-marcus-aurelius-is-born-hadrian1900/	"Happy 1900th birthday, Marcus Aurelius! 🎉
Marcus Aurelius Antoninus was born on 26 April 121 in Rome during the reign of Hadrian to an aristocratic family of Italo-Hispanic origin, the gens Annia. The family had settled in the southern Spanish province of Baetica, in the small town of Ucubi (modern-day Espejo), a few miles southeast of Corduba (modern-day Córdoba). It came to prominence and became wealthy through olive oil production in Spain.
His father was the praetor Marcus Annius Verus, who came from a wealthy senatorial family. His mother was Domitia Lucilla, the heiress of a wealthy family that owned a tile factory near Rome. She was well educated and could read Greek and Latin.
—
Marcus’ year of birth, 121, was known in the Roman records as the year when his grandfather, Marcus Annius Verus, was consul for the second time. Verus was a close friend of Hadrian and would assume a third consulship in 126, an enormous mark of honour. Marcus’ grandfather married Rupilia Faustina, a daughter of the niece of Trajan, Salonia Matidia, Hadrian’s beloved mother-in-law (see here), and her elder half-sisters were Matidia Minor and Vibia Sabina, the wife of Hadrian. So Marcus’ paternal grandmother was the sister-in-law and third cousin of Hadrian. The couple had three children, Annia Galeria Faustina (Faustina the Elder), a future Empress who would marry Antoninus Pius (Hadrian’s successor), Marcus Annius Libo, a future consul (AD 128), and Marcus Annius Verus, the future father of Marcus Aurelius. Marcus’ grandfather Verus died in 138, nearly aged ninety. Marcus would later say of him in his Meditations:
From my grandfather Verus [I learned] good morals and the government of my temper.
121 was also known as the year 874 Ab urbe condita (‘from the founding of the City’, the dating system of classical Rome). Just a few days before Marcus’ birth, Hadrian celebrated Rome’s 874th birthday with chariot races in the Circus Maximus and the establishment of a new festival, the Romaia (see here).
—
Marcus was originally named Marcus Annius Catilius Severus, after his maternal great-grandfather, Lucius Catilius Severus. He would later be called Marcus Annius Verus after his father. He was raised as a boy in the family’s house on the Caelian, one of the hills of Rome with many aristocratic villas, a district Marcus would affectionately refer to as “my Caelian”.
Marcus was only three years old when his father died in 124. Though he can hardly have known him, Marcus wrote in his Meditations that he had learned “modesty and manliness” from his father’s memories and the man’s posthumous reputation.
From the reputation and remembrance of my father [I learned] modesty and a manly character.
—
At his father’s death, Marcus was adopted by his paternal grandfather Verus, but his maternal great-grandfather also participated in his upbringing. He was brought up in his mother’s home (the Horti Domitiae Lucillae) on the Caelian Hill, although she probably did not spend much time with her son. As was the custom in aristocratic households, Marcus Aurelius was in the care of nurses. However, he was to credit her with teaching him religious piety and how to avoid the manners of the rich.
From my mother, [I learned] piety and beneficence, and abstinence, not only from evil deeds, but even from evil thoughts; and further, simplicity in my way of living, far removed from the habits of the rich.
He was a solemn child from the very beginning; and as soon as he passed beyond the age when children are brought up under the care of nurses, he was handed over to advanced instructors and attained to a knowledge of philosophy. HA Marcus 2.1
Hadrian seems to have favoured the young Marcus by the frankness of his character as he nicknamed him Verissimus, meaning “most truthful”. In 127, at the age of six, Marcus was enrolled in the order of the equites on the recommendation of Hadrian, and the following year he was made salius Palatinus, the highly respected priesthood dedicated to Mars that dates back to the early days of the settlement of Rome on the Palatine Hill. While Marcus was a Salii an auspicious omen heralding Marcus Aurelius’ future rule occurred. One day, when the members of the college were throwing their crowns onto the banqueting couch of the gods, as was customary, Marcus Aurelius’ crown fell on the brow of Mars (HA Marcus 4.1-4)
His elementary education began in 128 when Marcus reached the age of seven. His first teachers were Euphoric, Geminus, and an unnamed tutor. Euphoric probably taught him Greek and Geminus Latin, while his tutor was probably charged with his general development. At the age of twelve, Marcus would have been ready for secondary education under the grammatici. He had a series of tutors in oratory and rhetoric, including Diognetus, who was to introduce him to philosophical texts. The most eminent of his tutors was Marcus Cornelius Fronto, best known as Fronto, the greatest Latin orator and rhetorician of his time. The pair would become life-long friends and keep in close correspondence for many years afterwards. Their surviving letters throw an attractive light on Marcus’ daily routine as a young man and as Caesar.
1) A day in the life of a young heir to the throne. In this delightful letter, a twenty-something Marcus Aurelius describes his day: ""Today I worked at my studies from 3am to 8am with some snacks. Then for an hour I cheerfully paced around my bedroom wearing only my slippers.. pic.twitter.com/LBHas4Cm4a — Gareth Harney (@OptimoPrincipi) April 24, 2021
In 136, in his fifteenth year, Marcus took on the toga virilis, symbolizing his passage into manhood. Shortly after, Hadrian arranged for his engagement to Ceionia Fabia, the daughter of the respected politician Lucius Ceionius Commodus (soon to become Lucius Aelius Caesar). Marcus was then made prefect of the city during the feriae Latinae. In the same year, Marcus met Apollonius of Chalcedon, a Stoic philosopher who taught Commodus. Apollonius would have a significant impact on Marcus and would study regularly with him. Like Hadrian, Marcus admired philosophy, a subject that he prized above all and would have the greatest influence on the young man. Fronto wanted him to become a rhetorician, but Marcus to become a student of philosophy instead.
He studied philosophy intensely, even when he was still a boy. When he was twelve years old he embraced the dress of a philosopher, and later, the endurance – studying in a Greek cloak and sleeping on the ground. However, (with some difficulty) his mother persuaded him to sleep on a couch spread with skins. HA Marcus 2.6
—
In late 136, Hadrian fell ill and almost died from a haemorrhage at his villa at Tivoli. With such uncertainty about the future, Hadrian selected Ceionius Commodus (who was married to Marcus Aurelius’ aunt Faustina) as his successor and adopted him as his son. Hadrian spent 300 million sesterces on publicly celebrating the adoption with lavish games in the Circus Maximus and distributing gifts to the public and military. As part of his adoption, Commodus took the name Lucius Aelius Caesar and was designated consul for the second time for the year 137. Although Lucius lacked military experience, he was sent to the Danube frontier and returned to Rome a year later. He was to deliverer a speech to the Senate on the first day of 138 but grew ill the night before and died of a haemorrhage.
On 24 January 138, while celebrating his sixty-second birthday, Hadrian selected Aurelius Antoninus as his new successor, formally adopting him the following month. As part of Hadrian’s wishes, Antoninus adopted his own nephew, the 17-year-old Marcus, and 7-year-old Lucius Commodus (his future co-emperor), the young son of Lucius Aelius Caesar, thus securing the succession for another generation. Marcus became M. Aelius Aurelius Verus and Lucius became L. Aelius Aurelius Commodus. Further, as stipulated by Hadrian, Antoninus’ daughter Faustina was betrothed to Lucius (his betrothal to Ceionia Fabia would be annulled).
The adoption was later depicted on the so-called Parthian Monument from Ephesus, now in the Ephesos Museum in Vienna.
—
The Historia Augusta records that on the night of his adoption Marcus Aurelius had a dream:
And it was on the day that Verus  was adopted that he dreamed that he had shoulders of ivory, and when he asked if they were capable of bearing a burden, he found them much stronger than before. When he discovered, moreover, that Hadrian had adopted him, he was appalled rather than overjoyed, and when told to move to the private home of Hadrian, reluctantly departed from his mother’s villa. And when the members of his household asked him why he was sorry to receive royal adoption, he enumerated to them the evil things that sovereignty involved. HA Marcus 5.2
In 138, as his health had deteriorated steadily, Hadrian left for Baiae, a seaside resort on the Campanian coast. He died in the presence of his adopted son on 10 July. Antoninus succeeded to the throne, finalised Hadrian’s burial arrangements, and Marcus Aurelius held gladiatorial games at Rome. Antoninus had Aurelius’ betrothal to Ceionia Fabia annulled and arranged a marriage between him and Antoninus’ daughter Anna Galeria Faustina (the future Faustina the Younger).
—
Marcus held the consulship jointly with Antoninus in 140, then he and Antoninus were consuls again for the year 145. In late spring 145, Marcus Aurelius married Faustina Antoninus’ daughter, as planned since 138. Marcus took an increasingly important role in his adoptive father’s government for the next 23 years. His apprenticeship under Antoninus is illuminated by the correspondence between him and his teacher Fronto. Aurelius also devotes a long passage of praise to his adopted father in his Meditations, in which he lists the emperor’s impressive qualities (Book I.16).
In my father I observed mildness of temper, and unchangeable resolution in the things which he had determined after due deliberation; and no vainglory in those things which men call honours; and a love of labour and perseverance; and a readiness to listen to those who had anything to propose for the common weal; and undeviating firmness in giving to every man according to his deserts; and a knowledge derived from experience of the occasions for vigorous action and for remission.
—
On the death of Antoninus Pius in March 161, Marcus Aurelius became emperor and made Lucius Verus his colleague in government, keeping with Hadrian’s original designs. They ruled jointly until Lucius’ death in January 169. During their reign, the Empire entered a period troubled by natural disasters, plague and floods, and by invasions of barbarians. To console himself, Marcus Aurelius recorded his private notes to himself and ideas on Stoic philosophy. These are now known as his Meditations, and they reveal a mind of great humanity and natural humility.
He would go on to become on the last of the “Five Good Emperors” of Rome and a prominent Stoic philosopher. These successions of adoptions became known as the Antonine Dynasty. This era of more than 80 years was described by the 18th-century historian Edward Gibbon as the height of Roman power and glory and ‘the happiest times of humanity’.
—
“What we do now echoes in eternity.” Marcus Aurelius, Meditations
—
“All is ephemeral — fame and the famous as well.”
—
“When you arise in the morning think of what a privilege it is to be alive, to think, to enjoy, to love…”
—
“Look back over the past, with its changing empires that rose and fell, and you foresee the future too.”
—
“Very little is needed to make a happy life; it is all within yourself, in your way of thinking.”
—
“Do every act of your life as though it were the very last act of your life.”
“Often injustice lies in what you aren’t doing, not only in what you are doing.”
—
‘Never let the future disturb you. You will meet it, if you have to, with the same weapons of reason which today arm you against the present.’
—
“It is not death that a man should fear, but he should fear never beginning to live.”
—
Let’s celebrate! 🎈
Happy 1,900th birthday anniversary to Marcus Aurelius pic.twitter.com/lZX4iM6Rpn — Ripesh (@ripesh_bista) April 26, 2021
Happy 1900th Birthday to one of the 5 good #Roman Emperors, Marcus Aurelius. He was born in Rome on 26th April AD121 to Marcus Annius Verus & Domitia Lucilla . His great grandmother was Salonia Matidia, niece of Emperor Trajan & his great aunt was Sabina, wife of Emperor Hadrian pic.twitter.com/T3HJPymp9z — Trimontium Trust (@TrimontiumTrust) April 26, 2021
Born on this day 1900 years ago, Marcus Aurelius. Happy Birthday to The Philosopher King!@StoicWeek pic.twitter.com/dnxHnc2tdD — What Is Stoicism? 🌷 (@WhatIsStoicism) April 26, 2021
Happy Birthday, dear Marcus Aurelius! “If it is not ceasing to live that you are afraid of but never beginning to properly live then you are worthy of the world that made you.”
(Meditations 12.1) Worry not to die, rather worry never beginning to live. pic.twitter.com/D6rY0DvAG0 — A Stoic Student (@a_stoic_student) April 26, 2021
""What we do now echoes in eternity."" 1900 years of Marcus Aurelius' echoes. pic.twitter.com/q2Zeiy7n9r — Pranav Sharma (@Pranavv_Sharma) April 25, 2021
Links and further reading:
Like this: Like Loading..."	13329
history	['View All Posts Followinghadrian', 'I Came', 'I Saw', 'I Photographed... Follow Me In The Footsteps Of Hadrian']	2021-03-21 00:00:00	"In the early year of AD 121, Pompeia Plotina, the greatly respected widow of the emperor Trajan, sent Hadrian a letter asking him to help the Epicurean school in Athens solve an issue regarding the rights of succession. According to Roman law, the head of the school was obliged to appoint a new leader from among Roman citizens only with a will written in Latin. This restriction made the number of candidates too limited and prevented the best members of the school from being appointed.

The head of the Epicurean school (scholarch) at the time was a man called Popillius Theotimus, a Roman citizen. As he could not legally name as his successor (diadochus) any foreigner (peregrinus), nor could the will be written in Greek, he enlisted the Dowager Empress to petition the Emperor for a change in the law on his behalf. The relevant letters (IG II² 1099) were preserved in a series of inscriptions discovered in Athens in 1890. They consist of three separate sections. The first section contains the letter from Plotina addressed to Hadrian in Latin in the consulship of Marcus Annius Verus with Gnaeus Arrius Augur as his colleague, which gives the year of AD 121 (before April when two new consuls were elected). The second section contains Hadrian’s brief reply in Latin to Popillius Theotimus, whilst the third is a long letter in Greek from Plotina to “all her friends” at the Epicurean school. The letters were inscribed on slabs of Pentelic marble in Athens in AD 121.

Hadrian agreed to Plotina’s request and granted Popillius Theotimus and his successors permission to extend the succession rights to Greek peregrine and draw up their testaments in Greek.

Plotina wrote to Hadrian as follows: (Translation by R. van Bremen, 2005)

How greatly I favor the school of Epicurus you know full well, my lord. The succession therein needs your help, for since none but a Roman citizen may be elected head of the school the choice is narrowly limited. I pray therefore on behalf of Popillius Theotimus, who is now the head at Athens, that you will allow him to provide by will in Greek concerning that part of his instructions which pertains to the regulation of the headship, and to name a successor to himself of non-citizen status if he is so persuaded by the attainments of the person; and that future heads of the school may hereafter exercise with the same right the privilege you grant to Theotimus, all the more so because the practice is that whenever an error has been made by the testator concerning the choice of a head, he who will be best is, by common consent, selected by the students of the school, and this will be easier if he can be chosen from a larger number.

The Emperor answered: (Translation by R. van Bremen, 2005)

I, Imperator Caesar Traianus Hadrianus Augustus permit Popillius Theotimus to testate in Greek concerning those matters which pertain to the succession of the Epicurean sect, and since it will be easier for him to select a successor if the option exists also to appoint from among the peregrine I guarantee this too and it will be permitted from now on to those who have obtained the succession that this right be transmitted either to a peregrine or to a Roman citizen.

Plotina then passes on the news that her request has been granted to all the Athenian Epicureans: (Translation by R. van Bremen, 2005)

Plotina Augusta to all her friends, greetings. We now have what we were keen to achieve. For it has been granted to each Successor who will lead the School of Epicurus in Athens, both to make dispositions about the entire administration concerning the School by means of Greek testament, and to choose at will, either a Greek or Roman Head of the School. Because this excellent extension of authority has been granted to us – which binds us to express true gratitude to him who is verily a benefactor and guardian of all culture and therefore a most venerable emperor; to me personally also most dear in all respects as both an outstanding master and a good son – it is proper that each of those who have been trusted with the decision concerning the headship always tries to appoint in his own place the best of those who share in the doctrine and to attach more importance to the view of the community as a whole than to his own predilections for particular individuals. It would therefore please me if he did not show preference to anyone over those who are acknowledged as outstanding in the power of our doctrines and, accordingly, in the superiority of their moral conduct. If this were not to be the case, not because of the particular nature of the matter, but because of our own weakness, or through some other accidental impediment, I consider it proper that he who will plan ahead for the common observances will aim for that which will please all in common and not that which will please him personally. But, by Zeus, I do not think that he who has grasped the benefit which has come to him from the teachings and is grateful for the wonderful insight it brings, by virtue of his adhering to a reasoned principle which does not permit him to abuse the magnitude of that gift, will fail to dispose by testament in such a way that both the preservation of the dignity of that place which contains the (…) will be firmly secures and equally the opinion concerning the successor of our saviour, which… when… became master of the school, since Epicurus… according to other specific qualities, not according to the pre-eminence of…

—

Epicureanism is the philosophy taught by Epicurus (341 BC – 270 BC), an Athenian philosopher who identified the goal of human life as pleasure and happiness and who maintained that the universe consisted of nothing but space and atoms. Epicurus was born on the island of Samos in 341 BC to Athenian parents who later moved to Colophon in Asia Minor. He began teaching in Mytilene on the island of Lesbos when he was around thirty years old and soon proceeded to Lampsacus on the nearby mainland, where he founded a school, taught and gathered followers. In 307/6 BC, Epicurus purchased a house with a large walled garden (kēpos) just outside Athens and established a school that became known as the “Garden”. There, amidst the lush and pleasure groves of his Garden, Epicurus taught his hedonistic and materialistic philosophy until his death in 270 BC at seventy-two.

At this time, the two other established Athenian philosophical schools which attracted students from throughout the Greek world were Plato’s Academy and Aristotle’s Lyceum. First founded at the beginning of the 4th century BC was the Academy, located outside Athens’ walls to the north. It remained open until AD 83. Next came the Lyceum in 335 BC to the east of the city. Then came Epicurus’ Garden. In De Finibus V, Cicero places the Garden’s location on the way to Plato’s Academy and indicates that the Academy is situated three-quarters of a mile (1,100 meters) from the Dipylon Gate, the main entrance to the city. Candace H. Smith, in The Hellenistic Philosophers Volume I, provides an illustration in accordance with this portrayal (see here).

We have been born once and there can be no second birth. For all eternity we shall no longer be. But you, although you are not master of tomorrow, are postponing your happiness. We waste away our lives in delaying, and each of us dies without having enjoyed leisure – Epicurus, Sententiae Vaticanae 14

Much of what is known about Epicurus’s principles come from the writings of his later followers. Diogenes Laertius (3rd century AD) is the primary source for the original letters of Epicurus to his disciples (Herodotus, Menoeceus and Pythocles) and the brief sayings called “Principal Doctrines” (Kuriai Doxai). Diogenes gives a somewhat sympathetic account of Epicurus in the tenth and final book of his Lives of Eminent Philosophers. Also preserved is a collection of eighty maxims of ethical content in what is best known as the Vatican Sayings. In addition, Titus Lucretius Carus, an Epicurean poet of the late Roman republican era (1st century BC), wrote a Latin hexameter poem On the Nature of Things devoted to Epicurean physics. Short citations of Epicurus’ works also appear in other writers such as Seneca, Plutarch who were very knowledgeable about Epicurean teachings.

Among the most important later sources for Epicurean philosophy are two remarkable sets of texts discovered in the eighteenth and nineteenth centuries. The first texts were recovered in carbonized condition from the Villa of the Papyri library just outside Herculaneum, which was buried in the eruption of Vesuvius in AD 79. They are books by the Epicurean philosopher and epigrammatist Philodemus of Gadara (ca. 110 – ca. 30 BC), who studied in the Epicurean school at Athens under Zeno of Sidon (c. 150 – c. 75 BC) and later moved to Italy. The majority of the burnt papyri scrolls found in the Villa of the Papyri were written in Greek and contained Philodemus’ philosophical works, including books that reflect upon the fundamental aspects of Epicurean doctrine (PHerc. 1232, 1289), including Epicurus’ lost magnum opus, On Nature.

Don’t fear god,

Don’t worry about death;

What is good is easy to get,

What is terrible is easy to endure

The second set of texts is from the monumental Epicurean inscription from Oinoanda in Lycia (modern Turkey) erected in the AD 120s by a wealthy notable named Diogenes. The surviving fragments of the inscription (about 305 to date), originally extending onto a portico wall about 80 metres in length and 4 metres in height, sets out Epicurus’ teachings on physics, epistemology, and ethics. The inscription was discovered at the end of the nineteenth century, but research gained momentum only after 1968 when British scholar Martin Ferguson Smith investigated the site and the inscription. Diogenes, who was an old man when he had the inscription set up, addresses his fellow citizens and visitors from all over the world. He reminds them that happiness (eudaimonia) gained by pleasure is the aim of all human endeavour, that fear of the gods, death and pain is a particular obstacle of happiness, and warns them of the dangers of unfettered desires.

Having already reached the sunset of my life (being almost on the verge of departure from the world on account of old age), I wanted, before being overtaken by death, to compose a [fine] anthem [to celebrate the] fullness [of pleasure] and so to help now those who are well-constituted. – Diogenes

To these sources, we must also add an inscription from Apamea on the Orontes in Syria, dated to Hadrian’s era or a little later (Smith, 1996). It is a Greek dedication made by one Aurelius Belius Philippus, who appears as diadochus of the Epicureans in Apamea.

On the order of the greatest holy god Bel, Aurelius Belius Philippus, priest and diadochos of the Epicureans in Apamea.

The Epicurean inscription from Oinoanda and the correspondence between Plotina, Hadrian and the Epicurean community at Athens show that Epicureanism continued to have a considerable following during the 2nd century AD, in Athens, but also in other parts of the Roman Empire, such as Asia Minor and Syria.

We do not know when Plotina adhered to Epicureanism, at any rate towards the last years of her life (she died in late 122 or early 123). Her letter to Hadrian reveals that she acted as an intermediary for the Athenian Epicureans with whom she appears to have enjoyed a personal relationship. Furthermore, the favour she obtained for them by her mediation with Hadrian sheds light on the respect she received from the man she helped put on the throne. Their relationship was a very close one, and the two surely had an intellectual bond. Hadrian honoured her on coins and commented that “though she asked a great deal of me, I refused her nothing.”

Her requests were of such a character that they neither burdened me nor afforded me any justification for opposing them. Dio 69.10-3

After her passing, Hadrian honoured her memory by wearing black for nine days. He then had her deified (like Matidia, her mother-in-law), commemorated her with a basilica in her native city (Nemausus in Gaul) and rededicated the temple he had built for Trajan to the Divine Trajan and the Divine Plotina.

—

As a Hellenophile, Hadrian was familiar with the work of the philosophers Epictetus, the famous stoic, and a certain Heliodorus, an Epicurean, both of whom he considered intimate friends (HA Had. 16.10). The philosophy of the Garden may have appealed to Hadrian himself, although it is difficult to assert that Hadrian had Epicurean interests of his own (Birley, 1997 & Brennan, 1998). The Heliodorus mentioned in the Historia Augusta is said be the same philosopher named on another Epicurean document at Athens; a set of letters written by Hadrian four years later in AD 125 (SEG III 226 + IG II² 1097), which was again engraved on marble. The epigraphic document consists of two fragmented and poorly preserved texts, reconstructed by Simone Follet (1994). The first text is addressed to the Epicureans, in which Hadrian confirms the decision he made in 121 to grant them the right to choose a successor regardless of citizenship with a will written in either Greek or Latin. The second text is addressed to Heliodorus, the current head of the school and possibly the successor of Popillius Theotimus of our AD 121 letter. In this case, however, Hadrian refuses a request to construct new buildings and expensive offerings, judging them superfluous (Follet, 1994). According to Anthony Birley, there is a good chance that this Heliodorus was in fact C. Avidius Heliodorus, a Syrian from Cyrrhus, who served as Hadrian’s ab epistulis Graecis (secretary for Greek correspondence) and was prefect of Egypt between AD 138 and 142, and who is further known as the father of the usurper Avidius Cassius.

This was not the only favour which Hadrian bestowed on the philosophers. Immediately after his accession, the Emperor granted philosophers, but also rhetors, grammarians and doctors a wide range of privileges, including exemptions from taxes, and freedom from the burdens of civic offices and liturgies (Dig. 27,1,6,8). Hadrian granted similar privileges to the association of Dionysiac Artists (professional actors and musical performers who presented the great dramas and comedies everywhere in the Greek world).

Sources & references:

Birley, Anthony R., (1997). Hadrian. The restless emperor, Routledge London New York pp. 109 & 182

Van Bremen, R. (2005) ‘Plotina to all her friends: The Letter(s) of the Empress Plotina to the Epicureans in Athens’, Chiron 35:499–532.

Longo, A. and Patrizia Taormina, D. (2016) Plotinus and Epicurus: Matter, Perception, Pleasure, Cambridge; New York: Cambridge University Press pp. 31-34

Clay, Diskin. “The Philosophical Inscription of Diogenes of Oinoanda.” Bulletin of the Institute of Classical Studies. Supplement, no. 94 (2007): 283-91.

Smith, Martin Ferguson. “Fifty-Five New Fragments of Diogenes of Oenoanda.” Anatolian Studies 28 (1978): 39-92. (JSTOR)

Smith, Martin Ferguson. “An Epicurean Priest from Apamea in Syria.” Zeitschrift Für Papyrologie Und Epigraphik 112 (1996): 120-30. (JSTOR)

Oliver, James Henry. “An Inscription concerning the Epicurean School at Athens.” Transactions and Proceedings of the American Philological Association 69 (1938): 494-99. (JSTOR)

Brennan, T.C. 2018. Sabina Augusta: An Imperial Journey. Oxford and New York. pp. 52-54

Konstan, David, “Epicurus“, The Stanford Encyclopedia of Philosophy (Summer 2018 Edition), Edward N. Zalta (ed.).

Blank, David, “Philodemus“, The Stanford Encyclopedia of Philosophy (Spring 2019 Edition), Edward N. Zalta (ed.).

Like this: Like Loading..."	https://followinghadrian.com/2021/03/21/early-ad-121-plotina-writes-to-hadrian-on-behalf-of-the-epicurean-school-in-athens-hadrian1900/	"In the early year of AD 121, Pompeia Plotina, the greatly respected widow of the emperor Trajan, sent Hadrian a letter asking him to help the Epicurean school in Athens solve an issue regarding the rights of succession. According to Roman law, the head of the school was obliged to appoint a new leader from among Roman citizens only with a will written in Latin. This restriction made the number of candidates too limited and prevented the best members of the school from being appointed.
The head of the Epicurean school (scholarch) at the time was a man called Popillius Theotimus, a Roman citizen. As he could not legally name as his successor (diadochus) any foreigner (peregrinus), nor could the will be written in Greek, he enlisted the Dowager Empress to petition the Emperor for a change in the law on his behalf. The relevant letters (IG II² 1099) were preserved in a series of inscriptions discovered in Athens in 1890. They consist of three separate sections. The first section contains the letter from Plotina addressed to Hadrian in Latin in the consulship of Marcus Annius Verus with Gnaeus Arrius Augur as his colleague, which gives the year of AD 121 (before April when two new consuls were elected). The second section contains Hadrian’s brief reply in Latin to Popillius Theotimus, whilst the third is a long letter in Greek from Plotina to “all her friends” at the Epicurean school. The letters were inscribed on slabs of Pentelic marble in Athens in AD 121.
Hadrian agreed to Plotina’s request and granted Popillius Theotimus and his successors permission to extend the succession rights to Greek peregrine and draw up their testaments in Greek.
Plotina wrote to Hadrian as follows: (Translation by R. van Bremen, 2005)
How greatly I favor the school of Epicurus you know full well, my lord. The succession therein needs your help, for since none but a Roman citizen may be elected head of the school the choice is narrowly limited. I pray therefore on behalf of Popillius Theotimus, who is now the head at Athens, that you will allow him to provide by will in Greek concerning that part of his instructions which pertains to the regulation of the headship, and to name a successor to himself of non-citizen status if he is so persuaded by the attainments of the person; and that future heads of the school may hereafter exercise with the same right the privilege you grant to Theotimus, all the more so because the practice is that whenever an error has been made by the testator concerning the choice of a head, he who will be best is, by common consent, selected by the students of the school, and this will be easier if he can be chosen from a larger number.
The Emperor answered: (Translation by R. van Bremen, 2005)
I, Imperator Caesar Traianus Hadrianus Augustus permit Popillius Theotimus to testate in Greek concerning those matters which pertain to the succession of the Epicurean sect, and since it will be easier for him to select a successor if the option exists also to appoint from among the peregrine I guarantee this too and it will be permitted from now on to those who have obtained the succession that this right be transmitted either to a peregrine or to a Roman citizen.
Plotina then passes on the news that her request has been granted to all the Athenian Epicureans: (Translation by R. van Bremen, 2005)
Plotina Augusta to all her friends, greetings. We now have what we were keen to achieve. For it has been granted to each Successor who will lead the School of Epicurus in Athens, both to make dispositions about the entire administration concerning the School by means of Greek testament, and to choose at will, either a Greek or Roman Head of the School. Because this excellent extension of authority has been granted to us – which binds us to express true gratitude to him who is verily a benefactor and guardian of all culture and therefore a most venerable emperor; to me personally also most dear in all respects as both an outstanding master and a good son – it is proper that each of those who have been trusted with the decision concerning the headship always tries to appoint in his own place the best of those who share in the doctrine and to attach more importance to the view of the community as a whole than to his own predilections for particular individuals. It would therefore please me if he did not show preference to anyone over those who are acknowledged as outstanding in the power of our doctrines and, accordingly, in the superiority of their moral conduct. If this were not to be the case, not because of the particular nature of the matter, but because of our own weakness, or through some other accidental impediment, I consider it proper that he who will plan ahead for the common observances will aim for that which will please all in common and not that which will please him personally. But, by Zeus, I do not think that he who has grasped the benefit which has come to him from the teachings and is grateful for the wonderful insight it brings, by virtue of his adhering to a reasoned principle which does not permit him to abuse the magnitude of that gift, will fail to dispose by testament in such a way that both the preservation of the dignity of that place which contains the (…) will be firmly secures and equally the opinion concerning the successor of our saviour, which… when… became master of the school, since Epicurus… according to other specific qualities, not according to the pre-eminence of…
—
Epicureanism is the philosophy taught by Epicurus (341 BC – 270 BC), an Athenian philosopher who identified the goal of human life as pleasure and happiness and who maintained that the universe consisted of nothing but space and atoms. Epicurus was born on the island of Samos in 341 BC to Athenian parents who later moved to Colophon in Asia Minor. He began teaching in Mytilene on the island of Lesbos when he was around thirty years old and soon proceeded to Lampsacus on the nearby mainland, where he founded a school, taught and gathered followers. In 307/6 BC, Epicurus purchased a house with a large walled garden (kēpos) just outside Athens and established a school that became known as the “Garden”. There, amidst the lush and pleasure groves of his Garden, Epicurus taught his hedonistic and materialistic philosophy until his death in 270 BC at seventy-two.
At this time, the two other established Athenian philosophical schools which attracted students from throughout the Greek world were Plato’s Academy and Aristotle’s Lyceum. First founded at the beginning of the 4th century BC was the Academy, located outside Athens’ walls to the north. It remained open until AD 83. Next came the Lyceum in 335 BC to the east of the city. Then came Epicurus’ Garden. In De Finibus V, Cicero places the Garden’s location on the way to Plato’s Academy and indicates that the Academy is situated three-quarters of a mile (1,100 meters) from the Dipylon Gate, the main entrance to the city. Candace H. Smith, in The Hellenistic Philosophers Volume I, provides an illustration in accordance with this portrayal (see here).
We have been born once and there can be no second birth. For all eternity we shall no longer be. But you, although you are not master of tomorrow, are postponing your happiness. We waste away our lives in delaying, and each of us dies without having enjoyed leisure – Epicurus, Sententiae Vaticanae 14
Much of what is known about Epicurus’s principles come from the writings of his later followers. Diogenes Laertius (3rd century AD) is the primary source for the original letters of Epicurus to his disciples (Herodotus, Menoeceus and Pythocles) and the brief sayings called “Principal Doctrines” (Kuriai Doxai). Diogenes gives a somewhat sympathetic account of Epicurus in the tenth and final book of his Lives of Eminent Philosophers. Also preserved is a collection of eighty maxims of ethical content in what is best known as the Vatican Sayings. In addition, Titus Lucretius Carus, an Epicurean poet of the late Roman republican era (1st century BC), wrote a Latin hexameter poem On the Nature of Things devoted to Epicurean physics. Short citations of Epicurus’ works also appear in other writers such as Seneca, Plutarch who were very knowledgeable about Epicurean teachings.
Among the most important later sources for Epicurean philosophy are two remarkable sets of texts discovered in the eighteenth and nineteenth centuries. The first texts were recovered in carbonized condition from the Villa of the Papyri library just outside Herculaneum, which was buried in the eruption of Vesuvius in AD 79. They are books by the Epicurean philosopher and epigrammatist Philodemus of Gadara (ca. 110 – ca. 30 BC), who studied in the Epicurean school at Athens under Zeno of Sidon (c. 150 – c. 75 BC) and later moved to Italy. The majority of the burnt papyri scrolls found in the Villa of the Papyri were written in Greek and contained Philodemus’ philosophical works, including books that reflect upon the fundamental aspects of Epicurean doctrine (PHerc. 1232, 1289), including Epicurus’ lost magnum opus, On Nature.
Don’t fear god,
Don’t worry about death;
What is good is easy to get,
What is terrible is easy to endure
The second set of texts is from the monumental Epicurean inscription from Oinoanda in Lycia (modern Turkey) erected in the AD 120s by a wealthy notable named Diogenes. The surviving fragments of the inscription (about 305 to date), originally extending onto a portico wall about 80 metres in length and 4 metres in height, sets out Epicurus’ teachings on physics, epistemology, and ethics. The inscription was discovered at the end of the nineteenth century, but research gained momentum only after 1968 when British scholar Martin Ferguson Smith investigated the site and the inscription. Diogenes, who was an old man when he had the inscription set up, addresses his fellow citizens and visitors from all over the world. He reminds them that happiness (eudaimonia) gained by pleasure is the aim of all human endeavour, that fear of the gods, death and pain is a particular obstacle of happiness, and warns them of the dangers of unfettered desires.
Having already reached the sunset of my life (being almost on the verge of departure from the world on account of old age), I wanted, before being overtaken by death, to compose a [fine] anthem [to celebrate the] fullness [of pleasure] and so to help now those who are well-constituted. – Diogenes
To these sources, we must also add an inscription from Apamea on the Orontes in Syria, dated to Hadrian’s era or a little later (Smith, 1996). It is a Greek dedication made by one Aurelius Belius Philippus, who appears as diadochus of the Epicureans in Apamea.
On the order of the greatest holy god Bel, Aurelius Belius Philippus, priest and diadochos of the Epicureans in Apamea.
The Epicurean inscription from Oinoanda and the correspondence between Plotina, Hadrian and the Epicurean community at Athens show that Epicureanism continued to have a considerable following during the 2nd century AD, in Athens, but also in other parts of the Roman Empire, such as Asia Minor and Syria.
We do not know when Plotina adhered to Epicureanism, at any rate towards the last years of her life (she died in late 122 or early 123). Her letter to Hadrian reveals that she acted as an intermediary for the Athenian Epicureans with whom she appears to have enjoyed a personal relationship. Furthermore, the favour she obtained for them by her mediation with Hadrian sheds light on the respect she received from the man she helped put on the throne. Their relationship was a very close one, and the two surely had an intellectual bond. Hadrian honoured her on coins and commented that “though she asked a great deal of me, I refused her nothing.”
Her requests were of such a character that they neither burdened me nor afforded me any justification for opposing them. Dio 69.10-3
After her passing, Hadrian honoured her memory by wearing black for nine days. He then had her deified (like Matidia, her mother-in-law), commemorated her with a basilica in her native city (Nemausus in Gaul) and rededicated the temple he had built for Trajan to the Divine Trajan and the Divine Plotina.
—
As a Hellenophile, Hadrian was familiar with the work of the philosophers Epictetus, the famous stoic, and a certain Heliodorus, an Epicurean, both of whom he considered intimate friends (HA Had. 16.10). The philosophy of the Garden may have appealed to Hadrian himself, although it is difficult to assert that Hadrian had Epicurean interests of his own (Birley, 1997 & Brennan, 1998). The Heliodorus mentioned in the Historia Augusta is said be the same philosopher named on another Epicurean document at Athens; a set of letters written by Hadrian four years later in AD 125 (SEG III 226 + IG II² 1097), which was again engraved on marble. The epigraphic document consists of two fragmented and poorly preserved texts, reconstructed by Simone Follet (1994). The first text is addressed to the Epicureans, in which Hadrian confirms the decision he made in 121 to grant them the right to choose a successor regardless of citizenship with a will written in either Greek or Latin. The second text is addressed to Heliodorus, the current head of the school and possibly the successor of Popillius Theotimus of our AD 121 letter. In this case, however, Hadrian refuses a request to construct new buildings and expensive offerings, judging them superfluous (Follet, 1994). According to Anthony Birley, there is a good chance that this Heliodorus was in fact C. Avidius Heliodorus, a Syrian from Cyrrhus, who served as Hadrian’s ab epistulis Graecis (secretary for Greek correspondence) and was prefect of Egypt between AD 138 and 142, and who is further known as the father of the usurper Avidius Cassius.
This was not the only favour which Hadrian bestowed on the philosophers. Immediately after his accession, the Emperor granted philosophers, but also rhetors, grammarians and doctors a wide range of privileges, including exemptions from taxes, and freedom from the burdens of civic offices and liturgies (Dig. 27,1,6,8). Hadrian granted similar privileges to the association of Dionysiac Artists (professional actors and musical performers who presented the great dramas and comedies everywhere in the Greek world).
Sources & references:
Birley, Anthony R., (1997). Hadrian. The restless emperor, Routledge London New York pp. 109 & 182
Van Bremen, R. (2005) ‘Plotina to all her friends: The Letter(s) of the Empress Plotina to the Epicureans in Athens’, Chiron 35:499–532.
Longo, A. and Patrizia Taormina, D. (2016) Plotinus and Epicurus: Matter, Perception, Pleasure, Cambridge; New York: Cambridge University Press pp. 31-34
Clay, Diskin. “The Philosophical Inscription of Diogenes of Oinoanda.” Bulletin of the Institute of Classical Studies. Supplement, no. 94 (2007): 283-91.
Smith, Martin Ferguson. “Fifty-Five New Fragments of Diogenes of Oenoanda.” Anatolian Studies 28 (1978): 39-92. (JSTOR)
Smith, Martin Ferguson. “An Epicurean Priest from Apamea in Syria.” Zeitschrift Für Papyrologie Und Epigraphik 112 (1996): 120-30. (JSTOR)
Oliver, James Henry. “An Inscription concerning the Epicurean School at Athens.” Transactions and Proceedings of the American Philological Association 69 (1938): 494-99. (JSTOR)
Brennan, T.C. 2018. Sabina Augusta: An Imperial Journey. Oxford and New York. pp. 52-54
Konstan, David, “Epicurus“, The Stanford Encyclopedia of Philosophy (Summer 2018 Edition), Edward N. Zalta (ed.).
Blank, David, “Philodemus“, The Stanford Encyclopedia of Philosophy (Spring 2019 Edition), Edward N. Zalta (ed.).
Like this: Like Loading..."	15718
history	[]	2020-07-21 00:00:00	"The term “cancel culture” has been widely used in recent years to refer to efforts by both the political left and right to aggressively annul closely held cultural beliefs and even individual personalities by one side or the other. With governments worldwide struggling to contain the Covid-19 pandemic, Turkey’s Islamist government implemented cancel culture on a massive scale with the conversion of the 6th Century Byzantine Church of Hagia Sophia (Holy Wisdom) into a mosque.

Before going further, I need to lay some groundwork for what follows. First, I am writing from the perspective of a secular humanist; this is, an atheist. No argument from Islamist zealots, modern Orthodox reactionaries, European or American Christians seeking to generate political capital from this event nor any other interest group have played any part in my thinking. Second, I am writing from the perspective of someone with an academic background in archaeology. The truth as revealed by the historical written record and the archaeological record is important to me above all else in this matter. Third, I am writing from the perspective of a practicing artist; this is, someone who makes a substantial part of my living marketing and selling my own art. Looking at art and thinking about the role of art in both the past and in modern society is vitally important to me. Fourth, I am writing from the perspective of someone with a family background in the Middle East (my mother was Assyrian), who also happens to have a decidedly left leaning political and social perspective.

The facts of this recent event, which went largely unnoticed by a distracted world, are quickly laid out: In early July of this year, Turkey’s Council of State debated annulment of the 1934 decision to convert Hagia Sophia to a museum. The outcome of this debate was a forgone conclusion, as the Council was packed with loyalists to President Erdogan, the Islamist head of state, who had also served as Prime Minister from 2004-2014. On July 14, the Council voted to proceed and it was announced that the first Islamic prayers would be held inside the building on July 24. The screen capture below, from the Turkish government’s website, shows how quickly this decision was implemented.

Hagia Sophia’s status as a museum since 1935 had been a powerful symbol of Turkey’s secular status. This was a uniquely Turkish national characteristic among Middle Eastern nations. But under a resurgent Islamist political party, the AKP, with Erdogan at its head, Turkey’s secular institutions have been slowly dismantled and power centralized under an increasingly authoritarian, anti-free press regime. The conversion announcement was condemned by UNESCO, various heads of state around the world, the Pope, heads of all the Orthodox churches and by the secular Turkish novelist and Nobel laureate Orhan Pamuk.

It should be pointed out that Hagia Sophia is not the first ancient Byzantine monument to be converted to a mosque under the current regime. There have been three previous such moves in recent years, including the famed Church of the Chora Monastery.

Byzantine civilization is perhaps the least well understood, at least in Europe and the Americas, of all great civilizations of the past. Mention Byzantium to the average American and you’ll evoke a puzzled silence brought about by a mental fumbling of exactly where Byzantium fits in the tide of human history. With that in mind, I’d like to briefly lay out a few simple facts to help underscore the great importance of Hagia Sophia and the culture that created it.

In the early 4th Century CE (CE is the widely accepted term that replaces AD), the Roman Empire — which had dominated the Mediterranean world, much of Northern Europe including Britain, the North African coastline, Egypt, what is now Turkey and much of the Near East for centuries — was facing ever growing threats from the east. These included new enemies, migratory people such as the Visigoths, Ostrogoths, Huns and many others who had been slowly moving westward from their original homelands in the Central Asian Steppe regions. And old enemies, the resurgent Persian Empire under the Sassanian Kings. With a need to centralize command and control in a location better suited to meet these threats than the old capital at Rome, in 330 CE the Emperor Constantine I established a “new Rome” on the Bosporus Straights. It was the site of an ancient Greek town, providing access to the Mediterranean through the sea of Marmara to the west, the Black Sea to the east, and sitting on the connection point of Rome’s Greek speaking Asian provinces and Latin speaking western provinces. It was, of course, called Constantinopolis (Constantinople).

Nearly two centuries later, in 537 CE, under the Emperor Justinian I, Hagia Sophia was inaugurated. Designed and built by two mathematicians, Isidore and Anthemius, it was the principle location of events of state and the seat of the Patriarchate of Constantinople and therefore the primary church of the Byzantine world (“Byzantine” is a term coined later by historians; the Byzantines always referred to themselves as Romans). There simply was nothing else like it in on Earth. It featured the largest dome in the world for a thousand years until the completion of St Peter’s in Rome, the dome covering a space of 65,000 square feet. It was constructed of the finest, most exotic marbles, the interiors surfaced with stunning mosaics, and furnished with rare, exotic and ancient objects brought from Rome and other major cities in the Mediterranean world. Emissaries from European kingdoms, barbarian tribes, Vikings, Persians and others were left dumbfounded by Hagia Sophia’s opulence. With the Ottoman Turkish conquest of the Byzantine Empire in 1453, which by then comprised little more than the City of Constantinople itself, the great church was looted of its remaining valuables, its portable artworks hacked to bits and its clergy killed or sold into slavery, with the building itself converted to a mosque. It remained so until the secular Turkish government of Kemal Attaturk converted it to a museum.

Exterior view of the dome of Hagia Sophia as seen from the south.

Interior view of Hagia Sophia, showing part of the dome and hemi-domes. Credit: Tranxen / CC BY-SA (https://creativecommons.org/licenses/by-sa/3.0)

Gold mosaic inside Hagia Sophia showing the Emperor John Komnenos II and his wife Irene. On the left, Irene is holding a scroll; to the right is John carrying a bag of coins. These symbolize donations to the Church. In the middle, Virgin Mary wearing blue robes, and holding Christ in her arms. 12th Century.

9th Century gold mosaic of Virgin Mary sitting on a backless throne, with Christ on her lap. Situated in the apse of the hemi-dome above the altar.

Edward Gibbon’s monumental 18th Century work The Decline and Fall of the Roman Empire saw Byzantium as merely a corrupted poor relation of the earlier Roman Empire. Due in part to this and a European preference for a (perceived) secularized Roman world of the Republic and Empire, as opposed to the religiously inclined Byzantine world, and perhaps a European guilt at the sacking and temporary occupation of Constantinople during the Fourth Crusade, western historians long dismissed Byzantine studies.

In recent decades there has been a resurgence of interest in the Byzantine world, and a much deeper appreciation of Byzantium as a dynamic, multi-cultural society. Western scholarship has finally begun to acknowledge the vital role played by Byzantium in preserving classical Greek and Roman literature, transmitting this to the west, mainly through the Italian commercial republics (Venice, Florence, Genoa) from the 13th through 15th Centuries and helping kick start the Italian Renaissance. Once viewed by the west as monolithic and fossilized, Byzantine history, society and art have more recently been recognized as complex, subtle, sophisticated in ways that European society would not be until much later in its history (illiteracy was virtually unknown among the middle and upper classes in the Byzantine world), and capable of producing sublime works of visual art and architecture. Hagia Sophia is the most grand of these works. And all of this doesn’t even touch upon the profound political, artistic and theological impact Byzantium had upon Eastern Europe, Russia and the Islamic world for many centuries.

Shortly after their decision to convert Hagia Sophia from a museum to a mosque, the Turkish authorities issued a vague statement outlining a bizarre plan to cover the mosaics and other Byzantine representational images inside the building with laser light or draperies during Muslim prayers. Based on what the current Turkish government has done with the physical remains of Byzantine heritage thus far, this is surely the next step in a creeping long term plan to “cancel” the remaining vestiges of Byzantium.

It was the early 20th Century English travel writer Robert Byron who described the Byzantine Empire as the fusion of a Roman body, a Greek mind and a mystical soul. A leading advocate for the refurbishment of Byzantium’s reputation, he may have been strongly influenced by the Romantic poets, who had their own misconceptions about Byzantium. But on the whole, I believe his assessment was accurate. For this reason alone, Byzantium, it’s last Emperor Constantine XI, who died fighting at the breach in Constantinople’s walls on its last day, Tuesday, the 29th of May, 1453, and the incomparable achievement of Hagia Sophia itself, deserve better than the disgraceful treatment outlined above.

It would require a great many posts in this blog to attempt a meaningful summary of Byzantine art and history; that is a task best undertaken by others. But for those less familiar with Byzantium and its achievements, here is a very short list of websites, documentaries and books I believe to be of some value (all links open in a new tab):

WEBSITES & DOCUMENTARIES

Dumbarton Oaks Research Library and Collection in Georgetown, Washington, DC, features a stunning collection of Byzantine and related art. Their online collections include Byzantine textiles, coins, seals and manuscripts. https://www.doaks.org/resources/online-collections

Some years ago, renowned British archaeologist John Romer, who has a great talent for story telling, created a 4 part series on Byzantium. It can be accessed for free on YouTube. I’ve included links to all 4 episodes. #1 https://youtu.be/WW1J1VH966c #2 https://youtu.be/l3aUL_YM89Y #3 https://youtu.be/j9l6hzKPRlY #4 https://youtu.be/4yC-gif0H-s

BOOKS

The Glory of Byzantium: Art and Culture of the Middle Byzantine Era, AD 843-1261. This monumental work, which accompanied a 1997 exhibition of the same name at the Metropolitan Museum of Art in New York, may be downloaded for free or read online from the Met’s website. https://www.metmuseum.org/art/metpublications/The_Glory_of_Byzantium_Art_and_Culture_of_the_Middle_Byzantine_Era_AD_843_1261?Tag=&title=&author=&pt=0&tc={F52F45EC-2E28-4BC3-96B6-879A33F0B139}&dept=0&fmt=0

A Short History of Byzantium. John Julius Norwich. Alfred A Knopf, New York, 1997. This thick single volume is an abbreviated version, based on a great three volume work.

PLEASE NOTE: Comments for this blog entry have been turned off, as there will surely be partisans of every religious, political and philosophical persuasion demanding my head after reading this."	https://clioantiquities.wordpress.com/2020/07/21/with-the-world-distracted-turkeys-government-cancels-byzantine-history/	"The term “cancel culture” has been widely used in recent years to refer to efforts by both the political left and right to aggressively annul closely held cultural beliefs and even individual personalities by one side or the other. With governments worldwide struggling to contain the Covid-19 pandemic, Turkey’s Islamist government implemented cancel culture on a massive scale with the conversion of the 6th Century Byzantine Church of Hagia Sophia (Holy Wisdom) into a mosque.
Before going further, I need to lay some groundwork for what follows. First, I am writing from the perspective of a secular humanist; this is, an atheist. No argument from Islamist zealots, modern Orthodox reactionaries, European or American Christians seeking to generate political capital from this event nor any other interest group have played any part in my thinking. Second, I am writing from the perspective of someone with an academic background in archaeology. The truth as revealed by the historical written record and the archaeological record is important to me above all else in this matter. Third, I am writing from the perspective of a practicing artist; this is, someone who makes a substantial part of my living marketing and selling my own art. Looking at art and thinking about the role of art in both the past and in modern society is vitally important to me. Fourth, I am writing from the perspective of someone with a family background in the Middle East (my mother was Assyrian), who also happens to have a decidedly left leaning political and social perspective.
The facts of this recent event, which went largely unnoticed by a distracted world, are quickly laid out: In early July of this year, Turkey’s Council of State debated annulment of the 1934 decision to convert Hagia Sophia to a museum. The outcome of this debate was a forgone conclusion, as the Council was packed with loyalists to President Erdogan, the Islamist head of state, who had also served as Prime Minister from 2004-2014. On July 14, the Council voted to proceed and it was announced that the first Islamic prayers would be held inside the building on July 24. The screen capture below, from the Turkish government’s website, shows how quickly this decision was implemented.
Hagia Sophia’s status as a museum since 1935 had been a powerful symbol of Turkey’s secular status. This was a uniquely Turkish national characteristic among Middle Eastern nations. But under a resurgent Islamist political party, the AKP, with Erdogan at its head, Turkey’s secular institutions have been slowly dismantled and power centralized under an increasingly authoritarian, anti-free press regime. The conversion announcement was condemned by UNESCO, various heads of state around the world, the Pope, heads of all the Orthodox churches and by the secular Turkish novelist and Nobel laureate Orhan Pamuk.
It should be pointed out that Hagia Sophia is not the first ancient Byzantine monument to be converted to a mosque under the current regime. There have been three previous such moves in recent years, including the famed Church of the Chora Monastery.
Byzantine civilization is perhaps the least well understood, at least in Europe and the Americas, of all great civilizations of the past. Mention Byzantium to the average American and you’ll evoke a puzzled silence brought about by a mental fumbling of exactly where Byzantium fits in the tide of human history. With that in mind, I’d like to briefly lay out a few simple facts to help underscore the great importance of Hagia Sophia and the culture that created it.
In the early 4th Century CE (CE is the widely accepted term that replaces AD), the Roman Empire — which had dominated the Mediterranean world, much of Northern Europe including Britain, the North African coastline, Egypt, what is now Turkey and much of the Near East for centuries — was facing ever growing threats from the east. These included new enemies, migratory people such as the Visigoths, Ostrogoths, Huns and many others who had been slowly moving westward from their original homelands in the Central Asian Steppe regions. And old enemies, the resurgent Persian Empire under the Sassanian Kings. With a need to centralize command and control in a location better suited to meet these threats than the old capital at Rome, in 330 CE the Emperor Constantine I established a “new Rome” on the Bosporus Straights. It was the site of an ancient Greek town, providing access to the Mediterranean through the sea of Marmara to the west, the Black Sea to the east, and sitting on the connection point of Rome’s Greek speaking Asian provinces and Latin speaking western provinces. It was, of course, called Constantinopolis (Constantinople).
Nearly two centuries later, in 537 CE, under the Emperor Justinian I, Hagia Sophia was inaugurated. Designed and built by two mathematicians, Isidore and Anthemius, it was the principle location of events of state and the seat of the Patriarchate of Constantinople and therefore the primary church of the Byzantine world (“Byzantine” is a term coined later by historians; the Byzantines always referred to themselves as Romans). There simply was nothing else like it in on Earth. It featured the largest dome in the world for a thousand years until the completion of St Peter’s in Rome, the dome covering a space of 65,000 square feet. It was constructed of the finest, most exotic marbles, the interiors surfaced with stunning mosaics, and furnished with rare, exotic and ancient objects brought from Rome and other major cities in the Mediterranean world. Emissaries from European kingdoms, barbarian tribes, Vikings, Persians and others were left dumbfounded by Hagia Sophia’s opulence. With the Ottoman Turkish conquest of the Byzantine Empire in 1453, which by then comprised little more than the City of Constantinople itself, the great church was looted of its remaining valuables, its portable artworks hacked to bits and its clergy killed or sold into slavery, with the building itself converted to a mosque. It remained so until the secular Turkish government of Kemal Attaturk converted it to a museum.
Exterior view of the dome of Hagia Sophia as seen from the south.
Interior view of Hagia Sophia, showing part of the dome and hemi-domes. Credit: Tranxen / CC BY-SA (https://creativecommons.org/licenses/by-sa/3.0)
Gold mosaic inside Hagia Sophia showing the Emperor John Komnenos II and his wife Irene. On the left, Irene is holding a scroll; to the right is John carrying a bag of coins. These symbolize donations to the Church. In the middle, Virgin Mary wearing blue robes, and holding Christ in her arms. 12th Century.
9th Century gold mosaic of Virgin Mary sitting on a backless throne, with Christ on her lap. Situated in the apse of the hemi-dome above the altar.
Edward Gibbon’s monumental 18th Century work The Decline and Fall of the Roman Empire saw Byzantium as merely a corrupted poor relation of the earlier Roman Empire. Due in part to this and a European preference for a (perceived) secularized Roman world of the Republic and Empire, as opposed to the religiously inclined Byzantine world, and perhaps a European guilt at the sacking and temporary occupation of Constantinople during the Fourth Crusade, western historians long dismissed Byzantine studies.
In recent decades there has been a resurgence of interest in the Byzantine world, and a much deeper appreciation of Byzantium as a dynamic, multi-cultural society. Western scholarship has finally begun to acknowledge the vital role played by Byzantium in preserving classical Greek and Roman literature, transmitting this to the west, mainly through the Italian commercial republics (Venice, Florence, Genoa) from the 13th through 15th Centuries and helping kick start the Italian Renaissance. Once viewed by the west as monolithic and fossilized, Byzantine history, society and art have more recently been recognized as complex, subtle, sophisticated in ways that European society would not be until much later in its history (illiteracy was virtually unknown among the middle and upper classes in the Byzantine world), and capable of producing sublime works of visual art and architecture. Hagia Sophia is the most grand of these works. And all of this doesn’t even touch upon the profound political, artistic and theological impact Byzantium had upon Eastern Europe, Russia and the Islamic world for many centuries.
Shortly after their decision to convert Hagia Sophia from a museum to a mosque, the Turkish authorities issued a vague statement outlining a bizarre plan to cover the mosaics and other Byzantine representational images inside the building with laser light or draperies during Muslim prayers. Based on what the current Turkish government has done with the physical remains of Byzantine heritage thus far, this is surely the next step in a creeping long term plan to “cancel” the remaining vestiges of Byzantium.
It was the early 20th Century English travel writer Robert Byron who described the Byzantine Empire as the fusion of a Roman body, a Greek mind and a mystical soul. A leading advocate for the refurbishment of Byzantium’s reputation, he may have been strongly influenced by the Romantic poets, who had their own misconceptions about Byzantium. But on the whole, I believe his assessment was accurate. For this reason alone, Byzantium, it’s last Emperor Constantine XI, who died fighting at the breach in Constantinople’s walls on its last day, Tuesday, the 29th of May, 1453, and the incomparable achievement of Hagia Sophia itself, deserve better than the disgraceful treatment outlined above.
It would require a great many posts in this blog to attempt a meaningful summary of Byzantine art and history; that is a task best undertaken by others. But for those less familiar with Byzantium and its achievements, here is a very short list of websites, documentaries and books I believe to be of some value (all links open in a new tab):
WEBSITES & DOCUMENTARIES
Dumbarton Oaks Research Library and Collection in Georgetown, Washington, DC, features a stunning collection of Byzantine and related art. Their online collections include Byzantine textiles, coins, seals and manuscripts. https://www.doaks.org/resources/online-collections
Some years ago, renowned British archaeologist John Romer, who has a great talent for story telling, created a 4 part series on Byzantium. It can be accessed for free on YouTube. I’ve included links to all 4 episodes. #1 https://youtu.be/WW1J1VH966c #2 https://youtu.be/l3aUL_YM89Y #3 https://youtu.be/j9l6hzKPRlY #4 https://youtu.be/4yC-gif0H-s
BOOKS
The Glory of Byzantium: Art and Culture of the Middle Byzantine Era, AD 843-1261. This monumental work, which accompanied a 1997 exhibition of the same name at the Metropolitan Museum of Art in New York, may be downloaded for free or read online from the Met’s website. https://www.metmuseum.org/art/metpublications/The_Glory_of_Byzantium_Art_and_Culture_of_the_Middle_Byzantine_Era_AD_843_1261?Tag=&title=&author=&pt=0&tc={F52F45EC-2E28-4BC3-96B6-879A33F0B139}&dept=0&fmt=0
A Short History of Byzantium. John Julius Norwich. Alfred A Knopf, New York, 1997. This thick single volume is an abbreviated version, based on a great three volume work.
PLEASE NOTE: Comments for this blog entry have been turned off, as there will surely be partisans of every religious, political and philosophical persuasion demanding my head after reading this."	11474
history	['Theme']	2018-07-08 11:44:56+00:00	"By Marguerite Johnson

It was a well-kept secret among historians during the late 19th and early 20th centuries that the practice of magic was widespread in the ancient Mediterranean. Historians wanted to keep the activity low-key because it did not support their idealised view of the Greeks and Romans. Today, however, magic is a legitimate area of scholarly enquiry, providing insights into ancient belief systems as well as cultural and social practices.

While magic was discouraged and sometimes even punished in antiquity, it thrived all the same. Authorities publicly condemned it, but tended to ignore its powerful hold.

Erotic spells were a popular form of magic. Professional magic practitioners charged fees for writing erotic charms, making enchanted dolls (sometimes called poppets), and even directing curses against rivals in love.

Magic is widely attested in archaeological evidence, spell books and literature from both Greece and Rome, as well as Egypt and the Middle East. The Greek Magical Papyri, for example, from Graeco-Roman Egypt, is a large collection of papyri listing spells for many purposes. The collection was compiled from sources dating from the second century BC to the fifth century AD, and includes numerous spells of attraction.

Some spells involve making dolls, which were intended to represent the object of desire (usually a woman who was either unaware or resistant to a would-be admirer). Instructions specified how an erotic doll should be made, what words should be said over it, and where it should be deposited.

Such an object is a form of sympathetic magic; a type of enchantment that operates along the principle of “like affects like”. When enacting sympathetic magic with a doll, the spell-caster believes that whatever action is performed on it – be it physical or psychic – will be transferred to the human it represents.

The best preserved and most notorious magical doll from antiquity, the so-called “Louvre Doll” (4th century AD), depicts a naked female in kneeling position, bound, and pierced with 13 needles. Fashioned from unbaked clay, the doll was found in a terracotta vase in Egypt. The accompanying spell, inscribed on a lead tablet, records the woman’s name as Ptolemais and the man who made the spell, or commissioned a magician to do so, as Sarapammon.

Violent, brutal language

The spells that accompanied such dolls and, indeed, the spells from antiquity on all manner of topics, were not mild in the language and imagery employed. Ancient spells were often violent, brutal and without any sense of caution or remorse. In the spell that comes with the Louvre Doll, the language is both frightening and repellent in a modern context. For example, one part of the spell directed at Ptolemais reads:

Do not allow her to eat, drink, hold out, venture out, or find sleep …

Another part reads:

Drag her by the hair, by the guts, until she no longer scorns me …

Such language is hardly indicative of any emotion pertaining to love, or even attraction. Especially when combined with the doll, the spell may strike a modern reader as obsessive (perhaps reminiscent of a stalker or online troll) and even misogynistic. Indeed, rather than seeking love, the intention behind the spell suggests seeking control and domination. Such were the gender and sexual dynamics of antiquity.

But in a masculine world, in which competition in all aspects of life was intense, and the goal of victory was paramount, violent language was typical in spells pertaining to anything from success in a court case to the rigging of a chariot race. Indeed, one theory suggests that the more ferocious the words, the more powerful and effective the spell.

Love potions

Most ancient evidence attests to men as both professional magical practitioners and their clients. There was a need to be literate to perform most magic (most women were not educated) and to be accessible to clients (most women were not free to receive visitors or have a business). However, some women also engaged in erotic magic (although the sources on this are relatively scarce).

In ancient Athens, for example, a woman was taken to court on the charge of attempting to poison her husband. The trial was recorded in a speech delivered on behalf of the prosecution (dated around 419 BC). It includes the woman’s defence, which stated that she did not intend to poison her husband but to administer a love philtre to reinvigorate the marriage.

The speech, entitled Against the Stepmother for Poisoning by Antiphon, clearly reveals that the Athenians practised and believed in love potions and may suggest that this more subtle form of erotic magic (compared to the casting of spells and the making of enchanted dolls) was the preserve of women.

Desire between women

Within the multiplicity of spells found in the Greek Magical Papyri, two deal specifically with female same sex desire. In one of these, a woman by the name of Herais attempts to magically entreat a woman by the name of Serapis. In this spell, dated to the second century AD, the gods Anubis and Hermes are called upon to bring Serapis to Herais and to bind Serapis to her.

In the second spell, dated to the third or fourth century AD, a woman called Sophia seeks out a woman by the name of Gorgonia. This spell, written on a lead tablet, is aggressive in tone; for example:

Burn, set on fire, inflame her soul, heart, liver, spirit, with love for Sophia …

Gods and goddesses were regularly summoned in magic. In the spell to attract Serapis, for example, Anubis is included based on his role as the god of the secrets of Egyptian magic. Hermes, a Greek god, was often included because as a messenger god, he was a useful choice in spells that sought contact with someone.

The tendency to combine gods from several cultures was not uncommon in ancient magic, indicative of its eclectic nature and perhaps a form of hedging one’s bets (if one religion’s god won’t listen, one from another belief system may).

Deities with erotic connections were also inscribed on gems to induce attraction. The Greek god of eroticism, Eros was a popular figure to depict on a gemstone, which could then be fashioned into a piece of jewellery.

The numerous erotic spells in antiquity – from potions to dolls to enchanted gems and rituals – not only provide information about magic in the ancient Mediterranean world, but the intricacies and cultural conventions around sexuality and gender.

The rigid system of clearly demarcated gender roles of active (male) and passive (female) partners, based on a patriarchy that championed dominance and success at all costs, underpinned the same societies’ magical practices. Yet it is important to note that even in magic featuring people of the same sex, aggressive language is employed because of the conventions that underlined ancient spells.

Still magic remains, in part, a mystery when it comes to erotic practice and conventions. The two same-sex spells from the Greek Magical Papyri, for example, attest to the reality of erotic desire among ancient women, but do not shed light on whether this type of sexuality was condoned in Roman Egypt. Perhaps such desires were not socially approved; hence the recourse to magic. Perhaps the desires of Sarapammon for Ptolemais were also outside the bounds of acceptability, which led him to the surreptitious and desperate world of magic.

Marguerite Johnson is Professor of Classics, University of Newcastle

This article was first published in The Conversation

Sponsored Content"	https://www.historyoftheancientworld.com/2018/07/spells-charms-erotic-dolls-love-magic-in-the-ancient-mediterranean/	"By Marguerite Johnson
It was a well-kept secret among historians during the late 19th and early 20th centuries that the practice of magic was widespread in the ancient Mediterranean. Historians wanted to keep the activity low-key because it did not support their idealised view of the Greeks and Romans. Today, however, magic is a legitimate area of scholarly enquiry, providing insights into ancient belief systems as well as cultural and social practices.
While magic was discouraged and sometimes even punished in antiquity, it thrived all the same. Authorities publicly condemned it, but tended to ignore its powerful hold.
Erotic spells were a popular form of magic. Professional magic practitioners charged fees for writing erotic charms, making enchanted dolls (sometimes called poppets), and even directing curses against rivals in love.
Magic is widely attested in archaeological evidence, spell books and literature from both Greece and Rome, as well as Egypt and the Middle East. The Greek Magical Papyri, for example, from Graeco-Roman Egypt, is a large collection of papyri listing spells for many purposes. The collection was compiled from sources dating from the second century BC to the fifth century AD, and includes numerous spells of attraction.
Some spells involve making dolls, which were intended to represent the object of desire (usually a woman who was either unaware or resistant to a would-be admirer). Instructions specified how an erotic doll should be made, what words should be said over it, and where it should be deposited.
Such an object is a form of sympathetic magic; a type of enchantment that operates along the principle of “like affects like”. When enacting sympathetic magic with a doll, the spell-caster believes that whatever action is performed on it – be it physical or psychic – will be transferred to the human it represents.
The best preserved and most notorious magical doll from antiquity, the so-called “Louvre Doll” (4th century AD), depicts a naked female in kneeling position, bound, and pierced with 13 needles. Fashioned from unbaked clay, the doll was found in a terracotta vase in Egypt. The accompanying spell, inscribed on a lead tablet, records the woman’s name as Ptolemais and the man who made the spell, or commissioned a magician to do so, as Sarapammon.
Violent, brutal language
The spells that accompanied such dolls and, indeed, the spells from antiquity on all manner of topics, were not mild in the language and imagery employed. Ancient spells were often violent, brutal and without any sense of caution or remorse. In the spell that comes with the Louvre Doll, the language is both frightening and repellent in a modern context. For example, one part of the spell directed at Ptolemais reads:
Do not allow her to eat, drink, hold out, venture out, or find sleep …
Another part reads:
Drag her by the hair, by the guts, until she no longer scorns me …
Such language is hardly indicative of any emotion pertaining to love, or even attraction. Especially when combined with the doll, the spell may strike a modern reader as obsessive (perhaps reminiscent of a stalker or online troll) and even misogynistic. Indeed, rather than seeking love, the intention behind the spell suggests seeking control and domination. Such were the gender and sexual dynamics of antiquity.
But in a masculine world, in which competition in all aspects of life was intense, and the goal of victory was paramount, violent language was typical in spells pertaining to anything from success in a court case to the rigging of a chariot race. Indeed, one theory suggests that the more ferocious the words, the more powerful and effective the spell.
Love potions
Most ancient evidence attests to men as both professional magical practitioners and their clients. There was a need to be literate to perform most magic (most women were not educated) and to be accessible to clients (most women were not free to receive visitors or have a business). However, some women also engaged in erotic magic (although the sources on this are relatively scarce).
In ancient Athens, for example, a woman was taken to court on the charge of attempting to poison her husband. The trial was recorded in a speech delivered on behalf of the prosecution (dated around 419 BC). It includes the woman’s defence, which stated that she did not intend to poison her husband but to administer a love philtre to reinvigorate the marriage.
The speech, entitled Against the Stepmother for Poisoning by Antiphon, clearly reveals that the Athenians practised and believed in love potions and may suggest that this more subtle form of erotic magic (compared to the casting of spells and the making of enchanted dolls) was the preserve of women.
Desire between women
Within the multiplicity of spells found in the Greek Magical Papyri, two deal specifically with female same sex desire. In one of these, a woman by the name of Herais attempts to magically entreat a woman by the name of Serapis. In this spell, dated to the second century AD, the gods Anubis and Hermes are called upon to bring Serapis to Herais and to bind Serapis to her.
In the second spell, dated to the third or fourth century AD, a woman called Sophia seeks out a woman by the name of Gorgonia. This spell, written on a lead tablet, is aggressive in tone; for example:
Burn, set on fire, inflame her soul, heart, liver, spirit, with love for Sophia …
Gods and goddesses were regularly summoned in magic. In the spell to attract Serapis, for example, Anubis is included based on his role as the god of the secrets of Egyptian magic. Hermes, a Greek god, was often included because as a messenger god, he was a useful choice in spells that sought contact with someone.
The tendency to combine gods from several cultures was not uncommon in ancient magic, indicative of its eclectic nature and perhaps a form of hedging one’s bets (if one religion’s god won’t listen, one from another belief system may).
Deities with erotic connections were also inscribed on gems to induce attraction. The Greek god of eroticism, Eros was a popular figure to depict on a gemstone, which could then be fashioned into a piece of jewellery.
The numerous erotic spells in antiquity – from potions to dolls to enchanted gems and rituals – not only provide information about magic in the ancient Mediterranean world, but the intricacies and cultural conventions around sexuality and gender.
The rigid system of clearly demarcated gender roles of active (male) and passive (female) partners, based on a patriarchy that championed dominance and success at all costs, underpinned the same societies’ magical practices. Yet it is important to note that even in magic featuring people of the same sex, aggressive language is employed because of the conventions that underlined ancient spells.
Still magic remains, in part, a mystery when it comes to erotic practice and conventions. The two same-sex spells from the Greek Magical Papyri, for example, attest to the reality of erotic desire among ancient women, but do not shed light on whether this type of sexuality was condoned in Roman Egypt. Perhaps such desires were not socially approved; hence the recourse to magic. Perhaps the desires of Sarapammon for Ptolemais were also outside the bounds of acceptability, which led him to the surreptitious and desperate world of magic.
Marguerite Johnson is Professor of Classics, University of Newcastle
This article was first published in The Conversation
Sponsored Content"	7524
history	['Theme']	2018-07-30 03:35:56+00:00	"Since the 16th century, Basel has been home to a mysterious papyrus. With mirror writing on both sides, it has puzzled generations of researchers. A research team from the University of Basel has now discovered that it is an unknown medical document from late antiquity. The text was likely written by the famous Roman physician Galen.

The Basel papyrus collection comprises 65 papers in five languages, which were purchased by the university in 1900 for the purpose of teaching classical studies – with the exception of two papyri. These arrived in Basel back in the 16th century, and likely formed part of Basilius Amerbach’s art collection.

One of these Amerbach papyri was regarded until now as unique in the world of papyrology. With mirror writing on both sides, it has puzzled generations of researchers. It was only through ultraviolet and infrared images produced by the Basel Digital Humanities Lab that it was possible to determine that this 2,000-year-old document was not a single papyrus at all, but rather several layers of papyrus glued together. A specialist papyrus restorer was brought to Basel to separate the sheets, enabling the Greek document to be decoded for the first time.

“This is a sensational discovery,” says Sabine Huebner, Professor of Ancient History at the University of Basel. “The majority of papyri are documents such as letters, contracts and receipts. This is a literary text, however, and they are vastly more valuable.”

What’s more, it contains a previously unknown text from antiquity. “We can now say that it’s a medical text from late antiquity that describes the phenomenon of ‘hysterical apnea’,” says Huebner. “We therefore assume that it is either a text from the Roman physician Galen, or an unknown commentary on his work.” After Hippocrates, Galen is regarded as the most important physician of antiquity.

The decisive evidence came from Italy – an expert saw parallels to the famous Ravenna papyri from the chancery of the Archdiocese of Ravenna. These include many antique manuscripts from Galen, which were later used as palimpsests and written over. The Basel papyrus could be a similar case of medieval recycling, as it consists of multiple sheets glued together and was probably used as a book binding. The other Basel Amerbach papyrus in Latin script is also thought to have come from the Archdiocese of Ravenna. At the end of the 15th century, it was then stolen from the archive and traded by art collectors as a curiosity.

Utilizing digital opportunities in research

Huebner made the discovery in the course of an editing project funded by the Swiss National Science Foundation. For three years, she has been working with an interdisciplinary team in collaboration with the University of Basel’s Digital Humanities Lab to examine the papyrus collection, which in the meantime has been digitalized, transcribed, annotated and translated. The project team has already presented the history of the papyrus collection through an exhibition in the University Library last year. They plan to publish all their findings at the start of 2019.

With the end of the editing project, the research on the Basel papyri will enter into a new phase. Huebner hopes to provide additional impetus to papyrus research, particularly through sharing the digitalized collection with international databases. As papyri frequently only survive in fragments or pieces, exchanges with other papyrus collections are essential. “The papyri are all part of a larger context. People mentioned in a Basel papyrus text may appear again in other papyri, housed for example in Strasbourg, London, Berlin or other locations. It is digital opportunities that enable us to put these mosaic pieces together again to form a larger picture.”

Sponsored Content"	https://www.historyoftheancientworld.com/2018/07/basel-papyrus-is-an-ancient-medical-text-researchers-find/	"Since the 16th century, Basel has been home to a mysterious papyrus. With mirror writing on both sides, it has puzzled generations of researchers. A research team from the University of Basel has now discovered that it is an unknown medical document from late antiquity. The text was likely written by the famous Roman physician Galen.
The Basel papyrus collection comprises 65 papers in five languages, which were purchased by the university in 1900 for the purpose of teaching classical studies – with the exception of two papyri. These arrived in Basel back in the 16th century, and likely formed part of Basilius Amerbach’s art collection.
One of these Amerbach papyri was regarded until now as unique in the world of papyrology. With mirror writing on both sides, it has puzzled generations of researchers. It was only through ultraviolet and infrared images produced by the Basel Digital Humanities Lab that it was possible to determine that this 2,000-year-old document was not a single papyrus at all, but rather several layers of papyrus glued together. A specialist papyrus restorer was brought to Basel to separate the sheets, enabling the Greek document to be decoded for the first time.
“This is a sensational discovery,” says Sabine Huebner, Professor of Ancient History at the University of Basel. “The majority of papyri are documents such as letters, contracts and receipts. This is a literary text, however, and they are vastly more valuable.”
What’s more, it contains a previously unknown text from antiquity. “We can now say that it’s a medical text from late antiquity that describes the phenomenon of ‘hysterical apnea’,” says Huebner. “We therefore assume that it is either a text from the Roman physician Galen, or an unknown commentary on his work.” After Hippocrates, Galen is regarded as the most important physician of antiquity.
The decisive evidence came from Italy – an expert saw parallels to the famous Ravenna papyri from the chancery of the Archdiocese of Ravenna. These include many antique manuscripts from Galen, which were later used as palimpsests and written over. The Basel papyrus could be a similar case of medieval recycling, as it consists of multiple sheets glued together and was probably used as a book binding. The other Basel Amerbach papyrus in Latin script is also thought to have come from the Archdiocese of Ravenna. At the end of the 15th century, it was then stolen from the archive and traded by art collectors as a curiosity.
Utilizing digital opportunities in research
Huebner made the discovery in the course of an editing project funded by the Swiss National Science Foundation. For three years, she has been working with an interdisciplinary team in collaboration with the University of Basel’s Digital Humanities Lab to examine the papyrus collection, which in the meantime has been digitalized, transcribed, annotated and translated. The project team has already presented the history of the papyrus collection through an exhibition in the University Library last year. They plan to publish all their findings at the start of 2019.
With the end of the editing project, the research on the Basel papyri will enter into a new phase. Huebner hopes to provide additional impetus to papyrus research, particularly through sharing the digitalized collection with international databases. As papyri frequently only survive in fragments or pieces, exchanges with other papyrus collections are essential. “The papyri are all part of a larger context. People mentioned in a Basel papyrus text may appear again in other papyri, housed for example in Strasbourg, London, Berlin or other locations. It is digital opportunities that enable us to put these mosaic pieces together again to form a larger picture.”
Sponsored Content"	3780
history	['Theme']	2018-06-13 16:57:49+00:00	"By Nicky Nielsen

It’s a great place to sit in the shade and enjoy a gelato. The base of the Flaminian Obelisk in the Piazza del Popolo on the northern end of Rome’s ancient quarter offers views of the twin churches of Santa Maria dei Miracoli and Santa Maria di Montesanto. But while enjoying the outlook, take a few minutes to marvel at how this 23-metre chunk of granite ended up where it has.

The Flaminian Obelisk was carved at the height of Egypt’s New Kingdom, during the reign of Seti I (1290 to 1279 BCE), the father of Ramesses the Great. “Carved” is a rather clinical expression for an astounding feat of engineering. Quarrying and moving a 263-ton chunk of granite – with the additional issue of not having access to any metal harder than bronze – is no mean feat.

The process used by the Egyptians was surprisingly straightforward. Initially, they levelled off the ground above a vein of granite. Then the rough shape of the obelisk was marked using hard stone pounders. Channels were carved in the rock around the shape of the obelisk before it was separated from the bedrock entirely by carving under its bulk.

Afterwards, the obelisk was shipped on barges nearly 900km north to the Temple of Heliopolis near modern Cairo and dedicated to the sun god Re-Horakhty – and of course to the memory of both Seti and Ramesses.

Egypt in vogue

Though much of our current obsessive cultural interest in ancient Egypt can be traced to key events such as the discovery of Tutankhamun’s tomb in 1922, other cultures at other times in history have had an equal interest in the land of the Pharaohs – and a similar penchant for creatively misrepresenting it.

At the height of the Roman Empire, “Egyptianising” architectural elements became very popular. Sites such as the Villa Adriana in Tivoli, built in the second century CE as a retreat for Emperor Hadrian, is positively lousy with Egyptianised statues and architectural elements – including an Egyptian-style shrine dedicated to the emperor’s lover, Antinous.

While these imitations of Egyptian styles and fashions (creatively altered for a Roman audience) were extremely popular, several Roman rulers went a step further. Rather than simply imitating Egyptian architecture, they brought some home with them from Egypt.

After the defeat of Cleopatra and Mark Anthony in 30 BCE, the first Roman emperor, Augustus Caesar, set his sights on the Flaminian Obelisk which had remained for more than 1,200 years at Heliopolis. To commemorate his comprehensive victory, Augustus opted to bring the obelisk back to Rome on a specially designed vessel, which was later destroyed in a fire in Puteoli.

Upon its arrival in Rome, Augustus added a Latin inscription underneath the far older hieroglyphs of the obelisk, extolling his own triumphs as the new ruler of Egypt. To show off his achievement, he ordered the obelisk raised at Circus Maximus.

As Christianity rose to prominence and became the official state religion of the Roman Empire, the arena fell into decay and flooding eventually toppled the obelisk. It was gradually buried in alluvial soil, lying undiscovered for nearly 1,000 years until it was unearthed at the height of the Italian Renaissance in 1587.

Renaissance renewal

A product of the Italian Renaissance, Pope Sixtus V (1521-1590) embarked on a wide-ranging programme of urban renewal in Rome shortly after his election to the Papal Throne. Ironically, while he is credited with reerecting no less than four ancient obelisks in Rome, he had very little appreciation for the city’s own antiquity, ordering several ancient monuments demolished and the stone reused as building material.

When the Flaminian Obelisk was rediscovered in 1587, Sixtus charged the noted architect Domenico Fontana (1543-1607) with the task of raising the monolith in Piazza del Popolo (at that time a place of public executions), a task which he accomplished in 1589. Fontana was experienced in the art of raising obelisks – three years earlier, he had been responsible for placing the Vatican obelisk (which is heavier than the Flaminian obelisk by nearly 100 tons) in St Peter’s Square. In an attempt to detract from the quite obvious pagan nature of the monuments, both were crowned with large crosses.

With this, the journey of the Flaminian Obelisk from an ancient Egyptian tribute to the sun god to a Renaissance curio was completed. But the monument’s impact on history continued – in 1921, a year before seizing power after the March on Rome, Benito Mussolini (1883-1945) led a march past the obelisk during the Third Fascist Congress. Later on, the Flaminian Obelisk and the many other Egyptian and Roman obelisks found throughout the city prompted the dictator to create his own: a massive 300-ton marble behemoth which still stands in Foro Italico (then Foro Mussolini) bearing the Latin inscription MVSSOLINI DVX(Mussolini, the Leader).

The Flaminian Obelisk is a multicultural monument in many ways. It remains today in its square, a physical testament to the grandiose ideas of three rulers – each in their own way both secular and divine: Pharaoh Seti I, Emperor Augustus Caesar and Pope Sixtus V.

Nicky Nielsen is Lecturer in Egyptology, University of Manchester

This article was first published in The Conversation

Sponsored Content"	https://www.historyoftheancientworld.com/2018/06/romes-flaminian-obelisk-an-epic-journey-from-divine-egyptian-symbol-to-tourist-attraction/	"By Nicky Nielsen
It’s a great place to sit in the shade and enjoy a gelato. The base of the Flaminian Obelisk in the Piazza del Popolo on the northern end of Rome’s ancient quarter offers views of the twin churches of Santa Maria dei Miracoli and Santa Maria di Montesanto. But while enjoying the outlook, take a few minutes to marvel at how this 23-metre chunk of granite ended up where it has.
The Flaminian Obelisk was carved at the height of Egypt’s New Kingdom, during the reign of Seti I (1290 to 1279 BCE), the father of Ramesses the Great. “Carved” is a rather clinical expression for an astounding feat of engineering. Quarrying and moving a 263-ton chunk of granite – with the additional issue of not having access to any metal harder than bronze – is no mean feat.
The process used by the Egyptians was surprisingly straightforward. Initially, they levelled off the ground above a vein of granite. Then the rough shape of the obelisk was marked using hard stone pounders. Channels were carved in the rock around the shape of the obelisk before it was separated from the bedrock entirely by carving under its bulk.
Afterwards, the obelisk was shipped on barges nearly 900km north to the Temple of Heliopolis near modern Cairo and dedicated to the sun god Re-Horakhty – and of course to the memory of both Seti and Ramesses.
Egypt in vogue
Though much of our current obsessive cultural interest in ancient Egypt can be traced to key events such as the discovery of Tutankhamun’s tomb in 1922, other cultures at other times in history have had an equal interest in the land of the Pharaohs – and a similar penchant for creatively misrepresenting it.
At the height of the Roman Empire, “Egyptianising” architectural elements became very popular. Sites such as the Villa Adriana in Tivoli, built in the second century CE as a retreat for Emperor Hadrian, is positively lousy with Egyptianised statues and architectural elements – including an Egyptian-style shrine dedicated to the emperor’s lover, Antinous.
While these imitations of Egyptian styles and fashions (creatively altered for a Roman audience) were extremely popular, several Roman rulers went a step further. Rather than simply imitating Egyptian architecture, they brought some home with them from Egypt.
After the defeat of Cleopatra and Mark Anthony in 30 BCE, the first Roman emperor, Augustus Caesar, set his sights on the Flaminian Obelisk which had remained for more than 1,200 years at Heliopolis. To commemorate his comprehensive victory, Augustus opted to bring the obelisk back to Rome on a specially designed vessel, which was later destroyed in a fire in Puteoli.
Upon its arrival in Rome, Augustus added a Latin inscription underneath the far older hieroglyphs of the obelisk, extolling his own triumphs as the new ruler of Egypt. To show off his achievement, he ordered the obelisk raised at Circus Maximus.
As Christianity rose to prominence and became the official state religion of the Roman Empire, the arena fell into decay and flooding eventually toppled the obelisk. It was gradually buried in alluvial soil, lying undiscovered for nearly 1,000 years until it was unearthed at the height of the Italian Renaissance in 1587.
Renaissance renewal
A product of the Italian Renaissance, Pope Sixtus V (1521-1590) embarked on a wide-ranging programme of urban renewal in Rome shortly after his election to the Papal Throne. Ironically, while he is credited with reerecting no less than four ancient obelisks in Rome, he had very little appreciation for the city’s own antiquity, ordering several ancient monuments demolished and the stone reused as building material.
When the Flaminian Obelisk was rediscovered in 1587, Sixtus charged the noted architect Domenico Fontana (1543-1607) with the task of raising the monolith in Piazza del Popolo (at that time a place of public executions), a task which he accomplished in 1589. Fontana was experienced in the art of raising obelisks – three years earlier, he had been responsible for placing the Vatican obelisk (which is heavier than the Flaminian obelisk by nearly 100 tons) in St Peter’s Square. In an attempt to detract from the quite obvious pagan nature of the monuments, both were crowned with large crosses.
With this, the journey of the Flaminian Obelisk from an ancient Egyptian tribute to the sun god to a Renaissance curio was completed. But the monument’s impact on history continued – in 1921, a year before seizing power after the March on Rome, Benito Mussolini (1883-1945) led a march past the obelisk during the Third Fascist Congress. Later on, the Flaminian Obelisk and the many other Egyptian and Roman obelisks found throughout the city prompted the dictator to create his own: a massive 300-ton marble behemoth which still stands in Foro Italico (then Foro Mussolini) bearing the Latin inscription MVSSOLINI DVX(Mussolini, the Leader).
The Flaminian Obelisk is a multicultural monument in many ways. It remains today in its square, a physical testament to the grandiose ideas of three rulers – each in their own way both secular and divine: Pharaoh Seti I, Emperor Augustus Caesar and Pope Sixtus V.
Nicky Nielsen is Lecturer in Egyptology, University of Manchester
This article was first published in The Conversation
Sponsored Content"	5293
history	['Theme']	2018-06-12 02:48:12+00:00	"The landmark 50th issue of the journal Internet Archaeology is featuring pioneering research that is investigating new ways of analysing millions of Roman artefacts associated with the consumption of food and drink.

Current knowledge of everyday consumption practices for the majority living in the Roman Empire remains uneven, however, and little is known about how, where and with whom most people ate their meals, or what aspects of this social practice might have conveyed a universal sense of shared behaviour.

The ‘Big Data on the Roman Table’ (BDRT) research network, which is led by Professor Penelope Allison from the University of Leicester’s School of Archaeology and Ancient History and Professor Martin Pitts from the University of Exeter, has explored theoretical and technological approaches to analysing the large amount of available artefactual data from the Roman world, so that social behaviour associated with food-consumption practices can indeed be investigated.

The network, funded by the Arts & Humanities Research Council (AHRC), has focused on the first to second centuries CE – the period in which Roman ways of life and material culture were established across the Roman Empire.

Millions of artefacts associated with eating and drinking have been recorded by archaeologists since the eighteenth century and are the main ‘big data’ component from the Roman world.

With fresh analytical approaches to this data archaeologists will be able to shed new light on how food and drink was consumed during the Roman period, where these activities took place, how these social practices varied across different contexts and reflected different cultural preferences, and how they changed over time.

Professor Allison said: “The processes explored by this network can change the ways archaeologists and the public think about the value of material culture, and particularly ceramics, for understanding social behaviour in the past.”

The network drew on several substantial artefact datasets, many encompassing both legacy data (i.e. non-digital data from old excavations) and data from recent excavations, such as those from the abandoned first-century CE Italian urban site of Pompeii, well-preserved or extensive military sites in the NW provinces (e.g. Nijmegen, Vindolanda, and sites on the Antonine Wall in Scotland), and large assemblages from other urban sites which are the product of more typical archaeological site formation processes (e.g. London and Colchester in Britain, Libarna in northern Italy, and cities in the East).

Between 2015 and 2016 the international BDRT research network developed interdisciplinary, quantitative and qualitative approaches for analysing the material-cultural evidence for social relations around food and drink.

Two workshops were held, at the University of Leicester (September 2015) and the University of Exeter (July 2016), each with about 50 participants from academic, government and consultancy organisations, from UK, Netherland, Belgium, France, Germany, Spain, Italy, Croatia, Egypt, Canada, and USA.

Professor Allison added: “These two workshops stimulated vibrant discussion between archaeologists, museum curators, mathematicians, and computer scientists on how best to handle the immense amounts of archaeological evidence from eating and drinking in the Roman world so that they can be comprehensively analysed to better understand how different people in different parts of the vast Roman world socialised around eating and drinking.”

Internet Archaeology has been publishing on the web since 1996 and is the premier e-journal for archaeology. Click here to access Issue 50: Big Data on the Roman Table.

Sponsored Content"	https://www.historyoftheancientworld.com/2018/06/research-network-sheds-new-light-on-drinking-and-eating-habits-in-the-roman-world/	"The landmark 50th issue of the journal Internet Archaeology is featuring pioneering research that is investigating new ways of analysing millions of Roman artefacts associated with the consumption of food and drink.
Current knowledge of everyday consumption practices for the majority living in the Roman Empire remains uneven, however, and little is known about how, where and with whom most people ate their meals, or what aspects of this social practice might have conveyed a universal sense of shared behaviour.
The ‘Big Data on the Roman Table’ (BDRT) research network, which is led by Professor Penelope Allison from the University of Leicester’s School of Archaeology and Ancient History and Professor Martin Pitts from the University of Exeter, has explored theoretical and technological approaches to analysing the large amount of available artefactual data from the Roman world, so that social behaviour associated with food-consumption practices can indeed be investigated.
The network, funded by the Arts & Humanities Research Council (AHRC), has focused on the first to second centuries CE – the period in which Roman ways of life and material culture were established across the Roman Empire.
Millions of artefacts associated with eating and drinking have been recorded by archaeologists since the eighteenth century and are the main ‘big data’ component from the Roman world.
With fresh analytical approaches to this data archaeologists will be able to shed new light on how food and drink was consumed during the Roman period, where these activities took place, how these social practices varied across different contexts and reflected different cultural preferences, and how they changed over time.
Professor Allison said: “The processes explored by this network can change the ways archaeologists and the public think about the value of material culture, and particularly ceramics, for understanding social behaviour in the past.”
The network drew on several substantial artefact datasets, many encompassing both legacy data (i.e. non-digital data from old excavations) and data from recent excavations, such as those from the abandoned first-century CE Italian urban site of Pompeii, well-preserved or extensive military sites in the NW provinces (e.g. Nijmegen, Vindolanda, and sites on the Antonine Wall in Scotland), and large assemblages from other urban sites which are the product of more typical archaeological site formation processes (e.g. London and Colchester in Britain, Libarna in northern Italy, and cities in the East).
Between 2015 and 2016 the international BDRT research network developed interdisciplinary, quantitative and qualitative approaches for analysing the material-cultural evidence for social relations around food and drink.
Two workshops were held, at the University of Leicester (September 2015) and the University of Exeter (July 2016), each with about 50 participants from academic, government and consultancy organisations, from UK, Netherland, Belgium, France, Germany, Spain, Italy, Croatia, Egypt, Canada, and USA.
Professor Allison added: “These two workshops stimulated vibrant discussion between archaeologists, museum curators, mathematicians, and computer scientists on how best to handle the immense amounts of archaeological evidence from eating and drinking in the Roman world so that they can be comprehensively analysed to better understand how different people in different parts of the vast Roman world socialised around eating and drinking.”
Internet Archaeology has been publishing on the web since 1996 and is the premier e-journal for archaeology. Click here to access Issue 50: Big Data on the Roman Table.
Sponsored Content"	3698
history	['Brittany Britanniae']	2021-02-26 12:13:54+00:00	"In light of this year is a bit different in the world of sports, let us take a look at Ancient Roman Gladiator ads. These ads in a time with no television or newspaper was possible through billboard status or graffiti. This post will be highlighting some graffiti inscriptions that showcase fans adoring different gladiators. In order to provide some background to some basic gladiator roles, I have included a basic description of the following.

Ancient Roman Gladiator:

The word gladiator meaning “swordsman”, from the word gladius “sword.” This is interesting because not all gladiators use a sword. Also, gladiators were often slaves with a few minor exceptions. The following are types of gladiators:

Retiarius fought with a trident and net .

fought with a trident and net Secutor was armed with a sword and they carried a shield and wore a smooth helmet.

was armed with a sword and they carried a shield and wore a smooth helmet. Murmillo or fish-man, wore a heavy helmet and fought with a sword, and carried a shield.

or fish-man, wore a heavy helmet and fought with a sword, and carried a shield. Hoplomachus fought with a lance and a dagger and carried a small circular shield.

fought with a lance and a dagger and carried a small circular shield. Thraex was dressed like a warrior from Thrace in northern Greece and was armed with a curved sword and carried a small shield.

was dressed like a warrior from Thrace in northern Greece and was armed with a curved sword and carried a small shield. Samnite was heavily armed with a short sword and heavy shield.

was heavily armed with a short sword and heavy shield. Provocator were the only type of gladiators to wear a full breastplate and they also wore a helmet with a visor and were armed with a sword and shield.

were the only type of gladiators to wear a full breastplate and they also wore a helmet with a visor and were armed with a sword and shield. Eques entered the arena mounted on a horse. They started their fights on horseback with lances but finished on foot with a sword.

entered the arena mounted on a horse. They started their fights on horseback with lances but finished on foot with a sword. Essedarius rode into the arena on chariots pulled by horses and was armed with both a lance and a sword.

rode into the arena on chariots pulled by horses and was armed with both a lance and a sword. Dimachaerius fought with two daggers and little armor to weigh him down.

fought with two daggers and little armor to weigh him down. Laquerarius was just like a retiarius (see above), but instead of a net, they used a lasso to trap their opponent.

was just like a retiarius (see above), but instead of a net, they used a lasso to trap their opponent. Sagittarius was armed with a bow and wore a lightweight pointed helmet.

was armed with a bow and wore a lightweight pointed helmet. Andabatus carried lances and wore helmets without eye holes and charged blindly at their opponents.

Ancient Roman Gladiator Ads:

Cumis gladiatorum paria XX et eorum suppositicii pugnabunt Kalendis Octobribus III pridie Nonas Octobres. Cruciarii, venatio et vela erunt. Curiculus scriptor Lucceio salutem.

In Cumae, twenty pairs of gladiators and their replacements will fight in October kalends, the day before and the day before the October nones. There will be crucified people, a hunt, in tents. Curiculus, who writes, greets Lucceius.

Cnaei Allei Nigidi Mai quinquennalis sine impensa publica gladiatorum paria XX et eorum suppositicii pugnabunt Pompeis. Telephe summa rudis instrumentum muneris ubique vale . Diadumeno et Pyladioni feliciter.

20 pairs of gladiators of Gnaeus Alleius Nigidius Maius, quinquennial, and their substitutes will fight without any public expense at Pompeii. Greetings to Gavillius Tigellus and Clodius. Greetings to Telephus, head gladiator instructor. Good luck, Diadumenus and Pyladio (C.I.L. IV 7991 House of Trebius Valens iii.2.1)

Suspirium puellarum Celadus thraex.

Celadus the Thracier makes the girls moan! (C.I.L. IV, 4397; in the barracks of the gladiators)

So, to learn more about Gladiator Graffiti inscriptions click here.

Furthermore, next month we will be taking a look at another type of graffiti and how it is similar to social media and texting."	https://blogs.transparent.com/latin/ancient-roman-gladiator-ads/	"In light of this year is a bit different in the world of sports, let us take a look at Ancient Roman Gladiator ads. These ads in a time with no television or newspaper was possible through billboard status or graffiti. This post will be highlighting some graffiti inscriptions that showcase fans adoring different gladiators. In order to provide some background to some basic gladiator roles, I have included a basic description of the following.
Ancient Roman Gladiator:
The word gladiator meaning “swordsman”, from the word gladius “sword.” This is interesting because not all gladiators use a sword. Also, gladiators were often slaves with a few minor exceptions. The following are types of gladiators:
Retiarius fought with a trident and net .
fought with a trident and net Secutor was armed with a sword and they carried a shield and wore a smooth helmet.
was armed with a sword and they carried a shield and wore a smooth helmet. Murmillo or fish-man, wore a heavy helmet and fought with a sword, and carried a shield.
or fish-man, wore a heavy helmet and fought with a sword, and carried a shield. Hoplomachus fought with a lance and a dagger and carried a small circular shield.
fought with a lance and a dagger and carried a small circular shield. Thraex was dressed like a warrior from Thrace in northern Greece and was armed with a curved sword and carried a small shield.
was dressed like a warrior from Thrace in northern Greece and was armed with a curved sword and carried a small shield. Samnite was heavily armed with a short sword and heavy shield.
was heavily armed with a short sword and heavy shield. Provocator were the only type of gladiators to wear a full breastplate and they also wore a helmet with a visor and were armed with a sword and shield.
were the only type of gladiators to wear a full breastplate and they also wore a helmet with a visor and were armed with a sword and shield. Eques entered the arena mounted on a horse. They started their fights on horseback with lances but finished on foot with a sword.
entered the arena mounted on a horse. They started their fights on horseback with lances but finished on foot with a sword. Essedarius rode into the arena on chariots pulled by horses and was armed with both a lance and a sword.
rode into the arena on chariots pulled by horses and was armed with both a lance and a sword. Dimachaerius fought with two daggers and little armor to weigh him down.
fought with two daggers and little armor to weigh him down. Laquerarius was just like a retiarius (see above), but instead of a net, they used a lasso to trap their opponent.
was just like a retiarius (see above), but instead of a net, they used a lasso to trap their opponent. Sagittarius was armed with a bow and wore a lightweight pointed helmet.
was armed with a bow and wore a lightweight pointed helmet. Andabatus carried lances and wore helmets without eye holes and charged blindly at their opponents.
Ancient Roman Gladiator Ads:
Cumis gladiatorum paria XX et eorum suppositicii pugnabunt Kalendis Octobribus III pridie Nonas Octobres. Cruciarii, venatio et vela erunt. Curiculus scriptor Lucceio salutem.
In Cumae, twenty pairs of gladiators and their replacements will fight in October kalends, the day before and the day before the October nones. There will be crucified people, a hunt, in tents. Curiculus, who writes, greets Lucceius.
Cnaei Allei Nigidi Mai quinquennalis sine impensa publica gladiatorum paria XX et eorum suppositicii pugnabunt Pompeis. Telephe summa rudis instrumentum muneris ubique vale . Diadumeno et Pyladioni feliciter.
20 pairs of gladiators of Gnaeus Alleius Nigidius Maius, quinquennial, and their substitutes will fight without any public expense at Pompeii. Greetings to Gavillius Tigellus and Clodius. Greetings to Telephus, head gladiator instructor. Good luck, Diadumenus and Pyladio (C.I.L. IV 7991 House of Trebius Valens iii.2.1)
Suspirium puellarum Celadus thraex.
Celadus the Thracier makes the girls moan! (C.I.L. IV, 4397; in the barracks of the gladiators)
So, to learn more about Gladiator Graffiti inscriptions click here.
Furthermore, next month we will be taking a look at another type of graffiti and how it is similar to social media and texting."	4247
history	['Brittany Britanniae']	2021-01-27 15:38:58+00:00	"Salvete Omnes,

In light of this year being an inauguration year, let us take a look at Roman Political Graffiti or ads. This post will be highlighting some graffiti inscriptions that showcase men vying for particular political positions. In order to provide some background to some basic political positions – I have included a basic description of the following positions.

Political Positions:

Quaestor: A quaestor handled finances in Rome or the provinces and held a seat in the Senate.

Praetor: A Roman magistrate, responsible for the administration of justice, they served as the supreme civil judges for legal cases. They also acted as deputies to the consuls, in particular regarding the administration of the provinces.

Aedile: A magistrate who looked after the city of Rome, its corn supply, municipal regulations, and games. The office of aedile came between quaestor and praetor in the cursus honorum (the ‘sequence of offices’ in the career of a Roman politician).

Duoviri:, a magistracy of two men. Duoviri perduellionis were two judges, selected by the chief magistrate, who tried cases of crime against the state.

The following inscriptions are from Pompeii:

CIL IV 107

C(aium) I(ulium) Priscum.

‘Gaius Iulius Priscus.’ CIL IV 108

C(aium) I(ulium) P(riscum) IIvir(um).

‘Gaius Iulius Pricsus, (for) duovir.’ CIL IV 429 = ILS 6412e

C(aium) Iulium Polybium / aed(ilem) o(ro) v(os) f(aciatis) panem bonum fert.

‘We ask for Gaius Iulius Polybius for aedile, he has good bread.’ CIL IV 134 = ILS 6412ab

C(aium) Iulium Polybium / IIvir(um) muliones rog(ant).

‘The muleteers ask you to elect Gaius Iulius Polybius, duovir.’ CIL IV 316

C(aium) I(ulium) Polybium d(uumvirum) i(ure) d(icundo) d(ignum) r(ei) p(ublicae).

‘Gaius Iulius Polybius for duovir with judicial power, worthy of public office.’ CIL IV 909

C(aium) I(ulium) P(olybium) d(uumvirum) i(ure) d(icundo).

‘Gaius Iulius Polybius for duovir with judicial power.’ CIL IV 230

M(arcum) Cerrinium Vatiam aed(ilem) dignum rei / Messenio rog(at) scr(ipsit) Infantio cum Floro et Fructo et / Sabino hic ubique.

‘Marcus Cerrinius Vatia for aedile: he is worthy of this commonwealth. Messenio supports this. Written by Infantio with Florus and Fructus and Sabinus, here and everywhere.’ M(arcum) Cerrinium / Vatiam aed(ilem) o(ro) v(os) f(aciatis) seribibi / universi rogant / scr(ipsit) Florus cum Fructo.

All the late drinkers ask you to elect Marcus Cerrinius Vatia aedile. Florus and Fructus wrote this. CIL 04, 03775

L(ucium) Statium Receptum

IIvir(um) i(ure) d(icundo) o(ro) v(os) f(aciatis) vicini dig(num)

scr(ibsit) Aemilius Celer vic(ini)

invidiose

qui deles

ae[g]rotes

Neighbors beg you to elect Lucius Statius Receptus duumvir with judicial

power, a worthy man. Aemilius Celer wrote this, a neighbour. You jealous

one who destroys this, may you fall ill.

Next month we will be taking a look at another type of ad – sports ads."	https://blogs.transparent.com/latin/roman-political-graffiti/	"Salvete Omnes,
In light of this year being an inauguration year, let us take a look at Roman Political Graffiti or ads. This post will be highlighting some graffiti inscriptions that showcase men vying for particular political positions. In order to provide some background to some basic political positions – I have included a basic description of the following positions.
Political Positions:
Quaestor: A quaestor handled finances in Rome or the provinces and held a seat in the Senate.
Praetor: A Roman magistrate, responsible for the administration of justice, they served as the supreme civil judges for legal cases. They also acted as deputies to the consuls, in particular regarding the administration of the provinces.
Aedile: A magistrate who looked after the city of Rome, its corn supply, municipal regulations, and games. The office of aedile came between quaestor and praetor in the cursus honorum (the ‘sequence of offices’ in the career of a Roman politician).
Duoviri:, a magistracy of two men. Duoviri perduellionis were two judges, selected by the chief magistrate, who tried cases of crime against the state.
The following inscriptions are from Pompeii:
CIL IV 107
C(aium) I(ulium) Priscum.
‘Gaius Iulius Priscus.’ CIL IV 108
C(aium) I(ulium) P(riscum) IIvir(um).
‘Gaius Iulius Pricsus, (for) duovir.’ CIL IV 429 = ILS 6412e
C(aium) Iulium Polybium / aed(ilem) o(ro) v(os) f(aciatis) panem bonum fert.
‘We ask for Gaius Iulius Polybius for aedile, he has good bread.’ CIL IV 134 = ILS 6412ab
C(aium) Iulium Polybium / IIvir(um) muliones rog(ant).
‘The muleteers ask you to elect Gaius Iulius Polybius, duovir.’ CIL IV 316
C(aium) I(ulium) Polybium d(uumvirum) i(ure) d(icundo) d(ignum) r(ei) p(ublicae).
‘Gaius Iulius Polybius for duovir with judicial power, worthy of public office.’ CIL IV 909
C(aium) I(ulium) P(olybium) d(uumvirum) i(ure) d(icundo).
‘Gaius Iulius Polybius for duovir with judicial power.’ CIL IV 230
M(arcum) Cerrinium Vatiam aed(ilem) dignum rei / Messenio rog(at) scr(ipsit) Infantio cum Floro et Fructo et / Sabino hic ubique.
‘Marcus Cerrinius Vatia for aedile: he is worthy of this commonwealth. Messenio supports this. Written by Infantio with Florus and Fructus and Sabinus, here and everywhere.’ M(arcum) Cerrinium / Vatiam aed(ilem) o(ro) v(os) f(aciatis) seribibi / universi rogant / scr(ipsit) Florus cum Fructo.
All the late drinkers ask you to elect Marcus Cerrinius Vatia aedile. Florus and Fructus wrote this. CIL 04, 03775
L(ucium) Statium Receptum
IIvir(um) i(ure) d(icundo) o(ro) v(os) f(aciatis) vicini dig(num)
scr(ibsit) Aemilius Celer vic(ini)
invidiose
qui deles
ae[g]rotes
Neighbors beg you to elect Lucius Statius Receptus duumvir with judicial
power, a worthy man. Aemilius Celer wrote this, a neighbour. You jealous
one who destroys this, may you fall ill.
Next month we will be taking a look at another type of ad – sports ads."	2895
economy	['Siddiq']	2020-05-22 00:00:00	"Abstract Recent work with the Economic Complexity Index (ECI) has shown that a country’s productive structure constrains its level of economic growth and income inequality. Building on previous research that identified an increasing gap between Latin America and the Caribbean (LAC) with that of China and other High Performing East Asian Economies (HPEA), this paper compares how the diverging productive structures of the Venezuelan and Indonesian economies have impacted the generation and distribution of income in each country. I use time-series regression analysis to show the impact of economic complexity – controlling for additional economic and political variables. Moreover, this paper considers how industrial policies and the resource curse have impacted the underlying productive structure of the economy in both nations. The paper argues that Indonesia’s success in diversifying its economy away from oil exportation and building a more complex economy has led to lower income inequality. Conversely, Venezuela’s increasing reliance on oil has led to higher income inequality despite substantial redistributive policies. Finally, the paper discusses how LAC reliance on the exportation of commodities has serious implications for income inequality because the productive structures of an economy also act as structural constraints on income inequality.

Economic Complexity and Income Inequality

Increasing income inequality has been a point of concern in many nations over the last 30 years. Political scientists and economists to better understand the causes of rising income inequality; however, research remains in dispute. Rising income inequality has been shown to lead to slower economic growth by depressing aggregate demand and stagnating wages, while also leaving households relying on debt to maintain spending. New research done at the Massachusetts Institute of Technology has found the underlying productive structures of a country’s economy as a contributing factor for the level of income inequality observed (Hartmann, Guevara, Jara-Figueroa, Aristarán, & Hidalgo, 2017). According to the World Bank, rising income inequality in developing countries has been linked to the underlying productive structure of a country’s economy. The majority of new studies that measure the productive structures of an economy use the new Economic Complexity Index (ECI).

Analysis of countries’ productive structures done during the twentieth century was simplistic: only measuring economic diversity by assessing the percentage of individuals employed by agriculture, manufacturing and service sectors (Hartmann et al., 2017). This approach was developed by Albert O. Hirschman, an economist. Despite its intuitive nature, it was too broad a measure, in part due to sectors not being adequately defined or uniformly measured. The limitations of Hirschman and others’ approaches necessitated the need for the Economic Complexity Index (ECI), which is designed to measure the sophistication of a country’s productive structure by combining information about the diversity of its products (the number of products) and the ubiquity of its products (the number of countries that export each product)(Hartmann et al., 2017). A higher ECI is a representation of a more complex economy. Conversely, a lower ECI indicates low diversity and high levels of ubiquity in the goods a country produces.

More simply, the ECI provides a useful rank order that places nations with similar exports near each other. Using United Nations (UN) export data, the ECI sorts the diversity and ubiquity of all countries’ exports for a given year and generates a ranking using both measures to correct for each other mathematically. Note that diversity captures how many exports a country is competitive in and ubiquity captures what type of exports. The ECI combines ubiquity and diversity and attempts to infer about countries productive capabilities or complexity by making relative comparisons across export baskets (Mealy, Farmer, & Teytelboym, 2017).

Figure 1: Visual Representation of the Economic Complexity Index (ECI) from “A New Interpretation of the Economic Complexity Index” by Mealy, Farmer, & Teytelboym, 2017

Figure 1 illustrates how the ECI provides a rank ordering of countries in terms of export similarity. For example, developed countries, such as the United States, Japan, and Germany have high ECI and produce complex goods like medical equipment and other capital goods. Conversely, developing countries, such as Bangladesh, Laos, and Cambodia have low ECI and produce simpler labor-intensive goods like textiles. Panel (a) shows how countries with a positive ECI are on the right segment of the graph and countries with negative ECI fall on the left segment. The network visualization in Panel (b) depicts country nodes with color corresponding their ECI value; shades of green represents positive ECI values and shades of pink represent negative values. Connections between nodes represent similar exports.

The ECI is a new measure of underlying productive structures, but prior endogenous growth models have also emphasized the importance of knowledge accumulation as well as product diversification and specialization as critical drivers of economic growth (Natarajan, 2013). In this respect, the ECI is not revolutionary as much as it is evolutionary as it attempts to quantify the productive knowledge in a given country’s economy by observing the goods it produces.

As mentioned, the Atlas of Economic Complexity uses UN export data to compute the ECI; this reliance on export data means it may be a less accurate measurement for the productive capabilities of larger economies, such as the US, which are service-based on not substantial exporters (Natarajan, 2013). Theoretically, the ECI formulas could be modified to use data on the composition of both goods and services, but the lack of a comprehensive data set and uniform categorization of services make this difficult if not impossible. Despite these issues, the ECI remains relevant when considering developing countries where exports are a substantial part of the domestic economy. With regards to Indonesia and Venezuela exports represented over 25% of both countries’ GDP, on average, for the time period under consideration (World Bank, 2016).

Statistical analysis from previous studies using the ECI has found it a significant predictor of economic growth and income inequality (Hartmann et al., 2017; Hartmann, Jara-Figueroa, Guevara, Simoes, & Hidalgo, 2016; C. A. Hidalgo & Hausmann, 2009). The ECI is statistically a more accurate predictor of per capita GDP growth than measures of governance, competitiveness (World Economic Forum's Global Competitiveness Index), and human capital (as measured in terms of educational attainment) (Hausmann et al., 2011). Hartmann’s most recent study observed that the variety of products a country produces acts as a predictor of an economy’s given path of specialization and level of economic growth (see Figure 2). Multiple other studies have also observed a strong correlation between a country’s level of economic complexity (ECI) and the corresponding level of income inequality between 1968 and 2008 (Gala, Rocha, & Magacho, 2018; Hartmann et al., 2017, 2016).

Figure 2: Bivariate relationship between economic complexity and income inequality from “Linking Economic Complexity, Institutions and Income Inequality” by D. Hartmann, Guevara, Jara-Figueroa, Aristarán, & Hidalgo, 2017

Figure 2 illustrates a bivariate relationship, using the Economic Complexity Index (ECI) as the independent variable, between economic complexity and income inequality across different decades from 1963-2008. The graphs illustrate how the ECI is negatively correlated with the Gini coefficient – after controlling for GDP per capita and other macroeconomic indicators. However, the negative relationship between economic complexity and income inequality is stronger (R2=0.58) than the relationship between income inequality and GDP per capita (R2=0.36). Furthermore, the figure shows that the negative relationship between income inequality (Gini) and economic complexity (ECI) remains stable from 1963-2008. Several other studies have also quantified and shown that, across all decades, there exists a negative relationship between economic complexity and income inequality (Gala et al., 2018; Hartmann et al., 2016; Hausmann et al., 2011).

There are multiple reasons why the economic complexity (ECI) of a given country is associated not only with economic growth, but also with a country’s average level of income inequality. The complexity and diversity of products that a country exports are a good proxy for the knowledge available in an economy that is not captured by other measures of human capital, such as schooling, which is measured by the percentage of the population with a secondary education (Hartmann et al., 2017). For example, complex industries, such as advanced medical equipment, require better-educated and more creative workers and institutions that are able to include the creative inputs of workers into the activities of firms (Hartmann et al., 2016). A nation’s degree of economic complexity can be seen as a proxy for factors such as the productive knowledge and the inclusiveness of institutions which profoundly affect economic growth and inequality, but which are typically difficult to measure directly. Without sound institutions maintaining the rule of law and high levels of human capital, it is impossible to have a complex economy. The quality of institutions and education are responsible for, and develop alongside, a complex growing economy (Hartmann et al., 2016). The ECI is sufficiently broad that a stronger statistical correlation can be found between it and income inequality over other aggregate measures like years of schooling (Hartmann et al., 2016).

Research using the ECI has bought new attention to large disparities in the productive structure between Latin American countries (LAC) and High-Performing East Asian countries (HPEA). These disparities are shown when observing how the ECI has diverged across nations in the two regions despite a degree of convergence in per capita GDP (PPP). Further research using the ECI has shown a gap in the productive capabilities and opportunities for inequality reduction of LAC and HPEA. HPEA have been capable of increasing their level of economic complexity and thus overcoming structural constraints on income inequality as expressed in the decline of income inequality (Hartmann et al., 2016). However, LAC are still strongly constrained by their natural resource centric productive structures, and income inequality has remained high despite comparable per capita economic output. To understand this trend, it would be beneficial to compare individual nations in the two regions and explore how their economic and political development have impacted the level of income inequality.

This paper compares Indonesia and Venezuela to illustrate how both nations have diverged and are proxies for broader economic and political trends across Latin America (LAC) and East Asia (HPEA). The primary focus is comparing how income inequality in both countries has been impacted by changes in the underlying productive structures of each is economy. The ECI is the primary independent variable of interest, and it has substantial benefits over using only values such as per capita GDP that does not capture changes in the underlying productive structures of an economy. The ECI is also an important benchmark for the relative success of economic development strategies that rely on Export-Oriented Industrialization (EOI) since the index measures the diversity and ubiquity of goods that an economy produces.

Indonesia (East Asia) and Venezuela (Latin America)

The focus on Indonesia and Venezuela is due to each nation’s representation of broader trends between High-Performing East Asian countries (HPAE) and Latin American countries (LAC). Neither of the aforementioned countries have reached the late stages of economic development, seen in Japan or Chile. Despite a degree of convergence in GDP per capita values in the two regions, there remain large disparities in income inequality which can be tied to differences in the knowledge and productive capabilities of nations in each region (Hartmann et al., 2016). These disparities are echoed when looking at Indonesia and Venezuela.

Both nations have distinct macroeconomic and historical similarities and differences that must be understood in order to fully comprehend each nation’s development (or lack thereof) and levels of income inequality over the last 60 years. Macroeconomically, both are classified as middle-income countries with per capita gross national incomes between $1026 and $12,475 (2011 US Dollars). They have comparable levels of human capital (years of schooling) and similar demographic structures with neither nation exhibiting irregularities in gender or age distribution (World Bank, 2016). Another important similarity is that both nations, in the 1960s and ‘70s, had economies that were heavily dependent on oil extraction and exportation. Venezuela is well known for having the largest proven oil reserves in the world (Fisher & Taub, 2017b). Indonesia was also a considerable exporter of petroleum products, and was the only East Asian nation in the Organization for Petroleum Exporting Countries (OPEC) until recently (Salna & Rusmana, 2017). The presence of natural resources in both countries is relevant when considering the resource curse where oil wealth plagues non-resource-related development and inhibits institutional development (Michael L. Ross, 1999). Unlike other East Asian nations, Indonesia did not lack natural resources and could have feasibly relied on oil revenues and not diversified its economy; however, it chose to pursue export-led industrialization which has been responsible for faster growth and lower levels of income inequality.

After WWII, Indonesia, like many nations in Southeast Asia, found itself in a vulnerable position as an economically and militarily weak state that was plagued by religious and ethnic conflicts. Indonesian leaders transformed those fears of war and instability into a remarkable developmental energy that eventually became a binding agent for growth. The pursuit of wealth in the context of stability, security, and catching up with the rest of the world were vastly more effective in generating developmental energy than a general appeal to increase welfare, which is a common thread among Latin American “populism.”(Woo-Cumings, 1999).

The absence in Latin America of the kind of vulnerability and urgency one finds in East Asia has to do with Latin American nations attaining political independence and beginning industrialization a century earlier. Latin American nations, including Venezuela, were never “late developers,” and thus never had nor developed the sustained nationalism needed to provide the impetus to modernize (Woo-Cumings, 1999). Additionally, the wealth of natural resources available to Latin American nations, combined with their relatively small populations, provided an easy alternative to the difficult task of industrialization. The timing and manner of Latin America’s involvement with the world economy and relative economic position within it was structurally different from that of many High-Performing East Asian Countries (HPEA). These fundamental differences between the two regions meant contrasting growth models with HPEA (Indonesia) adopting Export Oriented Industrialization (EOI), while LAC (Venezuela) relied on Import Substitution (ISI) and over-valued exchange rates (Müller, 2014).

Methodology

Datasets were compiled from the University of Texas Inequality Project (GINI EHII), World Bank, MIT Observatory of Economic Complexity and the Center for Systemic Peace. The most recent observations across the variables are from 2013, and income inequality (Gini) data is unavailable before 1964. Due to lack of reliable income distribution data for Venezuela from 2007 onward, the data set and all regression analysis between the ECI and Gini coefficient occurs from 1965-2006. The data available on Indonesia is more comprehensive, and the data set includes Gini coefficients from 1970-2013.

Time series regression analysis is used as it is the most practical and efficient method of measuring the relationship among income inequality (Gini), economic complexity (ECI), and political stability (Polity IV) while also considering additional factors that influence income inequality.

The Gini-coefficient is the dependent variable and is intended to measure income inequality. It does this by plotting the distribution of a given country’s income on a Lorenz curve and then deducting the area occupied by the curve from the line of equality (Mcgreevey, 2013). More simply, the Gini index is a number that lies between 0 (perfect equality—everyone has the same) and 100 (perfect inequality, with one person having everything). Income inequality is assessed rather than wealth inequality because the distribution of wealth in an economy is the product of past economic structures while the income distribution is representative of current structures. All Gini data is taken from the University of Texas Income Inequality project (GINI EHII).

Real GDP Per Capita is calculated by dividing a country’s total Gross Domestic Product (GDP), a measure of its total output, by its population to roughly estimate the total output of each individual in that country. Real per capita GDP is used in comparing Indonesia and Venezuela because it shows each country’s relative performance (Ilter, 2017). Furthermore, to effectively compare countries, differing price levels must be considered and thus, all real GDP figures are adjusted to purchasing power parity (PPP) dollars. Note that all GDP data used was taken from the World Bank’s World Development Indicators (World Bank, 2016).

As mentioned previously, the Economic Complexity Index (ECI) is the primary independent variable of interest and is used as a measure for measuring the productive structures of a respective country’s economy. Note that a modified measure of economic complexity called the ECI+ was recently developed and new research has shown it is a more accurate predictor of future economic growth; it was not in included because it was published after data analysis for this paper had been completed (Albeaik, Kaltenberg, Alsaleh, & Hidalgo, 2017). The ECI data was accessed from the MIT Observatory of Economic Complexity website (C. Hidalgo & Simoes, 2018).

Lastly, the Polity IV score, sometimes called the democracy score, is used as a measure of a country’s democratic institutions and free nature. The score ranges from -10 (worst) to 10 (best) and the minimum score for a country is contingent on whether a country is classified as a democracy, anocracy or autocracy. A degree of political stability by no means guarantees low levels of income inequality, but the type of regime could have a substantial impact on distribution of income via redistributive polices. Polity IV score data is available at the Center of Systematic Peace (2013).

Results

Indonesia

Figure 3: Economic Complexity and Income Inequality in Indonesia (1970-2013). Source: GINI EHII and the MIT Observatory of Economic Complexity

The Indonesian economy rose from being the 96th most complex economy in the world in 1980 to the 65th most complex in 2010 (AJS & CA, 2011). This improvement in the productive structures of its economy corresponded to a substantial decline in income inequality, with the Gini coefficient falling from 51.19 (1980) to 35.57 (2010). Figure 3 illustrates changes in Indonesia’s Economic Complexity Index (ECI) and income inequality (Gini) coefficient from 1970 to 2013.

The sustained rise in economic complexity in Indonesia coincided with its industrialization push in the mid-1980s. The Suharto administration devalued the rupiah (Indonesian currency) and deregulated the financial sector. These polices helped to aid export competitiveness by reducing the cost of exports and encouraging foreign investment. Suharto’s industrialization push came after the collapse of global oil prices in early 1980 and has reduced dependence on oil exports (Schwarz, 1999). The rise in economic complexity continued until the Asian Financial Crisis in mid-1997 when the Indonesian economy collapsed. It was particularly vulnerable due to a lack of formal state controls, specifically a pilot agency, such as the Ministry of International Trade and Industry (MITI) in Japan, which would have prioritized macroeconomic stability and mitigated the severe economic distortions that eventually eroded investor confidence in the Indonesian economy and led to substantial capital flight (Müller, 2014; Woo-Cumings, 1999)

The regression analysis in Table 1 also shows that the Indonesian economy has followed the global general trend with higher economic complexity (ECI) being a significant predictor of lower income inequality as measured by the Gini. Overall the model has relatively high predictive value with an R2 value of 0.6448, indicating that 64.48% of the change in the dependent variable is due to change in the independent variables.

The model specifies the parameter estimates below:

The intercept of the Gini coefficient is 64.7937 when all independent variables equal zero. This value is only representative and is not relevant.

There is a 7.1337 decline in income inequality (Gini coefficient) for every $1000 rise in GDP per capita (PPP, 2005 U.S Dollars) on average, ceteris paribus.

There is a 5.880801 decline in income inequality (Gini coefficient) for every 1 unit increase in the Economic Complexity Index (ECI) on average, ceteris paribus.

There is a 0.0583531 decline in income inequality (Gini coefficient) for every 1 unit increase in the Polity IV on average, ceteris paribus.

From the above results, we can see the most significant reduction in income inequality comes from an increase in GDP per capita, with the Economic Complexity Index coming in at a close second. This is consistent with previous research that has found a robust correlation between ECI and GDP per capita. An important consideration is that ECI is also shown to predict future economic growth, so improvements in productive structures have yielded both reductions in income inequality and greater levels of economic growth (Hartmann et al., 2017).

Although the above regression analysis showed that Polity IV was not statistically significant at α = 0.1 for Indonesia, it still warrants mentioning. For much of the time period under consideration (1970-2013), Indonesia was considered an authoritarian regime, and only since Suharto’s resignation in 1998 after the Asian Financial Crisis (AFC) have we seen a strengthening of democratic processes that culminated in genuine elections. The Polity IV rose from -7 (autocracy) in 1997 to 6 (democracy) in 2004 after the presidential election. Note that from 1997 to 2013, there was no substantial improvement in economic complexity, but income inequality still fell from 48.98 to 39.47.

Venezuela

Figure 4: Economic Complexity and Income Inequality in Venezuela (1965-2006). Source: GINI EHII and the MIT Observatory of Economic Complexity

The Venezuelan economy fell from the 91st (1980) most complex economy in the world to the 106th most complex in 2006(AJS & CA, 2011). This decline in the productive structures came with a gradual rise in the level of income inequality with the Gini coefficient rising from 39.23 (1980) to 46.64 (2006). This general rise in income inequality occurred despite substantial redistributive policies initiated by Hugo Chavez’s administration from 1999 to 2013 (Roth, 2016). Those same redistributive policies are likely responsible for throwing off the regression analysis in Table 2 where rises in ECI corresponds with a rise in income inequality (Gini coefficient). Figure 4 illustrates changes in the Economic Complexity Index (ECI) and income inequality (Gini) from 1965 to 2006.

From 1965 to 1980, one can see that as economic complexity of the Venezuelan economy rose, income inequality fell, and the inverse is true from 1999 onwards. There is an anomalous period from 1980 to 1999 when there was a rise in ECI, but also a rise in income inequality, which is likely related to the global collapse of energy prices. During the 1980s, the surplus of crude oil caused by a combination of reduced demand and increased production led to substantial decline in the price of oil. Venezuela’s economy was (and continues to be) highly dependent on oil exports; the decline in oil prices destabilized the economy, and despite cycles of crisis and reform, the government was unable to stabilize the economy (Corrales, 1999). The outcome was slow growth with substantial inflation. Governmental corruption in Venezuela combined with hyperinflation left the government with little revenue to fund its expansive welfare programs that were initially responsible for lowering levels of income inequality (Buxton, 2014). The decline in government redistribution outstripped improvements in Venezuela’s productive structures that came from the government’s attempts to stabilize, liberalize and diversify the economy. The rapid decline in economic complexity from 1999 onwards was due to the Chavez administration’s policy of focusing the entire economy on oil production. The Venezuelan economy is currently highly dependent on oil revenues for both the government budget and the foreign exchange reserves needed to import all consumer goods because practically nothing but oil is produced in the country (Fisher & Taub, 2017b).

The regression analysis in Table 2 also shows that the Venezuelan economy deviates from the global general trend with higher economic complexity (ECI) being a significant predictor of lower income inequality as measured by the Gini. Overall, the model has a high predictive value with an R2 value of 0.7173 indicating, that 71.73% of the change in the dependent variable is due to change in the independent variables.

The model specifies the parameter estimates below:

The intercept of the Gini coefficient is 66.80267 when all independent variables equal zero. This value is only representative and is not relevant.

There is 0.778 decline in income inequality (Gini coefficient) for every $1000 rise in GDP per capita (PPP, 2005 U.S Dollars) on average, ceteris paribus.

There is 2.832747 rise in income inequality (Gini coefficient) for every 1 unit increase in the Economic Complexity Index (ECI) on average, ceteris paribus.

There is 1.93866 fall in income inequality (Gini coefficient) for every 1 unit increase in the Polity IV on average, ceteris paribus.

From the results, we can see the most significant reduction in income inequality comes from changes in Polity IV with increases in GDP per capita not having a significant impact on income inequality. An important consideration is that the maximum value of the Polity IV is 10, and for the time period under consideration (1964-2006), Venezuela only rose once in from a score of 6 in 1967 to 9 in 1970 until 1991 – after which it gradually fell to 5 in 2006. During the aforementioned period, the rise in the Polity IV coincided with a fall in income inequality from 44.35 (1967) to 43.08(1970) and hit a low of 39 in 1979. This period was marked by heavy government spending on social welfare programs enabled by the rapid rise in global oil prices, caused by an oil embargo from the Organization of Petroleum Exporting Countries (OPEC), of which Venezuela was a founding member (McCaughan, 2011). The reductions in income inequality in Venezuela have been enabled by governmental redistributive polices that are constrained by the underlying structure of the economy and vulnerable to the cyclical nature of oil prices.

The positive correlation between ECI and the Gini coefficient does not necessarily indicate deficiencies in the approach, but corresponds to an anomaly where resource-rich nations violate global statistical trends. Generally, countries that exhibit greater economic complexity than expected, given their level of per capita GDP, tend to grow faster and have lower levels of income inequality. Venezuela and Saudi Arabia both deviate from this trend by having both higher GDP per capita and higher levels of income inequality than expected for their current level of economic complexity, since both countries have economies primarily based on oil (Hartmann et al., 2017). Note that several other countries in LA, like Brazil, also deviate from global trends because their economies rely heavily on commodity exportation. In this respect, Venezuela’s position of having relatively high per capita GDP, low complexity, and high levels of income inequality is the norm regionally even though it is a global outlier (Hartmann et al., 2016).

Discussion

Indonesia has gradually risen from being a poor country with an undiversified, oil-reliant economy in the 1970s to the largest economy in Southeast Asia. This meteoric rise was enabled by its export-led growth model, and its success can be seen in how the export structure has changed from 1970 to 2010 (see Figure 5), diversifying away from commodity exportation to exporting both labor and capital-intensive goods. This economic modernization was initiated by the Suharto administration, whose policies helped to stabilize Indonesia’s currency and encourage foreign investment. Like other East Asian nations during the Cold War, Indonesia established an extensive economic and security relationship with the United States, which allowed for greater market access in the Cold War (Müller, 2014). Many of these critical policies were crafted by a group of Indonesian economists educated in the U.S. called the “Berkley Mafia.” Their placement in various semi-permanent technocratic positions enabled them to push for liberalization and deregulation both before and after the rise in oil prices in the 1970s. The ability of these technocrats to influence economic policy differed considerably from Venezuela where rapid changes in economic conditions (associated with large fluctuations in the price of oil) led to political crises and the election of new leaders that seldom maintained any consistency in policy.

Unfortunately, economic diversification and growth did not yield improvements in political stability or personal freedoms. Indonesia was classified as autocracy and was led by President Suharto until 1997. While authoritarianism can overcome political constraints and mobilize the population to sacrifice current prosperity for the sake of developmental projects, this was never completely the case in Indonesia (Müller, 2014; Woo-Cumings, 1999). Rather than cultivate a relationship between the state and business through industrial policy, as in South Korea, it was done through ethnic division of labor with development, not for the sake of shared prosperity, but rather for the sake of payoffs. Suharto and his family were the owners of many conglomerates that benefited from export-led growth, and the minority ethnic Chinese that represented the business class or entrepreneurial element of the economy were under constant threat of persecution. Only after Suharto resigned in 1997 did democratization begin and Indonesians see a substantial rise in political stability and personal freedoms (Schwarz, 1999).

Venezuela was once considered a shining example for Latin American countries in the 1950s as a wealthy, stable democracy. Decades later, the country and its economy are in ruins, with unprecedented hyperinflation and recession that has left most of the population on the brink of starvation (Fisher & Taub, 2017b). Despite a gradual improvement in smaller sectors of the Venezuelan economy, it remains more dependent than ever on crude oil exportation (see Figure 5). Like other Latin American nations, Venezuela has suffered from the resource curse (paradox of plenty), where oil wealth plagues non-resource-related development and inhibits institutional development (M. L. Ross, 1999). Despite governmental efforts supporting industrialization in the 1960s and ‘70s with import substitution policies, said policies never truly came to fruition. The failure of industrial policy can be partially linked to “Dutch disease”, where an increase in natural resource revenues undermines export-based manufacturing because substantial commodity exports cause exchange rate appreciation (Fardmanesh, 1991). This “disease” has pulled capital and labor from non-resource related sectors and made Venezuela further reliant on oil revenue.

Politically, Venezuela was classified as a democracy until the early 2000s, but it is currently considered an authoritarian regime. Much of this regression occurred during the Chavez administration. Hugo Chavez, the former president of Venezuela, was a democratic populist who came to power in 1999 after two decades economic stagnation. Under Chavez’s rule, Venezuelans prospered through generous social welfare programs and redistributive policies funded through oil revenues (McCaughan, 2011). However, he became increasingly frustrated with courts blocking his legislation and media outlets criticizing his programs. Soon after, Chavez began to actively undermine the courts by removing judges, and he sought to silence journalists with anti-defamation laws. These discrete legal changes undermined Venezuelan democracy and centralized power with the executive branch of the government. Chavez’s actions have been characterized as in line with “democratic backsliding,” which is when the government attempt to systematically erode institutions like the press and independent judiciaries that enable democracy to function (Fisher & Taub, 2017a).

Figure 5: (A) Indonesia exports in 1970, (B) Indonesia exports in 2013, (C) Venezuela Exports in 1965 and (D) Venezuela exports in 2006. Source: atlas.media.mit.edu

The divergent productive structures of Indonesia and Venezuela’s economies are a product of each country’s history, culture, and policies, but this also illustrates a broader disparity between countries in Latin America (LAC) and newly-industrialized High-Performing East Asian countries (HPEA). HPEA primarily went through the process of industrialization in the latter half of the twentieth century with consistent policies; however, Latin American countries, including Venezuela, have had waves of extreme industrialization through import-substitution followed by equally extreme efforts to liberalize and reform (Hartmann et al., 2016). Neither state intervention nor liberalization have successfully diversified LAC economies, which remain heavily dependent on natural resource extraction (Hartmann et al., 2016).

Figure 6: (A) Chinese exports to the world, (B) Latin America Countries’ (LAC) exports to the world from “The Structural Constraints of Income Inequality in Latin America” by D. Hartmann, Jara-Figueroa, Guevara, Simoes, & Hidalgo.

Figure 6 illustrates the vast differences in the productive capabilities in between LAC and China. China’s exports primarily consist of complex machinery and other finished goods that require sophisticated factories and indicate a high degree of economic complexity. Latin American countries’ (LAC) exports consist largely of natural resources like crude oil, metals, and non-processed agricultural goods.

While HPEA, like China, have diversified, LAC remain reliant on the exportation of commodities. These structural differences have serious implications for income inequality in both regions because the productive structures of an economy also act as structural constraints on income inequality. A country with high levels of economic complexity will usually not exhibit high levels of income inequality because the structure of the economy is more inclusive and is also not reliant on governmental redistributive policies to achieve lower income inequality (Hartmann et al., 2016).

Many Latin American countries have had generous welfare policies that are intended to reduce income inequality, but these policies tend to be constrained since government revenues are usually reliant on no more than a few sectors of the economy (Hardesty, 2017). Venezuela is a commonly cited example because the Chavez and Maduro administrations focused the economy on oil production and attempted to use the revenues to fund programs and provide direct cash subsidies. However, despite these policies, income inequality has been rising since Chavez took office in 1998. Reliance on the concentrated economic power of the national oil industry led to rent-seeking behavior, mismanagement, and a grossly-unequal distribution of resources (McCaughan, 2011). The Venezuelan government maintained a monopoly on oil production by distributing the benefits and, therefore, feeding a relationship of dependency among citizens and private interests who, over time, became accustomed to thinking of oil wealth as something the government guarantees and thus its absence was deeply destructive.

Low levels of economic complexity are accompanied by high levels of income inequality because the structure of the economy is less inclusive and economic power more concentrated. Thus, redistributive policies to achieve lower income inequality have hard limits and must be accompanied by efforts to diversify the economy (Hartmann et al., 2016). Resource-rich European Welfare States like Norway have succeeded in part because they were able to use oil wealth to diversify their economies. Since the 1970 oil boom, the Norwegian government has provided funding and established incentives to aid the development of shipping, engineering, and drilling technologies. This formed the foundation for growth across a range of other industries, including fish farming, bio-refining, logging, and mining. Non-resource-rich European nations, like Finland, have also succeeded by making large investments in human capital and becoming research and development centers for technology (López, 2017).

There exist inextricable links between political and economic development that are shown to have a substantial impact on the level of income inequality both within individual nations and across regions. This case study of Indonesia and Venezuela illustrates how both nations have diverged and are proxies for broader economic and political trends across Latin America and East Asia. Literature on cross-national and regional similarities and differences have emphasized per capita economic performance, with less attention given to nations with comparable levels of per capita GDP but vastly different distributions of income. This omission was in part related to contemporary economic thinking which saw distribution of income as less relevant and primarily impacted by government redistributive policies rather than the underlying structure of the economy. A larger study involving panel data from Latin America Countries (LAC) and High Performing Asia Economies (HPEA) should be done to better understand the impact of diverging economic development on income inequality in these regions.

References

AJS, S., & CA, H. (2011). The Economic Complexity Obervatory: An Analytical Tool for Understanding the Dynamics of Economic Development. Workshops at the Twenty-Fifth AAAI Conference on Artifical Intelligence. Retrieved September 1, 2018, from https://atlas.media.mit.edu/en/resources/permissions/

Albeaik, S., Kaltenberg, M., Alsaleh, M., & Hidalgo, C. A. (2017). Improving the Economic Complexity Index. Arixv Working Paper (Vol. arXiv:1707). Retrieved from http://arxiv.org/abs/1707.05826

Buxton, J. (2014). Social policy in Venezuela: Bucking neoliberalism or unsustainable clientelism. UNRISD Working Paper 2014-16, (November). Retrieved from http://link.springer.com/chapter/10.1057/978-1-137-53377-7_13

Center for Systematic Peace Polity IV Datset. (2013). Retrieved September 1, 2018, from http://www.systemicpeace.org/inscrdata.html

Corrales, J. (1999). Venezuela in the 1980s, the 1990s and beyond. Harvard Review of Latin America, pp. 26–29. Retrieved from https://revista.drclas.harvard.edu/book/venezuela-1980s-1990s-and-beyond

Fardmanesh, M. (1991). Dutch disease economics and oil syndrome: An empirical study. World Development, 19(6), 711–717. https://doi.org/10.1016/0305-750X(91)90205-V

Fisher, M., & Taub, A. (2017a). How Does Populism Turn Authoritarian? Venezuela Is a Case in Point. Retrieved September 1, 2018, from https://www.nytimes.com/2017/04/01/world/americas/venezuela-populism-authoritarianism.html

Fisher, M., & Taub, A. (2017b). How Venezuela went from the richest economy in South America to the brink of financial ruin. Retrieved September 1, 2018, from https://www.independent.co.uk/news/long_reads/how-venezuela-went-from-the-richest-economy-in-south-america-to-the-brink-of-financial-ruin-a7740616.html

Gala, P., Rocha, I., & Magacho, G. (2018). The structuralist revenge: economic complexity as an important dimension to evaluate growth and development. Brazilian Journal of Political Economy. https://doi.org/10.1590/0101-31572018v38n02a01

GINI EHII. (2015). Retrieved July 20, 2012, from https://utip.lbj.utexas.edu/data.html

Hardesty, L. (2017). Income inequality linked to export “complexity.” Retrieved September 1, 2018, from http://news.mit.edu/2017/income-inequality-linked-export-complexity-0217

Hartmann, D., Guevara, M. R., Jara-Figueroa, C., Aristarán, M., & Hidalgo, C. A. (2017). Linking Economic Complexity, Institutions, and Income Inequality. World Development, 93(September), 75–93. https://doi.org/10.1016/j.worlddev.2016.12.020

Hartmann, D., Jara-Figueroa, C., Guevara, M., Simoes, A., & Hidalgo, C. A. (2016). The structural constraints of income inequality in Latin America. Integration & Trade Journal, (40), 70–85. Retrieved from https://publications.iadb.org/handle/11319/7667?locale-attribute=en

Hausmann, R., Hidalgo, C. A., Bustos, S., Coscia, M., Chung, S., Jimenez, J., … Yildirim, M. A. (2011). The Atlas of Economic Complexity.

Hidalgo, C. A., & Hausmann, R. (2009). The building blocks of economic complexity. Proceedings of the National Academy of Sciences, 106(26), 10570–10575. https://doi.org/10.1073/pnas.0900943106

Hidalgo, C., & Simoes, A. (2018). The Observatory of Economic Complexity.

Ilter, C. (2017). What Economic and Social Factors Affect GDP Per Capita? A Study on 40 Countries. Ssrn, 1–13. https://doi.org/10.2139/ssrn.2914765

López, L. (2017). Oil Has Cursed Venezuela—But Could Also Save the Country. The Atlantic. Retrieved from https://www.theatlantic.com/international/archive/2017/12/fixing-venezuela/548465/

McCaughan, M. (2011). The Battle of Venezuela. New York: Seven Stories Press. Retrieved from http://books.google.ch/books?id=VdYHt8EBsJUC

Mcgreevey, W. (2013). Angus Deaton, The Great Escape: Health, Wealth, and the Origins of Inequality. Population and Development Review, 39(4), 717–721. https://doi.org/10.1111/j.1728-4457.2013.00638.x

Mealy, P., Farmer, J. D., & Teytelboym, A. (2017). A New Interpretation of the Economic Complexity Index. SSRN. https://doi.org/10.2139/ssrn.3075591

Müller, A. R. (2014). South Korea: Food security, development and the developmental state. New Challenges to Food Security: From Climate Change to Fragile States, (Timmer 2005), 298–320. https://doi.org/10.4324/9780203371176

Natarajan, G. (2013). An assessment of “The Economic Complexity Atlas.” Retrieved from http://gulzar05.blogspot.com/2013/03/an-assessment-of-economic-complexity.html

Ross, M. L. (1999). The political economy of the resource curse. World Politics, 51(2), 297–322. https://doi.org/10.1017/S0043887100008200

Ross, M. L. (1999). The political economy of the resource curse. World Politics, 51(2), 297–322. https://doi.org/10.1017/S0043887100008200

Roth, C. (2016). Venezuela’s Economy Under Chavez, by the Numbers. Retrieved September 1, 2018, from https://blogs.wsj.com/economics/2013/03/06/venezuelas-economy-under-chavez-by-the-numbers/

Salna, K., & Rusmana, Y. (2017, August 14). Whatever Happened to Indonesia’s Mightly Oil and Gas Industry? Bloomberg News. Retrieved from https://www.bloomberg.com/news/articles/2017-08-14/whatever-happened-to-indonesia-s-mighty-oil-and-gas-industry

Schwarz, A. J. (1999). A Nation in Waiting: Indonesia in the 1990s. Boulder: Westview Press.

Woo-Cumings, M. (1999). The Developmental State. Cornell Studies in Political Economy.

World Bank. (2016). The World Bank DataBank. Retrieved September 1, 2018, from http://databank.worldbank.org/data/home.aspx

Appendix

Click here to access the appendix."	http://www.inquiriesjournal.com/articles/1794/the-impact-of-economic-complexity-on-productive-structure-and-income-inequality-in-indonesia-and-venezuela	"Recent work with the Economic Complexity Index (ECI) has shown that a country’s productive structure constrains its level of economic growth and income inequality. Building on previous research that identified an increasing gap between Latin America and the Caribbean (LAC) with that of China and other High Performing East Asian Economies (HPEA), this paper compares how the diverging productive structures of the Venezuelan and Indonesian economies have impacted the generation and distribution of income in each country. I use time-series regression analysis to show the impact of economic complexity – controlling for additional economic and political variables. Moreover, this paper considers how industrial policies and the resource curse have impacted the underlying productive structure of the economy in both nations. The paper argues that Indonesia’s success in diversifying its economy away from oil exportation and building a more complex economy has led to lower income inequality. Conversely, Venezuela’s increasing reliance on oil has led to higher income inequality despite substantial redistributive policies. Finally, the paper discusses how LAC reliance on the exportation of commodities has serious implications for income inequality because the productive structures of an economy also act as structural constraints on income inequality.
Economic Complexity and Income Inequality
Increasing income inequality has been a point of concern in many nations over the last 30 years. Political scientists and economists to better understand the causes of rising income inequality; however, research remains in dispute. Rising income inequality has been shown to lead to slower economic growth by depressing aggregate demand and stagnating wages, while also leaving households relying on debt to maintain spending. New research done at the Massachusetts Institute of Technology has found the underlying productive structures of a country’s economy as a contributing factor for the level of income inequality observed (Hartmann, Guevara, Jara-Figueroa, Aristarán, & Hidalgo, 2017). According to the World Bank, rising income inequality in developing countries has been linked to the underlying productive structure of a country’s economy. The majority of new studies that measure the productive structures of an economy use the new Economic Complexity Index (ECI).
Analysis of countries’ productive structures done during the twentieth century was simplistic: only measuring economic diversity by assessing the percentage of individuals employed by agriculture, manufacturing and service sectors (Hartmann et al., 2017). This approach was developed by Albert O. Hirschman, an economist. Despite its intuitive nature, it was too broad a measure, in part due to sectors not being adequately defined or uniformly measured. The limitations of Hirschman and others’ approaches necessitated the need for the Economic Complexity Index (ECI), which is designed to measure the sophistication of a country’s productive structure by combining information about the diversity of its products (the number of products) and the ubiquity of its products (the number of countries that export each product)(Hartmann et al., 2017). A higher ECI is a representation of a more complex economy. Conversely, a lower ECI indicates low diversity and high levels of ubiquity in the goods a country produces.
More simply, the ECI provides a useful rank order that places nations with similar exports near each other. Using United Nations (UN) export data, the ECI sorts the diversity and ubiquity of all countries’ exports for a given year and generates a ranking using both measures to correct for each other mathematically. Note that diversity captures how many exports a country is competitive in and ubiquity captures what type of exports. The ECI combines ubiquity and diversity and attempts to infer about countries productive capabilities or complexity by making relative comparisons across export baskets (Mealy, Farmer, & Teytelboym, 2017).
Figure 1: Visual Representation of the Economic Complexity Index (ECI) from “A New Interpretation of the Economic Complexity Index” by Mealy, Farmer, & Teytelboym, 2017
Figure 1 illustrates how the ECI provides a rank ordering of countries in terms of export similarity. For example, developed countries, such as the United States, Japan, and Germany have high ECI and produce complex goods like medical equipment and other capital goods. Conversely, developing countries, such as Bangladesh, Laos, and Cambodia have low ECI and produce simpler labor-intensive goods like textiles. Panel (a) shows how countries with a positive ECI are on the right segment of the graph and countries with negative ECI fall on the left segment. The network visualization in Panel (b) depicts country nodes with color corresponding their ECI value; shades of green represents positive ECI values and shades of pink represent negative values. Connections between nodes represent similar exports.
The ECI is a new measure of underlying productive structures, but prior endogenous growth models have also emphasized the importance of knowledge accumulation as well as product diversification and specialization as critical drivers of economic growth (Natarajan, 2013). In this respect, the ECI is not revolutionary as much as it is evolutionary as it attempts to quantify the productive knowledge in a given country’s economy by observing the goods it produces.
As mentioned, the Atlas of Economic Complexity uses UN export data to compute the ECI; this reliance on export data means it may be a less accurate measurement for the productive capabilities of larger economies, such as the US, which are service-based on not substantial exporters (Natarajan, 2013). Theoretically, the ECI formulas could be modified to use data on the composition of both goods and services, but the lack of a comprehensive data set and uniform categorization of services make this difficult if not impossible. Despite these issues, the ECI remains relevant when considering developing countries where exports are a substantial part of the domestic economy. With regards to Indonesia and Venezuela exports represented over 25% of both countries’ GDP, on average, for the time period under consideration (World Bank, 2016).
Statistical analysis from previous studies using the ECI has found it a significant predictor of economic growth and income inequality (Hartmann et al., 2017; Hartmann, Jara-Figueroa, Guevara, Simoes, & Hidalgo, 2016; C. A. Hidalgo & Hausmann, 2009). The ECI is statistically a more accurate predictor of per capita GDP growth than measures of governance, competitiveness (World Economic Forum's Global Competitiveness Index), and human capital (as measured in terms of educational attainment) (Hausmann et al., 2011). Hartmann’s most recent study observed that the variety of products a country produces acts as a predictor of an economy’s given path of specialization and level of economic growth (see Figure 2). Multiple other studies have also observed a strong correlation between a country’s level of economic complexity (ECI) and the corresponding level of income inequality between 1968 and 2008 (Gala, Rocha, & Magacho, 2018; Hartmann et al., 2017, 2016).
Figure 2: Bivariate relationship between economic complexity and income inequality from “Linking Economic Complexity, Institutions and Income Inequality” by D. Hartmann, Guevara, Jara-Figueroa, Aristarán, & Hidalgo, 2017
Figure 2 illustrates a bivariate relationship, using the Economic Complexity Index (ECI) as the independent variable, between economic complexity and income inequality across different decades from 1963-2008. The graphs illustrate how the ECI is negatively correlated with the Gini coefficient – after controlling for GDP per capita and other macroeconomic indicators. However, the negative relationship between economic complexity and income inequality is stronger (R2=0.58) than the relationship between income inequality and GDP per capita (R2=0.36). Furthermore, the figure shows that the negative relationship between income inequality (Gini) and economic complexity (ECI) remains stable from 1963-2008. Several other studies have also quantified and shown that, across all decades, there exists a negative relationship between economic complexity and income inequality (Gala et al., 2018; Hartmann et al., 2016; Hausmann et al., 2011).
There are multiple reasons why the economic complexity (ECI) of a given country is associated not only with economic growth, but also with a country’s average level of income inequality. The complexity and diversity of products that a country exports are a good proxy for the knowledge available in an economy that is not captured by other measures of human capital, such as schooling, which is measured by the percentage of the population with a secondary education (Hartmann et al., 2017). For example, complex industries, such as advanced medical equipment, require better-educated and more creative workers and institutions that are able to include the creative inputs of workers into the activities of firms (Hartmann et al., 2016). A nation’s degree of economic complexity can be seen as a proxy for factors such as the productive knowledge and the inclusiveness of institutions which profoundly affect economic growth and inequality, but which are typically difficult to measure directly. Without sound institutions maintaining the rule of law and high levels of human capital, it is impossible to have a complex economy. The quality of institutions and education are responsible for, and develop alongside, a complex growing economy (Hartmann et al., 2016). The ECI is sufficiently broad that a stronger statistical correlation can be found between it and income inequality over other aggregate measures like years of schooling (Hartmann et al., 2016).
Research using the ECI has bought new attention to large disparities in the productive structure between Latin American countries (LAC) and High-Performing East Asian countries (HPEA). These disparities are shown when observing how the ECI has diverged across nations in the two regions despite a degree of convergence in per capita GDP (PPP). Further research using the ECI has shown a gap in the productive capabilities and opportunities for inequality reduction of LAC and HPEA. HPEA have been capable of increasing their level of economic complexity and thus overcoming structural constraints on income inequality as expressed in the decline of income inequality (Hartmann et al., 2016). However, LAC are still strongly constrained by their natural resource centric productive structures, and income inequality has remained high despite comparable per capita economic output. To understand this trend, it would be beneficial to compare individual nations in the two regions and explore how their economic and political development have impacted the level of income inequality.
This paper compares Indonesia and Venezuela to illustrate how both nations have diverged and are proxies for broader economic and political trends across Latin America (LAC) and East Asia (HPEA). The primary focus is comparing how income inequality in both countries has been impacted by changes in the underlying productive structures of each is economy. The ECI is the primary independent variable of interest, and it has substantial benefits over using only values such as per capita GDP that does not capture changes in the underlying productive structures of an economy. The ECI is also an important benchmark for the relative success of economic development strategies that rely on Export-Oriented Industrialization (EOI) since the index measures the diversity and ubiquity of goods that an economy produces.
Indonesia (East Asia) and Venezuela (Latin America)
The focus on Indonesia and Venezuela is due to each nation’s representation of broader trends between High-Performing East Asian countries (HPAE) and Latin American countries (LAC). Neither of the aforementioned countries have reached the late stages of economic development, seen in Japan or Chile. Despite a degree of convergence in GDP per capita values in the two regions, there remain large disparities in income inequality which can be tied to differences in the knowledge and productive capabilities of nations in each region (Hartmann et al., 2016). These disparities are echoed when looking at Indonesia and Venezuela.
Both nations have distinct macroeconomic and historical similarities and differences that must be understood in order to fully comprehend each nation’s development (or lack thereof) and levels of income inequality over the last 60 years. Macroeconomically, both are classified as middle-income countries with per capita gross national incomes between $1026 and $12,475 (2011 US Dollars). They have comparable levels of human capital (years of schooling) and similar demographic structures with neither nation exhibiting irregularities in gender or age distribution (World Bank, 2016). Another important similarity is that both nations, in the 1960s and ‘70s, had economies that were heavily dependent on oil extraction and exportation. Venezuela is well known for having the largest proven oil reserves in the world (Fisher & Taub, 2017b). Indonesia was also a considerable exporter of petroleum products, and was the only East Asian nation in the Organization for Petroleum Exporting Countries (OPEC) until recently (Salna & Rusmana, 2017). The presence of natural resources in both countries is relevant when considering the resource curse where oil wealth plagues non-resource-related development and inhibits institutional development (Michael L. Ross, 1999). Unlike other East Asian nations, Indonesia did not lack natural resources and could have feasibly relied on oil revenues and not diversified its economy; however, it chose to pursue export-led industrialization which has been responsible for faster growth and lower levels of income inequality.
After WWII, Indonesia, like many nations in Southeast Asia, found itself in a vulnerable position as an economically and militarily weak state that was plagued by religious and ethnic conflicts. Indonesian leaders transformed those fears of war and instability into a remarkable developmental energy that eventually became a binding agent for growth. The pursuit of wealth in the context of stability, security, and catching up with the rest of the world were vastly more effective in generating developmental energy than a general appeal to increase welfare, which is a common thread among Latin American “populism.”(Woo-Cumings, 1999).
The absence in Latin America of the kind of vulnerability and urgency one finds in East Asia has to do with Latin American nations attaining political independence and beginning industrialization a century earlier. Latin American nations, including Venezuela, were never “late developers,” and thus never had nor developed the sustained nationalism needed to provide the impetus to modernize (Woo-Cumings, 1999). Additionally, the wealth of natural resources available to Latin American nations, combined with their relatively small populations, provided an easy alternative to the difficult task of industrialization. The timing and manner of Latin America’s involvement with the world economy and relative economic position within it was structurally different from that of many High-Performing East Asian Countries (HPEA). These fundamental differences between the two regions meant contrasting growth models with HPEA (Indonesia) adopting Export Oriented Industrialization (EOI), while LAC (Venezuela) relied on Import Substitution (ISI) and over-valued exchange rates (Müller, 2014).
Methodology
Datasets were compiled from the University of Texas Inequality Project (GINI EHII), World Bank, MIT Observatory of Economic Complexity and the Center for Systemic Peace. The most recent observations across the variables are from 2013, and income inequality (Gini) data is unavailable before 1964. Due to lack of reliable income distribution data for Venezuela from 2007 onward, the data set and all regression analysis between the ECI and Gini coefficient occurs from 1965-2006. The data available on Indonesia is more comprehensive, and the data set includes Gini coefficients from 1970-2013.
Time series regression analysis is used as it is the most practical and efficient method of measuring the relationship among income inequality (Gini), economic complexity (ECI), and political stability (Polity IV) while also considering additional factors that influence income inequality.
The Gini-coefficient is the dependent variable and is intended to measure income inequality. It does this by plotting the distribution of a given country’s income on a Lorenz curve and then deducting the area occupied by the curve from the line of equality (Mcgreevey, 2013). More simply, the Gini index is a number that lies between 0 (perfect equality—everyone has the same) and 100 (perfect inequality, with one person having everything). Income inequality is assessed rather than wealth inequality because the distribution of wealth in an economy is the product of past economic structures while the income distribution is representative of current structures. All Gini data is taken from the University of Texas Income Inequality project (GINI EHII).
Real GDP Per Capita is calculated by dividing a country’s total Gross Domestic Product (GDP), a measure of its total output, by its population to roughly estimate the total output of each individual in that country. Real per capita GDP is used in comparing Indonesia and Venezuela because it shows each country’s relative performance (Ilter, 2017). Furthermore, to effectively compare countries, differing price levels must be considered and thus, all real GDP figures are adjusted to purchasing power parity (PPP) dollars. Note that all GDP data used was taken from the World Bank’s World Development Indicators (World Bank, 2016).
As mentioned previously, the Economic Complexity Index (ECI) is the primary independent variable of interest and is used as a measure for measuring the productive structures of a respective country’s economy. Note that a modified measure of economic complexity called the ECI+ was recently developed and new research has shown it is a more accurate predictor of future economic growth; it was not in included because it was published after data analysis for this paper had been completed (Albeaik, Kaltenberg, Alsaleh, & Hidalgo, 2017). The ECI data was accessed from the MIT Observatory of Economic Complexity website (C. Hidalgo & Simoes, 2018).
Lastly, the Polity IV score, sometimes called the democracy score, is used as a measure of a country’s democratic institutions and free nature. The score ranges from -10 (worst) to 10 (best) and the minimum score for a country is contingent on whether a country is classified as a democracy, anocracy or autocracy. A degree of political stability by no means guarantees low levels of income inequality, but the type of regime could have a substantial impact on distribution of income via redistributive polices. Polity IV score data is available at the Center of Systematic Peace (2013).
Results
Indonesia
Figure 3: Economic Complexity and Income Inequality in Indonesia (1970-2013). Source: GINI EHII and the MIT Observatory of Economic Complexity
The Indonesian economy rose from being the 96th most complex economy in the world in 1980 to the 65th most complex in 2010 (AJS & CA, 2011). This improvement in the productive structures of its economy corresponded to a substantial decline in income inequality, with the Gini coefficient falling from 51.19 (1980) to 35.57 (2010). Figure 3 illustrates changes in Indonesia’s Economic Complexity Index (ECI) and income inequality (Gini) coefficient from 1970 to 2013.
The sustained rise in economic complexity in Indonesia coincided with its industrialization push in the mid-1980s. The Suharto administration devalued the rupiah (Indonesian currency) and deregulated the financial sector. These polices helped to aid export competitiveness by reducing the cost of exports and encouraging foreign investment. Suharto’s industrialization push came after the collapse of global oil prices in early 1980 and has reduced dependence on oil exports (Schwarz, 1999). The rise in economic complexity continued until the Asian Financial Crisis in mid-1997 when the Indonesian economy collapsed. It was particularly vulnerable due to a lack of formal state controls, specifically a pilot agency, such as the Ministry of International Trade and Industry (MITI) in Japan, which would have prioritized macroeconomic stability and mitigated the severe economic distortions that eventually eroded investor confidence in the Indonesian economy and led to substantial capital flight (Müller, 2014; Woo-Cumings, 1999)
The regression analysis in Table 1 also shows that the Indonesian economy has followed the global general trend with higher economic complexity (ECI) being a significant predictor of lower income inequality as measured by the Gini. Overall the model has relatively high predictive value with an R2 value of 0.6448, indicating that 64.48% of the change in the dependent variable is due to change in the independent variables.
The model specifies the parameter estimates below:
The intercept of the Gini coefficient is 64.7937 when all independent variables equal zero. This value is only representative and is not relevant.
There is a 7.1337 decline in income inequality (Gini coefficient) for every $1000 rise in GDP per capita (PPP, 2005 U.S Dollars) on average, ceteris paribus.
There is a 5.880801 decline in income inequality (Gini coefficient) for every 1 unit increase in the Economic Complexity Index (ECI) on average, ceteris paribus.
There is a 0.0583531 decline in income inequality (Gini coefficient) for every 1 unit increase in the Polity IV on average, ceteris paribus.
From the above results, we can see the most significant reduction in income inequality comes from an increase in GDP per capita, with the Economic Complexity Index coming in at a close second. This is consistent with previous research that has found a robust correlation between ECI and GDP per capita. An important consideration is that ECI is also shown to predict future economic growth, so improvements in productive structures have yielded both reductions in income inequality and greater levels of economic growth (Hartmann et al., 2017).
Although the above regression analysis showed that Polity IV was not statistically significant at α = 0.1 for Indonesia, it still warrants mentioning. For much of the time period under consideration (1970-2013), Indonesia was considered an authoritarian regime, and only since Suharto’s resignation in 1998 after the Asian Financial Crisis (AFC) have we seen a strengthening of democratic processes that culminated in genuine elections. The Polity IV rose from -7 (autocracy) in 1997 to 6 (democracy) in 2004 after the presidential election. Note that from 1997 to 2013, there was no substantial improvement in economic complexity, but income inequality still fell from 48.98 to 39.47.
Venezuela
Figure 4: Economic Complexity and Income Inequality in Venezuela (1965-2006). Source: GINI EHII and the MIT Observatory of Economic Complexity
The Venezuelan economy fell from the 91st (1980) most complex economy in the world to the 106th most complex in 2006(AJS & CA, 2011). This decline in the productive structures came with a gradual rise in the level of income inequality with the Gini coefficient rising from 39.23 (1980) to 46.64 (2006). This general rise in income inequality occurred despite substantial redistributive policies initiated by Hugo Chavez’s administration from 1999 to 2013 (Roth, 2016). Those same redistributive policies are likely responsible for throwing off the regression analysis in Table 2 where rises in ECI corresponds with a rise in income inequality (Gini coefficient). Figure 4 illustrates changes in the Economic Complexity Index (ECI) and income inequality (Gini) from 1965 to 2006.
From 1965 to 1980, one can see that as economic complexity of the Venezuelan economy rose, income inequality fell, and the inverse is true from 1999 onwards. There is an anomalous period from 1980 to 1999 when there was a rise in ECI, but also a rise in income inequality, which is likely related to the global collapse of energy prices. During the 1980s, the surplus of crude oil caused by a combination of reduced demand and increased production led to substantial decline in the price of oil. Venezuela’s economy was (and continues to be) highly dependent on oil exports; the decline in oil prices destabilized the economy, and despite cycles of crisis and reform, the government was unable to stabilize the economy (Corrales, 1999). The outcome was slow growth with substantial inflation. Governmental corruption in Venezuela combined with hyperinflation left the government with little revenue to fund its expansive welfare programs that were initially responsible for lowering levels of income inequality (Buxton, 2014). The decline in government redistribution outstripped improvements in Venezuela’s productive structures that came from the government’s attempts to stabilize, liberalize and diversify the economy. The rapid decline in economic complexity from 1999 onwards was due to the Chavez administration’s policy of focusing the entire economy on oil production. The Venezuelan economy is currently highly dependent on oil revenues for both the government budget and the foreign exchange reserves needed to import all consumer goods because practically nothing but oil is produced in the country (Fisher & Taub, 2017b).
The regression analysis in Table 2 also shows that the Venezuelan economy deviates from the global general trend with higher economic complexity (ECI) being a significant predictor of lower income inequality as measured by the Gini. Overall, the model has a high predictive value with an R2 value of 0.7173 indicating, that 71.73% of the change in the dependent variable is due to change in the independent variables.
The model specifies the parameter estimates below:
The intercept of the Gini coefficient is 66.80267 when all independent variables equal zero. This value is only representative and is not relevant.
There is 0.778 decline in income inequality (Gini coefficient) for every $1000 rise in GDP per capita (PPP, 2005 U.S Dollars) on average, ceteris paribus.
There is 2.832747 rise in income inequality (Gini coefficient) for every 1 unit increase in the Economic Complexity Index (ECI) on average, ceteris paribus.
There is 1.93866 fall in income inequality (Gini coefficient) for every 1 unit increase in the Polity IV on average, ceteris paribus.
From the results, we can see the most significant reduction in income inequality comes from changes in Polity IV with increases in GDP per capita not having a significant impact on income inequality. An important consideration is that the maximum value of the Polity IV is 10, and for the time period under consideration (1964-2006), Venezuela only rose once in from a score of 6 in 1967 to 9 in 1970 until 1991 – after which it gradually fell to 5 in 2006. During the aforementioned period, the rise in the Polity IV coincided with a fall in income inequality from 44.35 (1967) to 43.08(1970) and hit a low of 39 in 1979. This period was marked by heavy government spending on social welfare programs enabled by the rapid rise in global oil prices, caused by an oil embargo from the Organization of Petroleum Exporting Countries (OPEC), of which Venezuela was a founding member (McCaughan, 2011). The reductions in income inequality in Venezuela have been enabled by governmental redistributive polices that are constrained by the underlying structure of the economy and vulnerable to the cyclical nature of oil prices.
The positive correlation between ECI and the Gini coefficient does not necessarily indicate deficiencies in the approach, but corresponds to an anomaly where resource-rich nations violate global statistical trends. Generally, countries that exhibit greater economic complexity than expected, given their level of per capita GDP, tend to grow faster and have lower levels of income inequality. Venezuela and Saudi Arabia both deviate from this trend by having both higher GDP per capita and higher levels of income inequality than expected for their current level of economic complexity, since both countries have economies primarily based on oil (Hartmann et al., 2017). Note that several other countries in LA, like Brazil, also deviate from global trends because their economies rely heavily on commodity exportation. In this respect, Venezuela’s position of having relatively high per capita GDP, low complexity, and high levels of income inequality is the norm regionally even though it is a global outlier (Hartmann et al., 2016).
Discussion
Indonesia has gradually risen from being a poor country with an undiversified, oil-reliant economy in the 1970s to the largest economy in Southeast Asia. This meteoric rise was enabled by its export-led growth model, and its success can be seen in how the export structure has changed from 1970 to 2010 (see Figure 5), diversifying away from commodity exportation to exporting both labor and capital-intensive goods. This economic modernization was initiated by the Suharto administration, whose policies helped to stabilize Indonesia’s currency and encourage foreign investment. Like other East Asian nations during the Cold War, Indonesia established an extensive economic and security relationship with the United States, which allowed for greater market access in the Cold War (Müller, 2014). Many of these critical policies were crafted by a group of Indonesian economists educated in the U.S. called the “Berkley Mafia.” Their placement in various semi-permanent technocratic positions enabled them to push for liberalization and deregulation both before and after the rise in oil prices in the 1970s. The ability of these technocrats to influence economic policy differed considerably from Venezuela where rapid changes in economic conditions (associated with large fluctuations in the price of oil) led to political crises and the election of new leaders that seldom maintained any consistency in policy.
Unfortunately, economic diversification and growth did not yield improvements in political stability or personal freedoms. Indonesia was classified as autocracy and was led by President Suharto until 1997. While authoritarianism can overcome political constraints and mobilize the population to sacrifice current prosperity for the sake of developmental projects, this was never completely the case in Indonesia (Müller, 2014; Woo-Cumings, 1999). Rather than cultivate a relationship between the state and business through industrial policy, as in South Korea, it was done through ethnic division of labor with development, not for the sake of shared prosperity, but rather for the sake of payoffs. Suharto and his family were the owners of many conglomerates that benefited from export-led growth, and the minority ethnic Chinese that represented the business class or entrepreneurial element of the economy were under constant threat of persecution. Only after Suharto resigned in 1997 did democratization begin and Indonesians see a substantial rise in political stability and personal freedoms (Schwarz, 1999).
Venezuela was once considered a shining example for Latin American countries in the 1950s as a wealthy, stable democracy. Decades later, the country and its economy are in ruins, with unprecedented hyperinflation and recession that has left most of the population on the brink of starvation (Fisher & Taub, 2017b). Despite a gradual improvement in smaller sectors of the Venezuelan economy, it remains more dependent than ever on crude oil exportation (see Figure 5). Like other Latin American nations, Venezuela has suffered from the resource curse (paradox of plenty), where oil wealth plagues non-resource-related development and inhibits institutional development (M. L. Ross, 1999). Despite governmental efforts supporting industrialization in the 1960s and ‘70s with import substitution policies, said policies never truly came to fruition. The failure of industrial policy can be partially linked to “Dutch disease”, where an increase in natural resource revenues undermines export-based manufacturing because substantial commodity exports cause exchange rate appreciation (Fardmanesh, 1991). This “disease” has pulled capital and labor from non-resource related sectors and made Venezuela further reliant on oil revenue.
Politically, Venezuela was classified as a democracy until the early 2000s, but it is currently considered an authoritarian regime. Much of this regression occurred during the Chavez administration. Hugo Chavez, the former president of Venezuela, was a democratic populist who came to power in 1999 after two decades economic stagnation. Under Chavez’s rule, Venezuelans prospered through generous social welfare programs and redistributive policies funded through oil revenues (McCaughan, 2011). However, he became increasingly frustrated with courts blocking his legislation and media outlets criticizing his programs. Soon after, Chavez began to actively undermine the courts by removing judges, and he sought to silence journalists with anti-defamation laws. These discrete legal changes undermined Venezuelan democracy and centralized power with the executive branch of the government. Chavez’s actions have been characterized as in line with “democratic backsliding,” which is when the government attempt to systematically erode institutions like the press and independent judiciaries that enable democracy to function (Fisher & Taub, 2017a).
Figure 5: (A) Indonesia exports in 1970, (B) Indonesia exports in 2013, (C) Venezuela Exports in 1965 and (D) Venezuela exports in 2006. Source: atlas.media.mit.edu
The divergent productive structures of Indonesia and Venezuela’s economies are a product of each country’s history, culture, and policies, but this also illustrates a broader disparity between countries in Latin America (LAC) and newly-industrialized High-Performing East Asian countries (HPEA). HPEA primarily went through the process of industrialization in the latter half of the twentieth century with consistent policies; however, Latin American countries, including Venezuela, have had waves of extreme industrialization through import-substitution followed by equally extreme efforts to liberalize and reform (Hartmann et al., 2016). Neither state intervention nor liberalization have successfully diversified LAC economies, which remain heavily dependent on natural resource extraction (Hartmann et al., 2016).
Figure 6: (A) Chinese exports to the world, (B) Latin America Countries’ (LAC) exports to the world from “The Structural Constraints of Income Inequality in Latin America” by D. Hartmann, Jara-Figueroa, Guevara, Simoes, & Hidalgo.
Figure 6 illustrates the vast differences in the productive capabilities in between LAC and China. China’s exports primarily consist of complex machinery and other finished goods that require sophisticated factories and indicate a high degree of economic complexity. Latin American countries’ (LAC) exports consist largely of natural resources like crude oil, metals, and non-processed agricultural goods.
While HPEA, like China, have diversified, LAC remain reliant on the exportation of commodities. These structural differences have serious implications for income inequality in both regions because the productive structures of an economy also act as structural constraints on income inequality. A country with high levels of economic complexity will usually not exhibit high levels of income inequality because the structure of the economy is more inclusive and is also not reliant on governmental redistributive policies to achieve lower income inequality (Hartmann et al., 2016).
Many Latin American countries have had generous welfare policies that are intended to reduce income inequality, but these policies tend to be constrained since government revenues are usually reliant on no more than a few sectors of the economy (Hardesty, 2017). Venezuela is a commonly cited example because the Chavez and Maduro administrations focused the economy on oil production and attempted to use the revenues to fund programs and provide direct cash subsidies. However, despite these policies, income inequality has been rising since Chavez took office in 1998. Reliance on the concentrated economic power of the national oil industry led to rent-seeking behavior, mismanagement, and a grossly-unequal distribution of resources (McCaughan, 2011). The Venezuelan government maintained a monopoly on oil production by distributing the benefits and, therefore, feeding a relationship of dependency among citizens and private interests who, over time, became accustomed to thinking of oil wealth as something the government guarantees and thus its absence was deeply destructive.
Low levels of economic complexity are accompanied by high levels of income inequality because the structure of the economy is less inclusive and economic power more concentrated. Thus, redistributive policies to achieve lower income inequality have hard limits and must be accompanied by efforts to diversify the economy (Hartmann et al., 2016). Resource-rich European Welfare States like Norway have succeeded in part because they were able to use oil wealth to diversify their economies. Since the 1970 oil boom, the Norwegian government has provided funding and established incentives to aid the development of shipping, engineering, and drilling technologies. This formed the foundation for growth across a range of other industries, including fish farming, bio-refining, logging, and mining. Non-resource-rich European nations, like Finland, have also succeeded by making large investments in human capital and becoming research and development centers for technology (López, 2017).
There exist inextricable links between political and economic development that are shown to have a substantial impact on the level of income inequality both within individual nations and across regions. This case study of Indonesia and Venezuela illustrates how both nations have diverged and are proxies for broader economic and political trends across Latin America and East Asia. Literature on cross-national and regional similarities and differences have emphasized per capita economic performance, with less attention given to nations with comparable levels of per capita GDP but vastly different distributions of income. This omission was in part related to contemporary economic thinking which saw distribution of income as less relevant and primarily impacted by government redistributive policies rather than the underlying structure of the economy. A larger study involving panel data from Latin America Countries (LAC) and High Performing Asia Economies (HPEA) should be done to better understand the impact of diverging economic development on income inequality in these regions.
References
AJS, S., & CA, H. (2011). The Economic Complexity Obervatory: An Analytical Tool for Understanding the Dynamics of Economic Development. Workshops at the Twenty-Fifth AAAI Conference on Artifical Intelligence. Retrieved September 1, 2018, from https://atlas.media.mit.edu/en/resources/permissions/
Albeaik, S., Kaltenberg, M., Alsaleh, M., & Hidalgo, C. A. (2017). Improving the Economic Complexity Index. Arixv Working Paper (Vol. arXiv:1707). Retrieved from http://arxiv.org/abs/1707.05826
Buxton, J. (2014). Social policy in Venezuela: Bucking neoliberalism or unsustainable clientelism. UNRISD Working Paper 2014-16, (November). Retrieved from http://link.springer.com/chapter/10.1057/978-1-137-53377-7_13
Center for Systematic Peace Polity IV Datset. (2013). Retrieved September 1, 2018, from http://www.systemicpeace.org/inscrdata.html
Corrales, J. (1999). Venezuela in the 1980s, the 1990s and beyond. Harvard Review of Latin America, pp. 26–29. Retrieved from https://revista.drclas.harvard.edu/book/venezuela-1980s-1990s-and-beyond
Fardmanesh, M. (1991). Dutch disease economics and oil syndrome: An empirical study. World Development, 19(6), 711–717. https://doi.org/10.1016/0305-750X(91)90205-V
Fisher, M., & Taub, A. (2017a). How Does Populism Turn Authoritarian? Venezuela Is a Case in Point. Retrieved September 1, 2018, from https://www.nytimes.com/2017/04/01/world/americas/venezuela-populism-authoritarianism.html
Fisher, M., & Taub, A. (2017b). How Venezuela went from the richest economy in South America to the brink of financial ruin. Retrieved September 1, 2018, from https://www.independent.co.uk/news/long_reads/how-venezuela-went-from-the-richest-economy-in-south-america-to-the-brink-of-financial-ruin-a7740616.html
Gala, P., Rocha, I., & Magacho, G. (2018). The structuralist revenge: economic complexity as an important dimension to evaluate growth and development. Brazilian Journal of Political Economy. https://doi.org/10.1590/0101-31572018v38n02a01
GINI EHII. (2015). Retrieved July 20, 2012, from https://utip.lbj.utexas.edu/data.html
Hardesty, L. (2017). Income inequality linked to export “complexity.” Retrieved September 1, 2018, from http://news.mit.edu/2017/income-inequality-linked-export-complexity-0217
Hartmann, D., Guevara, M. R., Jara-Figueroa, C., Aristarán, M., & Hidalgo, C. A. (2017). Linking Economic Complexity, Institutions, and Income Inequality. World Development, 93(September), 75–93. https://doi.org/10.1016/j.worlddev.2016.12.020
Hartmann, D., Jara-Figueroa, C., Guevara, M., Simoes, A., & Hidalgo, C. A. (2016). The structural constraints of income inequality in Latin America. Integration & Trade Journal, (40), 70–85. Retrieved from https://publications.iadb.org/handle/11319/7667?locale-attribute=en
Hausmann, R., Hidalgo, C. A., Bustos, S., Coscia, M., Chung, S., Jimenez, J., … Yildirim, M. A. (2011). The Atlas of Economic Complexity.
Hidalgo, C. A., & Hausmann, R. (2009). The building blocks of economic complexity. Proceedings of the National Academy of Sciences, 106(26), 10570–10575. https://doi.org/10.1073/pnas.0900943106
Hidalgo, C., & Simoes, A. (2018). The Observatory of Economic Complexity.
Ilter, C. (2017). What Economic and Social Factors Affect GDP Per Capita? A Study on 40 Countries. Ssrn, 1–13. https://doi.org/10.2139/ssrn.2914765
López, L. (2017). Oil Has Cursed Venezuela—But Could Also Save the Country. The Atlantic. Retrieved from https://www.theatlantic.com/international/archive/2017/12/fixing-venezuela/548465/
McCaughan, M. (2011). The Battle of Venezuela. New York: Seven Stories Press. Retrieved from http://books.google.ch/books?id=VdYHt8EBsJUC
Mcgreevey, W. (2013). Angus Deaton, The Great Escape: Health, Wealth, and the Origins of Inequality. Population and Development Review, 39(4), 717–721. https://doi.org/10.1111/j.1728-4457.2013.00638.x
Mealy, P., Farmer, J. D., & Teytelboym, A. (2017). A New Interpretation of the Economic Complexity Index. SSRN. https://doi.org/10.2139/ssrn.3075591
Müller, A. R. (2014). South Korea: Food security, development and the developmental state. New Challenges to Food Security: From Climate Change to Fragile States, (Timmer 2005), 298–320. https://doi.org/10.4324/9780203371176
Natarajan, G. (2013). An assessment of “The Economic Complexity Atlas.” Retrieved from http://gulzar05.blogspot.com/2013/03/an-assessment-of-economic-complexity.html
Ross, M. L. (1999). The political economy of the resource curse. World Politics, 51(2), 297–322. https://doi.org/10.1017/S0043887100008200
Ross, M. L. (1999). The political economy of the resource curse. World Politics, 51(2), 297–322. https://doi.org/10.1017/S0043887100008200
Roth, C. (2016). Venezuela’s Economy Under Chavez, by the Numbers. Retrieved September 1, 2018, from https://blogs.wsj.com/economics/2013/03/06/venezuelas-economy-under-chavez-by-the-numbers/
Salna, K., & Rusmana, Y. (2017, August 14). Whatever Happened to Indonesia’s Mightly Oil and Gas Industry? Bloomberg News. Retrieved from https://www.bloomberg.com/news/articles/2017-08-14/whatever-happened-to-indonesia-s-mighty-oil-and-gas-industry
Schwarz, A. J. (1999). A Nation in Waiting: Indonesia in the 1990s. Boulder: Westview Press.
Woo-Cumings, M. (1999). The Developmental State. Cornell Studies in Political Economy.
World Bank. (2016). The World Bank DataBank. Retrieved September 1, 2018, from http://databank.worldbank.org/data/home.aspx
Appendix
Click here to access the appendix."	44797
economy	['Fesen', 'Zachary T']	2021-05-22 00:00:00	"Abstract Israel has increased the nation’s security presence around the Gaza Strip and in the West Bank. Here, the research project analyzes how transaction costs resulting from Israeli security policy impact the output of manufacturing activities in the Palestinian territories. The study examines transaction costs in the form of import restrictions and items Israel restricts from entering the Palestinian territories in the form of the dual-use goods list. Existing research outlines trade restrictions from Israeli security policy but does not explain how the restrictions impact Palestinian manufacturing productivity. I use a multivariable regression model to look for a correlation between a manufacturing activity’s exposure to Israeli security restrictions, in the form of the controlled dual-use goods list and import restrictions and a decrease in output. The regression breaks down the type of restriction a manufacturing activity could face into no restrictions, restrictions on chemical items, restriction on non-chemical items or restrictions on both chemical and non-chemical items. Furthermore, the project compares costs, procedures, processing times, and documentation associated with Palestinian importation through Israeli vs Jordanian ports. The study found a manufacturing activity's exposure to the dual-use goods list was not a statistically significant predictor of outcome. However, Palestinians face costs due to increased wait times, limitations of port security inspection infrastructure, increased transportation costs, storage fees, and security inspection costs as a result of Israeli security restrictions regardless of the port Palestinians use to import products. These findings demonstrate the need for the Israeli Government to increase the infrastructural capabilities and operating hours of Palestinian border crossings.

Today the Palestinian economy relies on the services sector for employment and output. Governmental social services are a staple in both the West Bank and Gazan economies. The expansion of the service sector means the manufacturing sector now plays a smaller role in the overall Palestinian GDP. In 2019, the service sector accounted for 73.4% Palestinian GDP while the agricultural sector decreased from 5.8% to 3.4% from 2005 to 2019 and the manufacturing sector from 25.8% to 22.9% (UNData, n.d.). Governmental social services make up 60% of the Gazan service sector GDP and 32% of West Bank service sector GDP (Al-Falah, 2013). While the Palestinian Authority (PNA) ran a debt-to-GDP ratio of 16% in 2018, decreases in foreign aid to the PNA means the budget deficit will enlarge (International Monetary Fund, 2018). As a result of the agricultural and manufacturing sectors’ decrease in output, a smaller PNA budget poses a large risk to the economic stability of the West Bank. Lastly, Israeli withholding of Palestinian import and value added taxes risks Palestinian job security due to the West Bank labor force’s public-sector employment reliance.

As Israel collects Palestinian import taxes, Israel also decides what imports Palestinians can bring into the Palestinian territories. Israel restricts items Palestinians may use in a military fashion via the dual-use restrictions list. The number of items Israel restricts vary based on the security threat of the Palestinian territory. As of April 17th, 2019, Israel restricted the import of 56 categories of items into the West Bank and 61 Gaza Strip specific restricted categories of items (The World Bank, 2019b). Israel first outlined the dual-use restrictions list in the 2007 Defense Export Control Law and the 2008 Defense Export Control Order (Gisha, n.d.). However, in July of 2010 Israel announced a Gaza Strip specific dual-use goods list in addition to the existing list (Gisha, 2016, p. 4). In March and November of 2015, the Israeli Defense Forces (IDF) added items to the Gaza Strip specific dual-use goods list (see list 3 Appendix B) (Gisha, 2016, p. 4). Because Hamas continues to pose a security threat to Israel, Israel will likely maintain restrictions on Gazan importation of military use items. Because Israel will likely not remove security restrictions on movement and trade in the near future, an analysis of the relationship between the security restrictions and Palestinian economic output may help academics understand the cost of Israel’s security presence on the Palestinian population.

This research project investigates how transaction costs from Israeli security policy impact manufacturing output in the West Bank and Gaza Strip. Specifically, the research examines how costs resulting from Israeli security policy impact the cost of importing for the Palestinian manufacturing sector. I use a multivariable regression model to look for a correlation between a manufacturing firm’s exposure to Israeli security restrictions, in the form of the controlled dual-use items list and import restrictions, and a decrease in output. Next, the project compares security related costs such as costs due to increased wait times, increased transportation costs, storage fees, and security inspection costs when Palestinian importers import through Israeli vs the Jordanian Ports The project uses dependency theory to show how Israeli security and economic concerns limit the growth potential of the Palestinian economy. Through understanding the causes of Palestinian economic struggles policymakers in Israel, the Palestinian territories, and the United States can decrease poverty and unemployment in the Palestinian territories. Together with improved Palestinian governance, the Palestinian economy will be able to support local Palestinian labor forces and increase economic growth.

The paper argues the dual-use goods list does not negatively correlate with manufacturing output in the Palestinian territories while Israeli security restrictions at ports and checkpoints directly increase costs for manufacturing activities. The dual-use goods list is the list of items Israel restricts from entry into the Palestinian territories because Palestinians could use the items for military purposes. The Literature Review covers existing academic scholarship on how the Israel-Palestinian customs union shaped Israeli-Palestinian economic relations and the relationship between Israeli security restrictions and Palestinian trade. The Analysis section runs a regression analyzing how a manufacturing activity’s exposure to the dual-use goods list impacts the activity’s output. The section compares the similarities and differences in security-related costs when a firm imports goods through Israeli vs Jordanian ports. The Discussion examines the contributions of the dependency theory to understanding Israeli-Palestinian trade relations and provides recommendations to decrease security-related costs.

Literature Review

Scholars argue the customs union established by the Paris Protocol on economic relations solidified an asymmetrical arrangement benefiting the Israeli economy at the expense of the Palestinian economy. The agreement still defines economic relations between Israel and the Palestinian territories today. Haddad (2016) and Samhouri (2016) argue the Paris Protocol flooded the Palestinian markets with cheap Israeli made products, consequently limiting the competitiveness of the Palestinian manufacturing sector (p. 102; pp. 581-583). Also, the Paris Protocols limited the ability of the PNA to by adjust instruments like fiscal, monetary, trade, and labor policy (Samhouri, 2016, p. 604). Similarly, under the Paris Protocol, Israel retained control over collecting Value-Added Taxes (VAT) and import taxes (Haddad, 2016, p. 102). While the literature covering the Paris Protocols answers how economic integration limits the competitiveness of the Palestinian manufacturing sector; the literature does not answer how hostilities between Israel and the Palestinian territories impact the customs union. To understand the production capability of the Palestinian manufacturing sector, scholars need to examine if Israeli security restrictions on the Palestinian territories limit the ability of Palestinian firms to move goods through Israeli territory under the customs union.

Additionally, Scholars are investigating whether limiting Palestinian access to Trade Facilitation Services (TFS) reduces exportation to international markets. Scholars have an interest in Palestinian access to TFS because the West Bank relies on Israel for most TFS. Previous studies evaluated how Israeli security concerns about the Palestinian territories alter Palestinian access to TFS and subsequently reduce Palestinian international market access (Eljafari, 2010, p. 748). Berends (2008) examines the Israeli-Palestinian TFS relationship by analyzing remedies to maximize international market access such as opening the Rafah crossing in Gaza for exports, and improving trade services (Berends, 2008, pp. 152-153). Research analyzing how easily Palestinians are able to access to TFS when importing could reveal if Israeli security policy limits access to inputs for Palestinian manufacturers. Further literature analyzing the ability of Palestinian businesses to access inputs for production may explain changes in the output of the manufacturing sector.

Existing academics frame integration between the Israeli and Palestinian economies through the lens of dependency theory. Dependency theory argues the underdevelopment of developing states is due to interactions between dominant and dependent states which reinforce unequal patterns of development. Past research argues the Palestinian economy is dependent on income earned by Palestinian laborers working in Israel (Zilberfarb, 2018, p. 785). The PNA is also dependent upon development focused foreign aid from developed countries and the United Nations to fill the government’s budget (Springer, 2015, p. 4). Lastly, scholars argue the PNA budget is dependent on VAT and import taxes Israel collects and has withheld in the past to punish to the PNA (Daoudi & Khalidi, 2008). However, existing research does not analyze through the lens of dependency theory how Israeli security restrictions on products entering the Palestinian territories limits economic output. Also, the literature does not examine whether the Israeli-Palestinian customs union creates a Palestinian reliance on Israeli trade facilitation.

This research adds to existing literature on barriers to Palestinian long-term economic growth by analyzing how transaction costs from Israeli security policy limit Palestinian manufacturing productivity. Existing research outlines trade restrictions from Israeli security policy but does not explain how the restrictions impact Palestinian manufacturing productivity. Furthermore, scholars have examined how the Israeli-Palestinian customs union decreases Palestinian manufacturing productivity but do not incorporate Israeli security restrictions into the research. Other literature has investigated how Palestinian reliance on Israeli TFS reduces Palestinian export access, but the research does not state whether Palestinians face increased security costs when importing using Israeli TFS. The present research analyzes the impact of Israeli security restrictions on Palestinian economic output and the Israeli-Palestinian customs union through the lens of dependency theory. My research will add to existing research by identifying how Israeli security policy impacts the output of Palestinian firms. Israel and Palestinian policymakers can only boost Palestinian economic development after policymakers understand the causes behind the Palestinian economic stagnation. Economic development would improve the quality of life in Palestinian territories while reducing Israeli security concerns.

Research Design

Dependency theory explains how unequal relationships between the developed and developing states in the world prevent economic growth of developing countries. The theory explains how the lack of growth in developing states comes at the benefit of developed states. Dependency theory labels developed as core states and developing states as periphery states (Shrum, 2001). Dependency theorists argue the impediments to economic growth in periphery states are imposed by developed countries who are pursuing domestic economic interests (Egeonu, 2017, p. 17). Dependency theorist Theontonio Dos Santos outlines the dependence of periphery states as “a situation in which the economy of certain countries (periphery states) is conditioned by the development and expansion of another economy (core states) to which the former is subjected” (Dos Santos, 1970, p. 231). Dependency is useful to understand how the Palestinian reliance on Israel for trade access alters output growth potential in the Palestinian territories. Moreover, the theory is useful to explain whether the Palestinian territories suffer from a reliance on Israel for customs duties, VAT, and import tax collection.

I collected data from the Palestinian Central Bureau of Statistics (PCBS) on the output, VAT, domestic taxes, and customs taxes of 30 West Bank and Gazan manufacturing activities (see Appendix A). The Palestinian Trade Center (PALTRADE) and the United Nations Conference on Trade and Development (UNCTAD) outlined specific costs associated with Israeli security restrictions. I listed items the Israeli Coordinator of Government Activities (COGAT) added to the Israeli dual use items list between 2007 to 2018. The dual use items list outlines items COGAT restricts Palestinians from importing because of Israeli Government fears Palestinians may use the items for military purposes (Gisha, n.d.). I gathered data from the PCBS’ Economic Surveys Series between 2007 and 2018 on the amount of customs taxes, VAT, and import purchases of a given manufacturing activity to determine whether a manufacturing activity was exposed to Israeli import restrictions. I gathered data on transportation, storage, and inspection costs as a result of Israeli security policy and the infrastructural capabilities of border crossings into the Palestinian territories from PALTRADE and UNCTAD. The time span between 2007 and 2018 is ideal to look for a potential relationship between Israeli security policy and Palestinian manufacturing output because Israel changed items listed on the dual-use items list in 2007, 2010, and 2015. A breakdown of specific costs to Palestinian importers as a result of Israeli security measures allows the project the compare the cost of importing through Israel vs Jordan.

To measure the impact of Israeli security restrictions on the productivity of the Palestinian manufacturing sector, I created a spreadsheet consolidating the output of each manufacturing activity and used binary coding to apply the import and controlled dual-use items restrictions to the activities. I used a multivariable regression model to look for a correlation between a manufacturing activity’s exposure to Israeli security restrictions, as in the controlled dual-use items list and import restrictions, and a change in output. I separated dual-use goods list restrictions into no restrictions, restrictions on chemical items, restrictions on non-chemical items or restrictions on chemical and non-chemical items. I coded a manufacturing activity a one in the categories NONE, CHEM, NCHEM, or BOTH based on whether the firm use a dual use goods list item in the production process and a zero in the remaining categories (see Table 1). I used VAT, import purchases, and customs taxes as a proxy for exposure to import restrictions. A multivariable regression is optimal because through statistical significance, a regression describes whether any potential relationship between a manufacturing activity’s exposure to security restrictions and the manufacturing activity’s output is causal or not.

The regression controlled for the C, E, WD, and T to account for other variables that might decrease the output of manufacturing firms (see Table 1). The variable WD is only used in regard to the West Bank while E is only used with the Gaza Strip. I used the yearly number of casualties due to the Israeli-Palestinian Conflict from the United Nations Office for the Coordination of Humanitarian Affairs (OCHA) and the Israeli NGO, B’Tselem, as a proxy to represent changes in the conflict level between Israel and the Palestine (B’Tselem, n.d.; OCHA, n.d.a). Next, I controlled for differences in wages between Israel and the West Bank for the same manufacturing activity from the Palestinian Labor Force Survey, between 2007 and 2018. From the Economic Surveys Series, I collected data on the domestic taxes each manufacturing activity paid per year not including the VAT or customs duties. I held constant both taxes and monetary compensation to understand the relationship between Israeli security restrictions and the output of the Palestinian manufacturing sector because low domestic taxes and low wages make the two sectors competitive in the region. I controlled for the level of violence in the Israeli-Palestinian Conflict per year because escalations in the conflict may decrease productivity and the present research wants to isolate the impact of the import restrictions and the dual-use list on Palestinian manufacturing productivity.

I used the multiple regression model (1) for the West Bank and (2) for the Gaza Strip,

To examine the relationship between the controlled dual-use list restrictions, import restrictions, and the output of a manufacturing activity, I used the statistical computing software R Studio to carry out the regression calculations. In the model (see table 1 for descriptions of variable symbols), Y represents the dependent variable, NONE, CHEM, NCHEM, BOTH, VAT, CU, and IM are the independent variables, while T, WD, E, and C are the control variables. b 2 , b 3 , and b 4 represent the change in output of a manufacturing activity due to the dual-use goods list; b 5 , b 6 , and b 7 are the change in output as a result of a change in VAT and customs duties paid and imports a firm purchases. As the subscript t labels the variables by year between 2007-2018, the model is optimal because it was able to compare the output of firms exposed to dual-use goods list as the Israeli Government changed the number of items on the list between 2007 and 2015. The model also explains the change in output due to factors other than the dual use goods list and import restrictions such as domestic tax burden, level of conflict, wage differential, and electricity usage.

To gain a deeper understanding of the specific security restrictions Palestinian importers face I compared the security restrictions firms encounter when importing products through Israeli vs Jordanian ports. The research project compares the costs of security inspections at ports and checkpoints along with transportation costs and storage costs due to wait times associated with security measures. First, the research breaks down the security and storage cost of importing a smaller 20ft shipping container vs a 40ft shipping container through Israeli ports vs the security-related cost through Jordanian ports. Lastly, the project compares the similarities and differences of security related costs when Palestinians move products through Jordan-West Bank vs Israeli-West Bank crossings. Analyzing the particular costs firms face when importing breaks down how Israeli security policy may harm the output potential of manufacturing activities reliant on imports. Moreover, comparing the security related costs when Palestinians import through Israel vs Jordan can help scholars understand whether the security-related costs alter the decision of where Palestinians want to import goods.

Analysis

Regression Findings: 2007-2018

The regression examining the impact of a manufacturing activity’s exposure to the dual-use goods list on the activity’s output shows Israel’s implementation of the dual-use goods list is not correlated with a decline in Palestinian manufacturing output. The project regresses separately manufacturing activities in the West Bank and the Gaza Strip. In the West Bank (column 1 of Table 1), Israeli restrictions on Palestinian access to chemical items or non-chemical items negatively relate to output but are statistically insignificant statistical. In the Gaza Strip (column 2 of Table 1), Israeli restrictions on Gazan access to chemical or non-chemical items positively correlates with output but are of statistically insignificance. As the smaller West Bank dual-use goods list has a negative relationship with output while the larger Gaza Strip dual-use goods list has a positive relationship with output, the regression fails to show a larger list of Israeli import restrictions impacts Palestinian output negatively. The lack of correlation between a Palestinian manufacturing firm’s exposure to the dual-use goods list and output reveals Palestinian manufacturing firms are able to increase production despite Israeli import restrictions.

In the West Bank, a manufacturing activity’s importation of products is a stronger predictor of output than exposure to the dual-use goods list. To represent whether a manufacturing activity relies on imports, the regression uses the amount of imports a firm purchases, along with the amount of VAT and customs duties a manufacturing firm pays. Palestinian companies purchasing imports pay VAT and customs duties when the products arrive at port. VAT is a strong predictor of a manufacturing activity’s output as VAT and output are positively correlated with a 99% confidence level (Table 1). The total customs duties a West Bank manufacturing firm pays is positively correlated with output with a 95% confidence level. The positive correlation between the activity’s purchase of imports and output shows manufacturing activities with higher levels of output in the West Bank are larger importers than activities with smaller output levels. As the data shows a correlation between VAT, customs, purchase of imports, and output, the data reveals the West Bank manufacturing sector is reliant upon imports to increase output. Consequently, Israeli imposed security costs at ports or Palestinian crossings would result in extra costs to a vital step of the manufacturing process.

Israel has changed the number of items the Israeli Defense Forces restricts from entering the Gaza Strip multiple times since 2008. Israel has added items to the list when Hamas expands it’s military capabilities and has subtracts items during peace negotiations or at the United Nation’s behest. After Hamas took power in the Gaza Strip in June of 2007, Israel limited imports into Gaza to only humanitarian items including, “ 70 items of foodstuffs, medicines, and grocery items (i.e. detergent, soap)” (Gisha, 2016, p. 1). Israel alleviated the near total blockade in May of 2010; On July 6th, 2010 Israel announced 23 categories of items the IDF would restrict Gazans from importing with the existing dual-use goods list the IDF outlined in 2008 for the Gaza Strip and West Bank (Gisha, 2016, p. 4). In March and November of 2015, Israel added items to the Gaza Strip’s specific dual-use goods list so Israel now restricts 61 categories of items from entering the Gaza Strip (Gisha, 2016, p. 4). The three different versions of the Gazan dual-use goods list means Israel has subjected Gazan manufacturers to three different intensity levels of import restrictions. Consequently, the study can investigate whether the correlation between a Gazan manufacturing activity’s exposure to the dual-use goods list and output vary when the dual-use goods list changes.

The impact of the dual-use goods list and trade restrictions on Gaza Strip manufacturing activities’ output vary based on the amount of items each dual-use goods list blocks. The first list, from 2007-2010, restricts the most items, the 2015-2018 list restricts fewer items, and the list from 2010-2015 is the least restrictive. Even when the project breaks down the regression based on Israeli changes to the Gaza Strip dual-use goods list, exposure to chemical, non-chemical, both types, or no restrictions, all remain statistically insignificant. During the period of Israel's near total blockade on the Gazan import of goods (2007-2010), and since Israel expanded the dual-use goods list (2015-2018), restrictions to chemical products show a negative relationship with output. When the IDF enforced a smaller list of dual-use goods restrictions (2010-2015), chemical, non-chemical, and both types of restrictions had a positive relationship with output but were statistically insignificant. Therefore, a reduction of the number of items as in 2010 may improve the Gazan economy. As the 2007-2010 and 2015-2018 versions of the list fail to correlate significantly with output, the data further proves the addition of items to the dual-use goods list does not restrict growth of Palestinian manufacturing output.

Output of a manufacturing activity in the Gaza Strip correlates more strongly with import purchases and electrical usage than exposure to the dual-use goods list. The study investigates the relationship between Gazan electrical usage and manufacturing output because Israel supplies the Gaza Strip with electricity and power plant fuel. The regression demonstrates manufacturing output in the Gaza Strip increases $41.24 when the amount of electricity a manufacturing activity bought increases $1. Because firms pay the VAT when importing products, the correlation between the total VAT a firm pays and output shows Gazan manufacturing firms with higher output rely on imported products (Table 2). The correlation between a manufacturing firm’s purchase of imports and output was statistically significant during the years of the near total Israeli blockade on imports while not significant between 2010 and 2018. The positive correlation between a manufacturing firm’s electricity usage and output means Israeli restrictions on Gazan electricity allocations limit Gazan manufacturing productivity. Furthermore, the correlation between VAT tax, purchase of imports, and manufacturing output means any additional security costs to Gazan imports would decrease Gazan manufacturing productivity.

Dual-Use Goods Restrictions and the Least Productive Manufacturing Activities

When the research project lists the specific items the dual-use list restricts a firm in the West Bank from obtaining, the list shows West Bank manufacturers with the lowest output more often cite restrictions of non-chemical items as barriers to production. Non-chemical items on the dual-use goods list include items like metals, machinery, vehicles, and tools. In the West Bank, the manufacture of vehicles and trailers, electrical products, machinery, and basic metals are the manufacturing sectors most commonly with the lowest output between 2008 and 2018 (PCBS, 2009, pp. 57-58, 2019, pp. 63-64). The dual-use goods list limits the output of the manufacture of basic metals, machinery, equipment, motor vehicles, trailers, and semi-trailers by restricting non-chemical items like metal working machinery, spare parts, and furnaces (Gisha, n.d.). The most commonly restricted items between the activities were sulfuric acid (an input for all four activities except the manufacture of vehicles and trailers) and lathe machines (an input for all four activities except the manufacture of electrical products) (Gisha, n.d.). As the four lowest output manufacturing activities cite restrictions of non-chemical items as impediments to production, the non-chemical restrictions limit even greater output from the four activities. While non-chemical restrictions on the dual-use list do not cause an overall decrease in manufacturing output, the non-chemical restrictions prevent the manufacturing sector from reaching it’s full productivity potential.

The lowest output activities in the Gaza Strip deal restrictions on construction related items while the West Bank does not deal with equivalent restrictions. The dual-use goods list for the West Bank is the same in 2018 as in 2008; Israel has changed the Gaza Strip dual-use goods list three times since 2008. The Gaza Strip's lowest output manufacturing activities include the manufacture of leather, vehicles and trailers, machinery and equipment, and paper products (PCBS, 2009, pp. 91-92, 2019, pp. 117-118). The manufacture of vehicles, trailers, machinery, and equipment do not have access to construction related inputs due to the 2015 Gazan dual-use goods list (Israel Ministry of Foreign Affairs, 2010). Specifically, Israeli restrictions on items such as, steel products, cables, resins, and sealants in 2010 with restrictions on welding equipment, winches, lifting equipment, copper, and aluminum products in 2015 limit the ability for Gazans to manufacture machinery, equipment, automobiles, and trailers (Israel Ministry of Foreign Affairs, 2010; Gisha, n.d.). As both the lowest productivity West Bank and Gazan manufacturing activities list non-chemical items as key items to production the dual-use goods list restricts, the non-chemical restrictions reduce how much the lowest output manufacturing activities can increase output. As neither Gazan nor West Bank manufacturing industries commonly cite chemical items as products in the production process, the chemical items restrictions do not prevent output growth.

The output of manufacturing sectors using construction-related materials decreased when Israel introduced additional construction or non-chemical items to the dual-use goods list. Manufacturing activities whose output decreased had used restricted construction materials as inputs for production. From 2009 to 2010, when Israel lifted the blockade, the output of manufacturing activities in the Gaza Strip increased from $158,323,400 to $316,646,800 as the blockade on the Gaza Strip limited all manufacturing sector’s access to inputs (PCBS, 2010, p. 91, 2011, pp. 119-120). As Israel’s 2015 expansion of the dual-use good list added Wood panels 1 cm thick and 5 cm wide, the output of furniture manufacturing decreased from $119,071,600 in 2015 to $57,412,500 in 2018 (PCBS, 2016, pp. 116-117, 2017, 2019, pp. 117-118). Manufacturing activities requiring heavy machinery such as electrical equipment manufacturing saw output decrease from 12,241,500 in 2015 to 10,322,100 in 2016; manufacturing of vehicles, trailers, and semi-trailers output decreased from 6,737,400 in 2015 to 5,366,400 in 2016 (PCBS, 2016, pp. 116-117, 2017, p. 115-116). As multiple Gazan manufacturing activities’ output decreased after Israel introduced additional non-chemical items to the Gazan dual-use goods list, Israeli restrictions on non-chemical items prevent the Gazan manufacturing sector from attaining a higher growth rate than possible with the restrictions on non-chemical items. Therefore, an Israeli reduction of restrictions on non-chemical items would increase the output of the Gazan manufacturing sector.

Palestinian Importation of Dual-Use Goods List Items

Israel does not completely block Palestinians from having dual-use list items. The IDF implements different levels of security screenings based on the dual-use good and subsequent destination. To import a dual-use goods list item into the West Bank, the Palestinian company applies for a transfer license from the Coordinator of Government Activities in the Territories’ (COGAT) Exceptions Committee (World Bank, 2019, p. 16). For a Gazan to import a dual-use goods item, the Israeli Customs Authority, Army, and intelligence service all screen the applying Gazan firm (The World Bank, 2019, p. 30-31). When companies import chemical products on the dual-use goods list, firms apply for “dealer permits” from the Israeli Civil Administration and Israeli Security Administration, who perform background checks on the firms (World Bank, 2019, p. 16). However, the IDF has not provided exact details on how Palestinian firms are to obtain a transfer license. Due to the vague description of items on the dual-use goods list, Palestinian manufacturers importing products are often unsure of particular items to declare as dual-use goods and therefore IDF personnel confiscate the dual-use goods without a license at Israeli-Palestinian checkpoints.

While Palestinians can apply to import dual-use goods list items into the West Bank or Gaza Strip, the process is complicated and cumbersome. Licenses for Palestinians to import dual-use goods vary depending on the type of product and destination. The World Bank’sEconomic Monitoring Report to the Ad Hoc Liaison Committee (2019) describes why Palestinians struggle to import dual-use items as COGAT accepted 95% of license applications for Palestinian companies to import dual-use goods in 2013; however, Palestinian companies only submitted 126 license requests. The World Bank labels the procedure for firms to get a license as a “long, nontransparent, and unpredictable.” Lastly, the license is only valid for 45 days and the company has to get permission from COGAT every time the company wants to import dual-use goods list items (World Bank, 2019, p. 16). As Palestinians find the process to obtain a permit as complicated, Palestinians will be less likely to import items on the dual-use goods list. The complicated nature of Israel’s dual-use goods permit policy means Palestinian importers don’t have an efficient mechanism to import dual-use goods list items.

Costs to Palestinians of Importing into the West Bank from Israel

At the Israeli Ports of Haifa and Ashdod Palestinian importers face multiple costs related to Israeli security policy. Foreign companies exporting to the Palestinian territories more often use the Israeli Ports than the Jordanian Port of Aqaba due to lower shipping costs at the Israeli Ports. Mustasim Elagraa, Randa Jamal, and Mahmoud Elkhafif in Trade Facilitation in the Occupied Palestinian Territory: Restrictions and Limitations (2014) describe security restrictions at Israeli ports as after a shipment enters the Port of Haifa or Ashdod, the Palestinian merchant makes an security check appointment with the Israeli Customs Authority and consequently waits one to seven days. At the security check, 70% of Palestinian imports go through a security scanning in electronic detectors while the Israeli Customs Authority either does not inspect or manually inspects the rest of imports. Costs to Palestinian importers from security screenings and manual inspections range from $210-$457 for smaller containers and $242-$484 for larger containers (see table 4) (Elagraa et al., 2014, pp. 14-18). Consequently, Palestinian manufacturers heavily reliant on imports are likely to spend a more revenue on security measures at Israeli ports. As foreign exporters more often ship products to the Ports of Haifa and Ashdod rather than the Port of Aqaba, Palestinian manufacturers reliant on imports are unlikely to be able to avoid the security costs.

West Bank importers acquire high transportation costs due to the limited number of Israeli-West Bank crossings. Israel reduced the number of crossings into the West Bank when Israel constructed the Israeli-West Bank separation wall following the Second Intifada. The 712-kilometer wall separating Israel and the West Bank limits the flow of imports between Israel and the West Bank to a smaller number of commercial crossings (OCHA, n.d.c). The Palestine Trade Center (n.d.) notes Palestinian importers must take less efficient routes to and from the West Bank crossing at one of four commercial crossings: Tarqumia, Betunia, Sha’ar Ephraim, and Al Jalameh (Image 1). The four commercial crossings have specific infrastructural restrictions; specifically, the Taybeh Checkpoint only handles containers for imports while Tarqumia only handles stone shipment containers (Palestine Trade Center, n.d.). As a result, Palestinian importers have to spend extra time and resources traveling to a crossing able to handle the needs of the importer. As the Palestinian importer continuously accumulates extra transportation costs, the costs are likely to reduce the firm’s ability to invest in other business areas.

Moreover, at Israeli ports and Israeli-West Bank checkpoints, Israeli security checks cause delays and higher costs for Palestinian importers. Wait times and infrastructural limitations at Israeli-West Bank crossings result in higher costs at Israeli ports. At Israeli-West Bank commercial crossings, Palestinians importing goods unload shipments from Israeli trucks where Israeli security personnel inspect the cargo and then move the goods to Palestinian trucks (World Bank, 2008, pp. 7-8). As Elagraa et al. (2018) states, the commute from the Israeli ports to the checkpoints is two hours, shipments must leave the ports by 1:00 pm to cross the checkpoints before the crossings close at 5:00 pm. Because of security checks, health, and sanitary screenings, and the limited hours of Israeli-West Bank commercial crossings, Palestinians imports usually have to pay storage fees at Israeli ports (see table 5) (Elagraa et al., 2014, pp. 19-20). The combination of limited hours of Israeli-West Bank checkpoints, security screenings, and delays at Israeli ports means Palestinian importers dedicate multiple days to moving an import from Israeli ports to the West Bank.

Despite the costs and wait times Palestinians face when importing through Israeli ports, imports fuel the Palestinian economy. As the Palestinian economy struggles to expand exports, the Palestinian territories maintain a high trade deficit. The Palestinian economy relies on imports as imports of goods and services accounted to 59.2% of Palestinian GDP in 2015 while exports were 18.3% of GDP (The World Bank, n.d., p. 8). Manufacturing activities in the West Bank and Gaza Strip imported $3,466,168 worth of products in 2008 and $6,539,590 in 2018 (PCBS, 2009, pp. 91-92, 2019, pp. 117-118). Since 2008 Palestinian companies have increasingly relied on imports for production; in the West Bank, the 3 largest manufacturing sectors: the manufacture of non-metallic mineral products, manufacture of food products, and manufacture of furniture had output increase 22%, 16% , and 33% from 2007 to 2018 (PCBS, 2019, p. 37). While the Palestinian economy already relies on imports, lowering security related costs on Palestinian imports would decrease costs for Palestinian manufactures. Consequently, a decrease in costs for Palestinian manufactures would boost the price competitiveness of Palestinian manufacturing products.

Costs to Palestinians of Importing from Jordan to the West Bank

Palestinians moving products into the West Bank from Jordan face costs resulting from the infrastructural limitations of the Jordan-West Bank Crossing at the King Hussein Bridge (KHB). As the KHB is the only crossing between the West Bank and Jordan, the bridge facilitates most trade between the West Bank and Jordan. Because the security scanner at the KHB only handles products on pallets rather than larger shipping containers, merchants at the Port of Aqaba move products from large containers to pallets no taller than 160cm (Palestine Trade Center, 2013, p. 99). As the KHB cannot handle large containers, Israeli Customs personnel divert large imports to the Sheikh Hussein crossing 49 miles from the KHB; consequently, containerized imports incur extra storage, transportation, and Israeli customs fees (Elagraa et al., 2014, p. 22). In 2005 the IDF designated the Palestinian territory near the Damya Bridge Crossing a closed military zone; subsequently, the KHB faces higher traffic and wait times (Palestinian Shippers' Council, 2012, p. 5). The security infrastructure limitations at the KHB mean Palestinians really on Israeli ports when importing goods to large for pallets. Therefore, Israeli security policy directly results in higher costs for Palestinian imports coming through Jordan.

Imports coming into the West Bank from Israeli ports and the Port at Aqaba both face Israeli security restrictions but in different forms. The variance in security-related cost depends on the size of the import. If a product fits onto pallets, then the Palestinian company will face lower costs through Aqaba (only $190) vs either Israeli port (between $210-$484) (Tables 4 and 6). If the product cannot fit onto pallets, Palestinian firms importing through the Port of Aqaba face costs up to $2,296 (Table 6). Security-related costs at Israeli ports usually will not exceed $548 and depend upon the Israeli Customs Authority’s inspection technique of the import (see table 4) along with how long Palestinians store the product at port (see table 5). Therefore, the main security related cost decreasing the competitiveness of firms importing through Jordan vs Israel is inadequate security inspection infrastructure. Thus, the KHB needs a security scanner able to handle shipping containers to decrease the cost to Palestinians of using the Jordanian Port.

While Palestinians face different security-related costs importing through Jordan vs Israel, Palestinians face a handful of common security measures regardless of port. Palestinians face common security measures at Israeli-West Bank and Jordan-West Bank crossings as Israeli security personnel staff all West Bank crossings. At both Israeli-West Bank and Jordanian-West Bank crossings, Palestinian importers unload products from Israeli or Jordanian trucks for Israeli security personnel to manually inspect and repackage into Palestinian trucks (Palestine Trade Center, n.d.). The manual security checks cause potential damage to goods, longer wait times, higher transportation costs, and spoilage of temperature sensitive goods as the checkpoints only provide unrefrigerated storage areas (The World Bank, n.d.). Lastly, Israeli personnel rather than PNA personnel perform security and operations on the Palestinian side of Israeli-West Bank and Jordanian-West Bank commercial checkpoints (The World Bank Group, 2017). Because of the security measures common at all West Bank crossings, Palestinian importers face some security related cost no matter the port a product enters from. Hence, an Israeli streamlining of the security inspection process would reduce costs and wait times at all crossings into the West Bank.

Despite costs due to security restrictions at Israeli ports, overall, Israeli ports are more attractive to shipping companies importing products to the West Bank. The Israeli ports are closer to Palestinian cities like Hebron, Ramallah, and Nablus. The Ports of Ashdod and Haifa are far cheaper than the port of Aqaba when the ship sails from African, American, or European ports. Additionally, the transportation costs to Palestinian cities is lower when the business imports from Israeli Ports vs Aqaba (see Table 7 and 8). When Palestinians import through Israeli ports Palestinian merchants do not have to worry about transferring products from containers to pallets as the Israeli-West Bank crossings are able to handle containers where the KHB cannot (Palestine Trade Center, 2013, p. 108). The higher security costs for containerized products through the Port of Aqaba with higher shipping costs for products going to the Jordanian port mean Palestinians overwhelmingly rely on Israeli ports to import products. Therefore, Palestinian importers are more likely to pay the security-related costs at Israeli ports.

Discussion

Dependency Theory

As Israel collects Palestinian VAT and import taxes while maintaining a security presence around the Palestinian territories, the Palestinian economy is dependent upon Israel for funding and trade access. As Dependency theory outlines how unequal relationships between developed and developing states prevent economic growth in developing countries, the theory is useful to explain Israeli-Palestinian economic relations. Elkhafif et al. (2014) notes despite the 1994 Paris Protocol’s creation of an Israeli-Palestinian customs union, Palestinians cannot carry out customs operations as the Palestinian Authority (PNA) lacks a presence at border-crossing checkpoints and rely on Israel to collect VAT, customs, and import taxes. The customs union promotes Palestinian reliance on Israeli products while making foreign non-Israeli products more expensive as the PNA cannot lower the VAT more than 2% below the Israeli VAT (Elkhafif et al., 2014, pp. 9-10). The present study found Palestinian manufacturers when importing goods face Israeli security related costs whether the Port of entry is Israeli or Jordanian. As Israel’s control of Palestinian VAT and import tax collection allows Israel to control a major Palestinian revenue source, the PNA is dependent upon Israel to fulfill the PNA’s budget. Israeli security measures increasing costs for Palestinian importers shows the unequal customs relationship between Israel and the Palestinian territories negatively impacts Palestinian economic growth as dependency theory states.

Israeli security related costs and exportation of low technology products to the Palestinian territories reduces Palestinian manufacturing competitiveness and benefits Israeli firms as dependency theory states. Israel’s use of its control of access to the West Bank to benefit Israeli firms while raising costs for Palestinian firms aligns with dependency theory’s explanation of how developed states may boost economic growth at the expense of developing states. Security-related costs do not hinder Israeli made imports because Israeli firms pay less in shipping costs and are not subject to the same security restrictions at Israeli Ports as other imports to the Palestinian territories (Bank of Israel, n.d.). Consequently, Israeli exports to the West Bank did not change when Israel implemented security restrictions due to the Second Intifada as imports from Israel were 70% of total imports before the Second Intifada in 1999 and 72% in 2006. (PCBS, 1999, pp. 38-45, 2006, pp. 46-51). Palestinian manufacturing GDP dropped from 21.2% of GDP in 1994 to 12.6% in 2007 (PCBS, 2003, p. 71, 2009, p. 73). Consequently, Israeli security restrictions on Palestinians benefited Israeli security and economic interests but decreased output potential of Palestinian manufacturing firms. Therefore, dependency theory clarifies how Israel’s presence in the Palestinian territories damages Palestinian economic growth potential but benefits Israeli security and economic concerns.

Policy Implications

Israel could reduce transport costs for Palestinian firms by equipping more checkpoints with necessary equipment to handle large imports and export loads. Israel needs to expand the number of checkpoints which can scan larger shipping containers. Containerization allows Palestinians to ship a larger number of products, decreases time spent at checkpoints, and reduces the amount of potential damage to goods (USAID, 2013, pp. 31). The Jordan-West Bank King Hussein Bridge (KHB) Crossing does not have the infrastructure to handle shipping containers while Israeli restrictions on which items Israeli security personnel can use in container scanners limits the use of container scanners at the Taybeh, Tarquima, and Al-Jalameh crossings (Palestine Trade Center, n.d.). A security scanner able to handle shipping containers at the KHB would mean Palestinians importing via Jordan using shipping containers would not have to spend $1,486 to transport goods via the Jordan-Israel Sheikh Hussein Crossing (Palestine Trade Center, 2013, p. 103). An Israeli expansion of security infrastructure to scan shipping containers would limit the number of trips a Palestinian firm would take to transport an equal amount of goods. A King Hussein Bridge Crossing able to handle shipping containers would reduce Palestinian reliance on Israeli ports.

To increase manufacturing production in the Gaza Strip, foreign state and non-state donors need to increase funding to support Gazan electrical and fuel purchases. Egypt, Israel, and the Gaza Strip’s lone power plant supply the Gaza Strip with electricity. In 2019, the Gaza Strip had an average unfulfilled daily electricity demand of 263 megawatts due to fuel shortages at the Gazan power plant and a lack of funding for Gazan electricity purchases from Israel (OCHA, n.d.b). The present study found Gazan manufacturing output increases $41.24 when the amount of electricity a firm purchases increases $1. Offshore natural gas could provide fuel for the Gazan power plant as a European funded pipeline able to transfer 35 billion cubic feet of natural gas a year to the Gaza Strip is under construction; but until the pipeline is completed the Gaza Strip will need international donors to fund Gazan purchases of electricity and fuel (The Times of Israel, 2020). Expanded Gaza Strip electricity access and a subsequent reduction in the amount of power outages would allow Gaza Strip manufacturers to use electricity intensive modes of production and increase productivity. A larger supply of electricity in the Gaza Strip would improve living standards through expanding health, water, and sanitation services’ capabilities in the Gaza Strip.

Conclusion

Palestinians face costs due to increased wait times, limitations of security inspection infrastructure, increased transportation costs, storage fees, and security inspection costs because of Israeli security restrictions regardless of the port Palestinians use to import products. However, Palestinians face higher security costs importing shipping containers through the Jordanian port at Aqaba and lower security costs at the Jordanian Port if the import is shipped on pallets because the Jordanian-West Bank crossing does not have security scanning infrastructure capable to handle shipping containers. If the import can only fit onto shipping containers then the security related cost when Palestinians ship through the Jordanian Port of Aqaba can exceed $2,296 while the cost related to security shipping through the Israeli Ports will not exceed $548 depending on the security risk of the import. But when the import fits onto pallets, the security related costs are $190 through the Jordanian Port and $210-$548 through Israeli Ports. Although, the Ports of Ashdod and Haifa are cheaper than the Jordanian Port when the ship sails from African, American, or European ports. Due to security related costs and lower shipping costs to the Israeli Ports, Palestinians are dependent upon Israeli ports for trade facilitation. As Israel controls access into and out of the West Bank, Palestinian trade access is dependent upon the security concerns of Israel.

References

Adnan, W. (2015). Who gets to cross the border? The impact of mobility restrictions on labor flows in the West Bank. Labour Economics, 34, 86-99. https://doi.org/10.1016/j.labeco.2015.03.016

Al-Falah, B. (2013). Structure of the Palestinian Services Sector and Its Economic Impact. Palestinian Economic Policy Research Institute.

Bank of Israel. (n.d.). Trade links between Israel and the Palestinian Authority. https://www.boi.org.il/he/NewsAndPublications/PressReleases/Documents/Israel-Palestinian%20trade.pdf

BBC (2012, September 24). Israel plans work permits for 5,000 Palestinians in West Bank. https://www.bbc.com/news/world-middle-east-19701090

Berends, G. (2008). Fear and Loading in West Bank/Gaza: The State of Palestinian Trade. Journal of World Trade, 42(1), 151–175. https://doi.org/10.1002/j.1538-165X.2008.tb00632.x

Brym, R. J., & Araj, B. (2008). Palestinian Suicide Bombing Revisited: A Critique of the Outbidding Thesis. Political Science Quarterly, 123(3), 485–500.

B’Tselem. (n.d.). Fatalities in the first Intifada [table]. https://www.btselem.org/statistics/first_intifada_tables

B’tselem. (2017, November 11). Hebron City Center. https://www.btselem.org/hebron

Cali, M., & Miaari, S. H. (2018). The Labour Market Impact of Mobility Restrictions: Evidence from the West Bank. Labour Economics, 51, 136-151. https://doi.org/10.1016/j.labeco.2017.12.005

Central Intelligence Agency. (2020a, January 30). The World Factbook: Gaza Strip. https://www.cia.gov/library/publications/the-world-factbook/geos/gz.html

Central Intelligence Agency. (2020b, January27). The World Factbook: West Bank. https://www.cia.gov/library/publications/the-world-factbook/geos/we.html

Cohen, R. S., Johnson, D. E., Thaler, D. E., Allen, B., Bartels, E. M., Cahill, J., & Efron, S.

(2017). https://doi.org/10.7249/RB9975

Daoudi, H., & Khalidi, R. (2008). The Palestinian War-Torn Economy: Aid, Development and State Formation. A Contrario, 5(1). https://doi.org/10.3917/aco.052.0023

Dos Santos, T. (1970). The Structure of Dependence. American Economics Review, 40(2), 231-236. https://EconPapers.repec.org/RePEc:aea:aecrev:v:60:y:1970:i:2:p:231-36

Elagraa, M., Jamal, R., & Elkhafif, M. (2014). Trade Facilitation in the Occupied Palestinian Territory: Restrictions and Limitations (pp. 1–43). United Nations Conference on Trade and Development (UNCTAD).

Eljafari, M. (2010). Palestinian Capacity Building Needs in Trade Policy and Trade Facilitation. The Journal of World Investment & Trade, 11(5), 731-751. https://doi.org/10.1163/221190010X00383

Elkhafif, M., Misyef, M., & Elagraa, M. (2014). Palestinian Fiscal Revenue Leakage to Israel under the Paris Protocol on Economic Relations (pp. 1–58). United Nations.

Egeonu, P. (2017). Third World Dependency, Theoretical Assumptions and African Underdevelopment: A Critique Analysis. Online Journal of Arts, Management and Social Sciences, 2(2), 16–28.

Gisha. (n.d.). Controlled dual-use items – in English. Gisha.

Gisha. (2016). Dark-gray lists. (pp. 1–6). Gisha. https://gisha.org/publication/4860

Haddad, T. (2018). Palestine Ltd.: Neoliberalism and Nationalism in the Occupied Territory. I.B. Taurus.

Israeli Defenses Forces, (n.d.). The Gaza Tunnel Industry. https://www.idf.il/en/minisites/hamas/hamas/the-gaza-tunnel-industry/#:~:text=One%20of%20the%20most%20pressing,the%20cost%20of%20civilian%20rehabilitation.

International Monetary Fund. (2018). West Bank and Gaza Report to the Ad Hoc Liason

Committee. https://www.imf.org/en/Publications/CR/Issues/2018/09/17/west-bank-gaza-report-to-the-ad-hoc-liaison-committee

Israel Ministry of Foreign Affairs. (2010, July, 04). Gaza: Lists of Controlled Entry Items. https://mfa.gov.il/mfa/foreignpolicy/peace/humanitarian/pages/lists_controlled_entry_items_4-jul-2010.aspx

Lloyd, R. B. (2012). On the Fence: Negotiating Israel's Security Barrier. The Journal of the Middle East and Africa, 3(2), 198-214. https://doi.org/10.1080/21520844.2012.741039

Mansour, H. (2010). The Effects of Labor Supply Shocks on Labor Market Outcomes: Evidence from the Israeli-Palestinian Conflict. Labour Economics, 17, 930-939. https://doi.org/10.1016/j.labeco.2010.04.001

Marcus, R. D. (2017). Learning ‘Under Fire’: Israel’s improvised military adaptation to Hamas tunnel warfare. Journal of Strategic Studies, 42(3-4), 344-370. https://doi.org/10.1080/01402390.2017.1307744

Nashashibi, K., Gal, Y., & Rock, B. (2015). Palestinian- Israeli Economic Relations: Trade and Economic Regime. Office of the Quartet Representative.

Palestinian Central Bureau of Statistics (PCBS). (1999). Registered Foreign Trade Statistics Goods and Services, 2018 Main Results (pp. 1–208). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx

Palestinian Central Bureau of Statistics (PCBS). (2003). National Accounts at Current and Constant Prices - (1994-2000), (pp. 1–101). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx

Palestinian Central Bureau of Statistics (PCBS). (2006). Registered Foreign Trade Statistics Goods and Services, 2018 Main Results (pp. 1–85). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx

Palestinian Central Bureau of Statistics. (2009). Economic Surveys Series, 2008 Main Results (pp. 1–144). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx

Palestinian Central Bureau of Statistics. (2010). Economic Surveys Series, 2009 Main Results (pp. 1–145). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx

Palestinian Central Bureau of Statistics. (2011). Economic Surveys Series, 2010 Main Results (pp. 1–182). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx

Palestinian Central Bureau of Statistics. (2016). Economic Surveys Series, 2015 Main Results (pp. 1–179). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx

Palestinian Central Bureau of Statistics. (2017). Economic Surveys Series, 2016 Main Results (pp. 1–183). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx

Palestinian Central Bureau of Statistics. (2019). Economic Surveys Series, 2018 Main Results (pp. 1–186). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx

Palestinian Central Bureau of Statistics. (2019). Palestinian Labour Force Survey Annual Report: 2018 (pp. 1–138). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx

Palestinian Central Bureau of Statistics. (2019). Registered Foreign Trade Statistics Goods and Services, 2018 Main Results (pp. 1–157). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx

Palestinian Shippers' Council. (2012). Capacity Development For Facilitating Palestinian Trade A Study on the Proposed Mobile Scanner at King Hussein Bridge (pp. 1–31). Palestinian Shippers’ Council.

Palestine Trade Center. (n.d.). Export Processing Path through Internal Commercial Crossings. https://www.paltrade.org/en_US/page/export-processing-path-through-internal-commercial-crossings

Palestine Trade Center. (2013). Exporting and Importing Via Jordanian and Israeli Ports Comparison Study (pp. 1–128). https://www.paltrade.org/en_US/page/sector-product-studies

Pressman, J. (2003). The Second Intifada: Background and Causes of the Israeli-Palestinian Conflict. Journal of Conflict Studies, 23(2). https://journals.lib.unb.ca/index.php/JCS/article/view/220

Samhouri, M. (2016). Revisiting the Paris Protocol: Israeli-Palestinian Economic Relations, 1994–2014. The Middle East Journal, 70(4), 679-607. https://www.muse.jhu.edu/article/634691.

Shrum, W. (2001). Science and Development . International Encyclopedia of the Social & Behavioral Sciences. https://doi.org/10.1016/B0-08-043076-7/03165-X

Smith, C. D. (2017). Palestine and the Arab-Israeli conflict a history with documents. Boston: Bedford/St. Martins.

Springer, J. E. (2015). Assessing Donor-driven Reforms in the Palestinian Authority: Building the State or Sustaining Status Quo? Journal of Peacebuilding & Development,10(2), 1-19. https://doi.org/10.1080/15423166.2015.1050796

The Times of Israel. (2019, May 26). Israel re-expands Gaza fishing zone to 15 nautical miles. https://www.timesofisrael.com/israel-re-expands-gaza-fishing-zone-to-15-nautical-miles/#:~:text=Israel%20announced%20Saturday%20night%20that,from%20the%20coastal%20Palestinian%20territory.

The Times of Israel. (January, 12, 2020). Israel pushes forward with plans for new gas pipeline to Gaza – report. (n.d.). https://www.timesofisrael.com/israel-pushes-forward-with-plans-for-new-gas-pipeline-to-gaza-report/

the United Nations Relief and Works Agency for Palestine Refugees in the Near East (UNRWA). (2017a, October 28). UNRWA Condemns Neutrality Violation in Gaza. https://www.unrwa.org/newsroom/official-statements/unrwa-condemns-neutrality-violation-gaza

the United Nations Relief and Works Agency for Palestine Refugees in the Near East (UNRWA). (2017b, June 09). UNRWA Condemns Neutrality Violation in Gaza in the Strongest Possible Terms. https://www.unrwa.org/newsroom/press-releases/unrwa-condemns-neutrality-violation-gaza-strongest-possible-terms

The World Bank. (n.d.). Unlocking the Trade Potential of the Palestinian Economy (pp. 1–116). The World Bank. https://documents.worldbank.org/en/publication/documents-reports/documentdetail/960071513228856631/unlocking-the-trade-potential-of-the-palestinian-economy-immediate-measures-and-a-long-term-vision-to-improve-palestinian-trade-and-economic-outcomes

The World Bank. (2019a). Palestine's Economic Update — October 2019. World Bank. https://www.worldbank.org/en/country/westbankandgaza/publication/economic-update-october-2019

The World Bank. (2019b, April 17).World Bank Calls for Reform to the Dual Use Goods System to Revive a Stagnant Palestinian Economy. https://www.worldbank.org/en/news/press-release/2019/04/17/world-bank-calls-for-reform-to-the-dual-use-goods-system-to-revive-a-stagnant-palestinian-economy

The World Bank Group. (2017). Unlocking the Trade Potential of the Palestinian Economy. Washington D.C.: The World Bank Group. https://documents.worldbank.org/en/publication/documents-reports/documentdetail/960071513228856631/unlocking-the-trade-potential-of-the-palestinian-economy-immediate-measures-and-a-long-term-vision-to-improve-palestinian-trade-and-economic-outcomes

United Nations Conference on Trade and Development (UNCTAD). (2013). Report on Unctad assistance to the Palestinian people: developments in the economy of the Occupied Palestinian Territory (pp. 1–19). Geneva.

UNData. (n.d.). State of Palestine. https://data.un.org/en/iso/ps.html

United Nations (n.d.). The Gaza Reconstruction Mechanism. Retrieved from https://grm.report

United Nations Office for Project Services. (n.d.a). GRM. https://grm.report

United Nations Office for Project Services. (n.d.b) The GRM Process. https://grm.report

United Nations Office for the Coordination of Humanitarian Affairs Occupied Palestinian territory (OCHA). (n.d.a). Data on casualties. https://www.ochaopt.org/data/casualties

United Nations Office for the Coordination of Humanitarian Affairs (OCHA). (n.d.b). Gaza Strip electricity supply. https://www.ochaopt.org/page/gaza-strip-electricity-supply

United Nations Office for the Coordination of Humanitarian Affairs Occupied Palestinian

territory (OCHA). (n.d.c). West Bank Barrier. https://www.ochaopt.org/theme/west-bank-

barrier

USAID. (2013). Trade Facilitation Project (pp. 1–52). USAID.

World Bank. (2008). West Bank and Gaza Palestinian Trade: West Bank Routes (pp. 1–29). World Bank.

World Bank. (2019). Economic Monitoring Report to the Ad Hoc Liaison Committee (English) (pp. 1-41). World Bank Group. https://www.worldbank.org/en/country/westbankandgaza/publication/economic-monitoring-report-to-the-ad-hoc-liaison-committee-april-2019

Zilber, N., & Al-Omari, G. (2018). State With No Army Army With No State. Washington D.C.: The Washington Institute For Near East Policy. washingtoninstitute.org/policy-analysis/view/state-with-no-army-army-with-no-state

Zilberfarb, B.-Z. (2018). The short- and long-term effects of the Six Day War on the Israeli economy. Israel Affairs, 24(5), 785–798. https://doi.org/10.1080/13537121.2018.1505476

Endnotes

1.) The dual-use goods list consists of items the Israeli Government thinks Palestinians could use “for the development, production, installation or enhancement of military capabilities” in addition to the civilian use of the item (Israel Ministry of Foreign Affairs, 2010)."	http://www.inquiriesjournal.com/articles/1869/a-barrier-to-prosperity-analyzing-the-impact-of-israeli-security-policy-on-palestinian-manufacturing-productivity	"Israel has increased the nation’s security presence around the Gaza Strip and in the West Bank. Here, the research project analyzes how transaction costs resulting from Israeli security policy impact the output of manufacturing activities in the Palestinian territories. The study examines transaction costs in the form of import restrictions and items Israel restricts from entering the Palestinian territories in the form of the dual-use goods list. Existing research outlines trade restrictions from Israeli security policy but does not explain how the restrictions impact Palestinian manufacturing productivity. I use a multivariable regression model to look for a correlation between a manufacturing activity’s exposure to Israeli security restrictions, in the form of the controlled dual-use goods list and import restrictions and a decrease in output. The regression breaks down the type of restriction a manufacturing activity could face into no restrictions, restrictions on chemical items, restriction on non-chemical items or restrictions on both chemical and non-chemical items. Furthermore, the project compares costs, procedures, processing times, and documentation associated with Palestinian importation through Israeli vs Jordanian ports. The study found a manufacturing activity's exposure to the dual-use goods list was not a statistically significant predictor of outcome. However, Palestinians face costs due to increased wait times, limitations of port security inspection infrastructure, increased transportation costs, storage fees, and security inspection costs as a result of Israeli security restrictions regardless of the port Palestinians use to import products. These findings demonstrate the need for the Israeli Government to increase the infrastructural capabilities and operating hours of Palestinian border crossings.
Today the Palestinian economy relies on the services sector for employment and output. Governmental social services are a staple in both the West Bank and Gazan economies. The expansion of the service sector means the manufacturing sector now plays a smaller role in the overall Palestinian GDP. In 2019, the service sector accounted for 73.4% Palestinian GDP while the agricultural sector decreased from 5.8% to 3.4% from 2005 to 2019 and the manufacturing sector from 25.8% to 22.9% (UNData, n.d.). Governmental social services make up 60% of the Gazan service sector GDP and 32% of West Bank service sector GDP (Al-Falah, 2013). While the Palestinian Authority (PNA) ran a debt-to-GDP ratio of 16% in 2018, decreases in foreign aid to the PNA means the budget deficit will enlarge (International Monetary Fund, 2018). As a result of the agricultural and manufacturing sectors’ decrease in output, a smaller PNA budget poses a large risk to the economic stability of the West Bank. Lastly, Israeli withholding of Palestinian import and value added taxes risks Palestinian job security due to the West Bank labor force’s public-sector employment reliance.
As Israel collects Palestinian import taxes, Israel also decides what imports Palestinians can bring into the Palestinian territories. Israel restricts items Palestinians may use in a military fashion via the dual-use restrictions list. The number of items Israel restricts vary based on the security threat of the Palestinian territory. As of April 17th, 2019, Israel restricted the import of 56 categories of items into the West Bank and 61 Gaza Strip specific restricted categories of items (The World Bank, 2019b). Israel first outlined the dual-use restrictions list in the 2007 Defense Export Control Law and the 2008 Defense Export Control Order (Gisha, n.d.). However, in July of 2010 Israel announced a Gaza Strip specific dual-use goods list in addition to the existing list (Gisha, 2016, p. 4). In March and November of 2015, the Israeli Defense Forces (IDF) added items to the Gaza Strip specific dual-use goods list (see list 3 Appendix B) (Gisha, 2016, p. 4). Because Hamas continues to pose a security threat to Israel, Israel will likely maintain restrictions on Gazan importation of military use items. Because Israel will likely not remove security restrictions on movement and trade in the near future, an analysis of the relationship between the security restrictions and Palestinian economic output may help academics understand the cost of Israel’s security presence on the Palestinian population.
This research project investigates how transaction costs from Israeli security policy impact manufacturing output in the West Bank and Gaza Strip. Specifically, the research examines how costs resulting from Israeli security policy impact the cost of importing for the Palestinian manufacturing sector. I use a multivariable regression model to look for a correlation between a manufacturing firm’s exposure to Israeli security restrictions, in the form of the controlled dual-use items list and import restrictions, and a decrease in output. Next, the project compares security related costs such as costs due to increased wait times, increased transportation costs, storage fees, and security inspection costs when Palestinian importers import through Israeli vs the Jordanian Ports The project uses dependency theory to show how Israeli security and economic concerns limit the growth potential of the Palestinian economy. Through understanding the causes of Palestinian economic struggles policymakers in Israel, the Palestinian territories, and the United States can decrease poverty and unemployment in the Palestinian territories. Together with improved Palestinian governance, the Palestinian economy will be able to support local Palestinian labor forces and increase economic growth.
The paper argues the dual-use goods list does not negatively correlate with manufacturing output in the Palestinian territories while Israeli security restrictions at ports and checkpoints directly increase costs for manufacturing activities. The dual-use goods list is the list of items Israel restricts from entry into the Palestinian territories because Palestinians could use the items for military purposes. The Literature Review covers existing academic scholarship on how the Israel-Palestinian customs union shaped Israeli-Palestinian economic relations and the relationship between Israeli security restrictions and Palestinian trade. The Analysis section runs a regression analyzing how a manufacturing activity’s exposure to the dual-use goods list impacts the activity’s output. The section compares the similarities and differences in security-related costs when a firm imports goods through Israeli vs Jordanian ports. The Discussion examines the contributions of the dependency theory to understanding Israeli-Palestinian trade relations and provides recommendations to decrease security-related costs.
Literature Review
Scholars argue the customs union established by the Paris Protocol on economic relations solidified an asymmetrical arrangement benefiting the Israeli economy at the expense of the Palestinian economy. The agreement still defines economic relations between Israel and the Palestinian territories today. Haddad (2016) and Samhouri (2016) argue the Paris Protocol flooded the Palestinian markets with cheap Israeli made products, consequently limiting the competitiveness of the Palestinian manufacturing sector (p. 102; pp. 581-583). Also, the Paris Protocols limited the ability of the PNA to by adjust instruments like fiscal, monetary, trade, and labor policy (Samhouri, 2016, p. 604). Similarly, under the Paris Protocol, Israel retained control over collecting Value-Added Taxes (VAT) and import taxes (Haddad, 2016, p. 102). While the literature covering the Paris Protocols answers how economic integration limits the competitiveness of the Palestinian manufacturing sector; the literature does not answer how hostilities between Israel and the Palestinian territories impact the customs union. To understand the production capability of the Palestinian manufacturing sector, scholars need to examine if Israeli security restrictions on the Palestinian territories limit the ability of Palestinian firms to move goods through Israeli territory under the customs union.
Additionally, Scholars are investigating whether limiting Palestinian access to Trade Facilitation Services (TFS) reduces exportation to international markets. Scholars have an interest in Palestinian access to TFS because the West Bank relies on Israel for most TFS. Previous studies evaluated how Israeli security concerns about the Palestinian territories alter Palestinian access to TFS and subsequently reduce Palestinian international market access (Eljafari, 2010, p. 748). Berends (2008) examines the Israeli-Palestinian TFS relationship by analyzing remedies to maximize international market access such as opening the Rafah crossing in Gaza for exports, and improving trade services (Berends, 2008, pp. 152-153). Research analyzing how easily Palestinians are able to access to TFS when importing could reveal if Israeli security policy limits access to inputs for Palestinian manufacturers. Further literature analyzing the ability of Palestinian businesses to access inputs for production may explain changes in the output of the manufacturing sector.
Existing academics frame integration between the Israeli and Palestinian economies through the lens of dependency theory. Dependency theory argues the underdevelopment of developing states is due to interactions between dominant and dependent states which reinforce unequal patterns of development. Past research argues the Palestinian economy is dependent on income earned by Palestinian laborers working in Israel (Zilberfarb, 2018, p. 785). The PNA is also dependent upon development focused foreign aid from developed countries and the United Nations to fill the government’s budget (Springer, 2015, p. 4). Lastly, scholars argue the PNA budget is dependent on VAT and import taxes Israel collects and has withheld in the past to punish to the PNA (Daoudi & Khalidi, 2008). However, existing research does not analyze through the lens of dependency theory how Israeli security restrictions on products entering the Palestinian territories limits economic output. Also, the literature does not examine whether the Israeli-Palestinian customs union creates a Palestinian reliance on Israeli trade facilitation.
This research adds to existing literature on barriers to Palestinian long-term economic growth by analyzing how transaction costs from Israeli security policy limit Palestinian manufacturing productivity. Existing research outlines trade restrictions from Israeli security policy but does not explain how the restrictions impact Palestinian manufacturing productivity. Furthermore, scholars have examined how the Israeli-Palestinian customs union decreases Palestinian manufacturing productivity but do not incorporate Israeli security restrictions into the research. Other literature has investigated how Palestinian reliance on Israeli TFS reduces Palestinian export access, but the research does not state whether Palestinians face increased security costs when importing using Israeli TFS. The present research analyzes the impact of Israeli security restrictions on Palestinian economic output and the Israeli-Palestinian customs union through the lens of dependency theory. My research will add to existing research by identifying how Israeli security policy impacts the output of Palestinian firms. Israel and Palestinian policymakers can only boost Palestinian economic development after policymakers understand the causes behind the Palestinian economic stagnation. Economic development would improve the quality of life in Palestinian territories while reducing Israeli security concerns.
Research Design
Dependency theory explains how unequal relationships between the developed and developing states in the world prevent economic growth of developing countries. The theory explains how the lack of growth in developing states comes at the benefit of developed states. Dependency theory labels developed as core states and developing states as periphery states (Shrum, 2001). Dependency theorists argue the impediments to economic growth in periphery states are imposed by developed countries who are pursuing domestic economic interests (Egeonu, 2017, p. 17). Dependency theorist Theontonio Dos Santos outlines the dependence of periphery states as “a situation in which the economy of certain countries (periphery states) is conditioned by the development and expansion of another economy (core states) to which the former is subjected” (Dos Santos, 1970, p. 231). Dependency is useful to understand how the Palestinian reliance on Israel for trade access alters output growth potential in the Palestinian territories. Moreover, the theory is useful to explain whether the Palestinian territories suffer from a reliance on Israel for customs duties, VAT, and import tax collection.
I collected data from the Palestinian Central Bureau of Statistics (PCBS) on the output, VAT, domestic taxes, and customs taxes of 30 West Bank and Gazan manufacturing activities (see Appendix A). The Palestinian Trade Center (PALTRADE) and the United Nations Conference on Trade and Development (UNCTAD) outlined specific costs associated with Israeli security restrictions. I listed items the Israeli Coordinator of Government Activities (COGAT) added to the Israeli dual use items list between 2007 to 2018. The dual use items list outlines items COGAT restricts Palestinians from importing because of Israeli Government fears Palestinians may use the items for military purposes (Gisha, n.d.). I gathered data from the PCBS’ Economic Surveys Series between 2007 and 2018 on the amount of customs taxes, VAT, and import purchases of a given manufacturing activity to determine whether a manufacturing activity was exposed to Israeli import restrictions. I gathered data on transportation, storage, and inspection costs as a result of Israeli security policy and the infrastructural capabilities of border crossings into the Palestinian territories from PALTRADE and UNCTAD. The time span between 2007 and 2018 is ideal to look for a potential relationship between Israeli security policy and Palestinian manufacturing output because Israel changed items listed on the dual-use items list in 2007, 2010, and 2015. A breakdown of specific costs to Palestinian importers as a result of Israeli security measures allows the project the compare the cost of importing through Israel vs Jordan.
To measure the impact of Israeli security restrictions on the productivity of the Palestinian manufacturing sector, I created a spreadsheet consolidating the output of each manufacturing activity and used binary coding to apply the import and controlled dual-use items restrictions to the activities. I used a multivariable regression model to look for a correlation between a manufacturing activity’s exposure to Israeli security restrictions, as in the controlled dual-use items list and import restrictions, and a change in output. I separated dual-use goods list restrictions into no restrictions, restrictions on chemical items, restrictions on non-chemical items or restrictions on chemical and non-chemical items. I coded a manufacturing activity a one in the categories NONE, CHEM, NCHEM, or BOTH based on whether the firm use a dual use goods list item in the production process and a zero in the remaining categories (see Table 1). I used VAT, import purchases, and customs taxes as a proxy for exposure to import restrictions. A multivariable regression is optimal because through statistical significance, a regression describes whether any potential relationship between a manufacturing activity’s exposure to security restrictions and the manufacturing activity’s output is causal or not.
The regression controlled for the C, E, WD, and T to account for other variables that might decrease the output of manufacturing firms (see Table 1). The variable WD is only used in regard to the West Bank while E is only used with the Gaza Strip. I used the yearly number of casualties due to the Israeli-Palestinian Conflict from the United Nations Office for the Coordination of Humanitarian Affairs (OCHA) and the Israeli NGO, B’Tselem, as a proxy to represent changes in the conflict level between Israel and the Palestine (B’Tselem, n.d.; OCHA, n.d.a). Next, I controlled for differences in wages between Israel and the West Bank for the same manufacturing activity from the Palestinian Labor Force Survey, between 2007 and 2018. From the Economic Surveys Series, I collected data on the domestic taxes each manufacturing activity paid per year not including the VAT or customs duties. I held constant both taxes and monetary compensation to understand the relationship between Israeli security restrictions and the output of the Palestinian manufacturing sector because low domestic taxes and low wages make the two sectors competitive in the region. I controlled for the level of violence in the Israeli-Palestinian Conflict per year because escalations in the conflict may decrease productivity and the present research wants to isolate the impact of the import restrictions and the dual-use list on Palestinian manufacturing productivity.
I used the multiple regression model (1) for the West Bank and (2) for the Gaza Strip,
To examine the relationship between the controlled dual-use list restrictions, import restrictions, and the output of a manufacturing activity, I used the statistical computing software R Studio to carry out the regression calculations. In the model (see table 1 for descriptions of variable symbols), Y represents the dependent variable, NONE, CHEM, NCHEM, BOTH, VAT, CU, and IM are the independent variables, while T, WD, E, and C are the control variables. b 2 , b 3 , and b 4 represent the change in output of a manufacturing activity due to the dual-use goods list; b 5 , b 6 , and b 7 are the change in output as a result of a change in VAT and customs duties paid and imports a firm purchases. As the subscript t labels the variables by year between 2007-2018, the model is optimal because it was able to compare the output of firms exposed to dual-use goods list as the Israeli Government changed the number of items on the list between 2007 and 2015. The model also explains the change in output due to factors other than the dual use goods list and import restrictions such as domestic tax burden, level of conflict, wage differential, and electricity usage.
To gain a deeper understanding of the specific security restrictions Palestinian importers face I compared the security restrictions firms encounter when importing products through Israeli vs Jordanian ports. The research project compares the costs of security inspections at ports and checkpoints along with transportation costs and storage costs due to wait times associated with security measures. First, the research breaks down the security and storage cost of importing a smaller 20ft shipping container vs a 40ft shipping container through Israeli ports vs the security-related cost through Jordanian ports. Lastly, the project compares the similarities and differences of security related costs when Palestinians move products through Jordan-West Bank vs Israeli-West Bank crossings. Analyzing the particular costs firms face when importing breaks down how Israeli security policy may harm the output potential of manufacturing activities reliant on imports. Moreover, comparing the security related costs when Palestinians import through Israel vs Jordan can help scholars understand whether the security-related costs alter the decision of where Palestinians want to import goods.
Analysis
Regression Findings: 2007-2018
The regression examining the impact of a manufacturing activity’s exposure to the dual-use goods list on the activity’s output shows Israel’s implementation of the dual-use goods list is not correlated with a decline in Palestinian manufacturing output. The project regresses separately manufacturing activities in the West Bank and the Gaza Strip. In the West Bank (column 1 of Table 1), Israeli restrictions on Palestinian access to chemical items or non-chemical items negatively relate to output but are statistically insignificant statistical. In the Gaza Strip (column 2 of Table 1), Israeli restrictions on Gazan access to chemical or non-chemical items positively correlates with output but are of statistically insignificance. As the smaller West Bank dual-use goods list has a negative relationship with output while the larger Gaza Strip dual-use goods list has a positive relationship with output, the regression fails to show a larger list of Israeli import restrictions impacts Palestinian output negatively. The lack of correlation between a Palestinian manufacturing firm’s exposure to the dual-use goods list and output reveals Palestinian manufacturing firms are able to increase production despite Israeli import restrictions.
In the West Bank, a manufacturing activity’s importation of products is a stronger predictor of output than exposure to the dual-use goods list. To represent whether a manufacturing activity relies on imports, the regression uses the amount of imports a firm purchases, along with the amount of VAT and customs duties a manufacturing firm pays. Palestinian companies purchasing imports pay VAT and customs duties when the products arrive at port. VAT is a strong predictor of a manufacturing activity’s output as VAT and output are positively correlated with a 99% confidence level (Table 1). The total customs duties a West Bank manufacturing firm pays is positively correlated with output with a 95% confidence level. The positive correlation between the activity’s purchase of imports and output shows manufacturing activities with higher levels of output in the West Bank are larger importers than activities with smaller output levels. As the data shows a correlation between VAT, customs, purchase of imports, and output, the data reveals the West Bank manufacturing sector is reliant upon imports to increase output. Consequently, Israeli imposed security costs at ports or Palestinian crossings would result in extra costs to a vital step of the manufacturing process.
Israel has changed the number of items the Israeli Defense Forces restricts from entering the Gaza Strip multiple times since 2008. Israel has added items to the list when Hamas expands it’s military capabilities and has subtracts items during peace negotiations or at the United Nation’s behest. After Hamas took power in the Gaza Strip in June of 2007, Israel limited imports into Gaza to only humanitarian items including, “ 70 items of foodstuffs, medicines, and grocery items (i.e. detergent, soap)” (Gisha, 2016, p. 1). Israel alleviated the near total blockade in May of 2010; On July 6th, 2010 Israel announced 23 categories of items the IDF would restrict Gazans from importing with the existing dual-use goods list the IDF outlined in 2008 for the Gaza Strip and West Bank (Gisha, 2016, p. 4). In March and November of 2015, Israel added items to the Gaza Strip’s specific dual-use goods list so Israel now restricts 61 categories of items from entering the Gaza Strip (Gisha, 2016, p. 4). The three different versions of the Gazan dual-use goods list means Israel has subjected Gazan manufacturers to three different intensity levels of import restrictions. Consequently, the study can investigate whether the correlation between a Gazan manufacturing activity’s exposure to the dual-use goods list and output vary when the dual-use goods list changes.
The impact of the dual-use goods list and trade restrictions on Gaza Strip manufacturing activities’ output vary based on the amount of items each dual-use goods list blocks. The first list, from 2007-2010, restricts the most items, the 2015-2018 list restricts fewer items, and the list from 2010-2015 is the least restrictive. Even when the project breaks down the regression based on Israeli changes to the Gaza Strip dual-use goods list, exposure to chemical, non-chemical, both types, or no restrictions, all remain statistically insignificant. During the period of Israel's near total blockade on the Gazan import of goods (2007-2010), and since Israel expanded the dual-use goods list (2015-2018), restrictions to chemical products show a negative relationship with output. When the IDF enforced a smaller list of dual-use goods restrictions (2010-2015), chemical, non-chemical, and both types of restrictions had a positive relationship with output but were statistically insignificant. Therefore, a reduction of the number of items as in 2010 may improve the Gazan economy. As the 2007-2010 and 2015-2018 versions of the list fail to correlate significantly with output, the data further proves the addition of items to the dual-use goods list does not restrict growth of Palestinian manufacturing output.
Output of a manufacturing activity in the Gaza Strip correlates more strongly with import purchases and electrical usage than exposure to the dual-use goods list. The study investigates the relationship between Gazan electrical usage and manufacturing output because Israel supplies the Gaza Strip with electricity and power plant fuel. The regression demonstrates manufacturing output in the Gaza Strip increases $41.24 when the amount of electricity a manufacturing activity bought increases $1. Because firms pay the VAT when importing products, the correlation between the total VAT a firm pays and output shows Gazan manufacturing firms with higher output rely on imported products (Table 2). The correlation between a manufacturing firm’s purchase of imports and output was statistically significant during the years of the near total Israeli blockade on imports while not significant between 2010 and 2018. The positive correlation between a manufacturing firm’s electricity usage and output means Israeli restrictions on Gazan electricity allocations limit Gazan manufacturing productivity. Furthermore, the correlation between VAT tax, purchase of imports, and manufacturing output means any additional security costs to Gazan imports would decrease Gazan manufacturing productivity.
Dual-Use Goods Restrictions and the Least Productive Manufacturing Activities
When the research project lists the specific items the dual-use list restricts a firm in the West Bank from obtaining, the list shows West Bank manufacturers with the lowest output more often cite restrictions of non-chemical items as barriers to production. Non-chemical items on the dual-use goods list include items like metals, machinery, vehicles, and tools. In the West Bank, the manufacture of vehicles and trailers, electrical products, machinery, and basic metals are the manufacturing sectors most commonly with the lowest output between 2008 and 2018 (PCBS, 2009, pp. 57-58, 2019, pp. 63-64). The dual-use goods list limits the output of the manufacture of basic metals, machinery, equipment, motor vehicles, trailers, and semi-trailers by restricting non-chemical items like metal working machinery, spare parts, and furnaces (Gisha, n.d.). The most commonly restricted items between the activities were sulfuric acid (an input for all four activities except the manufacture of vehicles and trailers) and lathe machines (an input for all four activities except the manufacture of electrical products) (Gisha, n.d.). As the four lowest output manufacturing activities cite restrictions of non-chemical items as impediments to production, the non-chemical restrictions limit even greater output from the four activities. While non-chemical restrictions on the dual-use list do not cause an overall decrease in manufacturing output, the non-chemical restrictions prevent the manufacturing sector from reaching it’s full productivity potential.
The lowest output activities in the Gaza Strip deal restrictions on construction related items while the West Bank does not deal with equivalent restrictions. The dual-use goods list for the West Bank is the same in 2018 as in 2008; Israel has changed the Gaza Strip dual-use goods list three times since 2008. The Gaza Strip's lowest output manufacturing activities include the manufacture of leather, vehicles and trailers, machinery and equipment, and paper products (PCBS, 2009, pp. 91-92, 2019, pp. 117-118). The manufacture of vehicles, trailers, machinery, and equipment do not have access to construction related inputs due to the 2015 Gazan dual-use goods list (Israel Ministry of Foreign Affairs, 2010). Specifically, Israeli restrictions on items such as, steel products, cables, resins, and sealants in 2010 with restrictions on welding equipment, winches, lifting equipment, copper, and aluminum products in 2015 limit the ability for Gazans to manufacture machinery, equipment, automobiles, and trailers (Israel Ministry of Foreign Affairs, 2010; Gisha, n.d.). As both the lowest productivity West Bank and Gazan manufacturing activities list non-chemical items as key items to production the dual-use goods list restricts, the non-chemical restrictions reduce how much the lowest output manufacturing activities can increase output. As neither Gazan nor West Bank manufacturing industries commonly cite chemical items as products in the production process, the chemical items restrictions do not prevent output growth.
The output of manufacturing sectors using construction-related materials decreased when Israel introduced additional construction or non-chemical items to the dual-use goods list. Manufacturing activities whose output decreased had used restricted construction materials as inputs for production. From 2009 to 2010, when Israel lifted the blockade, the output of manufacturing activities in the Gaza Strip increased from $158,323,400 to $316,646,800 as the blockade on the Gaza Strip limited all manufacturing sector’s access to inputs (PCBS, 2010, p. 91, 2011, pp. 119-120). As Israel’s 2015 expansion of the dual-use good list added Wood panels 1 cm thick and 5 cm wide, the output of furniture manufacturing decreased from $119,071,600 in 2015 to $57,412,500 in 2018 (PCBS, 2016, pp. 116-117, 2017, 2019, pp. 117-118). Manufacturing activities requiring heavy machinery such as electrical equipment manufacturing saw output decrease from 12,241,500 in 2015 to 10,322,100 in 2016; manufacturing of vehicles, trailers, and semi-trailers output decreased from 6,737,400 in 2015 to 5,366,400 in 2016 (PCBS, 2016, pp. 116-117, 2017, p. 115-116). As multiple Gazan manufacturing activities’ output decreased after Israel introduced additional non-chemical items to the Gazan dual-use goods list, Israeli restrictions on non-chemical items prevent the Gazan manufacturing sector from attaining a higher growth rate than possible with the restrictions on non-chemical items. Therefore, an Israeli reduction of restrictions on non-chemical items would increase the output of the Gazan manufacturing sector.
Palestinian Importation of Dual-Use Goods List Items
Israel does not completely block Palestinians from having dual-use list items. The IDF implements different levels of security screenings based on the dual-use good and subsequent destination. To import a dual-use goods list item into the West Bank, the Palestinian company applies for a transfer license from the Coordinator of Government Activities in the Territories’ (COGAT) Exceptions Committee (World Bank, 2019, p. 16). For a Gazan to import a dual-use goods item, the Israeli Customs Authority, Army, and intelligence service all screen the applying Gazan firm (The World Bank, 2019, p. 30-31). When companies import chemical products on the dual-use goods list, firms apply for “dealer permits” from the Israeli Civil Administration and Israeli Security Administration, who perform background checks on the firms (World Bank, 2019, p. 16). However, the IDF has not provided exact details on how Palestinian firms are to obtain a transfer license. Due to the vague description of items on the dual-use goods list, Palestinian manufacturers importing products are often unsure of particular items to declare as dual-use goods and therefore IDF personnel confiscate the dual-use goods without a license at Israeli-Palestinian checkpoints.
While Palestinians can apply to import dual-use goods list items into the West Bank or Gaza Strip, the process is complicated and cumbersome. Licenses for Palestinians to import dual-use goods vary depending on the type of product and destination. The World Bank’sEconomic Monitoring Report to the Ad Hoc Liaison Committee (2019) describes why Palestinians struggle to import dual-use items as COGAT accepted 95% of license applications for Palestinian companies to import dual-use goods in 2013; however, Palestinian companies only submitted 126 license requests. The World Bank labels the procedure for firms to get a license as a “long, nontransparent, and unpredictable.” Lastly, the license is only valid for 45 days and the company has to get permission from COGAT every time the company wants to import dual-use goods list items (World Bank, 2019, p. 16). As Palestinians find the process to obtain a permit as complicated, Palestinians will be less likely to import items on the dual-use goods list. The complicated nature of Israel’s dual-use goods permit policy means Palestinian importers don’t have an efficient mechanism to import dual-use goods list items.
Costs to Palestinians of Importing into the West Bank from Israel
At the Israeli Ports of Haifa and Ashdod Palestinian importers face multiple costs related to Israeli security policy. Foreign companies exporting to the Palestinian territories more often use the Israeli Ports than the Jordanian Port of Aqaba due to lower shipping costs at the Israeli Ports. Mustasim Elagraa, Randa Jamal, and Mahmoud Elkhafif in Trade Facilitation in the Occupied Palestinian Territory: Restrictions and Limitations (2014) describe security restrictions at Israeli ports as after a shipment enters the Port of Haifa or Ashdod, the Palestinian merchant makes an security check appointment with the Israeli Customs Authority and consequently waits one to seven days. At the security check, 70% of Palestinian imports go through a security scanning in electronic detectors while the Israeli Customs Authority either does not inspect or manually inspects the rest of imports. Costs to Palestinian importers from security screenings and manual inspections range from $210-$457 for smaller containers and $242-$484 for larger containers (see table 4) (Elagraa et al., 2014, pp. 14-18). Consequently, Palestinian manufacturers heavily reliant on imports are likely to spend a more revenue on security measures at Israeli ports. As foreign exporters more often ship products to the Ports of Haifa and Ashdod rather than the Port of Aqaba, Palestinian manufacturers reliant on imports are unlikely to be able to avoid the security costs.
West Bank importers acquire high transportation costs due to the limited number of Israeli-West Bank crossings. Israel reduced the number of crossings into the West Bank when Israel constructed the Israeli-West Bank separation wall following the Second Intifada. The 712-kilometer wall separating Israel and the West Bank limits the flow of imports between Israel and the West Bank to a smaller number of commercial crossings (OCHA, n.d.c). The Palestine Trade Center (n.d.) notes Palestinian importers must take less efficient routes to and from the West Bank crossing at one of four commercial crossings: Tarqumia, Betunia, Sha’ar Ephraim, and Al Jalameh (Image 1). The four commercial crossings have specific infrastructural restrictions; specifically, the Taybeh Checkpoint only handles containers for imports while Tarqumia only handles stone shipment containers (Palestine Trade Center, n.d.). As a result, Palestinian importers have to spend extra time and resources traveling to a crossing able to handle the needs of the importer. As the Palestinian importer continuously accumulates extra transportation costs, the costs are likely to reduce the firm’s ability to invest in other business areas.
Moreover, at Israeli ports and Israeli-West Bank checkpoints, Israeli security checks cause delays and higher costs for Palestinian importers. Wait times and infrastructural limitations at Israeli-West Bank crossings result in higher costs at Israeli ports. At Israeli-West Bank commercial crossings, Palestinians importing goods unload shipments from Israeli trucks where Israeli security personnel inspect the cargo and then move the goods to Palestinian trucks (World Bank, 2008, pp. 7-8). As Elagraa et al. (2018) states, the commute from the Israeli ports to the checkpoints is two hours, shipments must leave the ports by 1:00 pm to cross the checkpoints before the crossings close at 5:00 pm. Because of security checks, health, and sanitary screenings, and the limited hours of Israeli-West Bank commercial crossings, Palestinians imports usually have to pay storage fees at Israeli ports (see table 5) (Elagraa et al., 2014, pp. 19-20). The combination of limited hours of Israeli-West Bank checkpoints, security screenings, and delays at Israeli ports means Palestinian importers dedicate multiple days to moving an import from Israeli ports to the West Bank.
Despite the costs and wait times Palestinians face when importing through Israeli ports, imports fuel the Palestinian economy. As the Palestinian economy struggles to expand exports, the Palestinian territories maintain a high trade deficit. The Palestinian economy relies on imports as imports of goods and services accounted to 59.2% of Palestinian GDP in 2015 while exports were 18.3% of GDP (The World Bank, n.d., p. 8). Manufacturing activities in the West Bank and Gaza Strip imported $3,466,168 worth of products in 2008 and $6,539,590 in 2018 (PCBS, 2009, pp. 91-92, 2019, pp. 117-118). Since 2008 Palestinian companies have increasingly relied on imports for production; in the West Bank, the 3 largest manufacturing sectors: the manufacture of non-metallic mineral products, manufacture of food products, and manufacture of furniture had output increase 22%, 16% , and 33% from 2007 to 2018 (PCBS, 2019, p. 37). While the Palestinian economy already relies on imports, lowering security related costs on Palestinian imports would decrease costs for Palestinian manufactures. Consequently, a decrease in costs for Palestinian manufactures would boost the price competitiveness of Palestinian manufacturing products.
Costs to Palestinians of Importing from Jordan to the West Bank
Palestinians moving products into the West Bank from Jordan face costs resulting from the infrastructural limitations of the Jordan-West Bank Crossing at the King Hussein Bridge (KHB). As the KHB is the only crossing between the West Bank and Jordan, the bridge facilitates most trade between the West Bank and Jordan. Because the security scanner at the KHB only handles products on pallets rather than larger shipping containers, merchants at the Port of Aqaba move products from large containers to pallets no taller than 160cm (Palestine Trade Center, 2013, p. 99). As the KHB cannot handle large containers, Israeli Customs personnel divert large imports to the Sheikh Hussein crossing 49 miles from the KHB; consequently, containerized imports incur extra storage, transportation, and Israeli customs fees (Elagraa et al., 2014, p. 22). In 2005 the IDF designated the Palestinian territory near the Damya Bridge Crossing a closed military zone; subsequently, the KHB faces higher traffic and wait times (Palestinian Shippers' Council, 2012, p. 5). The security infrastructure limitations at the KHB mean Palestinians really on Israeli ports when importing goods to large for pallets. Therefore, Israeli security policy directly results in higher costs for Palestinian imports coming through Jordan.
Imports coming into the West Bank from Israeli ports and the Port at Aqaba both face Israeli security restrictions but in different forms. The variance in security-related cost depends on the size of the import. If a product fits onto pallets, then the Palestinian company will face lower costs through Aqaba (only $190) vs either Israeli port (between $210-$484) (Tables 4 and 6). If the product cannot fit onto pallets, Palestinian firms importing through the Port of Aqaba face costs up to $2,296 (Table 6). Security-related costs at Israeli ports usually will not exceed $548 and depend upon the Israeli Customs Authority’s inspection technique of the import (see table 4) along with how long Palestinians store the product at port (see table 5). Therefore, the main security related cost decreasing the competitiveness of firms importing through Jordan vs Israel is inadequate security inspection infrastructure. Thus, the KHB needs a security scanner able to handle shipping containers to decrease the cost to Palestinians of using the Jordanian Port.
While Palestinians face different security-related costs importing through Jordan vs Israel, Palestinians face a handful of common security measures regardless of port. Palestinians face common security measures at Israeli-West Bank and Jordan-West Bank crossings as Israeli security personnel staff all West Bank crossings. At both Israeli-West Bank and Jordanian-West Bank crossings, Palestinian importers unload products from Israeli or Jordanian trucks for Israeli security personnel to manually inspect and repackage into Palestinian trucks (Palestine Trade Center, n.d.). The manual security checks cause potential damage to goods, longer wait times, higher transportation costs, and spoilage of temperature sensitive goods as the checkpoints only provide unrefrigerated storage areas (The World Bank, n.d.). Lastly, Israeli personnel rather than PNA personnel perform security and operations on the Palestinian side of Israeli-West Bank and Jordanian-West Bank commercial checkpoints (The World Bank Group, 2017). Because of the security measures common at all West Bank crossings, Palestinian importers face some security related cost no matter the port a product enters from. Hence, an Israeli streamlining of the security inspection process would reduce costs and wait times at all crossings into the West Bank.
Despite costs due to security restrictions at Israeli ports, overall, Israeli ports are more attractive to shipping companies importing products to the West Bank. The Israeli ports are closer to Palestinian cities like Hebron, Ramallah, and Nablus. The Ports of Ashdod and Haifa are far cheaper than the port of Aqaba when the ship sails from African, American, or European ports. Additionally, the transportation costs to Palestinian cities is lower when the business imports from Israeli Ports vs Aqaba (see Table 7 and 8). When Palestinians import through Israeli ports Palestinian merchants do not have to worry about transferring products from containers to pallets as the Israeli-West Bank crossings are able to handle containers where the KHB cannot (Palestine Trade Center, 2013, p. 108). The higher security costs for containerized products through the Port of Aqaba with higher shipping costs for products going to the Jordanian port mean Palestinians overwhelmingly rely on Israeli ports to import products. Therefore, Palestinian importers are more likely to pay the security-related costs at Israeli ports.
Discussion
Dependency Theory
As Israel collects Palestinian VAT and import taxes while maintaining a security presence around the Palestinian territories, the Palestinian economy is dependent upon Israel for funding and trade access. As Dependency theory outlines how unequal relationships between developed and developing states prevent economic growth in developing countries, the theory is useful to explain Israeli-Palestinian economic relations. Elkhafif et al. (2014) notes despite the 1994 Paris Protocol’s creation of an Israeli-Palestinian customs union, Palestinians cannot carry out customs operations as the Palestinian Authority (PNA) lacks a presence at border-crossing checkpoints and rely on Israel to collect VAT, customs, and import taxes. The customs union promotes Palestinian reliance on Israeli products while making foreign non-Israeli products more expensive as the PNA cannot lower the VAT more than 2% below the Israeli VAT (Elkhafif et al., 2014, pp. 9-10). The present study found Palestinian manufacturers when importing goods face Israeli security related costs whether the Port of entry is Israeli or Jordanian. As Israel’s control of Palestinian VAT and import tax collection allows Israel to control a major Palestinian revenue source, the PNA is dependent upon Israel to fulfill the PNA’s budget. Israeli security measures increasing costs for Palestinian importers shows the unequal customs relationship between Israel and the Palestinian territories negatively impacts Palestinian economic growth as dependency theory states.
Israeli security related costs and exportation of low technology products to the Palestinian territories reduces Palestinian manufacturing competitiveness and benefits Israeli firms as dependency theory states. Israel’s use of its control of access to the West Bank to benefit Israeli firms while raising costs for Palestinian firms aligns with dependency theory’s explanation of how developed states may boost economic growth at the expense of developing states. Security-related costs do not hinder Israeli made imports because Israeli firms pay less in shipping costs and are not subject to the same security restrictions at Israeli Ports as other imports to the Palestinian territories (Bank of Israel, n.d.). Consequently, Israeli exports to the West Bank did not change when Israel implemented security restrictions due to the Second Intifada as imports from Israel were 70% of total imports before the Second Intifada in 1999 and 72% in 2006. (PCBS, 1999, pp. 38-45, 2006, pp. 46-51). Palestinian manufacturing GDP dropped from 21.2% of GDP in 1994 to 12.6% in 2007 (PCBS, 2003, p. 71, 2009, p. 73). Consequently, Israeli security restrictions on Palestinians benefited Israeli security and economic interests but decreased output potential of Palestinian manufacturing firms. Therefore, dependency theory clarifies how Israel’s presence in the Palestinian territories damages Palestinian economic growth potential but benefits Israeli security and economic concerns.
Policy Implications
Israel could reduce transport costs for Palestinian firms by equipping more checkpoints with necessary equipment to handle large imports and export loads. Israel needs to expand the number of checkpoints which can scan larger shipping containers. Containerization allows Palestinians to ship a larger number of products, decreases time spent at checkpoints, and reduces the amount of potential damage to goods (USAID, 2013, pp. 31). The Jordan-West Bank King Hussein Bridge (KHB) Crossing does not have the infrastructure to handle shipping containers while Israeli restrictions on which items Israeli security personnel can use in container scanners limits the use of container scanners at the Taybeh, Tarquima, and Al-Jalameh crossings (Palestine Trade Center, n.d.). A security scanner able to handle shipping containers at the KHB would mean Palestinians importing via Jordan using shipping containers would not have to spend $1,486 to transport goods via the Jordan-Israel Sheikh Hussein Crossing (Palestine Trade Center, 2013, p. 103). An Israeli expansion of security infrastructure to scan shipping containers would limit the number of trips a Palestinian firm would take to transport an equal amount of goods. A King Hussein Bridge Crossing able to handle shipping containers would reduce Palestinian reliance on Israeli ports.
To increase manufacturing production in the Gaza Strip, foreign state and non-state donors need to increase funding to support Gazan electrical and fuel purchases. Egypt, Israel, and the Gaza Strip’s lone power plant supply the Gaza Strip with electricity. In 2019, the Gaza Strip had an average unfulfilled daily electricity demand of 263 megawatts due to fuel shortages at the Gazan power plant and a lack of funding for Gazan electricity purchases from Israel (OCHA, n.d.b). The present study found Gazan manufacturing output increases $41.24 when the amount of electricity a firm purchases increases $1. Offshore natural gas could provide fuel for the Gazan power plant as a European funded pipeline able to transfer 35 billion cubic feet of natural gas a year to the Gaza Strip is under construction; but until the pipeline is completed the Gaza Strip will need international donors to fund Gazan purchases of electricity and fuel (The Times of Israel, 2020). Expanded Gaza Strip electricity access and a subsequent reduction in the amount of power outages would allow Gaza Strip manufacturers to use electricity intensive modes of production and increase productivity. A larger supply of electricity in the Gaza Strip would improve living standards through expanding health, water, and sanitation services’ capabilities in the Gaza Strip.
Conclusion
Palestinians face costs due to increased wait times, limitations of security inspection infrastructure, increased transportation costs, storage fees, and security inspection costs because of Israeli security restrictions regardless of the port Palestinians use to import products. However, Palestinians face higher security costs importing shipping containers through the Jordanian port at Aqaba and lower security costs at the Jordanian Port if the import is shipped on pallets because the Jordanian-West Bank crossing does not have security scanning infrastructure capable to handle shipping containers. If the import can only fit onto shipping containers then the security related cost when Palestinians ship through the Jordanian Port of Aqaba can exceed $2,296 while the cost related to security shipping through the Israeli Ports will not exceed $548 depending on the security risk of the import. But when the import fits onto pallets, the security related costs are $190 through the Jordanian Port and $210-$548 through Israeli Ports. Although, the Ports of Ashdod and Haifa are cheaper than the Jordanian Port when the ship sails from African, American, or European ports. Due to security related costs and lower shipping costs to the Israeli Ports, Palestinians are dependent upon Israeli ports for trade facilitation. As Israel controls access into and out of the West Bank, Palestinian trade access is dependent upon the security concerns of Israel.
References
Adnan, W. (2015). Who gets to cross the border? The impact of mobility restrictions on labor flows in the West Bank. Labour Economics, 34, 86-99. https://doi.org/10.1016/j.labeco.2015.03.016
Al-Falah, B. (2013). Structure of the Palestinian Services Sector and Its Economic Impact. Palestinian Economic Policy Research Institute.
Bank of Israel. (n.d.). Trade links between Israel and the Palestinian Authority. https://www.boi.org.il/he/NewsAndPublications/PressReleases/Documents/Israel-Palestinian%20trade.pdf
BBC (2012, September 24). Israel plans work permits for 5,000 Palestinians in West Bank. https://www.bbc.com/news/world-middle-east-19701090
Berends, G. (2008). Fear and Loading in West Bank/Gaza: The State of Palestinian Trade. Journal of World Trade, 42(1), 151–175. https://doi.org/10.1002/j.1538-165X.2008.tb00632.x
Brym, R. J., & Araj, B. (2008). Palestinian Suicide Bombing Revisited: A Critique of the Outbidding Thesis. Political Science Quarterly, 123(3), 485–500.
B’Tselem. (n.d.). Fatalities in the first Intifada [table]. https://www.btselem.org/statistics/first_intifada_tables
B’tselem. (2017, November 11). Hebron City Center. https://www.btselem.org/hebron
Cali, M., & Miaari, S. H. (2018). The Labour Market Impact of Mobility Restrictions: Evidence from the West Bank. Labour Economics, 51, 136-151. https://doi.org/10.1016/j.labeco.2017.12.005
Central Intelligence Agency. (2020a, January 30). The World Factbook: Gaza Strip. https://www.cia.gov/library/publications/the-world-factbook/geos/gz.html
Central Intelligence Agency. (2020b, January27). The World Factbook: West Bank. https://www.cia.gov/library/publications/the-world-factbook/geos/we.html
Cohen, R. S., Johnson, D. E., Thaler, D. E., Allen, B., Bartels, E. M., Cahill, J., & Efron, S.
(2017). https://doi.org/10.7249/RB9975
Daoudi, H., & Khalidi, R. (2008). The Palestinian War-Torn Economy: Aid, Development and State Formation. A Contrario, 5(1). https://doi.org/10.3917/aco.052.0023
Dos Santos, T. (1970). The Structure of Dependence. American Economics Review, 40(2), 231-236. https://EconPapers.repec.org/RePEc:aea:aecrev:v:60:y:1970:i:2:p:231-36
Elagraa, M., Jamal, R., & Elkhafif, M. (2014). Trade Facilitation in the Occupied Palestinian Territory: Restrictions and Limitations (pp. 1–43). United Nations Conference on Trade and Development (UNCTAD).
Eljafari, M. (2010). Palestinian Capacity Building Needs in Trade Policy and Trade Facilitation. The Journal of World Investment & Trade, 11(5), 731-751. https://doi.org/10.1163/221190010X00383
Elkhafif, M., Misyef, M., & Elagraa, M. (2014). Palestinian Fiscal Revenue Leakage to Israel under the Paris Protocol on Economic Relations (pp. 1–58). United Nations.
Egeonu, P. (2017). Third World Dependency, Theoretical Assumptions and African Underdevelopment: A Critique Analysis. Online Journal of Arts, Management and Social Sciences, 2(2), 16–28.
Gisha. (n.d.). Controlled dual-use items – in English. Gisha.
Gisha. (2016). Dark-gray lists. (pp. 1–6). Gisha. https://gisha.org/publication/4860
Haddad, T. (2018). Palestine Ltd.: Neoliberalism and Nationalism in the Occupied Territory. I.B. Taurus.
Israeli Defenses Forces, (n.d.). The Gaza Tunnel Industry. https://www.idf.il/en/minisites/hamas/hamas/the-gaza-tunnel-industry/#:~:text=One%20of%20the%20most%20pressing,the%20cost%20of%20civilian%20rehabilitation.
International Monetary Fund. (2018). West Bank and Gaza Report to the Ad Hoc Liason
Committee. https://www.imf.org/en/Publications/CR/Issues/2018/09/17/west-bank-gaza-report-to-the-ad-hoc-liaison-committee
Israel Ministry of Foreign Affairs. (2010, July, 04). Gaza: Lists of Controlled Entry Items. https://mfa.gov.il/mfa/foreignpolicy/peace/humanitarian/pages/lists_controlled_entry_items_4-jul-2010.aspx
Lloyd, R. B. (2012). On the Fence: Negotiating Israel's Security Barrier. The Journal of the Middle East and Africa, 3(2), 198-214. https://doi.org/10.1080/21520844.2012.741039
Mansour, H. (2010). The Effects of Labor Supply Shocks on Labor Market Outcomes: Evidence from the Israeli-Palestinian Conflict. Labour Economics, 17, 930-939. https://doi.org/10.1016/j.labeco.2010.04.001
Marcus, R. D. (2017). Learning ‘Under Fire’: Israel’s improvised military adaptation to Hamas tunnel warfare. Journal of Strategic Studies, 42(3-4), 344-370. https://doi.org/10.1080/01402390.2017.1307744
Nashashibi, K., Gal, Y., & Rock, B. (2015). Palestinian- Israeli Economic Relations: Trade and Economic Regime. Office of the Quartet Representative.
Palestinian Central Bureau of Statistics (PCBS). (1999). Registered Foreign Trade Statistics Goods and Services, 2018 Main Results (pp. 1–208). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx
Palestinian Central Bureau of Statistics (PCBS). (2003). National Accounts at Current and Constant Prices - (1994-2000), (pp. 1–101). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx
Palestinian Central Bureau of Statistics (PCBS). (2006). Registered Foreign Trade Statistics Goods and Services, 2018 Main Results (pp. 1–85). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx
Palestinian Central Bureau of Statistics. (2009). Economic Surveys Series, 2008 Main Results (pp. 1–144). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx
Palestinian Central Bureau of Statistics. (2010). Economic Surveys Series, 2009 Main Results (pp. 1–145). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx
Palestinian Central Bureau of Statistics. (2011). Economic Surveys Series, 2010 Main Results (pp. 1–182). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx
Palestinian Central Bureau of Statistics. (2016). Economic Surveys Series, 2015 Main Results (pp. 1–179). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx
Palestinian Central Bureau of Statistics. (2017). Economic Surveys Series, 2016 Main Results (pp. 1–183). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx
Palestinian Central Bureau of Statistics. (2019). Economic Surveys Series, 2018 Main Results (pp. 1–186). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx
Palestinian Central Bureau of Statistics. (2019). Palestinian Labour Force Survey Annual Report: 2018 (pp. 1–138). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx
Palestinian Central Bureau of Statistics. (2019). Registered Foreign Trade Statistics Goods and Services, 2018 Main Results (pp. 1–157). http://www.pcbs.gov.ps/pcbs_2012/Publications.aspx
Palestinian Shippers' Council. (2012). Capacity Development For Facilitating Palestinian Trade A Study on the Proposed Mobile Scanner at King Hussein Bridge (pp. 1–31). Palestinian Shippers’ Council.
Palestine Trade Center. (n.d.). Export Processing Path through Internal Commercial Crossings. https://www.paltrade.org/en_US/page/export-processing-path-through-internal-commercial-crossings
Palestine Trade Center. (2013). Exporting and Importing Via Jordanian and Israeli Ports Comparison Study (pp. 1–128). https://www.paltrade.org/en_US/page/sector-product-studies
Pressman, J. (2003). The Second Intifada: Background and Causes of the Israeli-Palestinian Conflict. Journal of Conflict Studies, 23(2). https://journals.lib.unb.ca/index.php/JCS/article/view/220
Samhouri, M. (2016). Revisiting the Paris Protocol: Israeli-Palestinian Economic Relations, 1994–2014. The Middle East Journal, 70(4), 679-607. https://www.muse.jhu.edu/article/634691.
Shrum, W. (2001). Science and Development . International Encyclopedia of the Social & Behavioral Sciences. https://doi.org/10.1016/B0-08-043076-7/03165-X
Smith, C. D. (2017). Palestine and the Arab-Israeli conflict a history with documents. Boston: Bedford/St. Martins.
Springer, J. E. (2015). Assessing Donor-driven Reforms in the Palestinian Authority: Building the State or Sustaining Status Quo? Journal of Peacebuilding & Development,10(2), 1-19. https://doi.org/10.1080/15423166.2015.1050796
The Times of Israel. (2019, May 26). Israel re-expands Gaza fishing zone to 15 nautical miles. https://www.timesofisrael.com/israel-re-expands-gaza-fishing-zone-to-15-nautical-miles/#:~:text=Israel%20announced%20Saturday%20night%20that,from%20the%20coastal%20Palestinian%20territory.
The Times of Israel. (January, 12, 2020). Israel pushes forward with plans for new gas pipeline to Gaza – report. (n.d.). https://www.timesofisrael.com/israel-pushes-forward-with-plans-for-new-gas-pipeline-to-gaza-report/
the United Nations Relief and Works Agency for Palestine Refugees in the Near East (UNRWA). (2017a, October 28). UNRWA Condemns Neutrality Violation in Gaza. https://www.unrwa.org/newsroom/official-statements/unrwa-condemns-neutrality-violation-gaza
the United Nations Relief and Works Agency for Palestine Refugees in the Near East (UNRWA). (2017b, June 09). UNRWA Condemns Neutrality Violation in Gaza in the Strongest Possible Terms. https://www.unrwa.org/newsroom/press-releases/unrwa-condemns-neutrality-violation-gaza-strongest-possible-terms
The World Bank. (n.d.). Unlocking the Trade Potential of the Palestinian Economy (pp. 1–116). The World Bank. https://documents.worldbank.org/en/publication/documents-reports/documentdetail/960071513228856631/unlocking-the-trade-potential-of-the-palestinian-economy-immediate-measures-and-a-long-term-vision-to-improve-palestinian-trade-and-economic-outcomes
The World Bank. (2019a). Palestine's Economic Update — October 2019. World Bank. https://www.worldbank.org/en/country/westbankandgaza/publication/economic-update-october-2019
The World Bank. (2019b, April 17).World Bank Calls for Reform to the Dual Use Goods System to Revive a Stagnant Palestinian Economy. https://www.worldbank.org/en/news/press-release/2019/04/17/world-bank-calls-for-reform-to-the-dual-use-goods-system-to-revive-a-stagnant-palestinian-economy
The World Bank Group. (2017). Unlocking the Trade Potential of the Palestinian Economy. Washington D.C.: The World Bank Group. https://documents.worldbank.org/en/publication/documents-reports/documentdetail/960071513228856631/unlocking-the-trade-potential-of-the-palestinian-economy-immediate-measures-and-a-long-term-vision-to-improve-palestinian-trade-and-economic-outcomes
United Nations Conference on Trade and Development (UNCTAD). (2013). Report on Unctad assistance to the Palestinian people: developments in the economy of the Occupied Palestinian Territory (pp. 1–19). Geneva.
UNData. (n.d.). State of Palestine. https://data.un.org/en/iso/ps.html
United Nations (n.d.). The Gaza Reconstruction Mechanism. Retrieved from https://grm.report
United Nations Office for Project Services. (n.d.a). GRM. https://grm.report
United Nations Office for Project Services. (n.d.b) The GRM Process. https://grm.report
United Nations Office for the Coordination of Humanitarian Affairs Occupied Palestinian territory (OCHA). (n.d.a). Data on casualties. https://www.ochaopt.org/data/casualties
United Nations Office for the Coordination of Humanitarian Affairs (OCHA). (n.d.b). Gaza Strip electricity supply. https://www.ochaopt.org/page/gaza-strip-electricity-supply
United Nations Office for the Coordination of Humanitarian Affairs Occupied Palestinian
territory (OCHA). (n.d.c). West Bank Barrier. https://www.ochaopt.org/theme/west-bank-
barrier
USAID. (2013). Trade Facilitation Project (pp. 1–52). USAID.
World Bank. (2008). West Bank and Gaza Palestinian Trade: West Bank Routes (pp. 1–29). World Bank.
World Bank. (2019). Economic Monitoring Report to the Ad Hoc Liaison Committee (English) (pp. 1-41). World Bank Group. https://www.worldbank.org/en/country/westbankandgaza/publication/economic-monitoring-report-to-the-ad-hoc-liaison-committee-april-2019
Zilber, N., & Al-Omari, G. (2018). State With No Army Army With No State. Washington D.C.: The Washington Institute For Near East Policy. washingtoninstitute.org/policy-analysis/view/state-with-no-army-army-with-no-state
Zilberfarb, B.-Z. (2018). The short- and long-term effects of the Six Day War on the Israeli economy. Israel Affairs, 24(5), 785–798. https://doi.org/10.1080/13537121.2018.1505476
Endnotes
1.) The dual-use goods list consists of items the Israeli Government thinks Palestinians could use “for the development, production, installation or enhancement of military capabilities” in addition to the civilian use of the item (Israel Ministry of Foreign Affairs, 2010)."	63027
economy	['Myers', 'David L']	2020-05-22 00:00:00	"Abstract This work aims to integrate postcolonial scholarship into some basic theoretical foundations of a mainstream economic curriculum. Noting the insufficiencies of neoclassical economics to deal with problems of cultural difference and priority, the work offers a basic critique of economics and its aspirations for universal applicability. It does this by building upon existing postcolonial critiques of economics as a social science and focuses specifically on economic notions of value. Using postcolonial and anthropological scholarship, it sketches out a broader, more inclusive theory of value than traditional Marxist and Utilitarian value theories. Disciplinary-Mimetic Valuation uses a dualistic framework foregrounded in a social ontology of power and agency. Through the works of Syed Hussein Alatas, Marshall Sahlins, Homi Bhabha, and Partha Chatterjee, the reaches and limitations of Disciplinary-Mimetic Valuation are explored. This work and Disciplinary-Mimetic Valuation offers new avenues for economic theorizing in conjunction with cultural studies and urges further theorizing on the links between economic and cultural theory.

Introduction

Significant work has been brought to the forefront of the Anglo-European world with issues concerning colonialism as well as postcolonial development. It is in the vein of those scholars who contributed heavily to exposing the faultiness of European generalizations of humanity and how those judgements were situated in a nexus of global power that this paper finds itself in. More specifically, it aims to target an issue that finds itself drawing from the interdisciplinary intersections of Economics, Anthropology, and Cultural Theory as it pertains to the concept of value. This paper aims to advance an argument that the colonial process across the globe followed a general pattern of coercive homogenization that penetrated pre-colonial societies and shifted their value orientation to partially or completely conform to European cultural values; I call this process Disciplinary-Mimetic Valuation (DMV). I conclude by considering the philosophical implications of DMV on a developing postcolonial economics as well as further commentary on some of the central aspects of DMV.

Economic Theories of Value and Modernity

In the history of economic thought, there have been two prominent theories of value: the Labor Theory of Value and the Utilitarian Theory of Value. The Labor Theory of Value postulates that economic value arises from the labor necessarily required to produce the object. The discipline of economics reorganized itself following Marxist criticism and the Labor Theory of Value gave way to the Utilitarian Theory in mainstream discourse. The Utilitarian Theory of Value is premised on rational choice theory and a flattening of the individual. Value is measured in how much satisfaction an action, good, etc. can give to a rational consumer; this is what is taught in most economics curriculums and is central to neoclassical modeling. All individuals are said to have ‘Utility functions’ that measure the relativity of value across sets of market goods. Furthermore, the demand and supply curve in market models are constructed with the idea in mind that individuals acquire goods when considering the marginal gains of utility. This theory of value fundamentally holds all humans as operating on the same logic of utility maximization; this type of transcendental claim, as we shall see, falls into a culturally specific and historically contingent way of carving up the world.

With the advent of the Enlightenment and triumph of scientific rationality, one central tendency of modernity has been the universalization and totalization of humans across cultures on Western ontological grounding and classificatory systems. In particular, it is the Cartesian individual and its placement outside the ties of culture and history that finds itself as the central locus of Utilitarian Value Theory and subsequently Economics as a scientific discipline. Nitasha Kaul is instructional here; she writes,

The economic ‘subject’ of this unlocated analysis is a self-transparent unembodied and unembedded amoral utility maximizer propped up by Cartesian dualisms and interested narratives of Enlightenment reason. The ‘method’ of accessing valid disciplinary (and disciplined) economic narrative of the subject is mathematical formalism, heroic role of assumptions in a theory deductive nomological explanation, operation of an extremum principle and so on.

The aim of economics (and its theory of value) is formulating the Cartesian individual in a purely scientific light that seeks to establish its claims as universally true and verifiable by positive analysis and intuitive normalization. The area of contestation for Kaul and I is the central focal point of rationality as a universalizing scientific principle for economic analysis. The invariance of rational preferences in the Utilitarian theory of value makes possible the mathematical formalization essential for the construction of utility functions (and other neoclassical economic modeling tools). Economics asks us to analyze economic activity without asking us who it is we are analyzing, where the subject of analysis is historically and culturally situated, and other factors that do not lend itself to neat formalized models. Again, Kaul poignantly points to the issues that economic theory suppresses:

This…formalization as the core of economic theory leaves unchallenged the underlying issues of universalism and modernist stereotypy that form the operational basis of mainstream economic methods. And it is not only formalism which requires that identity be a matter of universal essences. The particularities of inalienably situated historical contexts within which the ‘economic’ is experienced are simplified into general and universal denominations.

We are forced to ask how a purely positivistic and intuitive science can develop out of an analysis of the individual, yet in the same stroke ignore the various aspects of the human condition that are essential renderings of understanding particular individuals? The answer to this question is that it cannot and that the ontological grounding that neoclassical economics and subsequently the Utilitarian Theory of Value cannot be used in an economic inquiry concerned with empirical reality and the social complexes that form it. If the Utilitarian Theory of Value fails to obtain when considered in tandem with the socio-cultural nexuses that operate on the individual, we are left to develop an alternative theory of value; one that does not universalize, but instead focuses on the contextual elements that hold influence over people. I begin to develop one such contextualized theory of valuation in the following sections of this paper, however, let this not be a flattening and pegging of value to abstract numerical continuum as represented in economics literature. I intend this theory of value (or valuation properly) to exist and interact with various others that escape its domain.

Value/Values Connection and the Formulation of Disciplinary Valuation

The notion of DMV that I seek to develop in this paper hold a great debt to Nitasha Kaul’s discussion concerning conceptions of value/values. Furthermore, it draws significant influence from the disciplinary effects of developmental ideology that springs out of macroeconomic thought. Finally, it articulates its link to mimesis via Homi Bhabha. Kaul starts developing this connection first by examining Samuel Bailey’s critique on the Labor Theory of Value in classical political economy. In Baily, Kaul finds the core of Bailey’s argument focused specifically against the fixing of value as something nvariable. Kaul cites Baily critiquing James Mill’s notion that wine which sets in a basement for a year gains value through labor. For Bailey, Kaul notes that, “the only accumulation having occurred in this case was an arithmetical, not an actual one.” She further finds in Baily a reaffirmation of the milieu of factors that exerts itself on the mind when considering the value of a given object. In recognizing the variety of influences that operate on the mind in the question of value, she sees a connection to what she terms as valu-ation. She writes:

In my reading, this sense of value is amenable to admitting the processual considerations of valu-ation, and need not necessarily be something that applies only to commodities. Value can be aligned to values – worthy of espousing – and, since the subjective dimension of value and values is acknowledged, it allows for admitting the politicized nature of the contest for values.

This, more or less, gives the theoretical base that gives rise to DMV; it fundamentally recognizes that the notion of value extends far beyond pegging it to an invariance principle, but instead recognizes that other “non-economic” forces play a role in the process. Furthermore, it explicitly recognizes the contested nature of values which is a necessary precondition for any disciplining and mimesis to occur. Thus, the notion of valuation in DMV is lifted directly from Kaul’s work and the concept of valuation necessarily invites inter-disciplinary commentary given the incommensurability of value not located on an abstract invariance principle. Instead of value arising out of an invariance (labor or utility) it instead is subject to a wide variety of considerations that may not explicitly be “economic” in nature. For example, when I consider the value of a pen, its value does not merely arise out of satisfaction it gives me, but rather there’s a whole variety of psycho-cultural factors playing in my mind when I come to that decision. The pen might have been given to me by someone close and because the community I was socialized in values friendships a certain way, I may extract from it an additional type of value like sentimental value. In this way, valuation precedes any claim to the utility of a good and, instead, value becomes realized through a complex interrelation of social, individual, and cultural pressures. These pressures ultimately push us to explore disciplinarity.

Next, the idea of economic development presents in itself a teleological narrative sourced in an imaginative rendering of universal progress. The developmental ideology itself can be said to stem directly from the civilizing mission posited by colonial actors. The civilizing mission is premised on a singular distinction: that the European societies are superior to non-European societies. Such a distinction was carried out through a variety of methods either through race, religion, cultural prejudice, etc. Hence, the civilizing mission can be characterized as the European compulsion to “uplift” the “wild and barbarian peoples.” Its teleological ends derive explicitly from resolving the contradictions of the universalist principles of the liberal Enlightenment and the racial and ethnic markers (or any other essentialist rendering of difference) of colonial difference that justified brutal repression. Thus, colonial policy often admitted inconsistency and contradiction. In the so-called decolonized world, the functional goal of assimilative pressures is supported by the imposition of development as the fundamental ends of the decolonized society. Development in economics can be analyzed through numerical metrics and indexes like those concerning gross domestic product, unemployment, productivity, stability, etc. Now, it’s not merely the improvement of these metrics that specifically disciplines, but rather their relation to the prescriptive measures taken to raise such levels. Much like Baptism and Christianity were seen as prescriptions to civilize and assimilate colonized peoples, the dominance of the Washington Consensus, International Monetary Fund, and World Bank and the liberalization they propose can be seen as providing the essential prescriptions of development even if such policies prove counter-intuitive to national interest. Eiman O. Zein Elabdin and S. Charusheela frame the centralization of development in Economics:

As a discipline, it has upheld the narrative of ‘development’ as the center piece of its theoretical construction of formerly colonized regions, presuming the ontological precedence of modern European societies as a basis for its theory of history…As Feyerabend put it, the discourse on development in effect renders patterns of life outside the (Western) industrial world a mistake.

This basis of development on the European model finds its support in the active constraint of alternative developmental methods by institutions and the operational logic of international institutions. For example, a significant portion of Francophone Africa finds itself tethered to the paternalistic monetary union that ties national currency to the French Franc. The need to defend the convertibility peg actively constrains macroeconomic stimulus policy; it sets a hard limit (approximately these countries must keep 65% of currency reserves in the French Central Bank) on fiscal and monetary policy in the African countries. To overspend under the regime of the currency peg is to devalue one’s currency and subsequently contribute to vast inflationary pressure. Even more evidence to this are the numerous conditional lending programs sponsored by the IMF and the World Bank which provide aid under the condition that a set of prescriptive policies perform. Hence, I derive the disciplinary aspect of DMV; it sits in the uncomfortable intersections of disciplinary power and bio-power best resembling the norm that operates, much like the economic theories of value, on a point of invariance for it, “introduces, as a useful imperative and as a result of measurement, all shading of individual differences.” It is disciplinary in the sense that it seeks to meld individuals to a certain cultural point of being and touched with bio-power as this is a strategy not employed at any single individual, but entire cultural groups via civilizing ‘x’ group or developing ‘y’ country.

Lastly, the mimetic derives from Bhabha’s reflection on mimicry and in many ways this theory can be seen as drawing out some of the necessary elements of mimesis. Mimicry, more or less, is the performative adoption of specific cultural values from the colonizing culture. In defining mimicry, Bhabha puts up an important limiting factor for such an act, “almost the same, but not quite.” Given the historicity, the distinctions and limiting factors that constitute colonial power, and the agency and ties of the colonized individual, the assimilative drive of DMV is never fully complete, one does not mesh themselves completely into the culture of the dominator and lose themselves completely. DMV portrays a partial mode of mimicry, but never a complete homogenization and unity of the colonized and colonizer. From this observation though, I must distinguish between myself and Bhabha. While I am indebted to his reflection on the phenomena, the mimesis I describe does not touch on the disruptive factors to colonial power that Bhabha points to. Rather, my interest is to what degree the cultural values of the colonizer were the object of imitation and assimilation as this is a work concerned with a specific historical socio-economic process. A more detailed discussion on Bhabha’s work will follow as I draw out DMV.

Disciplinary-Mimetic Valuation is, in the historical empirics of this analysis, the process in which European colonial states sought to assimilate (or “civilize”) the people it colonized into its universalist cultural value schema and the extent to which the colonized people attempted to assimilate into or mimic such. In the more general sense, it can be attributed to any power that seeks to assimilate another society into its socio-cultural schema by coercive force insofar as someone seeks to apply these principles to imperialist phenomena escaping European colonialism proper. Thus, in the thrust of developing extractive colonial economies and building administrative systems to impose those desires, we have the foreground for DMV in action. What will follow throughout the rest of this work is understanding the ways DMV has played out in specific analyses. Since it has a fundamental historical character, each incidence observed will have its proper contextual elements that provide unique aspects to the phenomena of Disciplinary-Mimetic Valuation. Furthermore, before any serious analysis begins, DMV is situated specifically and does not occur uniformly or simultaneously; it is fractional and the effects are prone to conflicts of that nature.

Disciplinary-Mimetic Valuation: Cultural and Economic Homogenization in Action

Syed Hussein Alatas starts us off in describing the starting point of this valuation, he points to a significant causal reason in the homogenization of cultural values to European colonizers. It develops from, what he considers, the fundamental source for orientalist mischaracterizations of Malays: their unwillingness to engage in colonial capitalism. Alatas writes of the diminished attribution to natives, “Any type of labour which did not conform to this conception (capitalist timed labour) was rejected as a deviation. A community which did not enthusiastically and willingly adopt this conception of labour was regarded as indolent.” From this passage, we see the attributing of a negative image to function as a disciplinary tool. First, we establish the point of invariance, the norm so to speak (A specific labor relation) Alatas points us to. Next, we note the difference in empirical adherence: the Dutch and English adhere to this type of labor, but the Native does not. There next needs to be an explanation as to why this differential exists; we are left to question the moment and conditions of observation. Alatas does this for us as we are to recognize the material elements concerning Malay society, namely commercialized agriculture, “The root cause of this image was the Malays’ reaction to cash-crop agriculture and to working in colonial capitalist estates and plantations. They avoided the most exploitative kind of labour in 19th century colonial capitalist undertakings.” Hence, there exists an industry in want of labor and a people unwilling to become subservient to it. The explanation for the differences, given proper context, is rooted in the satiation of economic processes in which a specific set of disciplined labor is necessary. The characterization of Malays from European observers becomes centered around labor, Malays become “indolent and lazy,” as opposed to “industrious.” We see here a disciplinary tool in action, one aimed specifically at convincing the European reader of the inferiority of the native while also complementing the civilizing mission. We learn from this distinction that there are preliminary necessities for DMV to occur. First, there must be a difference imbued with power differentials. Second, there requires in the dominating party a belief in their supposed superiority explicitly or implicitly held. Alatas connects the image of the lazy Malay as an integral functionary in colonial ideology. Much to the same, DMV draws on a supporting ideology of superiority/inferiority of peoples.

While Alatas points us to the proper requirements for the occurrence of DMV, we find its empirical implementation exploiting fractures in colonized societies existing in the works of Anthropologist Marshal Sahlins. Sahlins, in his analysis, is concerned predominantly with what he terms a structure of conjuncture or the reproduction of a culture that also transforms it and reinscribes new values to old cultural categories. Sahlins sketches out a portrait of Hawaiian society organized around mana. Mana can roughly be described as power typically supernatural that legitimizes the right to rule. Sahlins himself describes mana in several different ways. When describing pre-colonial mana, Sahlins writes that it was won from the great feat of the chiefs, “The mana of the chiefly heros is extraordinary, but more appropriate to a human sphere than were the supernatural gifts of their predecessors. The chiefs win success by their subtlety, courage, skill and strength.” Sahlins gives another description of mana saying, “mana is the creative power Hawaiians describe as making visible what is invisible, causing things to be seen, which is the same as making them known or giving them form.” From this, we can see a high prioritization for the acquisition and accumulation of mana.

Now, Sahlins describes the receiving of Europeans on Hawaiians own cultural terms and through their adjustment and adaptation of their cultural schema. Following the thread on mana, European goods and people were considered to possess considerable mana and was used for appropriation to reinforce or disrupt precolonial structures prevalent within Hawaiian society. For example, the killing of Captain Cook represents the appropriation of mana and the interrelation of it to Englishness:

Through the appropriation of Cook’s bones, the mana of the Hawaiian kingship itself became British. And long after the English as men lost their godliness, the Hawaiian gods kept their Englishness. Moreover, the effect was to give the British a presence in Hawaiian affairs that was all out of proportion to their actual presence in Hawaiian waters, since they were rapidly displaced in the sandalwood trade by the Americans.

From this we can see the mimetic effects quite clearly, while retaining their own conceptual and cosmological categories, they begin to be partially adopted by the Hawaiians. Interestingly enough, Sahlins makes note of the economic withdrawal of British in the area, yet the presence of their cultural influence prevails despite the waning of hard British power. The disciplinary aspects, instead, become enforced by the own cosmological categories of the Hawaiians themselves and give way to the fractious exploitation of DMV. First, it did so by legitimizing Kamehameha’s conquest of the Hawaiian Islands and his European policy of friendship with the British; Sahlins writes:

Yet the means…by which the great Kamehameha turned that commerce into practical account was his own special relation to British power…Transmitted through the bones of Captain Cook, Kamehameha’s special relation to European mana gave him enough of it, in the forms of guns, ships and resident advisors, to conquer the islands.

Further, the chiefs even started to act and imitate the British themselves as a means of perpetuating and reinforcing their social dominance from their subjects. Clothing was of particular importance:

The Hawaiian chiefs seized upon the European distinctions between ‘plain’ and ‘fancy’ cloth to mark their own distance from the common people…European mana in the form of domestic possessions now replaced military supplies as the principal means of aristocratic competition.

Conversely, European interaction with the Hawaiians also led to conflict between the chiefs and the commoners. First, the commoners themselves had competing economic interests that ran converse to chiefly tabus on trade. Commoners had explicit interest in the accumulation of European mana observable in the exchange of sexual labor of the natives for European goods. Even in open hostility following Cook’s death, these relations still held in secrecy. In short, Sahlins summarizes the fractious effect of European contact on Hawaiian society; he writes:

The respective relations of chiefs and people to the European presence thus set them in practical opposition to one another. The complex exchanges that developed between Hawaiians and Europeans…brought the former into uncharacteristic conditions of internal conflict and contradiction.

Sahlins portrays the structure of conjuncture though not as an assimilative or mimetic experience, but one in which the Hawaiian social order appropriates these interactions to reassert and recast itself. I depart from this conjecture only insofar as the recasting and resetting of the Hawaiian social ordering produced mimetic effects, namely in the chiefdom, of European cultural distinctions. I emphasize heavily that the mimetic effects themselves are not all-consuming or even prominent at the early stages of colonial contact, but are ultimately partial. The stipulation that Europeans had superior mana and the dependencies those created established the base means for DMV’s internal logic to take root. Discipliniarity itself started in repositioning and fracturing of Hawaiian power and its social order.

Bhabha’s conception of hybridity may help us elucidate further how cultural mimesis in DMV works. Bhabha holds that postcolonial subjects exist at the “in-between” or “splitting” of cultural consciousness that inevitably places the colonized subject in a state of hybridity by creating a unique space of enunciation, “The social articulation of difference, from the minority perspective, is a complex, on-going negotiation that seeks to authorize cultural hybridities that emerge in moments of historical transformation.” What these postcolonial hybridities necessitate is some degree of mimicry for them to be constituted as a hybridized entity otherwise the very notion of a hybridized postcolonial identity fails to obtain. Mimicry, for Bhabha, creates an alternative knowledge or viewpoint within the cultural schema that simultaneously threatens its relation to truth concerning colonial images. Bhabha writes, “It is the process of fixation of the colonial as a form of cross-classificatory, discriminatory knowledge within an interdictory discourse and therefore necessarily raises the question of the authorization of colonial representation.” Hence, Bhabha sees in mimicry a way of undermining and de-authorizing negative colonial images and there is some truth in this. Insofar as DMV operates though, its mimesis first signals a capitulation and perhaps one that is unavoidable of the precolonial consciousness. I read Bhabha’s conception of mimicry as starting from an initial departure and transformation (although not necessarily self-conscious as Sahlins’s analysis of the Hawaiians shows) of the precolonial culture to become more amenable to the dominating one.

If Sahlins points to the early and fractious implementation of DMV, Chatterjee draws it out in more substantial and concrete terms when he describes the nationalist response to British occupation: a necessary privacy of the inner or spiritual domain of culture and the homogenization of the outer domain. Chatterjee’s analysis of Indian Nationalism points us to understanding DMV as it operates in nationalist contestation; disciplining the outer presentation while never quite penetrating the privacy of the spiritual domain. To understand the proper context in which Chatterjee introduces these concepts, we need to understand that Chatterjee postulates an idea of colonial difference that operates quite agreeably with the characterization of the civilizing mission and Alatas’ observations. Chatterjee covers some debates among British intellectuals and colonial administrators in relation to liberal and conservative colonial policy as they perceive colonial difference. Prior to the development of Hindu nationalism, India was subjected to the Rule of Colonial Difference which set race as its distinguishing marker. Such rule justified the conservative tendency to perceive the British colonial subjects as incapable of self-rule while simultaneously throwing the universalist claims of British liberalism into question. If the colonized subjects of the British Empire are incapable of self-rule based on differences according to race, the universalist mission of juridical liberal rights, equal protection under the law, and property come into direct conflict with the realities of colonial rule and discourse. Chatterjee himself makes note of this when considering Ripon’s failure in the Ilbert Bill affair, “What his ‘failure’ signaled was the inherent impossibility of completing the project of the modern state without superseding the conditions of colonial rule.” The immediate tension being, again, the so-called applicability of universalist values confronted with the epistemic and material alienation of the colonized and the discursive justification for their occupation. For colonized India, this meant adopting some of the material habits that fall into line with an enlightened, liberal criteria of humanity.

Hence, for the modern state to succeed in its mission, it needed to grant the conditions that fundamentally led to the disavowal of colonial rule itself. There needs to be some degree of mimetic or assimilative adjustment in the colonized culture to give way to the modern state. For Chatterjee, This begins with the spiritual and material modernization of the Indian national consciousness; this is also where we can find the machinations of DMV. It is most obvious in the material domain which Chatterjee distinguishes as, “the domain of law, administration, economy, and statecraft.” The homogenization and erasure of difference in this domain of culture shows the ascription of the administrative, legal, and economic aspects of Indian society into the teleological cultural schema of the Enlightenment. The modernization of the Indian state becomes pegged to abstract European notions of progress and development. We see in the material realm the purest form of DMV as the Indian economy begins to resemble Europe in administrative structure and economy. Chatterjee writes:

In this, it seemed to be reasserting precisely the claims to universality of the modern regime of power. And in the end, by successfully terminating the life of the colonial state, nationalism demonstrated that the project…could be carried forward only by superseding the conditions of colonial rule.

Disciplinarity becomes melded with the conditionality of political decolonization and independence; it is a necessary precondition that the Indian population mimic, in some respect, British materiality.

If in the outer domain, we can see the adoption of British material values, The Inner-Domain (language, religion, family and personal life) represents the exemplification of the partiality of mimesis. In the inner-domain, the nationalistic India posits the alternative knowledge that Bhabha gives, it creates a conception of Indian-ness as a legalistic and political equal once independence is achieved. However, these inner-distinctions themselves do not completely escape the influence of disciplinarity either through modular forms of expression (like the novel) or through institutional arrangements like the university system. More explicitly, we can see the effect of DMV reproducing the same class distinctions in post-colonial states that, although not exclusively occupied by a nationalistic middle class, lends itself to their expression by way of material domination. Chatterjee demonstrates this explicitly with his analysis on Bankim’s nationalism; he writes on the prospect of Bankim’s consideration of a true Bengali history, “The historical consciousness he is seeking to invoke is in no way an ‘indigenous’ consciousness’ because the preferred discursive form of his historiography is modern European.” Coupled with Bankim’s antipathy for Muslims and their representation of India, we can see mimesis and acculturative practices touching the inner aspect of nationalistic consciousness; the modular and methodological forms of transmission were instrumental at this point of contact. Nationalistic histories tended to perpetuate the exclusive aspects that were articulated by colonial difference and impose them on other colonized peoples. While one may make note of historical antagonisms prior to colonization between Muslims and Hindu peoples, this does not draw away from the mimetic representations and othering of Muslim populations in nationalistic imaginations. In particular, the othering of the Muslim is complementary with western orientalist renderings of Islam in general; Islam is portrayed as a functionary of stagnation and is given a “medieval” characterization in the nationalistic consciousness. Muslims become, “fanatical, bigoted, warlike, dissolute, and cruel.” Hence, DMV can also lead to the articulation of negative images shared in common with the dominating power. It is important to note that the mimesis of Bhabha draws heavily from his concept of cultural hybridization that is antithetical to nationalism itself; it is another distinction I draw from what I interpret as mimicry. Overall though, while partiality is given, the inner domain of culture is not entirely immune from DMV’s influence.

Nothing gives a more explicit portrayal of the intersections of mimesis and coercion than the first chapter of Black Skin, White Masks. Fanon portrays such a process as it inheres on the Black colonized subject and France and points explicitly to the opportunities it allows for individuals suffering under colonial occupation. In this particular occurrence, we can see Bhabha’s postulation of, “almost the same, but not quite.” Fanon points to unique material opportunities that are provided through the adoption of the colonizer culture, “The colonized is elevated above his jungle status in proportion to his adoption of the mother country’s cultural standards…In the Senegalese regiments, the black officers serve first of all as interpreters.” However, it is not only material opportunities, like that of an interpreter, that gives rise to mimesis, but also a feeling of psychological factors that contribute to a feeling of equality:

The wearing of European clothes…using European furniture and European forms of social intercourse; adorning the Native language with European expressions; using bombastic phrases in speaking or writing a European language; all these contribute to a feeling of equality with the European and his achievements.

Both of the reasoning for the adoption of European cultural values stems from the disciplining of racist images of colonial rule onto the colonized, Mimicry supposes an alternative way to remove the brutalizing force of colonial occupation and racism. Fanon, himself, asserts this when he writes, “Historically, it must be understood that the Negro wants to speak French because it is the key that can open doors which were still barred to him fifty years ago.” This mimetic pathway, however, is limited by the persistence of colonial difference still present in the consciousness of racist Europeans and the internal splitting that is required from the minority consciousness when faced with marginalization. The partiality of mimesis is not only preserved by the internal conditions of the colonized consciousness, but also supported by the vestiges of colonial difference be it presented in explicit and historical understandings of race and skin color or disparate material outcomes. Thus, DMV never completes the disciplining to the dominating culture, it cannot because complete homogenization eliminates any notion of mimesis. Rather, if we were to deny the inner domain of the colonized subject, we would only be speaking of absorption. Returning to Chatterjee, we can find a similar thought:

A relational opposition of power necessarily meant that the dominated had to be granted their own domain of subjectivity, where they were autonomous, undominated. If it were not so, the dominators would, in the exercise of their domination, wholly consume and obliterate the dominated.

Instead, partiality is preserved and with it a split consciousness: one in which the disciplining powers of the dominant European culture operates and influences, but never fully takes hold. This is because the historical markers of colonial difference that initially gave way to the discursive justification for domination, disciplining, and extraction persist and are realized to this day; what started this process also prevents its completion.

Necessary Questions, Philosophical Problems, and Concluding Remarks

I have demonstrated a fragmented account of a theory of cultural valuation that in and of itself is not entirely complete. Nor could it be, it requires fundamentally contextualized historical readings and interpretative accounts of European colonialism. Rather, as a theory of value it strives to fundamentally rethink how economic value is constituted. What will follow now is a reflection on some questions that might have relevance to this theory and its placement in the economics discourse.

Perhaps a prominent question is what distinguishes DMV from anthropological and sociological accounts of acculturation. DMV itself might be seen as a particular theoretical application of acculturation and I have no qualms with this, but I would stress unique markers of this theory and the potential of it for further work. First, I draw upon a tradition of poststructuralist and postcolonial thought to inform some of the ontological basis for this theory. First, I appropriate Foucault’s notions of disciplinary and bio-power and do the same for Bhabha’s mimicry. In particular, poststructuralist readings tend to draw significantly from semiotic accounts of signification. Sahlins, who is far from a post-structuralist, also recognizes the importance of sign analysis when he considers the following on value:

The value of any cultural category whatever such as ‘land’ is indeed arbitrary in the sense that it is constituted on principled distinctions among signs which, in relation to objects, are never the only possible distinctions. Even an ecological anthropology would recognize that the extent to which a particular tract of land is a ‘productive resource,’ if it is at all, depends on the cultural order in place. Economics might thus find a place in the general semiology that Saussure envisioned—while at the same time hedging the entrance requirement with restrictive clauses.

Second, I am primarily concerned with the colonial situation in developing this theory, but do not see it exclusively applicable to that situation. Thus, if this theory is to be designated as just another theory of acculturation; I would prefer it be one rendered in the connecting of Economics to cultural values on a poststructuralist and postcolonial ground.

What remains is how this theory can be integrated into an emancipatory politics. Bhabha certainly recognizes hybridity as the fundamental ontological grounding of the postcolonial world and I am inclined to agree with him because the sheer material and historical effects of colonialism appear to be cemented in the international world and the emergence of new social identities. Rather, I see Bhabha as expressing a social reality and find that the notion of a hybridized social identity itself points to a new individualism that supplants the European rationalistic one. This individual follows no rational choice theory, does not give way to the clear mathematical formalization of neoclassical economics and their subsequent limitations, but rather draws from a variety of historical situations and identities that contextualizes and situates decision making. It is not a totalizing individuality; the economic individual becomes contextualized and situated in a variety of cultural nexuses that resists the invariances of economic theories of value and the oppressive fixing of cultural differences necessary for a rationalization on the colonial rule.

However, work still needs to be done. Kaul calls upon bringing the cultural politics of recognition to theories of value overall which I do not believe I have adequately done. Given this is a fragmented, incomplete, and non-exclusive theory, there is also considerable expansion and revision necessary for it to achieve proper clarity and rigor. There is also room to develop more extensive, complementary or antagonistic theories of value. Furthermore, there exists work to be done that far escapes the reaches of my thought and I am only a historically situated individual. So, to answer the question about how this theory can be integrated into an emancipatory politics, I answer that it provides the basis for new theorizing and new histories that provide the theoretical basis for such a politics, while fundamentally displacing a Eurocentric universalism premised on a culturally specific reading of rationality.

References

Alatas, Syed Hussein. The Myth of the Lazy Native London: Frank Cass and Company Limited, 1977.

Bhabha, Homi. The Location of Culture. London: Routledge, 1994.

Charusheela, S., and Eiman Zein-Elabdin, eds.Postcolonialism Meets Economics. London: Routledge, 2004.

Chatterjee, Partha. The Nation and Its Fragments. Princeton: Princeton University Press, 1993.

Fanon, Frantz. Black Skin, White Masks. London: Pluto Press, 1986.

Foucault, Michele. Discipline and Punish. New York: Random House, 1995.

Kaul, Nitasha. Imagining Economics Otherwise: Encounters with Identity/Difference. Abingdon: Routledge, 2008.

Sahlins, Marshall. Historical Metaphors and Mythical Realities. Michigan: The University of Michigan Press, 1981.

Tricoire, Damien, ed.Enlightened Colonialism: Civilization Narratives and Imperial Politics in the Age of Reason. Cham: Springer International Publishing, 2017.

Williams, Oral, Tracy Polius, and Selvon Haze, “Reserve Pooling in the Eastern Caribbean Currency Union and the CFA Franc Zone: A Comparative Analysis,” Savings and Development 29, no. 1 pp. 39-60.

Endnotes

1.) . It should be noted that this is a gross over-simplification of the Labor Theory of Value and the unique takes presented by classical economists.

2.) . Nitasha Kaul, “Writing Economic theory anOther Way,” Postcolonialism Meets Economics, ed. S. Charusheela & Eiman Zein-Elabdin (London, Routledge, 2004) P.184.

3.) . Ibid., P.187

4.) . Nitasha Kaul, Imagining Economics Otherwise: Encounters with Identity/Difference (Abingdon, Routledge, 2008), P.210

5.) . Ibid., PP.210-211

6.) . Damien Tricoire, “Introduction,” Enlightened Colonialism: Civilization Narratives and Imperial Politics in The Age of Reason (Cham, Springer International Publishing AG, 2017), P.2

7.) . Ibid., P.9

8.) . Eiman O. Zein-Elabdin & S. Charusheela, “Introduction: Economics and Postcolonial Thought,” Postcolonialism Meets Economics, P. 2

9.) . Williams, Oral, Tracy Polius, and Selvon Haze, “Reserve Pooling in the Eastern Caribbean Currency Union and the CFA Franc Zone: A Comparative Analysis,” Savings and Development 29, no. 1 p. 43.

10.) . Michele Foucault, Discipline and Punish (New York, Random House, 1995), p. 84

11.) . Homi Bhabha, The Location of Culture (London, Routledge, 1994), p. 86

12.) . Syed Hussein Alatas, The Myth of the Lazy Native (London, Frank Cass and Company Limited, 1977)

13.) . Ibid., p. 70

14.) . Ibid., p. 74

15.) . Ibid., pp. 75-78

16.) . Ibid., pp. 1-2

17.) . Marshall Sahlins, Historical Metaphors and Mythical Realities (Michigan, The University of Michigan Press, 1981), pp. 5-8

18.) . Ibid., pp. 15-16

19.) . Ibid., p. 31

20.) . Ibid., p. 7

21.) . Ibid., p. 26

22.) . Ibid., p. 31

23.) . Ibid., pp. 43-46

24.) . Ibid., p. 50

25.) . Bhabha, Location of Culture, p. 2

26.) . Ibid., p. 90

27.) . Partha Chatterjee, The Nation and Its Fragments (Princeton, Princeton University Press, 1993)

28.) . Ibid., p. 18

29.) . Ibid., 21

30.) . Ibid., p. 26

31.) . Ibid., p. 77

32.) . Ibid., p. 102

33.) . Although Bhabha, himself, distances himself from Fanon’s analysis due to the fundamental factor in motivating the adoption of the dominating culture. Bhabha p. 88

34.) . Frantz Fanon, Black Skin, White Masks (London, Pluto Press, 1986), p. 18

35.) . Ibid., p. 25

36.) . Ibid., p. 38

37.) . Partha Chatterjee, The Nation and its Fragments, p. 161

38.) . Sahlins, Historical Metaphors and Mythical Reality, p. 6"	http://www.inquiriesjournal.com/articles/1843/a-postcolonial-theory-of-value-broadening-economic-scholarship-through-disciplinary-mimetic-valuation	"This work aims to integrate postcolonial scholarship into some basic theoretical foundations of a mainstream economic curriculum. Noting the insufficiencies of neoclassical economics to deal with problems of cultural difference and priority, the work offers a basic critique of economics and its aspirations for universal applicability. It does this by building upon existing postcolonial critiques of economics as a social science and focuses specifically on economic notions of value. Using postcolonial and anthropological scholarship, it sketches out a broader, more inclusive theory of value than traditional Marxist and Utilitarian value theories. Disciplinary-Mimetic Valuation uses a dualistic framework foregrounded in a social ontology of power and agency. Through the works of Syed Hussein Alatas, Marshall Sahlins, Homi Bhabha, and Partha Chatterjee, the reaches and limitations of Disciplinary-Mimetic Valuation are explored. This work and Disciplinary-Mimetic Valuation offers new avenues for economic theorizing in conjunction with cultural studies and urges further theorizing on the links between economic and cultural theory.
Introduction
Significant work has been brought to the forefront of the Anglo-European world with issues concerning colonialism as well as postcolonial development. It is in the vein of those scholars who contributed heavily to exposing the faultiness of European generalizations of humanity and how those judgements were situated in a nexus of global power that this paper finds itself in. More specifically, it aims to target an issue that finds itself drawing from the interdisciplinary intersections of Economics, Anthropology, and Cultural Theory as it pertains to the concept of value. This paper aims to advance an argument that the colonial process across the globe followed a general pattern of coercive homogenization that penetrated pre-colonial societies and shifted their value orientation to partially or completely conform to European cultural values; I call this process Disciplinary-Mimetic Valuation (DMV). I conclude by considering the philosophical implications of DMV on a developing postcolonial economics as well as further commentary on some of the central aspects of DMV.
Economic Theories of Value and Modernity
In the history of economic thought, there have been two prominent theories of value: the Labor Theory of Value and the Utilitarian Theory of Value. The Labor Theory of Value postulates that economic value arises from the labor necessarily required to produce the object. The discipline of economics reorganized itself following Marxist criticism and the Labor Theory of Value gave way to the Utilitarian Theory in mainstream discourse. The Utilitarian Theory of Value is premised on rational choice theory and a flattening of the individual. Value is measured in how much satisfaction an action, good, etc. can give to a rational consumer; this is what is taught in most economics curriculums and is central to neoclassical modeling. All individuals are said to have ‘Utility functions’ that measure the relativity of value across sets of market goods. Furthermore, the demand and supply curve in market models are constructed with the idea in mind that individuals acquire goods when considering the marginal gains of utility. This theory of value fundamentally holds all humans as operating on the same logic of utility maximization; this type of transcendental claim, as we shall see, falls into a culturally specific and historically contingent way of carving up the world.
With the advent of the Enlightenment and triumph of scientific rationality, one central tendency of modernity has been the universalization and totalization of humans across cultures on Western ontological grounding and classificatory systems. In particular, it is the Cartesian individual and its placement outside the ties of culture and history that finds itself as the central locus of Utilitarian Value Theory and subsequently Economics as a scientific discipline. Nitasha Kaul is instructional here; she writes,
The economic ‘subject’ of this unlocated analysis is a self-transparent unembodied and unembedded amoral utility maximizer propped up by Cartesian dualisms and interested narratives of Enlightenment reason. The ‘method’ of accessing valid disciplinary (and disciplined) economic narrative of the subject is mathematical formalism, heroic role of assumptions in a theory deductive nomological explanation, operation of an extremum principle and so on.
The aim of economics (and its theory of value) is formulating the Cartesian individual in a purely scientific light that seeks to establish its claims as universally true and verifiable by positive analysis and intuitive normalization. The area of contestation for Kaul and I is the central focal point of rationality as a universalizing scientific principle for economic analysis. The invariance of rational preferences in the Utilitarian theory of value makes possible the mathematical formalization essential for the construction of utility functions (and other neoclassical economic modeling tools). Economics asks us to analyze economic activity without asking us who it is we are analyzing, where the subject of analysis is historically and culturally situated, and other factors that do not lend itself to neat formalized models. Again, Kaul poignantly points to the issues that economic theory suppresses:
This…formalization as the core of economic theory leaves unchallenged the underlying issues of universalism and modernist stereotypy that form the operational basis of mainstream economic methods. And it is not only formalism which requires that identity be a matter of universal essences. The particularities of inalienably situated historical contexts within which the ‘economic’ is experienced are simplified into general and universal denominations.
We are forced to ask how a purely positivistic and intuitive science can develop out of an analysis of the individual, yet in the same stroke ignore the various aspects of the human condition that are essential renderings of understanding particular individuals? The answer to this question is that it cannot and that the ontological grounding that neoclassical economics and subsequently the Utilitarian Theory of Value cannot be used in an economic inquiry concerned with empirical reality and the social complexes that form it. If the Utilitarian Theory of Value fails to obtain when considered in tandem with the socio-cultural nexuses that operate on the individual, we are left to develop an alternative theory of value; one that does not universalize, but instead focuses on the contextual elements that hold influence over people. I begin to develop one such contextualized theory of valuation in the following sections of this paper, however, let this not be a flattening and pegging of value to abstract numerical continuum as represented in economics literature. I intend this theory of value (or valuation properly) to exist and interact with various others that escape its domain.
Value/Values Connection and the Formulation of Disciplinary Valuation
The notion of DMV that I seek to develop in this paper hold a great debt to Nitasha Kaul’s discussion concerning conceptions of value/values. Furthermore, it draws significant influence from the disciplinary effects of developmental ideology that springs out of macroeconomic thought. Finally, it articulates its link to mimesis via Homi Bhabha. Kaul starts developing this connection first by examining Samuel Bailey’s critique on the Labor Theory of Value in classical political economy. In Baily, Kaul finds the core of Bailey’s argument focused specifically against the fixing of value as something nvariable. Kaul cites Baily critiquing James Mill’s notion that wine which sets in a basement for a year gains value through labor. For Bailey, Kaul notes that, “the only accumulation having occurred in this case was an arithmetical, not an actual one.” She further finds in Baily a reaffirmation of the milieu of factors that exerts itself on the mind when considering the value of a given object. In recognizing the variety of influences that operate on the mind in the question of value, she sees a connection to what she terms as valu-ation. She writes:
In my reading, this sense of value is amenable to admitting the processual considerations of valu-ation, and need not necessarily be something that applies only to commodities. Value can be aligned to values – worthy of espousing – and, since the subjective dimension of value and values is acknowledged, it allows for admitting the politicized nature of the contest for values.
This, more or less, gives the theoretical base that gives rise to DMV; it fundamentally recognizes that the notion of value extends far beyond pegging it to an invariance principle, but instead recognizes that other “non-economic” forces play a role in the process. Furthermore, it explicitly recognizes the contested nature of values which is a necessary precondition for any disciplining and mimesis to occur. Thus, the notion of valuation in DMV is lifted directly from Kaul’s work and the concept of valuation necessarily invites inter-disciplinary commentary given the incommensurability of value not located on an abstract invariance principle. Instead of value arising out of an invariance (labor or utility) it instead is subject to a wide variety of considerations that may not explicitly be “economic” in nature. For example, when I consider the value of a pen, its value does not merely arise out of satisfaction it gives me, but rather there’s a whole variety of psycho-cultural factors playing in my mind when I come to that decision. The pen might have been given to me by someone close and because the community I was socialized in values friendships a certain way, I may extract from it an additional type of value like sentimental value. In this way, valuation precedes any claim to the utility of a good and, instead, value becomes realized through a complex interrelation of social, individual, and cultural pressures. These pressures ultimately push us to explore disciplinarity.
Next, the idea of economic development presents in itself a teleological narrative sourced in an imaginative rendering of universal progress. The developmental ideology itself can be said to stem directly from the civilizing mission posited by colonial actors. The civilizing mission is premised on a singular distinction: that the European societies are superior to non-European societies. Such a distinction was carried out through a variety of methods either through race, religion, cultural prejudice, etc. Hence, the civilizing mission can be characterized as the European compulsion to “uplift” the “wild and barbarian peoples.” Its teleological ends derive explicitly from resolving the contradictions of the universalist principles of the liberal Enlightenment and the racial and ethnic markers (or any other essentialist rendering of difference) of colonial difference that justified brutal repression. Thus, colonial policy often admitted inconsistency and contradiction. In the so-called decolonized world, the functional goal of assimilative pressures is supported by the imposition of development as the fundamental ends of the decolonized society. Development in economics can be analyzed through numerical metrics and indexes like those concerning gross domestic product, unemployment, productivity, stability, etc. Now, it’s not merely the improvement of these metrics that specifically disciplines, but rather their relation to the prescriptive measures taken to raise such levels. Much like Baptism and Christianity were seen as prescriptions to civilize and assimilate colonized peoples, the dominance of the Washington Consensus, International Monetary Fund, and World Bank and the liberalization they propose can be seen as providing the essential prescriptions of development even if such policies prove counter-intuitive to national interest. Eiman O. Zein Elabdin and S. Charusheela frame the centralization of development in Economics:
As a discipline, it has upheld the narrative of ‘development’ as the center piece of its theoretical construction of formerly colonized regions, presuming the ontological precedence of modern European societies as a basis for its theory of history…As Feyerabend put it, the discourse on development in effect renders patterns of life outside the (Western) industrial world a mistake.
This basis of development on the European model finds its support in the active constraint of alternative developmental methods by institutions and the operational logic of international institutions. For example, a significant portion of Francophone Africa finds itself tethered to the paternalistic monetary union that ties national currency to the French Franc. The need to defend the convertibility peg actively constrains macroeconomic stimulus policy; it sets a hard limit (approximately these countries must keep 65% of currency reserves in the French Central Bank) on fiscal and monetary policy in the African countries. To overspend under the regime of the currency peg is to devalue one’s currency and subsequently contribute to vast inflationary pressure. Even more evidence to this are the numerous conditional lending programs sponsored by the IMF and the World Bank which provide aid under the condition that a set of prescriptive policies perform. Hence, I derive the disciplinary aspect of DMV; it sits in the uncomfortable intersections of disciplinary power and bio-power best resembling the norm that operates, much like the economic theories of value, on a point of invariance for it, “introduces, as a useful imperative and as a result of measurement, all shading of individual differences.” It is disciplinary in the sense that it seeks to meld individuals to a certain cultural point of being and touched with bio-power as this is a strategy not employed at any single individual, but entire cultural groups via civilizing ‘x’ group or developing ‘y’ country.
Lastly, the mimetic derives from Bhabha’s reflection on mimicry and in many ways this theory can be seen as drawing out some of the necessary elements of mimesis. Mimicry, more or less, is the performative adoption of specific cultural values from the colonizing culture. In defining mimicry, Bhabha puts up an important limiting factor for such an act, “almost the same, but not quite.” Given the historicity, the distinctions and limiting factors that constitute colonial power, and the agency and ties of the colonized individual, the assimilative drive of DMV is never fully complete, one does not mesh themselves completely into the culture of the dominator and lose themselves completely. DMV portrays a partial mode of mimicry, but never a complete homogenization and unity of the colonized and colonizer. From this observation though, I must distinguish between myself and Bhabha. While I am indebted to his reflection on the phenomena, the mimesis I describe does not touch on the disruptive factors to colonial power that Bhabha points to. Rather, my interest is to what degree the cultural values of the colonizer were the object of imitation and assimilation as this is a work concerned with a specific historical socio-economic process. A more detailed discussion on Bhabha’s work will follow as I draw out DMV.
Disciplinary-Mimetic Valuation is, in the historical empirics of this analysis, the process in which European colonial states sought to assimilate (or “civilize”) the people it colonized into its universalist cultural value schema and the extent to which the colonized people attempted to assimilate into or mimic such. In the more general sense, it can be attributed to any power that seeks to assimilate another society into its socio-cultural schema by coercive force insofar as someone seeks to apply these principles to imperialist phenomena escaping European colonialism proper. Thus, in the thrust of developing extractive colonial economies and building administrative systems to impose those desires, we have the foreground for DMV in action. What will follow throughout the rest of this work is understanding the ways DMV has played out in specific analyses. Since it has a fundamental historical character, each incidence observed will have its proper contextual elements that provide unique aspects to the phenomena of Disciplinary-Mimetic Valuation. Furthermore, before any serious analysis begins, DMV is situated specifically and does not occur uniformly or simultaneously; it is fractional and the effects are prone to conflicts of that nature.
Disciplinary-Mimetic Valuation: Cultural and Economic Homogenization in Action
Syed Hussein Alatas starts us off in describing the starting point of this valuation, he points to a significant causal reason in the homogenization of cultural values to European colonizers. It develops from, what he considers, the fundamental source for orientalist mischaracterizations of Malays: their unwillingness to engage in colonial capitalism. Alatas writes of the diminished attribution to natives, “Any type of labour which did not conform to this conception (capitalist timed labour) was rejected as a deviation. A community which did not enthusiastically and willingly adopt this conception of labour was regarded as indolent.” From this passage, we see the attributing of a negative image to function as a disciplinary tool. First, we establish the point of invariance, the norm so to speak (A specific labor relation) Alatas points us to. Next, we note the difference in empirical adherence: the Dutch and English adhere to this type of labor, but the Native does not. There next needs to be an explanation as to why this differential exists; we are left to question the moment and conditions of observation. Alatas does this for us as we are to recognize the material elements concerning Malay society, namely commercialized agriculture, “The root cause of this image was the Malays’ reaction to cash-crop agriculture and to working in colonial capitalist estates and plantations. They avoided the most exploitative kind of labour in 19th century colonial capitalist undertakings.” Hence, there exists an industry in want of labor and a people unwilling to become subservient to it. The explanation for the differences, given proper context, is rooted in the satiation of economic processes in which a specific set of disciplined labor is necessary. The characterization of Malays from European observers becomes centered around labor, Malays become “indolent and lazy,” as opposed to “industrious.” We see here a disciplinary tool in action, one aimed specifically at convincing the European reader of the inferiority of the native while also complementing the civilizing mission. We learn from this distinction that there are preliminary necessities for DMV to occur. First, there must be a difference imbued with power differentials. Second, there requires in the dominating party a belief in their supposed superiority explicitly or implicitly held. Alatas connects the image of the lazy Malay as an integral functionary in colonial ideology. Much to the same, DMV draws on a supporting ideology of superiority/inferiority of peoples.
While Alatas points us to the proper requirements for the occurrence of DMV, we find its empirical implementation exploiting fractures in colonized societies existing in the works of Anthropologist Marshal Sahlins. Sahlins, in his analysis, is concerned predominantly with what he terms a structure of conjuncture or the reproduction of a culture that also transforms it and reinscribes new values to old cultural categories. Sahlins sketches out a portrait of Hawaiian society organized around mana. Mana can roughly be described as power typically supernatural that legitimizes the right to rule. Sahlins himself describes mana in several different ways. When describing pre-colonial mana, Sahlins writes that it was won from the great feat of the chiefs, “The mana of the chiefly heros is extraordinary, but more appropriate to a human sphere than were the supernatural gifts of their predecessors. The chiefs win success by their subtlety, courage, skill and strength.” Sahlins gives another description of mana saying, “mana is the creative power Hawaiians describe as making visible what is invisible, causing things to be seen, which is the same as making them known or giving them form.” From this, we can see a high prioritization for the acquisition and accumulation of mana.
Now, Sahlins describes the receiving of Europeans on Hawaiians own cultural terms and through their adjustment and adaptation of their cultural schema. Following the thread on mana, European goods and people were considered to possess considerable mana and was used for appropriation to reinforce or disrupt precolonial structures prevalent within Hawaiian society. For example, the killing of Captain Cook represents the appropriation of mana and the interrelation of it to Englishness:
Through the appropriation of Cook’s bones, the mana of the Hawaiian kingship itself became British. And long after the English as men lost their godliness, the Hawaiian gods kept their Englishness. Moreover, the effect was to give the British a presence in Hawaiian affairs that was all out of proportion to their actual presence in Hawaiian waters, since they were rapidly displaced in the sandalwood trade by the Americans.
From this we can see the mimetic effects quite clearly, while retaining their own conceptual and cosmological categories, they begin to be partially adopted by the Hawaiians. Interestingly enough, Sahlins makes note of the economic withdrawal of British in the area, yet the presence of their cultural influence prevails despite the waning of hard British power. The disciplinary aspects, instead, become enforced by the own cosmological categories of the Hawaiians themselves and give way to the fractious exploitation of DMV. First, it did so by legitimizing Kamehameha’s conquest of the Hawaiian Islands and his European policy of friendship with the British; Sahlins writes:
Yet the means…by which the great Kamehameha turned that commerce into practical account was his own special relation to British power…Transmitted through the bones of Captain Cook, Kamehameha’s special relation to European mana gave him enough of it, in the forms of guns, ships and resident advisors, to conquer the islands.
Further, the chiefs even started to act and imitate the British themselves as a means of perpetuating and reinforcing their social dominance from their subjects. Clothing was of particular importance:
The Hawaiian chiefs seized upon the European distinctions between ‘plain’ and ‘fancy’ cloth to mark their own distance from the common people…European mana in the form of domestic possessions now replaced military supplies as the principal means of aristocratic competition.
Conversely, European interaction with the Hawaiians also led to conflict between the chiefs and the commoners. First, the commoners themselves had competing economic interests that ran converse to chiefly tabus on trade. Commoners had explicit interest in the accumulation of European mana observable in the exchange of sexual labor of the natives for European goods. Even in open hostility following Cook’s death, these relations still held in secrecy. In short, Sahlins summarizes the fractious effect of European contact on Hawaiian society; he writes:
The respective relations of chiefs and people to the European presence thus set them in practical opposition to one another. The complex exchanges that developed between Hawaiians and Europeans…brought the former into uncharacteristic conditions of internal conflict and contradiction.
Sahlins portrays the structure of conjuncture though not as an assimilative or mimetic experience, but one in which the Hawaiian social order appropriates these interactions to reassert and recast itself. I depart from this conjecture only insofar as the recasting and resetting of the Hawaiian social ordering produced mimetic effects, namely in the chiefdom, of European cultural distinctions. I emphasize heavily that the mimetic effects themselves are not all-consuming or even prominent at the early stages of colonial contact, but are ultimately partial. The stipulation that Europeans had superior mana and the dependencies those created established the base means for DMV’s internal logic to take root. Discipliniarity itself started in repositioning and fracturing of Hawaiian power and its social order.
Bhabha’s conception of hybridity may help us elucidate further how cultural mimesis in DMV works. Bhabha holds that postcolonial subjects exist at the “in-between” or “splitting” of cultural consciousness that inevitably places the colonized subject in a state of hybridity by creating a unique space of enunciation, “The social articulation of difference, from the minority perspective, is a complex, on-going negotiation that seeks to authorize cultural hybridities that emerge in moments of historical transformation.” What these postcolonial hybridities necessitate is some degree of mimicry for them to be constituted as a hybridized entity otherwise the very notion of a hybridized postcolonial identity fails to obtain. Mimicry, for Bhabha, creates an alternative knowledge or viewpoint within the cultural schema that simultaneously threatens its relation to truth concerning colonial images. Bhabha writes, “It is the process of fixation of the colonial as a form of cross-classificatory, discriminatory knowledge within an interdictory discourse and therefore necessarily raises the question of the authorization of colonial representation.” Hence, Bhabha sees in mimicry a way of undermining and de-authorizing negative colonial images and there is some truth in this. Insofar as DMV operates though, its mimesis first signals a capitulation and perhaps one that is unavoidable of the precolonial consciousness. I read Bhabha’s conception of mimicry as starting from an initial departure and transformation (although not necessarily self-conscious as Sahlins’s analysis of the Hawaiians shows) of the precolonial culture to become more amenable to the dominating one.
If Sahlins points to the early and fractious implementation of DMV, Chatterjee draws it out in more substantial and concrete terms when he describes the nationalist response to British occupation: a necessary privacy of the inner or spiritual domain of culture and the homogenization of the outer domain. Chatterjee’s analysis of Indian Nationalism points us to understanding DMV as it operates in nationalist contestation; disciplining the outer presentation while never quite penetrating the privacy of the spiritual domain. To understand the proper context in which Chatterjee introduces these concepts, we need to understand that Chatterjee postulates an idea of colonial difference that operates quite agreeably with the characterization of the civilizing mission and Alatas’ observations. Chatterjee covers some debates among British intellectuals and colonial administrators in relation to liberal and conservative colonial policy as they perceive colonial difference. Prior to the development of Hindu nationalism, India was subjected to the Rule of Colonial Difference which set race as its distinguishing marker. Such rule justified the conservative tendency to perceive the British colonial subjects as incapable of self-rule while simultaneously throwing the universalist claims of British liberalism into question. If the colonized subjects of the British Empire are incapable of self-rule based on differences according to race, the universalist mission of juridical liberal rights, equal protection under the law, and property come into direct conflict with the realities of colonial rule and discourse. Chatterjee himself makes note of this when considering Ripon’s failure in the Ilbert Bill affair, “What his ‘failure’ signaled was the inherent impossibility of completing the project of the modern state without superseding the conditions of colonial rule.” The immediate tension being, again, the so-called applicability of universalist values confronted with the epistemic and material alienation of the colonized and the discursive justification for their occupation. For colonized India, this meant adopting some of the material habits that fall into line with an enlightened, liberal criteria of humanity.
Hence, for the modern state to succeed in its mission, it needed to grant the conditions that fundamentally led to the disavowal of colonial rule itself. There needs to be some degree of mimetic or assimilative adjustment in the colonized culture to give way to the modern state. For Chatterjee, This begins with the spiritual and material modernization of the Indian national consciousness; this is also where we can find the machinations of DMV. It is most obvious in the material domain which Chatterjee distinguishes as, “the domain of law, administration, economy, and statecraft.” The homogenization and erasure of difference in this domain of culture shows the ascription of the administrative, legal, and economic aspects of Indian society into the teleological cultural schema of the Enlightenment. The modernization of the Indian state becomes pegged to abstract European notions of progress and development. We see in the material realm the purest form of DMV as the Indian economy begins to resemble Europe in administrative structure and economy. Chatterjee writes:
In this, it seemed to be reasserting precisely the claims to universality of the modern regime of power. And in the end, by successfully terminating the life of the colonial state, nationalism demonstrated that the project…could be carried forward only by superseding the conditions of colonial rule.
Disciplinarity becomes melded with the conditionality of political decolonization and independence; it is a necessary precondition that the Indian population mimic, in some respect, British materiality.
If in the outer domain, we can see the adoption of British material values, The Inner-Domain (language, religion, family and personal life) represents the exemplification of the partiality of mimesis. In the inner-domain, the nationalistic India posits the alternative knowledge that Bhabha gives, it creates a conception of Indian-ness as a legalistic and political equal once independence is achieved. However, these inner-distinctions themselves do not completely escape the influence of disciplinarity either through modular forms of expression (like the novel) or through institutional arrangements like the university system. More explicitly, we can see the effect of DMV reproducing the same class distinctions in post-colonial states that, although not exclusively occupied by a nationalistic middle class, lends itself to their expression by way of material domination. Chatterjee demonstrates this explicitly with his analysis on Bankim’s nationalism; he writes on the prospect of Bankim’s consideration of a true Bengali history, “The historical consciousness he is seeking to invoke is in no way an ‘indigenous’ consciousness’ because the preferred discursive form of his historiography is modern European.” Coupled with Bankim’s antipathy for Muslims and their representation of India, we can see mimesis and acculturative practices touching the inner aspect of nationalistic consciousness; the modular and methodological forms of transmission were instrumental at this point of contact. Nationalistic histories tended to perpetuate the exclusive aspects that were articulated by colonial difference and impose them on other colonized peoples. While one may make note of historical antagonisms prior to colonization between Muslims and Hindu peoples, this does not draw away from the mimetic representations and othering of Muslim populations in nationalistic imaginations. In particular, the othering of the Muslim is complementary with western orientalist renderings of Islam in general; Islam is portrayed as a functionary of stagnation and is given a “medieval” characterization in the nationalistic consciousness. Muslims become, “fanatical, bigoted, warlike, dissolute, and cruel.” Hence, DMV can also lead to the articulation of negative images shared in common with the dominating power. It is important to note that the mimesis of Bhabha draws heavily from his concept of cultural hybridization that is antithetical to nationalism itself; it is another distinction I draw from what I interpret as mimicry. Overall though, while partiality is given, the inner domain of culture is not entirely immune from DMV’s influence.
Nothing gives a more explicit portrayal of the intersections of mimesis and coercion than the first chapter of Black Skin, White Masks. Fanon portrays such a process as it inheres on the Black colonized subject and France and points explicitly to the opportunities it allows for individuals suffering under colonial occupation. In this particular occurrence, we can see Bhabha’s postulation of, “almost the same, but not quite.” Fanon points to unique material opportunities that are provided through the adoption of the colonizer culture, “The colonized is elevated above his jungle status in proportion to his adoption of the mother country’s cultural standards…In the Senegalese regiments, the black officers serve first of all as interpreters.” However, it is not only material opportunities, like that of an interpreter, that gives rise to mimesis, but also a feeling of psychological factors that contribute to a feeling of equality:
The wearing of European clothes…using European furniture and European forms of social intercourse; adorning the Native language with European expressions; using bombastic phrases in speaking or writing a European language; all these contribute to a feeling of equality with the European and his achievements.
Both of the reasoning for the adoption of European cultural values stems from the disciplining of racist images of colonial rule onto the colonized, Mimicry supposes an alternative way to remove the brutalizing force of colonial occupation and racism. Fanon, himself, asserts this when he writes, “Historically, it must be understood that the Negro wants to speak French because it is the key that can open doors which were still barred to him fifty years ago.” This mimetic pathway, however, is limited by the persistence of colonial difference still present in the consciousness of racist Europeans and the internal splitting that is required from the minority consciousness when faced with marginalization. The partiality of mimesis is not only preserved by the internal conditions of the colonized consciousness, but also supported by the vestiges of colonial difference be it presented in explicit and historical understandings of race and skin color or disparate material outcomes. Thus, DMV never completes the disciplining to the dominating culture, it cannot because complete homogenization eliminates any notion of mimesis. Rather, if we were to deny the inner domain of the colonized subject, we would only be speaking of absorption. Returning to Chatterjee, we can find a similar thought:
A relational opposition of power necessarily meant that the dominated had to be granted their own domain of subjectivity, where they were autonomous, undominated. If it were not so, the dominators would, in the exercise of their domination, wholly consume and obliterate the dominated.
Instead, partiality is preserved and with it a split consciousness: one in which the disciplining powers of the dominant European culture operates and influences, but never fully takes hold. This is because the historical markers of colonial difference that initially gave way to the discursive justification for domination, disciplining, and extraction persist and are realized to this day; what started this process also prevents its completion.
Necessary Questions, Philosophical Problems, and Concluding Remarks
I have demonstrated a fragmented account of a theory of cultural valuation that in and of itself is not entirely complete. Nor could it be, it requires fundamentally contextualized historical readings and interpretative accounts of European colonialism. Rather, as a theory of value it strives to fundamentally rethink how economic value is constituted. What will follow now is a reflection on some questions that might have relevance to this theory and its placement in the economics discourse.
Perhaps a prominent question is what distinguishes DMV from anthropological and sociological accounts of acculturation. DMV itself might be seen as a particular theoretical application of acculturation and I have no qualms with this, but I would stress unique markers of this theory and the potential of it for further work. First, I draw upon a tradition of poststructuralist and postcolonial thought to inform some of the ontological basis for this theory. First, I appropriate Foucault’s notions of disciplinary and bio-power and do the same for Bhabha’s mimicry. In particular, poststructuralist readings tend to draw significantly from semiotic accounts of signification. Sahlins, who is far from a post-structuralist, also recognizes the importance of sign analysis when he considers the following on value:
The value of any cultural category whatever such as ‘land’ is indeed arbitrary in the sense that it is constituted on principled distinctions among signs which, in relation to objects, are never the only possible distinctions. Even an ecological anthropology would recognize that the extent to which a particular tract of land is a ‘productive resource,’ if it is at all, depends on the cultural order in place. Economics might thus find a place in the general semiology that Saussure envisioned—while at the same time hedging the entrance requirement with restrictive clauses.
Second, I am primarily concerned with the colonial situation in developing this theory, but do not see it exclusively applicable to that situation. Thus, if this theory is to be designated as just another theory of acculturation; I would prefer it be one rendered in the connecting of Economics to cultural values on a poststructuralist and postcolonial ground.
What remains is how this theory can be integrated into an emancipatory politics. Bhabha certainly recognizes hybridity as the fundamental ontological grounding of the postcolonial world and I am inclined to agree with him because the sheer material and historical effects of colonialism appear to be cemented in the international world and the emergence of new social identities. Rather, I see Bhabha as expressing a social reality and find that the notion of a hybridized social identity itself points to a new individualism that supplants the European rationalistic one. This individual follows no rational choice theory, does not give way to the clear mathematical formalization of neoclassical economics and their subsequent limitations, but rather draws from a variety of historical situations and identities that contextualizes and situates decision making. It is not a totalizing individuality; the economic individual becomes contextualized and situated in a variety of cultural nexuses that resists the invariances of economic theories of value and the oppressive fixing of cultural differences necessary for a rationalization on the colonial rule.
However, work still needs to be done. Kaul calls upon bringing the cultural politics of recognition to theories of value overall which I do not believe I have adequately done. Given this is a fragmented, incomplete, and non-exclusive theory, there is also considerable expansion and revision necessary for it to achieve proper clarity and rigor. There is also room to develop more extensive, complementary or antagonistic theories of value. Furthermore, there exists work to be done that far escapes the reaches of my thought and I am only a historically situated individual. So, to answer the question about how this theory can be integrated into an emancipatory politics, I answer that it provides the basis for new theorizing and new histories that provide the theoretical basis for such a politics, while fundamentally displacing a Eurocentric universalism premised on a culturally specific reading of rationality.
References
Alatas, Syed Hussein. The Myth of the Lazy Native London: Frank Cass and Company Limited, 1977.
Bhabha, Homi. The Location of Culture. London: Routledge, 1994.
Charusheela, S., and Eiman Zein-Elabdin, eds.Postcolonialism Meets Economics. London: Routledge, 2004.
Chatterjee, Partha. The Nation and Its Fragments. Princeton: Princeton University Press, 1993.
Fanon, Frantz. Black Skin, White Masks. London: Pluto Press, 1986.
Foucault, Michele. Discipline and Punish. New York: Random House, 1995.
Kaul, Nitasha. Imagining Economics Otherwise: Encounters with Identity/Difference. Abingdon: Routledge, 2008.
Sahlins, Marshall. Historical Metaphors and Mythical Realities. Michigan: The University of Michigan Press, 1981.
Tricoire, Damien, ed.Enlightened Colonialism: Civilization Narratives and Imperial Politics in the Age of Reason. Cham: Springer International Publishing, 2017.
Williams, Oral, Tracy Polius, and Selvon Haze, “Reserve Pooling in the Eastern Caribbean Currency Union and the CFA Franc Zone: A Comparative Analysis,” Savings and Development 29, no. 1 pp. 39-60.
Endnotes
1.) . It should be noted that this is a gross over-simplification of the Labor Theory of Value and the unique takes presented by classical economists.
2.) . Nitasha Kaul, “Writing Economic theory anOther Way,” Postcolonialism Meets Economics, ed. S. Charusheela & Eiman Zein-Elabdin (London, Routledge, 2004) P.184.
3.) . Ibid., P.187
4.) . Nitasha Kaul, Imagining Economics Otherwise: Encounters with Identity/Difference (Abingdon, Routledge, 2008), P.210
5.) . Ibid., PP.210-211
6.) . Damien Tricoire, “Introduction,” Enlightened Colonialism: Civilization Narratives and Imperial Politics in The Age of Reason (Cham, Springer International Publishing AG, 2017), P.2
7.) . Ibid., P.9
8.) . Eiman O. Zein-Elabdin & S. Charusheela, “Introduction: Economics and Postcolonial Thought,” Postcolonialism Meets Economics, P. 2
9.) . Williams, Oral, Tracy Polius, and Selvon Haze, “Reserve Pooling in the Eastern Caribbean Currency Union and the CFA Franc Zone: A Comparative Analysis,” Savings and Development 29, no. 1 p. 43.
10.) . Michele Foucault, Discipline and Punish (New York, Random House, 1995), p. 84
11.) . Homi Bhabha, The Location of Culture (London, Routledge, 1994), p. 86
12.) . Syed Hussein Alatas, The Myth of the Lazy Native (London, Frank Cass and Company Limited, 1977)
13.) . Ibid., p. 70
14.) . Ibid., p. 74
15.) . Ibid., pp. 75-78
16.) . Ibid., pp. 1-2
17.) . Marshall Sahlins, Historical Metaphors and Mythical Realities (Michigan, The University of Michigan Press, 1981), pp. 5-8
18.) . Ibid., pp. 15-16
19.) . Ibid., p. 31
20.) . Ibid., p. 7
21.) . Ibid., p. 26
22.) . Ibid., p. 31
23.) . Ibid., pp. 43-46
24.) . Ibid., p. 50
25.) . Bhabha, Location of Culture, p. 2
26.) . Ibid., p. 90
27.) . Partha Chatterjee, The Nation and Its Fragments (Princeton, Princeton University Press, 1993)
28.) . Ibid., p. 18
29.) . Ibid., 21
30.) . Ibid., p. 26
31.) . Ibid., p. 77
32.) . Ibid., p. 102
33.) . Although Bhabha, himself, distances himself from Fanon’s analysis due to the fundamental factor in motivating the adoption of the dominating culture. Bhabha p. 88
34.) . Frantz Fanon, Black Skin, White Masks (London, Pluto Press, 1986), p. 18
35.) . Ibid., p. 25
36.) . Ibid., p. 38
37.) . Partha Chatterjee, The Nation and its Fragments, p. 161
38.) . Sahlins, Historical Metaphors and Mythical Reality, p. 6"	43925
economy	['Fedorov', 'Gregory A']	2018-05-22 00:00:00	"Abstract Devastated by an economic collapse at the end of the 20th century, Japan’s economy entered a decade long period of stagnation. Now, Japan has found stable leadership, but attempts at new economic growth have fallen through. A combination of public desire for economic security through lifetime employment, reliance on “economic bureaucracy” of large corporations and pressure from international powers have left Prime Minister Shinzo Abe few options for fulfilling his “Abenomics” dream. Japanese leaders have continuously attempted to restructure the economy in an effort to jump-start growth. But in order to create long-term economic growth, Japan must embark on decades long restructuring of its underlying cultural values. Japan is burdened by “path dependence” and resistance to change created by the institutional power of the private sector and a culture obsessed with being normal. The central challenge of the country is the time-scale mismatch between the urgency of economic reforms, which must occur quickly, and the slow dynamics of social and cultural transformations, which require generational changes. Japan’s government must establish a new culture within the traditional culture of Japan, as a way to lower the social barriers for change, to ensure economic prosperity of the country and its people for years to come. This long-term strategy requires focusing on the younger generation in order to prepare the society for the economic and social uncertainties of the new reality emerging in Japan and globally. Abe’s government must establish a comprehensive program that provides numerous and sustained opportunities for the Japanese youth to become more creative and unorthodox in educational and professional development, emphasizing significant exposure to global opportunities from the earliest age. By breaking the traditional hierarchies in the society with an influx of global social and cultural experiences, the reorganization towards a more dynamic and robust economic model with significant entrepreneurship and innovation components would naturally emerge and become self-sustainable in Japan.

A complex hierarchy of relationships and mixed personal-cast-institutional dependencies in Japanese society has been established over the course of many centuries. The economic path towards establishment of large, highly integrated corporations with life-long employment contracts was a natural “culturally-driven” process for Japan, which brought significant economic benefits and competitive advantages during the period of rapid post-World War II industrialization. As has been examined in many publications [1], Williamson’s transaction cost theory behind the formation of large firms is directly applicable to Japan.[2] During the 20th century, mutual dependence of corporations and employees resulted in a rise of complex contractual obligation in the form of lifelong employment. However, in the last several decades and especially since the information technology revolution of the 21st century, the role of a human worker possessing only basic skills has greatly diminished in economic activity that requires quick adaptation to changes in a globalized world. On one hand, workers become forced to be more flexible in their employment opportunities, as they can no longer be guaranteed that a company will stay committed to them. On the other hand, the removal of lifelong contracts voids the fundamental prerequisite for the creation of corporate hierarchies. This makes an adherence to lifelong employment suicidal to both the workers and corporation, imposing a significant stress on Japan’s socio-economic system.

The necessary adaptation to new economic reality is difficult for Japan due to fundamental norms of Japanese culture emphasizing social stability and long-term security. If Abe continues to pursue standard Western strategies of jump-starting the economy through monetary and fiscal policies, he will likely fail. He needs to exploit a new regional free trade agreement to expand the Japanese export markets and to invoke bold societal changes challenging the Japanese social norms by stimulating entrepreneurship, economic independence of younger workers and making economic growth the people's’ priority. But to fully succeed in rebuilding the Japanese economy, Abe must not only engage in tactical near-term fixes, such as participating in free trade, but most importantly focus on long-term cultural transformation projects.

In this work, I attempt to examine and substantiate the above thesis by analyzing Japan’s cultural roots underpinning its socio-economic development over several centuries in the context of evolution of Japanese government structure and external influences from the Western world. I show how the hierarchy of institutions and practices, which have been imposed from the top over many centuries as “Japanese way of life,” has led to an inflexible social contract between the working population and the country’s governing elite. This social contract presents an overwhelming barrier to transforming a rigid and slowly degrading industrial machine of today’s Japan into a flexible and adaptive economic system. Ultimately, I pose a challenge to Japan’s leadership to find a balance between the aggressive monetary and fiscal policies and the long-term social transformation of Japanese society to support sustainable economic gains without sacrificing societal harmony and cultural tradition. The long time required for sustained social transformations and the complexity of “social engineering” approaches when applied to a country with strong cultural traditions make the bold social change agenda more pressing than economic policies.[3] It must start immediately and target the youth, with broad understanding and support from older generations and significant financial commitment from the government. This will give Japan the best chance to succeed in the rapidly developing world for centuries to come.

Pre-War Japan

Japan’s rapid economic growth due to its strong and unique labor system following a long period of isolation is unique among advanced industrialized nations. The roots of growth started with the Edo period under Tokugawa rule (1603-1867). Japan’s feudal system rose to nationwide prominence in the 1100s. The persistent presence of moral obligation as a means of class distinction allowed the system to last longer, until the Meiji Restoration. The generalization that Japan’s feudal system was universal until the Meiji restoration of 1868 and immediately fell apart following the restoration is false.[4] Japan’s economic sector today is still heavily influenced by principles of social hierarchy. The system of respect between samurai and peasants proved stronger than the European system, under which knights expected profit. While increased growth of a middle class and currency interactions were important to the demise of European feudalism, the European enlightenment movement in the 18th century was the biggest stifling factor of the feudal system.

Revolutions, rebellions and the idea of individual rights gave the working class a motivating factor to protest. Individualism motivated entrepreneurship, which quickly blended rigid European class distinctions. Sakoku , the Shogunate’s policy of isolation, did not completely hinder the spread of European ideas.[5] This lead to the Edo state loosening its isolation policies to learn more about the west. A specific example was the Dutch trading port in Dejima where scholars kept in tune with Yogaku and western studies.[5] The century-long isolation of Japan created a social atmosphere in which fast changes in people's mentality and adaptation to new workplace reality became fundamentally impossible. Japanese isolation from western revolutions and ideologies helped the Japanese transform into a modern economic society much smoother while maintaining many feudal beliefs.[5] However, reliance on the government and guaranteed employment at large corporations has greatly hindered Japanese innovation and entrepreneurship.

Most of the labor force during the Edo period engaged in agricultural production. Almost 90% of Edo society were peasants, who were not the lowest tier in the social hierarchy.[6] This was crucial to the system’s long-term survival because it put a larger economic focus on peasants rather than merchants. Since the peasants were the tax base, law dictated that peasants cannot move, but peasants ignored the regulations both for entertainment and better economic opportunity.[6] The local system of tax collection was important to the success of the shogunate. The system is a very early form of lifelong employment which has been a major backbone for economic success in Japan. The Daimyos practiced salutary neglect as long as farmers paid their taxes. Sengoku Jidai allowed for a large expansion in farmland, giving even more weight to farmers. During this period the farmers bribed field inspectors to understate crop output.[6] This early form of bribery marks the emergence of corruption among Japanese bureaucrats, which put a heavy economic burden on the country in the 20th century. Rapid expansion of farmland began spiraling out of control, eventually contributing to the demise of the Tokugawa Shogunate . The large farmer class was forced to compete over a small amount of land. The government was responsible for over-regulating the system, thus giving birth to the belief that economic opportunity is achieved through the government policies. Uprisings by the farmer class marked the end of feudal society as the government failed to adapt its policies.

Commodore Perry’s arrival in 1853 signaled the end of feudal isolationism to the Tokugawa shogunate. The Bakufu realized that Japan had been completely eclipsed economically and technologically. Perry’s demands to the Tokugawa were mutually beneficial in terms of trade.[5] A combination of economic allure and fear of the unknown forced the Tokugawa to agree on the terms set forth by the Americans. The Convention of Kanagawa, which opened ports in Shimoda and Hakodate, was the defining point in the end of Japanese isolationism and the beginning of not only an unstable economic climate but isolationism of imperial rule. Japan’s economy started to rapidly decline following the opening of the country. Tokugawa Yoshinobu was ousted, and political power violently transferred to Emperor Meiji. The Charter Oath outlined the goals of the new imperial government, which wanted to rapidly advance the country technologically through urbanizing the workforce.[7]

Under Emperor Meiji, The Land Tax Reform of 1873 truly marked the beginning of a labor revolution. It reformed the tax system in a way that farmers had to assume the financial penalty of agricultural market fluctuation. The farmer had to pay taxes based on the value of land not the annual crop yield. While the new system helped establish independent commerce and private land ownership, it indirectly exacerbated the issue of wealth inequality. An attempt to reform already cemented practices had fallen through due to economic damage to lower classes. In 1872, the ban on selling and trading farmland was lifted, and in 1873 Japanese citizens were given the power to use real estate as collateral to secure agricultural loans.[8] Landowners without prosperous land became impoverished, while more successful landowners gained more social and economic power. The Land Tax Reform of 1873 industrialized the country by pushing a large population of workers to urban centers. “It is a well-recognized fact that the outflow of the labor force from agriculture provides a big source of labor for the rapidly expanding non-agricultural industries.”[9] Although the urbanization push created new jobs, these jobs were still tied to the government and had not led to a fundamental change in people's mentality. Paired with rapid population growth in the 19th century, Japanese industrialization outpaced many European countries. One of the most distinctive features of the Japanese economy is that growth in the agricultural sector never increased during economic growth.[9] During the Meiji Restoration, in an attempt to build military and industrial might the government consolidated and became more centralized. This transformation played a critical role in how the government acted throughout the 20th century, both during the period of imperial expansion and economic modernization.

A pattern of reliance on the government has yielded short boosts of growth, but created long-term instability. During the Imperial expansion period and World War II, the government twisted the social contract by inhibiting natural rights in return for national expansion. The government invested heavily in factories and shipyards, which were sold to companies to help guide a powerful private sector.[10] The private sector was successful but not agile and independent, because it was comprised of large companies with demands to their products and services largely coming from government. The development of a national rail system and modernized communications were spearheaded with the help of western technology.[11] The successful economic climate led to a rise of large corporations, which became the main provider of labor security in the late 20th century. The rise of zaibatsu , large business conglomerates, such as Hitachi and Mitsubishi gave the Japanese government an efficient supplier of technology and consistent labor opportunities. The Zaibatsu quickly grew too big to fail, creating departments for almost everything the government needed: whether it was military supplies, nuclear energy, vehicles and chemicals. As time progressed, the Zaibatsu system expanded to other Asian countries. In 1996-97 sales from top 20 Zaibatsu, expressed as percentage of GDP, accounted for 87% in South Korea, 36% in Thailand and 30% in Indonesia.[12] The government saw large corporate companies as its fastest road to achieving global success in a modernizing world. With power centralized imperially, the government adopted a policy of expansion. The invasion of Taiwan in 1874 demonstrated the immense power that the military possessed in government affairs, as this was the first time the military ignored civilian government orders.[13] With increased trade with western nations, Japan managed to quickly grow a large and modern standing army which provided for sizable employment in the public sector. National conscription not only provided a new guaranteed labor force, but also gave Zaibatsu a secure buyer.

Meiji reformers utilized Shintoism’s imperial myth to attempt a removal of Buddhism and make new national interests popular. [15] However, full removal was impossible and Buddhism combined with Shintoism to form ryōbu shitō . With feudalism laid to rest, and people free to build their own future in the new capitalistic society, uneducated Japanese transitioned to new low paid working jobs.[15] The large influx of low-skill labor to urban centers yielded disastrous conditions for workers. As a result, groups of organized workers demanded expanded civil rights. Marxist views began to take form among not only workers but also intellectuals.[14] The government cemented a far-right stance as it formed the Tokko secret policy in an age of nationalism. However, the government made a decisive change in 1911 when new regulations regarding a national minimum wage and work hours took into effect.[15] Japan quickly grew into an economic juggernaut, but the working class remained neglected and labor unions began to take form. Although mistrust in government gave rise to individualism, the economic system that had been put in place was not adequate to support this change in attitude, rendering existing small businesses weak and offering no meaningful path for harnessing the economic benefits of individual entrepreneurship. Three stimulating factors which emphasize the cultural values of equality led to labor reform in pre-war Japan: gender, nationalism and social treatment.[16] Gender played an important role because the gender divide in mill jobs was greater than the class divide.[16] Nationalism had given a feeling of importance to many workers who were eager to give their power for the empire’s success, but bureaucratic regulators and business owners felt the glory was theirs.[16] Finally, Japanese workers felt that their social treatment in the labor system was more important at times than their wages.[16]

Workers mistrust in the government during this period played an important role in Japan’s rebuilding post World War. Even at the point leading up to the war not all groups of workers felt job security. Girls in Tokyo textile mills saw their employment as a temporary occupation rather than a permanent occupation.[16] Nevertheless, there was no sense of hopelessness, as a promise of new opportunities paired with Imperial expansion tempered negative sentiments. As the country gained new territories and grew stronger, more jobs were available both in the private and public sector in order to sustain the growth. A very distinct pattern emerged of entering the workforce for life in order to securely raise a family, which would be a symbol of contributing to the empire. The pattern had been repeated across generations, which allowed post-war citizens to adapt quickly to the system of lifelong employment.

The new system congealed following Japan’s reconstruction as a major power. Andrew Gordon explains that in the case of Japanese labor disputes almost none were caused by machines crowding out low-skill labor.[16] Rather, he suggests that most disputes were driven by a desire for respect and social status.[16] Japan has always had a limited internal labor force, making individual workers more powerful in their demands for job security (i.e., a seller’s market for labor). This observation highlights another pillar in the foundation for the rise of lifelong employment, that is, that a concern for job security is paramount among Japanese workers .

Japan established a large empire, and the increased inflow of resources stimulated further economic growth. Japan’s mining and manufacturing sector accounted for 30% of GDP in the 1930’s.[10] The country’s GDP was expanding at an incredible 5% per year with continued economic focus on making the military strong.[10] Although the Great Depression did not have a profound impact on the country, there was an offspring event which did. In 1927 the Showa financial crisis left most major Japanese banks without capital and gave the financial sectors of Zaibatsu most power in Japanese finances.[17]

Post War Japan

Japan’s expansionary interests eventually led to World War II. The nation overestimated its production capacity and industrial capability in a fight against the world. Japan’s leadership envisioned the Greater East Asia Co-Prosperity Sphere. In order to sustain a ballooned economy over a large and diverse range of human capital, the government required a more effective union of Pacific territory. The eventual goal rested in a self-sufficient union, where Japan would fulfill its labor requirements through colonies while also having a guaranteed consumer of Japanese exports.[18] The system would appeal to people’s desire for riskless jobs because the hegemonic nature of the Greater East Asia Co-Prosperity Sphere would require increased production to remain autarkic thus supporting endless employment for the populace. In many ways Japan used similar principles that the U.S. used during the phase of its initial growth. Three events in early American history clearly correspond to Japan’s ambitions during World War. Japan was mimicking steps which brought the U.S. to superpower status. Japan aspired to become a global superpower (ittô koku ) by eliminating impediments brought about by Western colonization.[18] However, the Greater East Asia Co-Prosperity Sphere failed due to the very same problem it sought to fix: labor inequality in colonial countries.[18] A clear conflict of interest became apparent when examining Japan’s government during this period. The bureaucracy had lost its unity by the 1940s to the military autocracy, army and emperor acting in different roles with almost unchecked responsibility. The Zaibatsu accumulated more power, as the government enforced few regulations and instead needed fast supplies. Yet, the real powerbrokers were “reform bureaucrats” and ultranationalists.[19] These events reinforced the limited economic role regular citizens could play independently in the social hierarchy dominated by government bureaucracy and military-industrial complex.

By the time the war had ended most of Japan’s labor supply was involved in military-related activities. People in Japan were so committed to the 15 year long war that liberation came as a shock.[20] The American liberators came with a plan to rebuild Japan rather than leave it to domestic forces.[20] This optimistic plan paired with the Japanese people’s fervor for social and economic stability made the job of initial recovery easier than in other nations and became known as the “economic miracle.” The government created a comprehensive plan for rebuilding. Large investment and manufacturing firms now known as Keiretsu were to be drivers of internal development. They had nearly one third of overall employment and were going to be given more individual power for carrying the Japanese economy through the period of low-interest rates and low regulations.[21] These are the same firms that led to the collapse of the economy in the 1990s after their power was never taken back by the government. In order to maintain the gift of monopolistic privilege they were expected to provide lifelong employment and make sure the Japanese work force remained competitive.[21]

Japan’s second strategy was to increase the quality of individual labor. The social welfare system suffered eventually as the Keiretsu became centers of bureaucratic deal making.[21] Although the system made it easy for Japan to recover quickly it greatly hindered the quality of social benefits for the poorer citizens. The American occupation is often not given enough credit for causing the path Japan winds up on later. John Dower explains that there are eight critical reasons why the United States succeeded in rebuilding Japan: “Discipline, moral legitimacy, well-defined and well-articulated objectives, a clear chain of command, tolerance and flexibility in policy formulation and implementation, confidence in the ability of the state to act constructively, the ability to operate abroad free of partisan politics back home, and the existence of a stable, resilient, sophisticated civil society on the receiving end of occupation policies – these political and civic virtues helped make it possible to move decisively during the brief window of a few years when defeated Japan itself was in flux and most receptive to radical change.”[22] The most important of these (for Japanese) was “the existence of a stable, resilient, sophisticated civil society”[22] that did not dwell on the past. Japanese people invested their efforts in improving the country by working for the government instead of pursuing individual entrepreneur-driven innovation. World War destroyed close to 40% of Japan’s industrial infrastructure, but following the war the country was able to equip itself with newer and more advanced infrastructure.[10] This made Japanese workers more efficient and capable compared to labor in many other countries in the world. The government's extreme focus on channeling the new flood of labor, who were formerly in the military, into Keiretsu centralized advancement into specific business conglomerates and transferred the responsibility of keeping good working conditions to Keiretsu. The Zaibatsu were one of MacArthur’s biggest targets initially. Under the original plan for reorganization, he wanted to completely dissolve large commercial conglomerates. In his view, the Zaibatsu acted for themselves and spreading wealth out was more beneficial. By that point, laborers grew accustomed to working for large and stable businesses. When the Zaibatsu collapsed, instead of looking for more self-reliance and independent employment workers went simply to work for new corporations.

MacArthur was following a “new deal” type program that in his view would make a more efficient workforce by diversifying industries and taking advantage of the returning working population. While he successfully eliminated several Zaibatsu, many just reformed into looser cross-shareholding Keiretsu. During the occupation six Keiretsu became the most influential: Mitsui, Mitsubishi, Sumitomo, Fuyo, Sanwa and Dai-Ichi.[25] These Keiretsu distinguished themselves from others by all being centered to one bank, and the bank loaned money to member companies in the Keiretsu while also holding equity positions in the Keiretsu companies.[26] Close relationship between the banks, companies and government regulators eventually became an insurmountable detriment to the lower-class workforce.

MacArthur started labor reform from the distribution of property in the land redistribution act. The reform’s purpose was to redistribute land from landlords who did not individually use the land to the low-class workers who rented the land and actually used it.[23] Although many believe this event acted as a de-industrializer of the labor force, it is important to understand the reform did not affect a majority of working Japanese. The land reform only helped solidify the path of corporate labor because by 1965, mining, construction and manufacturing accounted for 41% of the labor force, while agriculture only employed 26% of the population.[10] The land reform acted both as and industrial and political policy.[24] Although the policy had little effect on agricultural output, it dramatically increased the amount of conservative thinking in the urban population.[24]

A majority of economic reform pinned regular Japanese citizens as the backbone for success. In return, the society structured itself to guarantee a job for any hardworking and willing citizen, cementing the decade-long practice of prioritizing corporate vs individual employment. It was a special modification on the social contract, which had been in place for a while, and it helped solidify the reputation in the West that Japanese citizens love work in an uncompromising way. In order to return to a strong economic footing, the government revived traditional values of shame, sacrifice, responsibility and wise decision making.[21] The effort by the government is often described as an “iron triangle” where large corporations had easy financial breaks, yet they had to pay large prices for resources and make sure their labor force was well serviced.[9] This brings about one of the greatest contradictions to present “Western” economic strategies. In “Western” countries, the government more often over regulates the financial system and ignores the workers. An American strategy is making sure the key economic drivers are fair in hope it will eventually benefit the poor. However, the Japanese took a different rout inspired by Roosevelt’s “new deal” and motivated the general workforce to better the large corporate institutions. The new corporations understood that a motivated labor force could be well trained and given pay incentives.

Japanese people had put national GDP growth as their foremost personal priority.[30] Through the corporation's desire for capable labor, the government no longer had to worry about a social welfare net.[9] The “economic miracle” could not have been achieved without the costs the United States government incurred from reconstructing Japan. The United States made sure to sufficiently fund Japanese defense spending well until the end of the Cold War. Also, a complete rebuilding of infrastructure helped the Japanese government focus on molding its labor force. However, in the long-term, Japanese people are still suffering today from the initial labor policies.[9] If a stronger effort had been put into incentivizing self- employment, Japan would have taken a slower but more robust path to economic recovery. There is almost no social security or pension fund for retirees.[9] To the present day, a large portion of Japan’s population is elderly and the child fertility rate is low due to the young generation’s urgency for having a stable position in the unforgiving labor market.

Dower describes the relationship that the U.S. and Japan have had since World War II as the “San Francisco system.” The conservative government had an easy decision to make because the U.S. granted military protection and independence to Japan at little cost.[27] While Dower argues that the system has imposed eight problematic legacies on Japan - “Okinawa and two Japans, unresolved territorial issues, U.S. bases, rearmament, the nuclear umbrella, containment of China and Japan’s deflection from Asia, and subordinate independence” - there have been substantial economic benefits from the close protection umbrella that the U.S. has cast on Japan. Japan’s border disputes have not resulted in destabilizing conflicts and the U.S.’s heavy military armament of Japan has alleviated pressure on defense spending. The time and capital that Japan has saved has been meticulously applied to creating a high growth economy rather than social investment. During the U.S. occupation, two more reforms were passed that established a very rigid labor structure: The Labor Standards act of 1947 and Trade Union Act of 1949. They created a basic system of ensuring workers’ rights were met and made up for not having a strong safety net for the unemployed. The Labor Standards Act has 13 chapters which outline necessary work conditions for human beings.[28] An important inclusion of the act are wage and labor contract provisions. The Trade Union Act made sure workers had the ability to organize and assemble.[29]

Following the U.S. withdrawal, a functioning, well-educated urban professional class, strong bureaucracy and effective agriculture put the country on track for a 5% economic growth rate compared to the U.S.’s 3.8% by the 1980’s.[10] While United States reform was necessary to rebuild the manufacturing sector to pre-war conditions, the motivated and highly educated labor force did a majority of the work. Japan also had a fast modernization of technology due to substantiated corporate investment.[10] The overall system worked on “arbitrary motivation,” where companies were able to attract capable employees with new technology and promise of promotion. The corporations were growing so fast they could guarantee senior positions to almost all employees. Although there was no long-term capacity for management positions, the lifetime employment system made sure that only employees approaching retirement received positions. Also, the companies were so well endowed with resources, they could meaninglessly fill high paying positions with no regard of valuable yield. Recent studies have shown that the Japanese population's propensity to spend helped drive the economic recovery.[30] These claims fall perfectly into the other established characteristics of the Japanese economy for two main reasons.

First, the well trained and motivated workers had very good wages. In the 1960s, a combination of low population growth and increased industrialization resulted in increased wages.[10] Second, low interest rates that were used for rapid growth benefited individual spending habits. At the same time the workers who participated in the lifelong employment system had a friendly relationship with higher management, which allowed for very few labor disputes.[30] This also can be seen as a perpetuator of the Japanese spending habit. The promise of greater financial gains during the later years of a citizen’s career was too tempting, and it led to a demise of the riskier possibility of entrepreneurship. In fact, as supported by statistical data on the number of internal migrants in Japan from 1954 to 2004 (Table 2-37-a in reference [37]), the total number of internal migrants began to decline from 8,026,029 in 1974, and this downward trend continued through 2004 with almost 30% reduction reaching 5,771,921 migrants. This demographic indicator of population mobility, as a proxy for economic opportunism, provides indirect evidence of sustained transition to a less entrepreneurial workforce in Japan, which underpins the beginning of lifelong employment dominance in the 80s as one of the important precursors for the “lost decade.” Companies essentially guaranteed workers higher pay alongside job security as time went on making people more inclined to spend. The political system from the 1950s to 2000 can’t be ignored as Japan has been in control by one party (LDP), which advocated for conservative policies favoring business growth.[30] The LDP’s long standing and consistent platform further benefited workers’ security because the party kept close economic ties with the United States. Despite the Cold War sucking funds away from nations, the keiretsu were safe since the United States was still a reliable consumer of new technologies. The labor system which was so focused on education and motivation also painted its own destiny and security. Other global nations recognized the quality and reliability of Japanese exports, giving an international brand to companies such as Sony and Nintendo.[30]

The system which closely relied on uniform loyal participation from the labor force could not last forever. In the 1970s extreme environmental pollution accompanied by an environment of overworking and stress became a political issue.[10] The social contract in Japan had rapidly changed for centuries and at this point work was intolerable. The success of both the industrialized and agricultural economies heavily relied on the success of the labor market, with labor efficiency being undoubtedly the largest factor when considering the nation's propensity to export. In most well developed nations there are two limiting factors when considering successful international trade: an available buyer and an ability to efficiently manufacture. Japan’s guaranteed buyer was the U.S., but Japan pinned its entire industrial capability on its labor force, which for a long time ultra-efficiently manufactured goods. However, Japanese people no longer agreed that economic success should be the national policy.[30] Both Japanese workers and corporations understood the value of the lifelong employment system. Even though economic stressors such as the oil shock tested the reliability of the economy, quick recoveries from large production covered up the issues that would become apparent later. It continued to satisfy the workers' desire for job security and helped businesses by making training easy and loyalty a standard.

In response to growing displeasure of outdated policy by the labor force, the Liberal Democratic Party passed more social reforms to help create a larger safety net.[30] People quickly benefited from the high growth era in the 1980s with a growing stock market and strong real estate market.[30] The Keiretsu grew quickly out of the banking success stemming from Japanese people’s spending. Japanese society saw the growth as having a negative effect on the people, associating excessive spending with a loss of traditional values.[30] Japanese citizens of all ages preferred a humble lifestyle with no excesses that often come with personal success, and this provided a social background for negative perception of people striving for independent success. In an exaggerated way, it was as if people had tirelessly worked for 20 years then excessively reaped the benefits. A society that had been so focused on maintaining a neutral social structure as a mechanism for post-war national recovery evolved to establish a more distinct class structure than ever before. This is corroborated by statistical data on Japanese households’ income from side work, i.e., outside of primary employment, which can be used as a proxy for the diversity of economic activity among the population (Table 2-22 in reference [37]). The data shows that the number of households with “side business activity” peaked at 123,772 in 1970. By 1980, ten years later, the number of households dropped to 85,243. This suggests a trend of reduced entrepreneurial activity in population, which is typically stimulated by having sufficient number of workers seeking new income opportunities, in addition or instead of favoring stable employment at large corporations.

Nonetheless, as Japanese people bought more homes, banks saw the sky as the limit, issuing incredible amounts of loans, not all of which that were secure. Nobel Prize winning economist Paul Krugman states ""Japan's banks lent more, with less regard for quality of the borrower, than anyone else's. In so doing they helped inflate the bubble economy to “grotesque proportions"".[32] Banks were forced to issue more loans because large corporations grew too big and had to search for other places to finance their capital from bigger foreign banks.[30] Banks felt that due to the lifelong employment system they were relatively safe in issuing mass loans due to the strong job security most people had possessed. When the bubble eventually burst, the labor system was undoubtedly one of the biggest perpetrators in the collapse. Despite the loan bonanza enjoyed by large corporation, the banks did not invest as much into new businesses that could provide engine for economic growth and new opportunities. Statistical data (Table 14-8 in reference [37]) shows that during the 90s and early 2000s Shoko Chukin Bank, Credit Cooperation and Shinkumi Federation Bank actually decreased loans and deposits for small businesses. By 2003, these three banks, in addition to Shinkin Central Bank, Labour Credit Association and the National Federation of Labor Credit Associations all together issued fewer than 100,000 loans a year to new business ventures. In essence, the large banks had lost confidence in the potential for small businesses to succeed. This decrease in loans took place despite a major decrease in a number of reported bankruptcies in late 80’s, with a staggering ~75% drop in just 5 years between 1985 and 1990 (Table 6-17 in reference [37]). One can suggest that this decrease indicates a healthier economic climate which helps new businesses survive; however, since its onset occurred in 1985 when the economy of big enterprises started to collapse, the “positive” trend most likely is another indicator of a dramatic slowdown in entrepreneurial activity in Japan, simply because small businesses are much more susceptible to bankruptcy. In turn, this led to a reduced diversity and increased bureaucratization of the economy with more workers favoring employment at large corporations.

The decrease in private ventures from bankruptcies in the 1980s is also displayed in data showing the number of private institutions. Beginning 1981, the number of privately owned enterprises and individual proprietorships stagnated and slowly decreased by the thousands, especially in Aomori, Iwate, Akita, Tokyo, Ishikawa, Kyoto and Osaka (Table 6-8 in reference [37]. At the same time, the number of corporations and joint stock companies saw a dramatic increase. This further supports a conclusion that entrepreneurialism was no longer viewed as a viable path to achieve economic success and societal preference towards the risk averse economic development relying on larger corporations and stable labor employment policies.

When examining the reason for the unchecked growth many economists agree that inept public policy, corruption, greed, and sentiment that Japan was ""different"" were root causes.[30] As previously mentioned, the central bank kept interest rates low forcing banks to give out more speculative loans in order to maintain growth levels. However, pressure from the United States to maintain a competitive domestic industry through politics of weakening the U.S. dollar forced the Japanese government to rapidly increase its interest rates, which made risky borrowers unable to pay back loans. Japanese regulators were too close to the banks since bureaucrats weren't paid well and often decided that working for a large bank would be more profitable.[30] This issue stemmed out of the rigid labor practices because most of the salary increases were in private corporations while government jobs offered a low retirement age. Thus, government regulators were able to retire early and profit at Keiretsu. This fostered an environment where bureaucrats would focus their time to establishing good relationships with banks in order to guarantee a well-paid position after retirement.[30] Not only did such relationships promote a conflict of interest for regulators before retirement, it also meant they had very good connections and abusive strategies when working for banks after retirement. When the bubble eventually burst, growth became so dismal that huge corporations people called ""too big to fail"" were forced to scrap their system of obtaining domestic human capital.

Japanese corporations had to cut costs, let go of useless workers and move expensive manufacturing to other countries.[30] Loyalty from workers became unimportant when a company is struggled to stay afloat, and senior management positions which provided no value could no longer be tolerated. To maximize labor efficiency part time employment proliferated, which meant the end of low unemployment rates and financial security to workers. By 2009, non-permanent employees constituted more than 30% of the work force.[31] A massive stagnation in wages meant that workers could not spend money to stimulate the economy, and the rapid transition from low to high then back to low interest rates hurt the labor market structure. Savers and retirees were ruined due to their dependence on accumulating funds, which stopped following the collapse.[30] In a country with a large elderly population and low fertility rate, the low interest rates make a well paying and reliable job even more important. However, the leading corporations now outsource a large fraction of labor and are always looking for cheap labor rather than investing in domestic human capital. This pattern will likely persist as Japan enters the next Asian free trade agreement, making cheap transactions easier within the region. The crisis disabled long-term political solutions throughout the 1990s as the country constantly elected different leaders with no long-term vision.[30]

One of the crisis's overlooked impact is the social change it caused. People have not only given up looking for stable employment, but also given up on life all together. Stories are widespread of young people who don't see education as a useful tool and are pressured by the unstable job market lock themselves in their parents’ room and withdraw from society. Although the private sector's inability to stimulate better conditions for development of domestic human capital do not always lead to extreme cases such as that, Japanese standard of living and happiness levels have significantly plummeted.

“Abenomics” and Beyond

After a half century of unstable economic climate, the Japanese people have found a resurgence in nationalistic and self-sufficient thinking. Japanese leaders have seen close economic and defense relationships with the U.S. as the best means of achieving high growth. Prime Minister Shinzo Abe has taken a very different approach to returning Japan onto the mainstage of global economic superpowers. Abe's plan, commonly named ""Abenomics"", aims to use three ""arrows"" -- monetary policy, fiscal policy and economic growth strategy -- to increase private investment.[33] Abe's plan relies heavily on changing how the labor force traditionally thinks in Japan by transforming the social priority given to saving into spending, much like in the 1980s. Although many economists have agreed that Japan's macroeconomic situation is nearly unmanageable due to interest rates already at zero, Abenomics specifically sets negative interest rates to help push Japanese workers into entrepreneurship. This policy is one of Abe's hardest to be successful due to historical instability of monetary policy and cultural norms in Japan. Fluctuations of the capital market go against the cultural desire for stability over everything else. The government's inability to be consistent on maintaining interest rates and excessive focus on large corporations to remain stable puts pressure on regular middle class Japanese. It will be incredibly difficult for the government to find a way to dispel its past troubles especially since large businesses still dominate the financial sector.

For trust between people and government to emerge, Abe will have to either provide further tax incentives for small businesses or establish stronger regulations on large corporations. However, for either to work, he needs to create something similar to China's five year plan, which Japan used to rely on, where a commitment to the policy is laid out because otherwise people will fear that the tax breaks or low interest rates won't last long enough to develop their business idea. Nonetheless, even if Abe can do both of these things, the cultural barrier will present a steep obstacle. In Japan, those who do too much to stand out are looked down upon in a society that tries its hardest to be “the most normal.” Not surprisingly, the allure of the lifetime employment system still has young workers hoping they can catch the tailwind of this dying trend. To succeed, Abe must show that individual employment opportunities will be more stable long-term than the corporate ones, and a culture of economic individualism does not negate the possibility of a fair social contract that broadly benefit everyone and the society as a whole.

Instead of discouraging people who have potential to make Japan more prosperous, the country has to buy into the idea of a national goal similar to the post-war dogma. Abe's best plan to tackle the pressure that large corporations project on government would be to limit the government's bending to corporate ""screw-ups"". This would cement a free market approach with basic government planning, show entrepreneurs the government is committed to a leveled economic field with equal opportunities and encourage large corporations to be more scrupulous. For the ""monetary policy"" arrow to hit its mark in promoting growth through low interest rate investments, Abe has to develop a more aggressive foreign policy. By decreasing interest rates further, Japan will see a rise in inflation. This will serve to lower the cost of domestic goods, which can be used to create a larger international trade surplus. I would like to emphasize that this is a short term policy to stimulate growth, which will have to be corrected if growth becomes excessive. That being said in the short term this should help not only promote growth via export, but should also eventually increase wages, thus promoting demand in Japanese markets

Japan's lifetime employment system must also change. The system works only during a period of high growth, due to more tolerance to inefficiencies during the growth periods, and it can only be managed with an excess of wealth. The Ministry of Internal Affairs estimates there are currently 25 million company workers between the ages of late 40s and mid 60s, and in Japan the most productive age is 45, which endangers the company's profitability.[34] The lifetime labor system guarantees pay increases as you get older, so in a time of small or negative growth an excess of money is wasted on inefficient older workers. The excess of employees in pointless management positions has yielded a new term oidashi beya which means “expulsion room.”[35] The system targets workers in their 30s and 40s, people who were lucky to find lifetime employment during the “lost decade.”[35] Companies have no easy way to shed excess labor due to the lifetime employment system and are forced to find ways to make employees quit. Transferring workers to useless departments and social disconnection emerged as passive methods of encouraging workers to leave. While this is an obvious example that could be used by Abe’s government to demonstrate the fundamental deficiencies of the existing corporate culture in Japan, it has the potential to create the unintended effect of being a social stressor. If Abe chooses to emphasize this as the largest consequence of Japanese corporate labor, it could paralyze people instead of forcing them to change their views on employment,

Historically, statistical data shows that the Japanese government has been slow to address the social climate from a young age. Data (Table 26-24 in reference [37]) shows that although the overall number of educational classes, training courses and events increased until 2005, the youth leaders’ training category experienced a massive decline. After peaking in 1984 at 1,518 leadership classes and courses, the total number declined by more than 50% down to mere 797 in 1999. The total facilities and number of trainees focusing on developing the leadership skills also dramatically declined after experiencing peaks in the 1980s. Leaders’ training is an important aspect of teaching entrepreneurial qualities, so for entrepreneurship to be boosted the social environment that provides the widespread educational and leadership training opportunities must be urgently revamped and invigorated.

Looking ahead into the future, free trade agreements will have a significant impact on Japan’s domestic workforce. Whether the Comprehensive and Progressive Agreement for Trans-Pacific Partnership (CPTPP) survives the United States’ withdrawal, or China’s Regional Comprehensive Economic Partnership becomes the premier Asian free trade agreement, Japan will benefit. The largest challenge to Japan will be selling the deal to voters.[36] The deal will put heavy pressure on small scale labor, such as farming or low-skill workers, because of the intense competition from other countries with lower labor costs (China). The next free trade agreement will be the final strike onto the weak permanent employment system. Corporations will have even easier access to labor in other countries where it is not only cheaper but requires no permanent commitments. This should not be a surprise, since for the last decade large Japanese corporations have already efficiently outsourced labor. Corporate outsourcing could have a positive impact by directly forcing Japanese workers to seek new opportunities. Despite the negative effects the deal will have short term, it will accomplish what Abe set out to do-to: jumpstart the stagnant Japanese economy by giving more opportunities for Japan to increase its exports. Free trade’s primary goal is to reduce trade barriers between Pacific nations, giving Abe the perfect chance to export high quality Japanese goods, made cheaply, to more countries.

Abe’s greatest challenge will be balancing corporate power and the supply of domestic labor. Through free trade, Japanese corporations will want to reduce expensive Japanese labor and expand production to other countries. In addition to promoting domestic entrepreneurship Abe will have to find a way to make it favorable for Japanese corporations to hire from the domestic workforce and domestic manufacturing base. In this way, Abe will stimulate economic growth and give Japanese workers money to invest and consume while interest rates are low. The only way this is feasible is by making corporate taxes competitive with the other members of a free trade agreement and making improvements to the education system. But, ultimately, only bold and consistent transformation in Japan’s social fabric -- especially targeting younger population and emphasizing self-reliance and the power of an individual -- can produce success in this paradigm change of the Japanese economic culture. The only way Japan will recover its 20th century economic swagger is if the working people buy into Abe’s plan and Abe strikes a delicate balance between big business growth and individual success.

References

Aggarwal, Raj. Restructuring Japanese Business for Growth: Strategy, Finance, Management and Marketing Perspective. Boston: Kluwer Academic, 1999 Williamson, Oliver E. ""The Transaction Cost Economics Project: The Theory and Practice of the Governance of Contractual Relations."" December 8, 2009. http://www.nobelprize.org/nobel_prizes/economic-sciences/laureates/2009/williamson_lecture.pdf. Woodall, Brian. ""Response to the Japanese Challenge,"" Asia Pacific Community, vol. 27, pg. 63-80, Winter 1985. Hall, John Whitney, and Marius B. Jansen. Studies in the Institutional History of Early Modern Japan,. Princeton, New Jersey: Princeton University Press, 1968. Dower, John. ""MIT Visualizing Cultures."" MIT Visualizing Cultures. 2008. Accessed December 1, 2015. http://ocw.mit.edu/ans7870/21f/21f.027/black_ships_and_samurai/index.html. ""Edo Period."" Edo Period. Accessed December 1, 2015. http://www.grips.ac.jp/teacher/oono/hp/lecture_J/lec02.htm. Keene, Donald. Emperor of Japan: Meiji and His World, 1852-1912. New York, New York: Columbia University Press, 2002. Chaurasia, Radhey Shyam. History of Japan. New Delhi: Atlantic Publishers & Distributors, 2003. West, John. ""Japan's Social Contract under Severe Stress."" Asian Century Institute. Accessed December 2, 2015. http://www.asiancenturyinstitute.com/society/371-japan-s-social-contract-under-severe-stress. ""Japan - The Economy - PATTERNS OF DEVELOPMENT."" Japan - The Economy - PATTERNS OF DEVELOPMENT. Accessed December 2, 2015. http://countrystudies.us/japan/98.htm. Yamamura, Kozo. ""Success Illgotten? The Role of Meiji Militarism in Japan's Technological Progress."" The Journal of Economic History: 113-35. Suehiro, Akira, and Tom Gill. Catch-up Industrialization: The Trajectory and Prospects of East Asian Economies. Honolulu, Hawaii: University of Hawaii Press, 2008. Kerr, George H. Okinawa, the History of an Island People. Rutland, Vermont: C.E. Tuttle, 1958. Totman, Conrad D. A History of Japan. Malden, Massachusetts: Blackwell Publishers, 2000. ""The Meiji Restoration and Modernization."" The Meiji Restoration and Modernization | Asia for Educators | Columbia University. Accessed December 3, 2015. http://afe.easia.columbia.edu/special/japan_1750_meiji.htm. Gordon, Andrew. Labor and Imperial Democracy in Prewar Japan. Berkeley, California: University of California Press, 1991. Yamamura, Ko. The Economic Emergence of Modern Japan. Vol. 1. Cambridge: Cambridge University Press, 1997. Gordon, Bill. ""Greater East Asia Co-Prosperity Sphere."" Greater East Asia Co-Prosperity Sphere. March 1, 2000. Accessed December 3, 2015. http://wgordon.web.wesleyan.edu/papers/coprospr.htm. Woodall, Brian. Growing Democracy in Japan: The Parliamentary Cabinet System Since 1868. Lexington: University of Kentucky Press, 2014, pp. 69-71. Dower, John W. Embracing Defeat: Japan in the Wake of World War II. New York, New York: W.W. Norton & Company, 1999. ""Japan and Its Broken Social Contract."" Japan and Its Broken Social Contract. March 23, 2009. Accessed December 3, 2015. http://www.investoralist.com/japan-reform-employment-social-welfare/. Dower, John W. Cultures of War: Pearl Harbor, Hiroshima, 9-11, Iraq. New York, New York: W.W. Norton & Company, 2010. ""Cross Currents."" Cross Currents. Accessed December 3, 2015. http://www.crosscurrents.hawaii.edu/content.aspx?lang=eng&site=japan&theme=work&subtheme=AGRIC&unit=JWORK098. Kawagoe, Toshihiko. ""Agricultural Land Reform in Postwar Japan: Experiences and Issues."" Policy Research Working Papers, 1999. Lincoln, Edward J. Arthritic Japan the Slow pace of Economic Reform. Washington, D.C.: Brookings Institution Press, 2001. Macey, Jonathan. ""Corporate Governance and Banking in Germany, Japan, and the United States."" : Publications : The Federalist Society. June 1, 1997. Accessed December 3, 2015. http://www.fed-soc.org/publications/detail/corporate-governance-and-banking-in-germany-japan-and-the-united-states. Dower, John. ""The San Francisco System: Past, Present, Future in U.S.-Japan-China Relations."" February 24, 2014. Accessed December 3, 2015. http://japanfocus.org/-john_w_-dower/4079/article.html. ""Labor Standards Act."" Accessed December 3, 2015. http://www.ilo.org/dyn/travail/docs/2021/Labor Standards Act - www.cas.go.jp version.pdf. Pharr, Susan J., and Frank Jacob Schwartz. The state of civil society in Japan. Cambridge: Cambridge Univ. Press, 2003. Tsutsui, William. ""The Bubble Economy and the Lost Decade: Learning from the Japanese Economic Experience."" Lecture, 2009 Summer Teacher Institute - Understanding the Global Economy: Bringing the World Market into Your Classroom, Chicago, January 1, 2009. https://www.youtube.com/watch?v=EF93cr3HxZY Tabuchi, Hiroko. ""When Consumers Cut Back: An Object Lesson From Japan."" The New York Times. February 21, 2009. Accessed December 3, 2015. Krugman, Paul R. The return of depression economics. London: Penguin Books, 2000. ""Lexicon."" Abenomics Definition from Financial Times Lexicon. Accessed December 3, 2015. http://lexicon.ft.com/Term?term=abenomics. Brasor, Philip. ""Debating the Merits of Lifetime Employment | The Japan Times."" Japan Times RSS. November 1, 2014. Accessed December 3, 2015. http://www.japantimes.co.jp/news/2014/11/01/national/media-national/debating-merits-lifetime-employment/ Brasor, Philip. ""No Room for Subtleties When Laying off Workers | The Japan Times."" Japan Times RSS. January 26, 2013. Accessed December 3, 2015. http://www.japantimes.co.jp/news/2013/01/26/national/media-national/no-room-for-subtleties-when-laying-off-workers/#.WW1dZojytPY Koike, Yuriko. ""How the TPP Can Transform Japan's Economy | The Japan Times."" Japan Times RSS. November 3, 2015. Accessed December 3, 2015. http://www.japantimes.co.jp/opinion/2015/11/03/commentary/japan-commentary/tpp-can-transform-japans-economy/ ""Historical Statistics of Japan."" Statistics Bureau, Ministry of Internal Affairs and Communications. Accessed February 16, 2016. http://www.stat.go.jp/english/data/chouki/index.htm.

Endnotes

1.) A Japanese policy that would not allow the entry or exit of foreigners. Also, Japanese citizens could not leave the country unless they had official permission. The policy was present under the Tokugawa shogunate.

2.) Ruling military elite in control of Japan prior to the 1867-68 revolution.

3.) An island in the bay of Nagasaki created as a trading outpost.

4.) Western music learned by Japanese scholars at Dejima

5.) An unsteady period in Japan notable for its many military conflicts and societal transformations

6.) The feudal government that remained in power in Japan from 1603 to 1868.

7.) Perry was a Commodore of the U.S. Navy who substantially contributed to linking Japan to the West (April 10, 1794 – March 4, 1858).

8.) Also known as the Tokugawa Shogunate

9.) The last Tokugawa shogun of Japan.

10.) Large corporate and financial institutions during Imperial Japan

11.) The ethnic religion of the people of Japan.

12.) A religion formed from a combination of Buddhism and Shintoism

13.) A law enforcement entity founded in 1911 to control political organizations and movements.

14.) A first rate country.

15.) A type of organizational structure which is based on tight interconnection between shareholders and business interests.

16.) The banishment room or the expulsion room for pressuring employees.

Acknowledgement

My advisor Professor Brian Woodall (the Sam Nunn School of International Affairs, Georgia Tech’s Ivan Allen College of Liberal Arts) who guided me, helped develop ideas and advised me on complicated issues."	http://www.inquiriesjournal.com/articles/1729/japan-at-work-in-the-21st-century-an-analysis-of-developing-labor-practices-in-japan-and-institutional-barriers-to-reform	"Devastated by an economic collapse at the end of the 20th century, Japan’s economy entered a decade long period of stagnation. Now, Japan has found stable leadership, but attempts at new economic growth have fallen through. A combination of public desire for economic security through lifetime employment, reliance on “economic bureaucracy” of large corporations and pressure from international powers have left Prime Minister Shinzo Abe few options for fulfilling his “Abenomics” dream. Japanese leaders have continuously attempted to restructure the economy in an effort to jump-start growth. But in order to create long-term economic growth, Japan must embark on decades long restructuring of its underlying cultural values. Japan is burdened by “path dependence” and resistance to change created by the institutional power of the private sector and a culture obsessed with being normal. The central challenge of the country is the time-scale mismatch between the urgency of economic reforms, which must occur quickly, and the slow dynamics of social and cultural transformations, which require generational changes. Japan’s government must establish a new culture within the traditional culture of Japan, as a way to lower the social barriers for change, to ensure economic prosperity of the country and its people for years to come. This long-term strategy requires focusing on the younger generation in order to prepare the society for the economic and social uncertainties of the new reality emerging in Japan and globally. Abe’s government must establish a comprehensive program that provides numerous and sustained opportunities for the Japanese youth to become more creative and unorthodox in educational and professional development, emphasizing significant exposure to global opportunities from the earliest age. By breaking the traditional hierarchies in the society with an influx of global social and cultural experiences, the reorganization towards a more dynamic and robust economic model with significant entrepreneurship and innovation components would naturally emerge and become self-sustainable in Japan.
A complex hierarchy of relationships and mixed personal-cast-institutional dependencies in Japanese society has been established over the course of many centuries. The economic path towards establishment of large, highly integrated corporations with life-long employment contracts was a natural “culturally-driven” process for Japan, which brought significant economic benefits and competitive advantages during the period of rapid post-World War II industrialization. As has been examined in many publications [1], Williamson’s transaction cost theory behind the formation of large firms is directly applicable to Japan.[2] During the 20th century, mutual dependence of corporations and employees resulted in a rise of complex contractual obligation in the form of lifelong employment. However, in the last several decades and especially since the information technology revolution of the 21st century, the role of a human worker possessing only basic skills has greatly diminished in economic activity that requires quick adaptation to changes in a globalized world. On one hand, workers become forced to be more flexible in their employment opportunities, as they can no longer be guaranteed that a company will stay committed to them. On the other hand, the removal of lifelong contracts voids the fundamental prerequisite for the creation of corporate hierarchies. This makes an adherence to lifelong employment suicidal to both the workers and corporation, imposing a significant stress on Japan’s socio-economic system.
The necessary adaptation to new economic reality is difficult for Japan due to fundamental norms of Japanese culture emphasizing social stability and long-term security. If Abe continues to pursue standard Western strategies of jump-starting the economy through monetary and fiscal policies, he will likely fail. He needs to exploit a new regional free trade agreement to expand the Japanese export markets and to invoke bold societal changes challenging the Japanese social norms by stimulating entrepreneurship, economic independence of younger workers and making economic growth the people's’ priority. But to fully succeed in rebuilding the Japanese economy, Abe must not only engage in tactical near-term fixes, such as participating in free trade, but most importantly focus on long-term cultural transformation projects.
In this work, I attempt to examine and substantiate the above thesis by analyzing Japan’s cultural roots underpinning its socio-economic development over several centuries in the context of evolution of Japanese government structure and external influences from the Western world. I show how the hierarchy of institutions and practices, which have been imposed from the top over many centuries as “Japanese way of life,” has led to an inflexible social contract between the working population and the country’s governing elite. This social contract presents an overwhelming barrier to transforming a rigid and slowly degrading industrial machine of today’s Japan into a flexible and adaptive economic system. Ultimately, I pose a challenge to Japan’s leadership to find a balance between the aggressive monetary and fiscal policies and the long-term social transformation of Japanese society to support sustainable economic gains without sacrificing societal harmony and cultural tradition. The long time required for sustained social transformations and the complexity of “social engineering” approaches when applied to a country with strong cultural traditions make the bold social change agenda more pressing than economic policies.[3] It must start immediately and target the youth, with broad understanding and support from older generations and significant financial commitment from the government. This will give Japan the best chance to succeed in the rapidly developing world for centuries to come.
Pre-War Japan
Japan’s rapid economic growth due to its strong and unique labor system following a long period of isolation is unique among advanced industrialized nations. The roots of growth started with the Edo period under Tokugawa rule (1603-1867). Japan’s feudal system rose to nationwide prominence in the 1100s. The persistent presence of moral obligation as a means of class distinction allowed the system to last longer, until the Meiji Restoration. The generalization that Japan’s feudal system was universal until the Meiji restoration of 1868 and immediately fell apart following the restoration is false.[4] Japan’s economic sector today is still heavily influenced by principles of social hierarchy. The system of respect between samurai and peasants proved stronger than the European system, under which knights expected profit. While increased growth of a middle class and currency interactions were important to the demise of European feudalism, the European enlightenment movement in the 18th century was the biggest stifling factor of the feudal system.
Revolutions, rebellions and the idea of individual rights gave the working class a motivating factor to protest. Individualism motivated entrepreneurship, which quickly blended rigid European class distinctions. Sakoku , the Shogunate’s policy of isolation, did not completely hinder the spread of European ideas.[5] This lead to the Edo state loosening its isolation policies to learn more about the west. A specific example was the Dutch trading port in Dejima where scholars kept in tune with Yogaku and western studies.[5] The century-long isolation of Japan created a social atmosphere in which fast changes in people's mentality and adaptation to new workplace reality became fundamentally impossible. Japanese isolation from western revolutions and ideologies helped the Japanese transform into a modern economic society much smoother while maintaining many feudal beliefs.[5] However, reliance on the government and guaranteed employment at large corporations has greatly hindered Japanese innovation and entrepreneurship.
Most of the labor force during the Edo period engaged in agricultural production. Almost 90% of Edo society were peasants, who were not the lowest tier in the social hierarchy.[6] This was crucial to the system’s long-term survival because it put a larger economic focus on peasants rather than merchants. Since the peasants were the tax base, law dictated that peasants cannot move, but peasants ignored the regulations both for entertainment and better economic opportunity.[6] The local system of tax collection was important to the success of the shogunate. The system is a very early form of lifelong employment which has been a major backbone for economic success in Japan. The Daimyos practiced salutary neglect as long as farmers paid their taxes. Sengoku Jidai allowed for a large expansion in farmland, giving even more weight to farmers. During this period the farmers bribed field inspectors to understate crop output.[6] This early form of bribery marks the emergence of corruption among Japanese bureaucrats, which put a heavy economic burden on the country in the 20th century. Rapid expansion of farmland began spiraling out of control, eventually contributing to the demise of the Tokugawa Shogunate . The large farmer class was forced to compete over a small amount of land. The government was responsible for over-regulating the system, thus giving birth to the belief that economic opportunity is achieved through the government policies. Uprisings by the farmer class marked the end of feudal society as the government failed to adapt its policies.
Commodore Perry’s arrival in 1853 signaled the end of feudal isolationism to the Tokugawa shogunate. The Bakufu realized that Japan had been completely eclipsed economically and technologically. Perry’s demands to the Tokugawa were mutually beneficial in terms of trade.[5] A combination of economic allure and fear of the unknown forced the Tokugawa to agree on the terms set forth by the Americans. The Convention of Kanagawa, which opened ports in Shimoda and Hakodate, was the defining point in the end of Japanese isolationism and the beginning of not only an unstable economic climate but isolationism of imperial rule. Japan’s economy started to rapidly decline following the opening of the country. Tokugawa Yoshinobu was ousted, and political power violently transferred to Emperor Meiji. The Charter Oath outlined the goals of the new imperial government, which wanted to rapidly advance the country technologically through urbanizing the workforce.[7]
Under Emperor Meiji, The Land Tax Reform of 1873 truly marked the beginning of a labor revolution. It reformed the tax system in a way that farmers had to assume the financial penalty of agricultural market fluctuation. The farmer had to pay taxes based on the value of land not the annual crop yield. While the new system helped establish independent commerce and private land ownership, it indirectly exacerbated the issue of wealth inequality. An attempt to reform already cemented practices had fallen through due to economic damage to lower classes. In 1872, the ban on selling and trading farmland was lifted, and in 1873 Japanese citizens were given the power to use real estate as collateral to secure agricultural loans.[8] Landowners without prosperous land became impoverished, while more successful landowners gained more social and economic power. The Land Tax Reform of 1873 industrialized the country by pushing a large population of workers to urban centers. “It is a well-recognized fact that the outflow of the labor force from agriculture provides a big source of labor for the rapidly expanding non-agricultural industries.”[9] Although the urbanization push created new jobs, these jobs were still tied to the government and had not led to a fundamental change in people's mentality. Paired with rapid population growth in the 19th century, Japanese industrialization outpaced many European countries. One of the most distinctive features of the Japanese economy is that growth in the agricultural sector never increased during economic growth.[9] During the Meiji Restoration, in an attempt to build military and industrial might the government consolidated and became more centralized. This transformation played a critical role in how the government acted throughout the 20th century, both during the period of imperial expansion and economic modernization.
A pattern of reliance on the government has yielded short boosts of growth, but created long-term instability. During the Imperial expansion period and World War II, the government twisted the social contract by inhibiting natural rights in return for national expansion. The government invested heavily in factories and shipyards, which were sold to companies to help guide a powerful private sector.[10] The private sector was successful but not agile and independent, because it was comprised of large companies with demands to their products and services largely coming from government. The development of a national rail system and modernized communications were spearheaded with the help of western technology.[11] The successful economic climate led to a rise of large corporations, which became the main provider of labor security in the late 20th century. The rise of zaibatsu , large business conglomerates, such as Hitachi and Mitsubishi gave the Japanese government an efficient supplier of technology and consistent labor opportunities. The Zaibatsu quickly grew too big to fail, creating departments for almost everything the government needed: whether it was military supplies, nuclear energy, vehicles and chemicals. As time progressed, the Zaibatsu system expanded to other Asian countries. In 1996-97 sales from top 20 Zaibatsu, expressed as percentage of GDP, accounted for 87% in South Korea, 36% in Thailand and 30% in Indonesia.[12] The government saw large corporate companies as its fastest road to achieving global success in a modernizing world. With power centralized imperially, the government adopted a policy of expansion. The invasion of Taiwan in 1874 demonstrated the immense power that the military possessed in government affairs, as this was the first time the military ignored civilian government orders.[13] With increased trade with western nations, Japan managed to quickly grow a large and modern standing army which provided for sizable employment in the public sector. National conscription not only provided a new guaranteed labor force, but also gave Zaibatsu a secure buyer.
Meiji reformers utilized Shintoism’s imperial myth to attempt a removal of Buddhism and make new national interests popular. [15] However, full removal was impossible and Buddhism combined with Shintoism to form ryōbu shitō . With feudalism laid to rest, and people free to build their own future in the new capitalistic society, uneducated Japanese transitioned to new low paid working jobs.[15] The large influx of low-skill labor to urban centers yielded disastrous conditions for workers. As a result, groups of organized workers demanded expanded civil rights. Marxist views began to take form among not only workers but also intellectuals.[14] The government cemented a far-right stance as it formed the Tokko secret policy in an age of nationalism. However, the government made a decisive change in 1911 when new regulations regarding a national minimum wage and work hours took into effect.[15] Japan quickly grew into an economic juggernaut, but the working class remained neglected and labor unions began to take form. Although mistrust in government gave rise to individualism, the economic system that had been put in place was not adequate to support this change in attitude, rendering existing small businesses weak and offering no meaningful path for harnessing the economic benefits of individual entrepreneurship. Three stimulating factors which emphasize the cultural values of equality led to labor reform in pre-war Japan: gender, nationalism and social treatment.[16] Gender played an important role because the gender divide in mill jobs was greater than the class divide.[16] Nationalism had given a feeling of importance to many workers who were eager to give their power for the empire’s success, but bureaucratic regulators and business owners felt the glory was theirs.[16] Finally, Japanese workers felt that their social treatment in the labor system was more important at times than their wages.[16]
Workers mistrust in the government during this period played an important role in Japan’s rebuilding post World War. Even at the point leading up to the war not all groups of workers felt job security. Girls in Tokyo textile mills saw their employment as a temporary occupation rather than a permanent occupation.[16] Nevertheless, there was no sense of hopelessness, as a promise of new opportunities paired with Imperial expansion tempered negative sentiments. As the country gained new territories and grew stronger, more jobs were available both in the private and public sector in order to sustain the growth. A very distinct pattern emerged of entering the workforce for life in order to securely raise a family, which would be a symbol of contributing to the empire. The pattern had been repeated across generations, which allowed post-war citizens to adapt quickly to the system of lifelong employment.
The new system congealed following Japan’s reconstruction as a major power. Andrew Gordon explains that in the case of Japanese labor disputes almost none were caused by machines crowding out low-skill labor.[16] Rather, he suggests that most disputes were driven by a desire for respect and social status.[16] Japan has always had a limited internal labor force, making individual workers more powerful in their demands for job security (i.e., a seller’s market for labor). This observation highlights another pillar in the foundation for the rise of lifelong employment, that is, that a concern for job security is paramount among Japanese workers .
Japan established a large empire, and the increased inflow of resources stimulated further economic growth. Japan’s mining and manufacturing sector accounted for 30% of GDP in the 1930’s.[10] The country’s GDP was expanding at an incredible 5% per year with continued economic focus on making the military strong.[10] Although the Great Depression did not have a profound impact on the country, there was an offspring event which did. In 1927 the Showa financial crisis left most major Japanese banks without capital and gave the financial sectors of Zaibatsu most power in Japanese finances.[17]
Post War Japan
Japan’s expansionary interests eventually led to World War II. The nation overestimated its production capacity and industrial capability in a fight against the world. Japan’s leadership envisioned the Greater East Asia Co-Prosperity Sphere. In order to sustain a ballooned economy over a large and diverse range of human capital, the government required a more effective union of Pacific territory. The eventual goal rested in a self-sufficient union, where Japan would fulfill its labor requirements through colonies while also having a guaranteed consumer of Japanese exports.[18] The system would appeal to people’s desire for riskless jobs because the hegemonic nature of the Greater East Asia Co-Prosperity Sphere would require increased production to remain autarkic thus supporting endless employment for the populace. In many ways Japan used similar principles that the U.S. used during the phase of its initial growth. Three events in early American history clearly correspond to Japan’s ambitions during World War. Japan was mimicking steps which brought the U.S. to superpower status. Japan aspired to become a global superpower (ittô koku ) by eliminating impediments brought about by Western colonization.[18] However, the Greater East Asia Co-Prosperity Sphere failed due to the very same problem it sought to fix: labor inequality in colonial countries.[18] A clear conflict of interest became apparent when examining Japan’s government during this period. The bureaucracy had lost its unity by the 1940s to the military autocracy, army and emperor acting in different roles with almost unchecked responsibility. The Zaibatsu accumulated more power, as the government enforced few regulations and instead needed fast supplies. Yet, the real powerbrokers were “reform bureaucrats” and ultranationalists.[19] These events reinforced the limited economic role regular citizens could play independently in the social hierarchy dominated by government bureaucracy and military-industrial complex.
By the time the war had ended most of Japan’s labor supply was involved in military-related activities. People in Japan were so committed to the 15 year long war that liberation came as a shock.[20] The American liberators came with a plan to rebuild Japan rather than leave it to domestic forces.[20] This optimistic plan paired with the Japanese people’s fervor for social and economic stability made the job of initial recovery easier than in other nations and became known as the “economic miracle.” The government created a comprehensive plan for rebuilding. Large investment and manufacturing firms now known as Keiretsu were to be drivers of internal development. They had nearly one third of overall employment and were going to be given more individual power for carrying the Japanese economy through the period of low-interest rates and low regulations.[21] These are the same firms that led to the collapse of the economy in the 1990s after their power was never taken back by the government. In order to maintain the gift of monopolistic privilege they were expected to provide lifelong employment and make sure the Japanese work force remained competitive.[21]
Japan’s second strategy was to increase the quality of individual labor. The social welfare system suffered eventually as the Keiretsu became centers of bureaucratic deal making.[21] Although the system made it easy for Japan to recover quickly it greatly hindered the quality of social benefits for the poorer citizens. The American occupation is often not given enough credit for causing the path Japan winds up on later. John Dower explains that there are eight critical reasons why the United States succeeded in rebuilding Japan: “Discipline, moral legitimacy, well-defined and well-articulated objectives, a clear chain of command, tolerance and flexibility in policy formulation and implementation, confidence in the ability of the state to act constructively, the ability to operate abroad free of partisan politics back home, and the existence of a stable, resilient, sophisticated civil society on the receiving end of occupation policies – these political and civic virtues helped make it possible to move decisively during the brief window of a few years when defeated Japan itself was in flux and most receptive to radical change.”[22] The most important of these (for Japanese) was “the existence of a stable, resilient, sophisticated civil society”[22] that did not dwell on the past. Japanese people invested their efforts in improving the country by working for the government instead of pursuing individual entrepreneur-driven innovation. World War destroyed close to 40% of Japan’s industrial infrastructure, but following the war the country was able to equip itself with newer and more advanced infrastructure.[10] This made Japanese workers more efficient and capable compared to labor in many other countries in the world. The government's extreme focus on channeling the new flood of labor, who were formerly in the military, into Keiretsu centralized advancement into specific business conglomerates and transferred the responsibility of keeping good working conditions to Keiretsu. The Zaibatsu were one of MacArthur’s biggest targets initially. Under the original plan for reorganization, he wanted to completely dissolve large commercial conglomerates. In his view, the Zaibatsu acted for themselves and spreading wealth out was more beneficial. By that point, laborers grew accustomed to working for large and stable businesses. When the Zaibatsu collapsed, instead of looking for more self-reliance and independent employment workers went simply to work for new corporations.
MacArthur was following a “new deal” type program that in his view would make a more efficient workforce by diversifying industries and taking advantage of the returning working population. While he successfully eliminated several Zaibatsu, many just reformed into looser cross-shareholding Keiretsu. During the occupation six Keiretsu became the most influential: Mitsui, Mitsubishi, Sumitomo, Fuyo, Sanwa and Dai-Ichi.[25] These Keiretsu distinguished themselves from others by all being centered to one bank, and the bank loaned money to member companies in the Keiretsu while also holding equity positions in the Keiretsu companies.[26] Close relationship between the banks, companies and government regulators eventually became an insurmountable detriment to the lower-class workforce.
MacArthur started labor reform from the distribution of property in the land redistribution act. The reform’s purpose was to redistribute land from landlords who did not individually use the land to the low-class workers who rented the land and actually used it.[23] Although many believe this event acted as a de-industrializer of the labor force, it is important to understand the reform did not affect a majority of working Japanese. The land reform only helped solidify the path of corporate labor because by 1965, mining, construction and manufacturing accounted for 41% of the labor force, while agriculture only employed 26% of the population.[10] The land reform acted both as and industrial and political policy.[24] Although the policy had little effect on agricultural output, it dramatically increased the amount of conservative thinking in the urban population.[24]
A majority of economic reform pinned regular Japanese citizens as the backbone for success. In return, the society structured itself to guarantee a job for any hardworking and willing citizen, cementing the decade-long practice of prioritizing corporate vs individual employment. It was a special modification on the social contract, which had been in place for a while, and it helped solidify the reputation in the West that Japanese citizens love work in an uncompromising way. In order to return to a strong economic footing, the government revived traditional values of shame, sacrifice, responsibility and wise decision making.[21] The effort by the government is often described as an “iron triangle” where large corporations had easy financial breaks, yet they had to pay large prices for resources and make sure their labor force was well serviced.[9] This brings about one of the greatest contradictions to present “Western” economic strategies. In “Western” countries, the government more often over regulates the financial system and ignores the workers. An American strategy is making sure the key economic drivers are fair in hope it will eventually benefit the poor. However, the Japanese took a different rout inspired by Roosevelt’s “new deal” and motivated the general workforce to better the large corporate institutions. The new corporations understood that a motivated labor force could be well trained and given pay incentives.
Japanese people had put national GDP growth as their foremost personal priority.[30] Through the corporation's desire for capable labor, the government no longer had to worry about a social welfare net.[9] The “economic miracle” could not have been achieved without the costs the United States government incurred from reconstructing Japan. The United States made sure to sufficiently fund Japanese defense spending well until the end of the Cold War. Also, a complete rebuilding of infrastructure helped the Japanese government focus on molding its labor force. However, in the long-term, Japanese people are still suffering today from the initial labor policies.[9] If a stronger effort had been put into incentivizing self- employment, Japan would have taken a slower but more robust path to economic recovery. There is almost no social security or pension fund for retirees.[9] To the present day, a large portion of Japan’s population is elderly and the child fertility rate is low due to the young generation’s urgency for having a stable position in the unforgiving labor market.
Dower describes the relationship that the U.S. and Japan have had since World War II as the “San Francisco system.” The conservative government had an easy decision to make because the U.S. granted military protection and independence to Japan at little cost.[27] While Dower argues that the system has imposed eight problematic legacies on Japan - “Okinawa and two Japans, unresolved territorial issues, U.S. bases, rearmament, the nuclear umbrella, containment of China and Japan’s deflection from Asia, and subordinate independence” - there have been substantial economic benefits from the close protection umbrella that the U.S. has cast on Japan. Japan’s border disputes have not resulted in destabilizing conflicts and the U.S.’s heavy military armament of Japan has alleviated pressure on defense spending. The time and capital that Japan has saved has been meticulously applied to creating a high growth economy rather than social investment. During the U.S. occupation, two more reforms were passed that established a very rigid labor structure: The Labor Standards act of 1947 and Trade Union Act of 1949. They created a basic system of ensuring workers’ rights were met and made up for not having a strong safety net for the unemployed. The Labor Standards Act has 13 chapters which outline necessary work conditions for human beings.[28] An important inclusion of the act are wage and labor contract provisions. The Trade Union Act made sure workers had the ability to organize and assemble.[29]
Following the U.S. withdrawal, a functioning, well-educated urban professional class, strong bureaucracy and effective agriculture put the country on track for a 5% economic growth rate compared to the U.S.’s 3.8% by the 1980’s.[10] While United States reform was necessary to rebuild the manufacturing sector to pre-war conditions, the motivated and highly educated labor force did a majority of the work. Japan also had a fast modernization of technology due to substantiated corporate investment.[10] The overall system worked on “arbitrary motivation,” where companies were able to attract capable employees with new technology and promise of promotion. The corporations were growing so fast they could guarantee senior positions to almost all employees. Although there was no long-term capacity for management positions, the lifetime employment system made sure that only employees approaching retirement received positions. Also, the companies were so well endowed with resources, they could meaninglessly fill high paying positions with no regard of valuable yield. Recent studies have shown that the Japanese population's propensity to spend helped drive the economic recovery.[30] These claims fall perfectly into the other established characteristics of the Japanese economy for two main reasons.
First, the well trained and motivated workers had very good wages. In the 1960s, a combination of low population growth and increased industrialization resulted in increased wages.[10] Second, low interest rates that were used for rapid growth benefited individual spending habits. At the same time the workers who participated in the lifelong employment system had a friendly relationship with higher management, which allowed for very few labor disputes.[30] This also can be seen as a perpetuator of the Japanese spending habit. The promise of greater financial gains during the later years of a citizen’s career was too tempting, and it led to a demise of the riskier possibility of entrepreneurship. In fact, as supported by statistical data on the number of internal migrants in Japan from 1954 to 2004 (Table 2-37-a in reference [37]), the total number of internal migrants began to decline from 8,026,029 in 1974, and this downward trend continued through 2004 with almost 30% reduction reaching 5,771,921 migrants. This demographic indicator of population mobility, as a proxy for economic opportunism, provides indirect evidence of sustained transition to a less entrepreneurial workforce in Japan, which underpins the beginning of lifelong employment dominance in the 80s as one of the important precursors for the “lost decade.” Companies essentially guaranteed workers higher pay alongside job security as time went on making people more inclined to spend. The political system from the 1950s to 2000 can’t be ignored as Japan has been in control by one party (LDP), which advocated for conservative policies favoring business growth.[30] The LDP’s long standing and consistent platform further benefited workers’ security because the party kept close economic ties with the United States. Despite the Cold War sucking funds away from nations, the keiretsu were safe since the United States was still a reliable consumer of new technologies. The labor system which was so focused on education and motivation also painted its own destiny and security. Other global nations recognized the quality and reliability of Japanese exports, giving an international brand to companies such as Sony and Nintendo.[30]
The system which closely relied on uniform loyal participation from the labor force could not last forever. In the 1970s extreme environmental pollution accompanied by an environment of overworking and stress became a political issue.[10] The social contract in Japan had rapidly changed for centuries and at this point work was intolerable. The success of both the industrialized and agricultural economies heavily relied on the success of the labor market, with labor efficiency being undoubtedly the largest factor when considering the nation's propensity to export. In most well developed nations there are two limiting factors when considering successful international trade: an available buyer and an ability to efficiently manufacture. Japan’s guaranteed buyer was the U.S., but Japan pinned its entire industrial capability on its labor force, which for a long time ultra-efficiently manufactured goods. However, Japanese people no longer agreed that economic success should be the national policy.[30] Both Japanese workers and corporations understood the value of the lifelong employment system. Even though economic stressors such as the oil shock tested the reliability of the economy, quick recoveries from large production covered up the issues that would become apparent later. It continued to satisfy the workers' desire for job security and helped businesses by making training easy and loyalty a standard.
In response to growing displeasure of outdated policy by the labor force, the Liberal Democratic Party passed more social reforms to help create a larger safety net.[30] People quickly benefited from the high growth era in the 1980s with a growing stock market and strong real estate market.[30] The Keiretsu grew quickly out of the banking success stemming from Japanese people’s spending. Japanese society saw the growth as having a negative effect on the people, associating excessive spending with a loss of traditional values.[30] Japanese citizens of all ages preferred a humble lifestyle with no excesses that often come with personal success, and this provided a social background for negative perception of people striving for independent success. In an exaggerated way, it was as if people had tirelessly worked for 20 years then excessively reaped the benefits. A society that had been so focused on maintaining a neutral social structure as a mechanism for post-war national recovery evolved to establish a more distinct class structure than ever before. This is corroborated by statistical data on Japanese households’ income from side work, i.e., outside of primary employment, which can be used as a proxy for the diversity of economic activity among the population (Table 2-22 in reference [37]). The data shows that the number of households with “side business activity” peaked at 123,772 in 1970. By 1980, ten years later, the number of households dropped to 85,243. This suggests a trend of reduced entrepreneurial activity in population, which is typically stimulated by having sufficient number of workers seeking new income opportunities, in addition or instead of favoring stable employment at large corporations.
Nonetheless, as Japanese people bought more homes, banks saw the sky as the limit, issuing incredible amounts of loans, not all of which that were secure. Nobel Prize winning economist Paul Krugman states ""Japan's banks lent more, with less regard for quality of the borrower, than anyone else's. In so doing they helped inflate the bubble economy to “grotesque proportions"".[32] Banks were forced to issue more loans because large corporations grew too big and had to search for other places to finance their capital from bigger foreign banks.[30] Banks felt that due to the lifelong employment system they were relatively safe in issuing mass loans due to the strong job security most people had possessed. When the bubble eventually burst, the labor system was undoubtedly one of the biggest perpetrators in the collapse. Despite the loan bonanza enjoyed by large corporation, the banks did not invest as much into new businesses that could provide engine for economic growth and new opportunities. Statistical data (Table 14-8 in reference [37]) shows that during the 90s and early 2000s Shoko Chukin Bank, Credit Cooperation and Shinkumi Federation Bank actually decreased loans and deposits for small businesses. By 2003, these three banks, in addition to Shinkin Central Bank, Labour Credit Association and the National Federation of Labor Credit Associations all together issued fewer than 100,000 loans a year to new business ventures. In essence, the large banks had lost confidence in the potential for small businesses to succeed. This decrease in loans took place despite a major decrease in a number of reported bankruptcies in late 80’s, with a staggering ~75% drop in just 5 years between 1985 and 1990 (Table 6-17 in reference [37]). One can suggest that this decrease indicates a healthier economic climate which helps new businesses survive; however, since its onset occurred in 1985 when the economy of big enterprises started to collapse, the “positive” trend most likely is another indicator of a dramatic slowdown in entrepreneurial activity in Japan, simply because small businesses are much more susceptible to bankruptcy. In turn, this led to a reduced diversity and increased bureaucratization of the economy with more workers favoring employment at large corporations.
The decrease in private ventures from bankruptcies in the 1980s is also displayed in data showing the number of private institutions. Beginning 1981, the number of privately owned enterprises and individual proprietorships stagnated and slowly decreased by the thousands, especially in Aomori, Iwate, Akita, Tokyo, Ishikawa, Kyoto and Osaka (Table 6-8 in reference [37]. At the same time, the number of corporations and joint stock companies saw a dramatic increase. This further supports a conclusion that entrepreneurialism was no longer viewed as a viable path to achieve economic success and societal preference towards the risk averse economic development relying on larger corporations and stable labor employment policies.
When examining the reason for the unchecked growth many economists agree that inept public policy, corruption, greed, and sentiment that Japan was ""different"" were root causes.[30] As previously mentioned, the central bank kept interest rates low forcing banks to give out more speculative loans in order to maintain growth levels. However, pressure from the United States to maintain a competitive domestic industry through politics of weakening the U.S. dollar forced the Japanese government to rapidly increase its interest rates, which made risky borrowers unable to pay back loans. Japanese regulators were too close to the banks since bureaucrats weren't paid well and often decided that working for a large bank would be more profitable.[30] This issue stemmed out of the rigid labor practices because most of the salary increases were in private corporations while government jobs offered a low retirement age. Thus, government regulators were able to retire early and profit at Keiretsu. This fostered an environment where bureaucrats would focus their time to establishing good relationships with banks in order to guarantee a well-paid position after retirement.[30] Not only did such relationships promote a conflict of interest for regulators before retirement, it also meant they had very good connections and abusive strategies when working for banks after retirement. When the bubble eventually burst, growth became so dismal that huge corporations people called ""too big to fail"" were forced to scrap their system of obtaining domestic human capital.
Japanese corporations had to cut costs, let go of useless workers and move expensive manufacturing to other countries.[30] Loyalty from workers became unimportant when a company is struggled to stay afloat, and senior management positions which provided no value could no longer be tolerated. To maximize labor efficiency part time employment proliferated, which meant the end of low unemployment rates and financial security to workers. By 2009, non-permanent employees constituted more than 30% of the work force.[31] A massive stagnation in wages meant that workers could not spend money to stimulate the economy, and the rapid transition from low to high then back to low interest rates hurt the labor market structure. Savers and retirees were ruined due to their dependence on accumulating funds, which stopped following the collapse.[30] In a country with a large elderly population and low fertility rate, the low interest rates make a well paying and reliable job even more important. However, the leading corporations now outsource a large fraction of labor and are always looking for cheap labor rather than investing in domestic human capital. This pattern will likely persist as Japan enters the next Asian free trade agreement, making cheap transactions easier within the region. The crisis disabled long-term political solutions throughout the 1990s as the country constantly elected different leaders with no long-term vision.[30]
One of the crisis's overlooked impact is the social change it caused. People have not only given up looking for stable employment, but also given up on life all together. Stories are widespread of young people who don't see education as a useful tool and are pressured by the unstable job market lock themselves in their parents’ room and withdraw from society. Although the private sector's inability to stimulate better conditions for development of domestic human capital do not always lead to extreme cases such as that, Japanese standard of living and happiness levels have significantly plummeted.
“Abenomics” and Beyond
After a half century of unstable economic climate, the Japanese people have found a resurgence in nationalistic and self-sufficient thinking. Japanese leaders have seen close economic and defense relationships with the U.S. as the best means of achieving high growth. Prime Minister Shinzo Abe has taken a very different approach to returning Japan onto the mainstage of global economic superpowers. Abe's plan, commonly named ""Abenomics"", aims to use three ""arrows"" -- monetary policy, fiscal policy and economic growth strategy -- to increase private investment.[33] Abe's plan relies heavily on changing how the labor force traditionally thinks in Japan by transforming the social priority given to saving into spending, much like in the 1980s. Although many economists have agreed that Japan's macroeconomic situation is nearly unmanageable due to interest rates already at zero, Abenomics specifically sets negative interest rates to help push Japanese workers into entrepreneurship. This policy is one of Abe's hardest to be successful due to historical instability of monetary policy and cultural norms in Japan. Fluctuations of the capital market go against the cultural desire for stability over everything else. The government's inability to be consistent on maintaining interest rates and excessive focus on large corporations to remain stable puts pressure on regular middle class Japanese. It will be incredibly difficult for the government to find a way to dispel its past troubles especially since large businesses still dominate the financial sector.
For trust between people and government to emerge, Abe will have to either provide further tax incentives for small businesses or establish stronger regulations on large corporations. However, for either to work, he needs to create something similar to China's five year plan, which Japan used to rely on, where a commitment to the policy is laid out because otherwise people will fear that the tax breaks or low interest rates won't last long enough to develop their business idea. Nonetheless, even if Abe can do both of these things, the cultural barrier will present a steep obstacle. In Japan, those who do too much to stand out are looked down upon in a society that tries its hardest to be “the most normal.” Not surprisingly, the allure of the lifetime employment system still has young workers hoping they can catch the tailwind of this dying trend. To succeed, Abe must show that individual employment opportunities will be more stable long-term than the corporate ones, and a culture of economic individualism does not negate the possibility of a fair social contract that broadly benefit everyone and the society as a whole.
Instead of discouraging people who have potential to make Japan more prosperous, the country has to buy into the idea of a national goal similar to the post-war dogma. Abe's best plan to tackle the pressure that large corporations project on government would be to limit the government's bending to corporate ""screw-ups"". This would cement a free market approach with basic government planning, show entrepreneurs the government is committed to a leveled economic field with equal opportunities and encourage large corporations to be more scrupulous. For the ""monetary policy"" arrow to hit its mark in promoting growth through low interest rate investments, Abe has to develop a more aggressive foreign policy. By decreasing interest rates further, Japan will see a rise in inflation. This will serve to lower the cost of domestic goods, which can be used to create a larger international trade surplus. I would like to emphasize that this is a short term policy to stimulate growth, which will have to be corrected if growth becomes excessive. That being said in the short term this should help not only promote growth via export, but should also eventually increase wages, thus promoting demand in Japanese markets
Japan's lifetime employment system must also change. The system works only during a period of high growth, due to more tolerance to inefficiencies during the growth periods, and it can only be managed with an excess of wealth. The Ministry of Internal Affairs estimates there are currently 25 million company workers between the ages of late 40s and mid 60s, and in Japan the most productive age is 45, which endangers the company's profitability.[34] The lifetime labor system guarantees pay increases as you get older, so in a time of small or negative growth an excess of money is wasted on inefficient older workers. The excess of employees in pointless management positions has yielded a new term oidashi beya which means “expulsion room.”[35] The system targets workers in their 30s and 40s, people who were lucky to find lifetime employment during the “lost decade.”[35] Companies have no easy way to shed excess labor due to the lifetime employment system and are forced to find ways to make employees quit. Transferring workers to useless departments and social disconnection emerged as passive methods of encouraging workers to leave. While this is an obvious example that could be used by Abe’s government to demonstrate the fundamental deficiencies of the existing corporate culture in Japan, it has the potential to create the unintended effect of being a social stressor. If Abe chooses to emphasize this as the largest consequence of Japanese corporate labor, it could paralyze people instead of forcing them to change their views on employment,
Historically, statistical data shows that the Japanese government has been slow to address the social climate from a young age. Data (Table 26-24 in reference [37]) shows that although the overall number of educational classes, training courses and events increased until 2005, the youth leaders’ training category experienced a massive decline. After peaking in 1984 at 1,518 leadership classes and courses, the total number declined by more than 50% down to mere 797 in 1999. The total facilities and number of trainees focusing on developing the leadership skills also dramatically declined after experiencing peaks in the 1980s. Leaders’ training is an important aspect of teaching entrepreneurial qualities, so for entrepreneurship to be boosted the social environment that provides the widespread educational and leadership training opportunities must be urgently revamped and invigorated.
Looking ahead into the future, free trade agreements will have a significant impact on Japan’s domestic workforce. Whether the Comprehensive and Progressive Agreement for Trans-Pacific Partnership (CPTPP) survives the United States’ withdrawal, or China’s Regional Comprehensive Economic Partnership becomes the premier Asian free trade agreement, Japan will benefit. The largest challenge to Japan will be selling the deal to voters.[36] The deal will put heavy pressure on small scale labor, such as farming or low-skill workers, because of the intense competition from other countries with lower labor costs (China). The next free trade agreement will be the final strike onto the weak permanent employment system. Corporations will have even easier access to labor in other countries where it is not only cheaper but requires no permanent commitments. This should not be a surprise, since for the last decade large Japanese corporations have already efficiently outsourced labor. Corporate outsourcing could have a positive impact by directly forcing Japanese workers to seek new opportunities. Despite the negative effects the deal will have short term, it will accomplish what Abe set out to do-to: jumpstart the stagnant Japanese economy by giving more opportunities for Japan to increase its exports. Free trade’s primary goal is to reduce trade barriers between Pacific nations, giving Abe the perfect chance to export high quality Japanese goods, made cheaply, to more countries.
Abe’s greatest challenge will be balancing corporate power and the supply of domestic labor. Through free trade, Japanese corporations will want to reduce expensive Japanese labor and expand production to other countries. In addition to promoting domestic entrepreneurship Abe will have to find a way to make it favorable for Japanese corporations to hire from the domestic workforce and domestic manufacturing base. In this way, Abe will stimulate economic growth and give Japanese workers money to invest and consume while interest rates are low. The only way this is feasible is by making corporate taxes competitive with the other members of a free trade agreement and making improvements to the education system. But, ultimately, only bold and consistent transformation in Japan’s social fabric -- especially targeting younger population and emphasizing self-reliance and the power of an individual -- can produce success in this paradigm change of the Japanese economic culture. The only way Japan will recover its 20th century economic swagger is if the working people buy into Abe’s plan and Abe strikes a delicate balance between big business growth and individual success.
References
Aggarwal, Raj. Restructuring Japanese Business for Growth: Strategy, Finance, Management and Marketing Perspective. Boston: Kluwer Academic, 1999 Williamson, Oliver E. ""The Transaction Cost Economics Project: The Theory and Practice of the Governance of Contractual Relations."" December 8, 2009. http://www.nobelprize.org/nobel_prizes/economic-sciences/laureates/2009/williamson_lecture.pdf. Woodall, Brian. ""Response to the Japanese Challenge,"" Asia Pacific Community, vol. 27, pg. 63-80, Winter 1985. Hall, John Whitney, and Marius B. Jansen. Studies in the Institutional History of Early Modern Japan,. Princeton, New Jersey: Princeton University Press, 1968. Dower, John. ""MIT Visualizing Cultures."" MIT Visualizing Cultures. 2008. Accessed December 1, 2015. http://ocw.mit.edu/ans7870/21f/21f.027/black_ships_and_samurai/index.html. ""Edo Period."" Edo Period. Accessed December 1, 2015. http://www.grips.ac.jp/teacher/oono/hp/lecture_J/lec02.htm. Keene, Donald. Emperor of Japan: Meiji and His World, 1852-1912. New York, New York: Columbia University Press, 2002. Chaurasia, Radhey Shyam. History of Japan. New Delhi: Atlantic Publishers & Distributors, 2003. West, John. ""Japan's Social Contract under Severe Stress."" Asian Century Institute. Accessed December 2, 2015. http://www.asiancenturyinstitute.com/society/371-japan-s-social-contract-under-severe-stress. ""Japan - The Economy - PATTERNS OF DEVELOPMENT."" Japan - The Economy - PATTERNS OF DEVELOPMENT. Accessed December 2, 2015. http://countrystudies.us/japan/98.htm. Yamamura, Kozo. ""Success Illgotten? The Role of Meiji Militarism in Japan's Technological Progress."" The Journal of Economic History: 113-35. Suehiro, Akira, and Tom Gill. Catch-up Industrialization: The Trajectory and Prospects of East Asian Economies. Honolulu, Hawaii: University of Hawaii Press, 2008. Kerr, George H. Okinawa, the History of an Island People. Rutland, Vermont: C.E. Tuttle, 1958. Totman, Conrad D. A History of Japan. Malden, Massachusetts: Blackwell Publishers, 2000. ""The Meiji Restoration and Modernization."" The Meiji Restoration and Modernization | Asia for Educators | Columbia University. Accessed December 3, 2015. http://afe.easia.columbia.edu/special/japan_1750_meiji.htm. Gordon, Andrew. Labor and Imperial Democracy in Prewar Japan. Berkeley, California: University of California Press, 1991. Yamamura, Ko. The Economic Emergence of Modern Japan. Vol. 1. Cambridge: Cambridge University Press, 1997. Gordon, Bill. ""Greater East Asia Co-Prosperity Sphere."" Greater East Asia Co-Prosperity Sphere. March 1, 2000. Accessed December 3, 2015. http://wgordon.web.wesleyan.edu/papers/coprospr.htm. Woodall, Brian. Growing Democracy in Japan: The Parliamentary Cabinet System Since 1868. Lexington: University of Kentucky Press, 2014, pp. 69-71. Dower, John W. Embracing Defeat: Japan in the Wake of World War II. New York, New York: W.W. Norton & Company, 1999. ""Japan and Its Broken Social Contract."" Japan and Its Broken Social Contract. March 23, 2009. Accessed December 3, 2015. http://www.investoralist.com/japan-reform-employment-social-welfare/. Dower, John W. Cultures of War: Pearl Harbor, Hiroshima, 9-11, Iraq. New York, New York: W.W. Norton & Company, 2010. ""Cross Currents."" Cross Currents. Accessed December 3, 2015. http://www.crosscurrents.hawaii.edu/content.aspx?lang=eng&site=japan&theme=work&subtheme=AGRIC&unit=JWORK098. Kawagoe, Toshihiko. ""Agricultural Land Reform in Postwar Japan: Experiences and Issues."" Policy Research Working Papers, 1999. Lincoln, Edward J. Arthritic Japan the Slow pace of Economic Reform. Washington, D.C.: Brookings Institution Press, 2001. Macey, Jonathan. ""Corporate Governance and Banking in Germany, Japan, and the United States."" : Publications : The Federalist Society. June 1, 1997. Accessed December 3, 2015. http://www.fed-soc.org/publications/detail/corporate-governance-and-banking-in-germany-japan-and-the-united-states. Dower, John. ""The San Francisco System: Past, Present, Future in U.S.-Japan-China Relations."" February 24, 2014. Accessed December 3, 2015. http://japanfocus.org/-john_w_-dower/4079/article.html. ""Labor Standards Act."" Accessed December 3, 2015. http://www.ilo.org/dyn/travail/docs/2021/Labor Standards Act - www.cas.go.jp version.pdf. Pharr, Susan J., and Frank Jacob Schwartz. The state of civil society in Japan. Cambridge: Cambridge Univ. Press, 2003. Tsutsui, William. ""The Bubble Economy and the Lost Decade: Learning from the Japanese Economic Experience."" Lecture, 2009 Summer Teacher Institute - Understanding the Global Economy: Bringing the World Market into Your Classroom, Chicago, January 1, 2009. https://www.youtube.com/watch?v=EF93cr3HxZY Tabuchi, Hiroko. ""When Consumers Cut Back: An Object Lesson From Japan."" The New York Times. February 21, 2009. Accessed December 3, 2015. Krugman, Paul R. The return of depression economics. London: Penguin Books, 2000. ""Lexicon."" Abenomics Definition from Financial Times Lexicon. Accessed December 3, 2015. http://lexicon.ft.com/Term?term=abenomics. Brasor, Philip. ""Debating the Merits of Lifetime Employment | The Japan Times."" Japan Times RSS. November 1, 2014. Accessed December 3, 2015. http://www.japantimes.co.jp/news/2014/11/01/national/media-national/debating-merits-lifetime-employment/ Brasor, Philip. ""No Room for Subtleties When Laying off Workers | The Japan Times."" Japan Times RSS. January 26, 2013. Accessed December 3, 2015. http://www.japantimes.co.jp/news/2013/01/26/national/media-national/no-room-for-subtleties-when-laying-off-workers/#.WW1dZojytPY Koike, Yuriko. ""How the TPP Can Transform Japan's Economy | The Japan Times."" Japan Times RSS. November 3, 2015. Accessed December 3, 2015. http://www.japantimes.co.jp/opinion/2015/11/03/commentary/japan-commentary/tpp-can-transform-japans-economy/ ""Historical Statistics of Japan."" Statistics Bureau, Ministry of Internal Affairs and Communications. Accessed February 16, 2016. http://www.stat.go.jp/english/data/chouki/index.htm.
Endnotes
1.) A Japanese policy that would not allow the entry or exit of foreigners. Also, Japanese citizens could not leave the country unless they had official permission. The policy was present under the Tokugawa shogunate.
2.) Ruling military elite in control of Japan prior to the 1867-68 revolution.
3.) An island in the bay of Nagasaki created as a trading outpost.
4.) Western music learned by Japanese scholars at Dejima
5.) An unsteady period in Japan notable for its many military conflicts and societal transformations
6.) The feudal government that remained in power in Japan from 1603 to 1868.
7.) Perry was a Commodore of the U.S. Navy who substantially contributed to linking Japan to the West (April 10, 1794 – March 4, 1858).
8.) Also known as the Tokugawa Shogunate
9.) The last Tokugawa shogun of Japan.
10.) Large corporate and financial institutions during Imperial Japan
11.) The ethnic religion of the people of Japan.
12.) A religion formed from a combination of Buddhism and Shintoism
13.) A law enforcement entity founded in 1911 to control political organizations and movements.
14.) A first rate country.
15.) A type of organizational structure which is based on tight interconnection between shareholders and business interests.
16.) The banishment room or the expulsion room for pressuring employees.
Acknowledgement
My advisor Professor Brian Woodall (the Sam Nunn School of International Affairs, Georgia Tech’s Ivan Allen College of Liberal Arts) who guided me, helped develop ideas and advised me on complicated issues."	60113
economy	['Kakutani']	2017-05-22 00:00:00	"Abstract By using an incentives/disincentives model to map the divergent behaviors of multinational corporations (MNCs) confronted by a sanctioned economy, I explain why some economic sanctions work better than others at achieving their desired political outcomes. When presented with the opportunity to ""run the blockade,"" MNCs are incentivized to sanction bust by the allure of higher profit through rent extraction. At the same time, MNCs are disincentivized to sanction bust by the penalties for breaking the sanction, but only if MNCs believe sanction busting operations is inconspicuous enough to avoid detection. If the incentives to sanction bust outweigh the disincentives not to, then MNCs will trade with sanctioned states, as was the case with Rhodesia. Since MNCs were crucial to both the Rhodesian and the South African economies – as it provided oil to the former and operated a significant minority of the firms in the latter – the decisions of MNCs to remain engaged in Rhodesia and to disengage from South Africa had a significant impact on the economic and political life of the two apartheid regimes. Hence, while many economic and political indicators identified by literature predicted that Rhodesia would have a shorter life expectancy under economic sanctions, Rhodesia defied all expectations and survived twice as long as South Africa.

Introduction

Since the end of the Cold War, economic sanctions have been used to discourage ""any actions of a targeted nation that the targeting nation or group of nations disagree with.""2 Indeed, in just the last 25 years, the U.N. Security Council has authorized 24 economic sanctions; prior to 1990, the U.N. authorized only two.3 Furthermore, ""between 1960 and 1990, most of the sanctions were imposed unilaterally, most frequently by the United States, but in the 1990s, a large fraction was imposed by intergovernmental coalitions.""4

Given the sudden popularity of economic sanctions as a policy instrument, many scholars have studied it. But despite the proliferation of literatures on economic sanctions, few studies have compared the impact of economic sanctions against Rhodesia and South Africa on the political resolution of Apartheid. This gap in literature is surprising, because a comparison of the two Apartheid regimes under sanction allows for a great ""most-similar cases"" research design, in which most exogenous variables that may affect the result are kept constant. Rhodesia and South Africa were both post–colonial Apartheid states with resource extraction economies. Both states received a comprehensive U.N. sanction regime that banned the trade of nearly all goods with nearly all nations. But Rhodesian Apartheid held out for 15 years under sanction, while South African Apartheid barely survived seven. Why is there such a big difference in the effectiveness of sanctions?

Conventional literature points to state-centric factors, like differences in political systems or macroeconomic structures. Such factors fail to answer the question. Political and macroeconomic divergences between the Apartheids states were either nonexistent, or predicted that Rhodesia – not South Africa – would be weakened more by sanctions.

Instead, the key to understanding the mystery must be sought in non–state actors: the MNC. Rather than differences in state-centric factors, the divergent decisions of MNCs to sanction bust in Rhodesia – but not in South Africa – decided the fates of the two Apartheid states. Rational and profit-maximizing actors, the MNCs made this crucial decision by weighing the incentives and disincentives of sanction busting in a process that considers profitability, punishment, and conspicuousness. On one hand, the prospects of capturing large rents from sanctioned economies and hence earning high profits gave MNCs an incentive to run the blockade. On the other hand, the penalties threatened for disobeying the sanction regime dissuades MNCs from sanction busting, but only if the MNCs were caught. Like people, MNCs will respect laws only if they are enforced, and laws are enforceable only if the offenses are discovered. MNCs will sanction bust if and only if the incentives to do so outweigh disincentives not to. The Rhodesia/South Africa story has broader implications: policymakers should consider not just state-centric factors, but also the micro economies that regulate the incentives and disincentives for sanction busting. Individual market actors, not the state, are the vehicle of global trade; sanction regimes that do not secure the cooperation of these actors are bound to fail.

This study will develop the argument in six sections. The following section will summarize the history of Rhodesia and South Africa before and during sanctions for unfamiliar readers. The third section will condense the literature on what determines sanction success, and why they cannot explain the Rhodesian and South African cases. The fourth section will describe the incentive/ disincentive model. The fifth section will apply the model to the Rhodesian and South African cases to explain the divergent sanctions outcomes. The last section will present the conclusions of this study and the implications of the incentive/ disincentive model for the larger sanctions debate.

A Brief History of Rhodesia, South Africa Before and During Sanctions

Rhodesia and South Africa were both British colonies with a long history of racial segregation that endowed political, economic, and social privileges to the white settler‒colonist minorities at the expense of the indigenous African majority. In both countries, whites had suffrage, Africans did not; Whites farmed expropriated land, Africans were expelled to agriculturally unproductive areas; whites enjoyed access to higher education, Africans did not. These racist policies, collectively referred to as Apartheid policies, would provoke international outcry and economic sanctions in the late 20th century.5

Until the mid–20th century, segregated, colonialist societies faced little international opposition: Singapore, Australia, the United States, and many other countries all denied suffrage and other key civil rights to people of non–Caucasian descent without inviting international condemnation. However, with the rise of the civil rights movement and the decolonization movements around the world in the late 50s and early 60s, international norms turned against states that continued to racially segregate their people. Because of this change in international opinion, Rhodesia and South Africa were both subjected to a U.N. authorized sanction regime from 1965-80 and 1986-93, respectively. In both cases, the U.N. sanctions were intended to force the two states to abandon their Apartheid policies and grant suffrage to their disenfranchised African majorities. The sanctions aimed to change the policy, not the regimes; that said, many who were targeted by the sanctions felt that Apartheid was such a crucial aspect of their regime, that granting suffrage to Africans would undermine it.

The outcome of the U.N. sanctions could not have been any more different. Rhodesian Apartheid, against all odds, survived for 15 years under economic sanctions. Rhodesia accepted power‒sharing with African party members only when the African guerilla fighters made their military situation untenable. If Rhodesian military situations were more favorable, they may have survived the sanction regime for a lot longer, perhaps even indefinitely. In contrast, South African Apartheid barely lasted eight years, after which it extended suffrage to Africans and released Nelson Mandela from prison. Because was no military threat to the South African government, the South African decision to end racist policies was primarily an outcome of economic pressure.

Explaining Sanction Success

One of the most common determinants of the sanction's effectiveness is the nature of the sanction–targeted state.6 This line of thinking makes sense; the same sanction package may cripple a small nation with little domestic manufacturing capacity to meet internal demand, but barely affect a large nation with the domestic industries to achieve autarky. Likewise, a sanction may be lethal to an export-oriented economy, but a mere annoyance to an economy sustained by domestic consumption.

Another equally intuitive exercise will examine the breadth and depth of the sanction package.7 The breadth of a sanction package is the proportion of the global market that would participate in the sanction. Obviously, if the United States restricts access to its market, then the sanction would be much more effective than if the United States did not. The depth of a sanction package is the number of commodities sanctioned. In 1983, Australia refused to sell uranium to France; while the sanction did some harm to the French nuclear program, it had little impact on the health of the macroeconomy.8 If Australia had restricted French access to its entire market, the French economy would have been more acutely hurt.

While these two intuitive variables of comparing the national context or the sanction package to determine divergent sanction outcomes offer easy ways out, they are unhelpful in explaining the outcomes in Rhodesia and South Africa: both the national context and the sanction package remained constant between the two countries, which means neither could explain the differences in outcomes.

Almost all the key national characteristics of sanctioned Rhodesia and South Africa were astonishingly similar. Both states had a large African majority dominated by a white minority, an Apartheid political system, an isolated diplomatic status, tenuous trade relations, a Southern African geography, an Afrikaan-British heritage, a British colonial history, and an agricultural and mining-based export economy. If any one of these factors were different, it could at least partially explain the sanctions outcome. But with these key national characteristics held largely constant, none of them can explain the drastically different sanctions outcomes.

Both Rhodesia and South Africa were placed under comprehensive Hurbauer, Economic Sanctions Reconsidered, 91. economic sanctions with nearly universal participation from all major and most medium powers. For both countries, the sanctions seriously inhibited trade, depreciated their currencies, limited access to international finance, cut off access to foreign aid, and froze foreign assets held by the target government and citizens.9, 10 And yet, the sanctions in Rhodesia and South Africa still produced significantly different political results.

If similar sanction packages were imposed on similar countries, why did one collapse much more quickly than the other? When neither national context nor sanction package can explain sanction outcomes, the literature turns to micro variables for answers. Consider the following metaphor: if two people with similar phenotypes drank the same amount of milk, why might only one of them suffer indigestion? The doctors would answer that despite the similarities in the general circumstances, there was a miniscule, unclear, but nevertheless very crucial difference between the two individuals, perhaps a slight genetic disposition that makes one lactose intolerant. In a similar fashion, if proponents of sanctions attempted to resolve the Rhodesia-South Africa paradox, they would have answered that despite the similarities between the two sanctions, there must have been micro but very significant differences that contributed to the divergent outcomes. Unfortunately, these micro variables are inconclusive in respect to explaining the South Africa/ Rhodesia case.

So what could have been a micro difference? One of the most widely known micro variables used to explain divergent sanction outcomes is the vulnerability of a state to economic sanctions relies on the target state's economic concentration: ""the more a country's economy depends on one product, and the more its exports consist of one product… the more vulnerable is the country.""11 It is also reliant on commodity concentration, or the degree of consumer tastes that are concentrated on one commodity, and geographic concentration, the degree to which the target's trade partners are concentrated on one country. Here again, the implications of this micro foundation are counter-intuitive. According to a composite vulnerability index that considers these three types of concentration, sanctioned Rhodesia was the fourth most vulnerable state in the world, after Mauritius, Trinidad, and Panama (Galtung, year)12. There is no comparable measure of South Africa's economic vulnerability in 1985, but a comparison of the sum of net export and import, a rough indicator of a given economy's reliance on trade, reveals that Rhodesia was significantly more concentrated than South Africa. The import-export sum figure for South Africa at the time of the sanction was 22% of GDP,13 while the same figure for Rhodesia was a staggering 66% of GDP.14 These figures indicate that Rhodesia was an exceptionally concentrated, vulnerable state compared to South Africa, and predict the earlier demise of the former.

Brooks claims the regime type of the target influences the effectiveness of a sanction -democracies are more vulnerable to comprehensive sanctions, which target the macro-economic health of the state, while autocracies are weak to targeted sanctions that attack the economic health of key stakeholders in the government.15 However, regime type is an inconclusive explanation in this case.

With several possible explanations failing to resolve the Rhodesia‒South Africa paradox, many scholars argued whether the sanctions in Rhodesia and South Africa were effective at all in years during and immediately following the sanction.16 Citing the persistent economic rigor of the sanctioned Apartheid states, ""sanction skeptics"" have concluded that sanctions were inconsequential. Their most damning evidence came from Rhodesia. Despite the severe embargo, in 1973, eight years into the sanction, Rhodesia's exports grew, GDP rose by seven percent,17 and domestic manufacturing doubled compared to presanction levels.18 The nation also underwent significant political integration as Rhodesians ""rallied to the flag.""19 In South Africa, despite significant movement to disinvest, MNCs maintained an indirect presence in the nation.20 Therefore, capital flight and disinvestment from sanctioned South Africa may not have been as economically devastating as the raw economic figures suggest.

Despite these arguments, the academic consensus today is that sanctions on South Africa and Rhodesia did play a role in forcing political change.21 Many of the claims of Rhodesian strengths, made before 1975, did not consider the numerous hidden weaknesses directly caused by the economic sanctions, such as the dwindling state Foreign Exchange reserves, which broke into daylight only in the final days of Rhodesia.22 Galtung's articles, written in 1974, admit: ""we do not know how (the sanction) will all end, and primary source material of a crucial nature is not yet available.""23 But while today's scholars agree sanctions did play a role in the demise of Rhodesia, they still have yet to consider the case of Rhodesia in a comparative context in relation to South Africa.

Limitations in the Conventional Literature

The conventional literature fails to explain the Rhodesia-South Africa paradox because it suffers from a consistent over-emphasis of the role of state actors on sanction outcome. Baldwin and many other foundational scholars of the sanctions literature frame economic sanctions tools of ""economic statecraft,"" without mentioning the impact of behaviors by non-state actors on sanctions effectiveness.24 Furthermore, sanction scholars tend to predict the effectiveness of sanctions by relying on macroeconomic or political microfoundations, such as export/import reliance and economic concentration, or regime type and political decision makers, which are all influenced by or a direct outcome of state behaviors.

The reliance of sanctions literature on the behavior of state actors as an explanation follows a general partiality of political science literature to state actors. However, this bias has resulted in an unjustified presumption in recent sanctions literature that the effectiveness of economic sanctions can easily be maximized, and its collateral damage minimized through ""smarter"" sanctions policy with improved political and macroeconomic parameter setting by policy makers. Adherents of ""smarter policy, smarter sanctions"" risk reaching a reductive conclusion that policy makers who are aware of the political/macroeconomic factors affecting sanction effectiveness will produce more effective sanction policies.25 This claim is not empirically supported according to Drezner: despite the frequent implementation of ""smart sanctions,"" sanctions have not become significantly more effective.

""Smarter"" sanctions, however, have not translated into better outcomes. This is because smarter sanctions ultimately suffer from a deficit in the entire economic sanctions literature in general. All previous scholarship on sanctions are incorrect in assuming that the success or failure of a sanctions policy is largely determined just by the decisions of policy makers and their macroeconomic and political parameter setting. They fail to account for the fact that sanction effectiveness is also determined by the decisions of the MNCs and their microeconomic parameter setting. If policymakers want to draft a truly ""smart"" sanction, they must not only consider the influence of state behavior on macroeconomic and political parameters, but also the impact of behaviors of non-state actors, namely the MNCs, on sanctions policy.

The Role of MNCs and Their Microeconomic Parameter Setting on Sanctions Outcome

I will construct a model to explain MNC behaviors. Of course, actual MNC decision making is much more involved, but the model has satisfactory explanatory power. In the next section, the model will be used to explain the outcomes of the sanctions placed on Rhodesia and South Africa.

Future Profitability, Conspicuity, Max Potential Punishment

MNCs are international corporate entities with a single incentive: net profit maximization.26 Assuming MNCs will always choose options that will produce the largest net profit, if MNCs achieve positive net profit from sanctionbusting, they will sanction bust; if not, they will disengage from the sanctioned economy. ""Profit Maximization,"" however, is an aggregate product derived from many incentives and disincentives. Hence, to determine whether a profit-maximizing MNC will sanction-bust or not, the incentives and disincentives that shape the corporate environment must be studied.

The primary incentives and disincentives at work when a profit-maximizing MNC decides to engage with or disengage from a sanctioned economy are future profitability, conspicuity, and max potential punishment. Future Profitability is a positive incentive. It correlates with the perceived gross economic gain a MNC expects to obtain from engaging in sanction-busting trade. The larger the expected profit from sanctions-busting, the stronger the incentive will be for MNCs to engage with sanctioned economies. Conversely, the smaller the expected profit, the weaker the incentive to sanction-bust. Conspicuity is a negative disincentive. It correlates with the expected visibility of the MNC's sanctions-busting operation from the perspective of the sanctioning government and the public at large. High conspicuity significantly discourages MNCs from engaging in sanctionsbusting trade because it is more likely to invite punishment from the sanctioning government and the public. Max Potential Punishment (or Max Punishment) is a negative disincentive. It correlates with the expected severity of the gravest punitive action conceivable for engaging in sanction busting trade inflicted onto the MNC by the government and the public. Obviously, the more severe the max potential punishment is, the stronger the disincentive is for MNCs to engage in sanction-busting. In constructing the model, we assume that these three incentive/ disincentives are determined by several key input variables, listed below.

1) Future Profitability.

The expected future profitability of a given sanction-busting trade is an output value that is derived from three input variables. One of these inputs is the general economic desirability of the sanctioned economy. This variable represents the key economic facts, (such as market size or production capacity) that MNCs consider when they decide whether to engage with a market, whether sanctioned or not. Unless the sanctioned economy has some economic value that makes it valuable to the MNCs, they would not trade with them even without sanctions. The other input variables are relevant only to sanctioned economies. One of these is bargaining power. MNCs must gauge how much additional bargaining power they have in negotiating with sanctioned economies. These economies are not in a position to take the global market price due to their limited access to the world market, and hence will have to take higher prices for imports and lower prices for exports. This is to the advantage of MNCs. The greater the bargaining power, the greater the future profitability. Significant bargaining power permits MNCs to act as a monopolistic price-setter that can undersell the sanctioned economy. The strength of an MNC's bargaining power is determined by the answer to three questions. (1) Are other MNCs participating in sanction-busting operations? If so, the other players will force the MNC to buy and sell at prices that are more competitive, reducing their bargaining power. (2) To what extent has the sanctioned economy achieved autarky? If the sanctioned economy has achieved a significant degree of autarky and can domestically produce the product supplied by the MNC, then the MNC will have to set prices competitive with domestic products. Whether autarky is possible or not would at least partly depend on the mobility and the alternative availability of the particular product provided by the MNC. If the product is highly mobile (eg. a consultant), then the product can be quickly pulled out of a sanctioned economy before there is enough time to establish autarky and produce sanctioned goods domestically. If the product is available from an alternative source, either from other MNCs or a domestic firm, then autarky is much easier. Lastly, there is (3), the expected duration of the sanction. This is a significant determinant of future profitability. If the MNC perceives the sanction to be a short-term affair, losses from hiccup in trade is far outweighed by the benefits of a sustained, long-term trade relationship.

2) Conspicuity.

Information that is visible to governments is not necessarily visible to the public, and vice versa. For example, state intelligence may be aware of MNC sanction-busting, but intentionally not inform the public. Conversely, certain private actors with privileged access to insider information of MNC activities, like investigative journalists, may have information that the government does not. Therefore, a more accurate model than the one used in this study would take into account this information asymmetry. This would be done by disaggregating conspicuity into the conspicuity of the sanction-busting trade from the sanctioning government, and the conspicuity of the same trade from the public. Unfortunately, disaggregating conspicuity into these two incentives is beyond the scope of this study. Lacking access to key government archives, it is impossible to know whether information not known by the public at the time of the sanction was withheld by the government as classified knowledge, or was just as unknown to the sanctioner government as to the public.

This methodological obstacle is circumvented by assuming that what information was known by the public was also known by the government, and vice versa. Therefore, in this model, the degree of awareness by either the public or the sanctioner government of the MNC sanction-busting operation is the single input used to derive the conspicuity disincentive. This assumption is not problematic for Rhodesia and South Africa. The most economically significant sanctioner governments of Rhodesia and South Africa, the United States and those of Western Europe, were liberal democracies, where information that was visible to the government was also visible to the public. Even if either the government or the public did come to acquire information that one of them was not aware of, the information barrier was low enough that information about the key sanctionbusting trades in the two countries (particularly oil trade in Rhodesia), and the FDI engagement of MNCs in South Africa) quickly diffused from one actor to another.

3) Max Potential Punishment.

The value we use for total Max Potential Punishment is derived from both the Max Potential Punishment sustainable from the sanctioner government, and the Max Potential Punishment sustainable from the public. If the sanction-busting activity is illegal, the punishment from the sanctioner government may come in the form of a fine, imprisonment, or other legal recourse.. If the sanction-busting activity is technically legal, it could come in the form of closing of loopholes, severance of government contract to MNCs, and other extra-judicial punitive measures. The most common and effective form of punitive action by the public is the boycott, where consumers refuse to buy the products of the offending MNC or otherwise engage in transaction with them. While it may seem wise to disaggregate public punishment and government punishment, this is not necessary.

MNCs anticipate their actual punishment for engaging in sanction-busting trade to be much smaller than, and rarely equal to, the Max Punishment threatened. Such optimism is not exclusive to MNC. It is not uncommon for criminal entities to perceive the expected punishment as much lower than what is threatened. For example, students perceive the actual punishment of smoking marijuana to be much less serious than the Max Punishment because they are confident they won't be caught. This expectation as to what extent the Max Punishment will be realized is entirely dependent on conspicuity. Similarly, the students in the previous example will expect more serious consequences if, instead of smoking at home, they smoke in front of the police. Likewise, MNCs perceive that higher degrees of conspicuity makes it more likely for the government and the public to impose the Maximum Punishment, and less conspicuity makes such punishment less likely.

The sanctioner government is more likely to retaliate against a conspicuous sanctions-breaking operation for two reasons. First, if the MNC's sanctions-busting operation is illegal, government agencies are more likely to uncover and punish conspicuous sanctions-busting trade. Second, even if the MNC's operation is still technically legal due to loopholes, the governments are still more likely to interpret conspicuous sanctions-busting activities as an affront to government authority.The governments will have strong incentives to either amend the sanction legislation, close the loophole, or even resort to extrajudicial measures to punish the ""legal"" trade. While a government might look the other way if an illegal transaction is done under the table, it may be forced to take retaliatory measures if the same trade is done in broad daylight, in open defiance to the authority of the state.27

The public is more likely to retaliate against conspicuous sanctionbusting operations because the principal method of retaliation used by the public against MNC, boycotting, requires widespread mobilization.28 Boycotting is a worthwhile weapon only if enough people participate. However, people would consider a boycott only if they are aware of the MNC's sanction-busting operations. The public is more likely to be aware of MNC operations that are visible; thus, a conspicuous sanction-busting trade is necessary to activate civil society in an effective retaliatory boycott.

The Incentive/Disincentives Model: MNC Decision Making for Sanctioned Economies

Now that all the inputs behind each of the incentive and disincentives have been described, it's time to present the way in which the incentive/disincentives of future profitability, conspicuity, and max punishment produce the final MNC decision. The corporate calculus is:

P − C*(MP) = D,

where: P = Future Profitability, C = [0,1] = Conspicuity, MP = Max Punishment, D = MNC Decision,

And: IF D > 0, THEN sanction-bust,

And: IF D < 0, THEN don't sanction-bust.

The economic benefit of sanction busting is abstracted as P. Hence, a larger P correlates with a larger D, as higher the future profitability of a trade, the greater the incentive for MNCs to sanction bust. Future profitability only accounts for the gross future profitability of sanction-busting. However, sanction busting comes with its own expected cost that will depress net profits. This expected political and social cost of sanction busting is modeled by C*(MP). A larger MP correlates with a smaller D, as the more severe the maximum potential punishment is for sanctionbusting, the more MNCs are discouraged from engaging in such operations. However, the severity of the punishment imparts any meaningful weight on the MNC decision making only if sanction-busting trade is conspicuous enough for the government and the public to inflict all, or part of, the maximum possible punishment onto the MNC. In other words, no matter how severe the potential punishment is for sanction-busting, MNCs will only be discouraged if sanctionbusting is flagrant enough to invite punishment. Thus, the extent to which the max potential punishment disincentivizes sanction-busting depends on conspicuity, a variable which ranges from 0 to 1 in this model. If Conspicuity is 0, the sanctionbusting transaction is invisible and hence the MNC decision making is unaffected by the empty threat of punishment. If Conspicuity is 1, sanction-busting is visible and the MNC expects the full weight of the threatened punishment.

Figure 1: The MNC incentive/disincentive model

Maximum Possible Punishment from the public, and Maximum Possible Punishment from the sanctioner government were integrated into one incentive function. This abstraction was made possible by our assumption that the visibility and conspicuity from the perspective of the government and the public is the same:

IF: P − C public *(MP public ) − C government *(MP government ) = D,

AND C = C public = C government , MP = MP public + MP government ,

THEN: P − C*(MP public ) − C*(MP government ) = P − C*(MP public + MP government ) = P − C*(MP) = D.

Now that the incentive/disincentive model for MNCs has been established, let us apply it to South Africa and Rhodesia. In each case study, I will first establish that the decisions of MNCs to engage or disengage, Rhodesia for the former and South Africa for the latter, weakened the state's economic health and its capacity to reject the sanctioner demand to end apartheid. This shows that MNC decision did affect the outcome of the sanction. Second, I will trace the decision-making process of key MNCs where I find the measure of the input variables, derive the incentive/disincentive from the inputs and add the incentive/ disincentive to derive the final decision of the MNCs and its impact on the sanctioned economy.

MNC Decision Making in South Africa and Rhodesia

South Africa

The economic and political impact of MNC behavior on the end of Apartheid

Faced with the prospects of an imminent sanction, many key MNCs chose to disengage from the South African economy rather than participate in sanction-busting trade. Their decisions destroyed the South African economy. The most visible damage to the South African economy came from capital flight. Foreign capital fled Praetoria for safer investment options,29 leading to higher South African interest rates. In 1985, the sudden capital flight created a liquidity crisis,30 forcing the government to call a unilateral memorandum on all public debts. The memorandum made banks grow even more wary of investing in Praetoria, causing a confidence crisis for all South African debts, public and private. As a result, the prime interest rate in South Africa shot up to record rates, from nine percent in 1982 to 25% in 1985.31 The rates did normalize at a lower figure as the initial shock wore off, but investors remained cautious. The prime interest rate floated between 15% and 25% throughout the sanction. Less visible in the short term but just as corrosive in the long term was the withdrawal of FDI by MNC from the sanctioned state. Two hundred MNCs with FDIs in South Africa chose to disinvest32, selling off their physical assets to South Africans. Because of disinvestment, about 5 billion Rand of FDI was lost.33

Disinvestment harmed South African industries in two ways. First, it cut off local production facilities, previously owned by MNC subsidiaries, from the MNC's global supply and distribution network, leading to drastic reduction in corporate profits, wage, and unemployment. Second, because of disinvestment, the South African economy lost the practical and technical knowhow of the MNCs that was now integral to maintain South African competitiveness. The MNC decision to disinvest reduced South African access to global supply/distribution chains and MNC knowhow, which in turn paralyzed productivity growth from 1985 to 1993.34 This stagnation was caused by the sanction, as the economy recovered its high productivity growth after the sanction was lifted.

The combined impact of capital flight and disinvestment intensified the economic crisis. When capital fled, international interest rates rose, and foreign exchange became scarce, the South African economy required a competitive export market to acquire foreign currency if it was to have any chance of staying afloat. However, disinvestment reduced South African productivity, hamstringing international competitiveness and reducing its ability to earn foreign currency. The negative spiral was reflected in the meager GDP growth rate during the sanction. During the duration of the sanction from 1985 to 1993, GDP stagnated, growing only by 20 billion. In comparison, the South African economy grew 60 billion from 1975 to 1985, and 130 billion from 1995 to 2005.35

The economic downturn caused by MNC decisions created political pressure to end the sanction as median voters, most of whom suffered financially, steered the political tone of the government to one of reconciliation.36 The most apparent impact felt by the average South African came in the form of more expensive imports. Import prices inflated due to the depreciation of the Rand caused by capital flight. As a result, the South Africans were forced abstain from luxury, sometimes necessities, that were imported. The South African current account surplus grew in this period despite weaker exports, because reduced imports more than offset declines in import (South African Reserve Bank)37. As Rodman notes, the South African economy was not in a state of total collapse. However, for a people used to easy credits and no shortage of goods, the MNC's disengagement undermined the economic security that until then they had taken for granted. This created an imperative for policy change. The disinvestment of many famous MNCs also created a psychological imperative amongst decision makers to change direction. The simultaneous rejection of South African apartheid by private actors through disinvestment and by public actors through withholding of diplomatic recognition reinforced the perceived isolation of South Africa and undermined the legitimacy of Apartheid.38

One caveat to this narrative is that, since white-only suffrage excluded black and colored Africans from voting, the ""median voter"" of Apartheid South Africa was by no means synonymous with the ""median citizen."" In fact, the ""median voter,"" who was most likely white and affluent, and the ""median citizen,"" who was most likely black and poor, had different experiences in sanctioned South Africa. As in other historical examples of sanctioning, the population that the sanction was supposed to help suffered the brunt of the economic devastation; ""the median citizen""– the many Africans under Apartheid – experienced even more drastic economic collapse under the sanction regime than did white voters.39 For instance, in companies formerly operated by MNCs before disinvestment, internal corporate regulations that barred discriminatory practices were repealed once each MNC administration was replaced by domestic management. The change in administration exposed the African employees of these companies to even greater discrimination and economic damage.40 Given the severe, short-term economic devastation experienced by Africans, which dwarfed the inconveniences suffered by white voters, if the Africans did have electoral power, there would probably have been an even greater political pressure to end apartheid. If the ""median voter"" did include Africans, Apartheid may have collapsed even sooner under sanctions; but again, there would be no Apartheid if Africans had the vote.

Incentive/Disincentive Causal Mechanisms behind MNC behavior in South Africa

MNCs decided to disengage from South Africa because continued engagement with South Africa would have entailed low future profitability, high conspicuity, and high Max Punishment. There was a weak positive economic incentive and a strong negative political and social disincentive to sanction-bust, leading to a decision to disengage. The disincentive from political and social sanctions exceeded incentives from the expected profit MNCs could gain by sustained engagement. As these incentives/disincentives are derived from several input factors, I will examine each of these inputs to determine how MNCs establish their incentives/disincentives.

1) Low Future Profitability.

MNCs expected low future profitability from sanction-busting in South Africa due to expectations of low economic desirability of trading with South Africa, weak bargaining power in sanction-busting negotiation, and a long period of sanction. First, the South African economy was not a very desirable market for MNCs. For MNCs with FDI assets in South Africa, the revenue collected from operations in South Africa often amounted only to about two percent of total revenue, and thus was not essential for overall company future profitability.41 For international banks, historically South Africa's primary value had been a stable African investment option with higher interest rates than in the developed world. However, after MNCs with FDI assets retreated, destabilizing the economy in the process, the primary merit of South African investment – its stability – eroded, making South Africa a less desirable investment option. Second, the MNCs expected weak bargaining power from negotiation, so they were skeptical they could maintain competitive prices or profits because the MNCs would have to compete with competitive prices from budding but growing domestic manufacturers. In addition, numerous MNCs were engaged with pre-sanction South Africa; hence, in the ""fog of war"" where MNCs (like all market actors) are unaware of the decisions of their competitors, an MNC could not rule out the possibility that some of their competitors may also decide to sanction-bust, reducing their own bargaining power in undercutting South African prices. Finally, with the historical knowledge that the only other example of a comprehensive, U.N.-led sanction against another Apartheid African state, Rhodesia, was a protracted affair lasting 15 years despite early expectation that it would be over within the year, it would be expected that most MNCs probably anticipated the sanction against South Africa to be a long-term affair. Expectations for a longterm sanction reduced the need to maintain trade contact with South Africa in case commercial relations were normalized again soon. These input factors totaled to a weak, positive future profitability incentive to sanction-bust.

2) High Conspicuity.

MNCs perceived that any sanction-busting activity would be conspicuous and subject to a serious maximum potential punishment. This expectation was more a result of heightened public awareness than of government authority. Even before the sanctions were formally instated, several NGOs had dedicated themselves to exposing MNCs with engagements in South Africa. In the early 1960s, a decade before the mandatory U.N. sanctions were authorized, the Anti-Apartheid Movement (AAM) organized civilian boycotts of South African goods by naming and shaming companies engaged with South Africa.42The AAM fizzled out in the late 1960s, but other NGOs continued the fight. Several campus student organizations petitioned their universities to disinvest from MNCs engaged in trade with South Africa, often naming specific corporations that the universities were invested in.43 Because of this activism by an activated civil society, MNCs expected high visibility of sanction-busting operations in South Africa. This is not to say that the local black South Africans exercised little agency in combatting the Apartheid regime ruling over them. If anything, the civilian boycott in the West gained critical mass only after the world witnessed the intense resistance of black South Africans to the institution of the South African Constitution of 1983, which reaffirmed racial segregation.44 That said, external actors (inspired to action by the resistance of black South African internal actors) played the predominant role in shaping MNC behaviors.

3) High Maximum Potential Punishment.

NGOs also had a deep well of support they could readily mobilize for boycotts and public campaigns, raising the maximum punishment that the public could inflict upon MNCs. In 1971, the U.S.-based Interfaith Center on Corporate Responsibility (ICCR) pressured corporations operating in South Africa to adopt the Sullivan principles – a seven-point program that demanded non-segregated workplaces and ""equal and fair opportunities for all employees"" regardless of race.45 Since it was illegal under South African law to abide by the Sullivan principles, the ICCR pressured companies to leave South Africa. Those that did not leave South Africa faced massive consumer and investor boycotts organized by the ICCR. Furthermore, the widespread student protests meant that 155 university institutions had disinvested from MNCs trading with South Africans by 1988. As such, NGOs saw that they would receive a severe punishment in the form of disinvestment by universities, who are key institutional investors, if they continued to sanctionbust.46 The NGOs also significantly raised the maximum punishment that the sanctioner government could impose.by pressuring municipal, local, and state governments to rescind lucrative government contracts with MNCs engaged with South Africa. Combined, these conditions created the threat of high maximum potential punishment by both the public and the government. Furthermore, because of the visibility of sanction-busting operations, the MNCs expected the full extent of the maximum punishment if they did engage with South Africa. As a result, a powerful negative disincentive, fueled by high conspicuity and high maximum punishment, overrode the weak positive incentive and led to the MNC decision to disengage.

Rhodesia

The economic and political impact of MNCs behavior on Rhodesian apartheid

Although MNCs were pervasive in the Rhodesian economy, they still had an indispensable role in the Rhodesian economy that was equally important, if asymmetrical, to their functions in the South African economy because they provided all the Rhodesian energy requirement. Lacking any domestic energy sources, namely oil, Rhodesia was 100% dependent on oil imports, all of which were provided by oil giant MNCs.47 Rhodesia's dependency on oil MNCs was exacerbated by its landlocked geography, which forced the country to rely on pipelines to transport oil from the South East African coast, all of which were owned by the MNCs.48

The mediumto long-term decision of oil giant MNCs to stay engaged in Rhodesia was crucial to the country's economic health. Numerous commentators of the time noted that Rhodesia's economy would have collapsed very quickly without its oil imports.49 The continued oil imports permitted the rapid industrial expansion that was crucial for Rhodesia to skirt the worst effects of the sanction. Such an industrial expansion helped in two ways. First, an intensive import substitution industrialization program supplied Rhodesia with goods no longer imported under the sanction.50 Second, the domestic manufacturing expansion compensated for the economic atrophy of the former cash cows of Rhodesia under the sanction, such as the tobacco industry.51 The macroeconomic health of the country was maintained by shifting the source of growth away from such atrophied industries hit by the sanction to domestic manufacturing that supplied internal consumption. This industrial expansion was possible only through the continued supply of oil by MNCs. So, without oil imports, there would have been no industrial expansion, and the Rhodesian economy would have collapsed quickly due to shortage of imported goods and the collapse of source of income industries without another growth source to replace it.52 In addition to the industrial expansion, the ready availability of oil from MNCs softened the upward price pressure of oil per gallon. Cheaper oil was essential to Rhodesia, which was suffering from a foreign currency shortage, as expensive oil would have quickly depleted Rhodesian foreign currency reserves. 53

Rhodesia's ability to resist demands to abandon Apartheid was directly tied to the economic ramifications of the MNCs' continued engagement. The expansion of domestic manufacturing permitted by the oil import provided workers in sectors weakened by the sanction with alternative employment.54 Furthermore, the growing domestic manufacturing industry – and the economic health of the economy it helped maintain – also bolstered civilian morale. Secondly, Rhodesia could spend its foreign exchange on other vital economic concerns, as it did not need to pay a premium for smuggled oil. For example, without surplus cheap credit, Rhodesian agricultural subsidies for sanctioned tobacco goods would have quickly become unsustainable. The same foreign currency was also crucial to acquire weaponries needed to fight the guerilla war against Black nationalists.55 Finally, the abundance of oil meant the life of the median Rhodesian voter was largely unaffected. Rhodesians could still drive cars, as long as they carpooled every so often. If anything, what little oil rationing there was inspired a ""rally-to-the-flag"" effect on national unity that far outweighed the negative impact of the minor inconvenience.56 However, if oil shortages and the drop in white living standards were far more severe, Rhodesians could be expected to have behaved differently, perhaps demanding that the government cave to sanction demands.

Incentive/Disincentive Causal Mechanisms behind MNCs behavior

Although the MNC decision in the mediumto long-term was to sanction-bust in Rhodesia, this was not initially the case. At the very beginning of the sanction, incentives/disincentives motivated oil giant MNCs to disengage from the sanctioned economy. The oil giant MNCs initially cooperated with the British government, the primary sanctioner government, and agreed to stop oil shipments to Rhodesia. The MNCs chose not to sanction-bust as they expected only moderate future profitability in comparison to high conspicuity and high maximum punishment; the disincentives outweighed the incentives. However, overtime, these incentives/ disincentives changed, until they settled in a pattern of low future profitability, low conspicuity, and low Maximum Punishment. The new constellation of incentives/disincentives then encouraged MNCs to change their decisions from disengagement to engagement as although the low future profitability meant there was only a weak incentive, the low conspicuity and maximum punishment meant the negative disincentive to sanction-bust was even more marginal. Tracing MNC behaviors in Rhodesia in terms of the incentive/ disincentive model highlights that when incentives/disincentives change over time, MNC decisions will shift with the incentives.

1a) Initially, moderate future profitability.

At the beginning of the sanction, the constellation of key input variables meant oil giant MNCs expected moderate future profitability from the sanction. Given how small the Rhodesian demand for oil was, the Rhodesian market had limited economic desirability. The MNCs also expected medium bargaining power. The Rhodesians had no domestic oil industries to compete with which the MNCs, giving MNCs monopolistic control over price controls. Furthermore, since there were only five oil giant MNCs operating in Rhodesia, the withdrawal of any one competitor would significantly lower competitive pressure. In light of the optimistic statements about the quick capitulation of the Apartheid regime by the government and the public, the MNCs did not expect sanctions to last long. In total, the input variables of low general economic desirability, medium bargaining power, and short expected duration of sanction, resulted in the MNC expectation of moderate future profitability from sanction-busting.

1b) Over time, weak future profitability.

However, as the sanction progressed, the future profitability incentive begun to change. The sanction, which everyone claimed would be over very quickly, dragged on for years. Hence, the MNCs now expected a sanction with a longer duration. With lower expectation that trade relations would be normalized anytime soon, the MNCs felt less compelled to maintain trade contact. This change in this input meant its output, the future profitability incentive, changed: now, there was a weak future profitability incentive to continue trade.

2a) Initially, high conspicuity.

At the beginning of the sanction, the MNCs expected high conspicuity from engaging in sanction-busting because they expected visibility from the sanctioner government of United Kingdom. The British government declared that the British navy would enforce the sanction by identifying any ships carrying oil to ports that were conventionally used to transport oil to Rhodesia.57 Once the ships were identified, they could easily be tracked to their patron oil giant MNC; hence, the high conspicuity of MNC sanction busting.

3a) Initially, high maximum potential punishment.

When Joanne V, an oil tanker, defied the British blockade and sailed into Beira, a Mozambique port used to transport oil to Rhodesia, the British government – sanctioned by a U.N. resolution to use force if necessary to stop the oil trade – declared that any ships that unloaded oil destined to Rhodesia would be seized.58 The threat was taken seriously by MNCs: Joanne V left the Mozambique port without unloading its oil.59 Attempts by MNCs to break the blockade by using an oil pipeline of significant symbolic value as the connection between Rhodesia and South Africa were also shut down by the British government. Most MNCs saw the highly conspicuous, punishable nature of trading with Rhodesia to be a disincentive that far outweighed the paltry future profitability incentive and disengaged from Rhodesia.

2b) Over time, low conspicuity.

However, as time went on, the MNCs increasingly perceived low conspicuity since South African state practices changed the incentive structure. MNCs could deliver oil anonymously to Rhodesia as South Africa offered to purchase oil from MNCs and then sell it to Rhodesia. This permitted MNCs to claim plausible deniability – how could MNCs ""know,"" and so take responsibility for, what the South Africans were going to do with the oil once it was sold? In addition, the public attention to the sanction-busting operations was diverted from the sanction-busting MNCs to the South African regime that permitted it; thus, the increased South African notoriety caused reduced MNC conspicuity.

3b) Over time, low maximum potential punishment.

As for maximum punishment, South African state practice again changed the incentive structure. By threatening to impose a mutually painful countersanction onto the British economy, the South African regime prohibited the British from blockading a few Mozambique ports that were used to supply oil to both Rhodesia and South Africa. The official reason given for this prohibition by the South African government was that a blockade of the Mozambique ports would threaten South African access to oil. As a result, cargo ships transporting oil to these ports, whether with South African or Rhodesian contracts, were immune to British reprisals. Under the pretext of ""protecting national interest in protecting South African access to oil,"" the South Africans protected the Rhodesian oil trade. The MNCs saw that the British were spooked by the South African protests and, unwilling to risk a potential South African countersanction, would not enforce a comprehensive blockade of all ports.60 Because of South African state practice, the incentive structure changed, and given low future profitability, low conspicuity, and marginal maximum punishment, MNCs changed their commercial orientations and sanction-busted on behalf of Rhodesia for the rest of the sanction duration.

Figure 2: Summary of the MNC behaviors in sanctioned South Africa and Rhodesia

""Low"" implies negligible amounts that only marginally tips the balance of the equation. ""Moderate"" implies a considerable amount that can affect MNC decision-making unless it is overdetermined by a ""high"" value, which has the potential of single handedly determining the outcome.

Conclusion

The behaviors of MNC operating under sanctioned regimes offer insights into what make sanctions effective. Truly effective sanctions must consider the corporate calculus of MNCs, and shape the incentive/ disincentives driving corporate behavior to one of compliance with the law. The most important implication of the incentive/disincentive model is that makers of sanction policy should not only focus on threatening the MNCs, but also increasing their conspicuity in sanction-busting operations. Policymakers should prioritize raising the conspicuity of sanction-busting activities just as much as designing punishments for misbehavior; after all, the most severe punishments are paper tigers unless they are enforceable, and punishments are enforceable if and only if they can be detected in the first place. Furthermore, policymakers must be aware that, as observed in Rhodesia, third-party states can easily manipulate the MNC incentive structure to promote sanction-busting. South African intervention in the sanction structure for Rhodesia ultimately shaped MNC behavior in a way that limits sanction effectiveness. Policymakers must plan for another ""South Africa"" contingency by preparing to counter such attempts by third party states to undermine sanction effectiveness.

There are a few caveats to this model. First, the increased public awareness of MNC sanction-busting was assumed to encourage the public to enforce punishments. This may not necessarily the case if the public is indifferent to, or perhaps even hostile to, the government's sanction policy. If the public does not perceive the MNC sanction-busting as deplorable, they will not be motivated to punish the MNC for it. Second, the conspicuity of MNC behavior from the standpoints of the government and the public was assumed to be the same. This was not too much of an issue in these two cases, where the sanctioner regimes were liberal democracies where information was, in fact, shared between the public and government. This may not always be the case, especially if the sanctioner regime is not liberal democratic. In the case of the OPEC oil embargo, for example, autocratic sanctioner regimes may not readily share what they know with their citizens.

There is yet to be a method of determining – beyond qualitative terms – the amount of clout MNCs have in a sanctioned economy. In South Africa, the sheer number of MNCs implied heavy MNC clout, but clout does not necessarily require numbers. In Rhodesia, while fewer MNCs were operating, the MNC oil monopoly in the Rhodesian market made them punch far above their weight in terms of importance to the national economy. The more MNCs control crucial resources, the more impact they have on the national economy.

In this model, a linear relationship was assumed between conspicuity and the actual punishment sustained by the corporations. This may not necessarily be the case. The government and the public may tolerate MNC misdemeanors to a point and only retaliate if the MNCs overstep a critical line. Future research should clarify whether the relationship between conspicuity and actual punishment is actually linear or, if fact, incremental, where actual punishment remains unchanged for increasing conspicuity until a point, at which the actual punishment shoots up.

Lastly, this model assumed that a negative profit from sanction-busting behavior would encourage MNCs to disengage from the sanctioned economy. This is not always so. If a MNC loses even more money by disengaging from a sanctioned economy, it may prefer remaining engaged with a sanctioned economy, even if it operates at a net loss.

Despite its limitations, the incentive/disincentive model has large implications for the sanction literature. I hope this paper is the much needed intervention in sanction literature to distance it away from state-centric solutions. Perhaps further research can investigate the impact of other nonstate actors on sanction effectiveness. In particular, the impact of NGOs on sanction effectiveness seems to be a natural next step. I tangentially referred to the indirect role of NGOs in determining sanction effectiveness by exploring how NGO activities affected the incentive structures of MNCs, but NGOs deserve a much more thought-out investigation on how they affect sanction effectiveness in their own right.

The incentive/disincentive model also can hopefully assist analysts in evaluating the non-state mechanisms that determine the effectiveness of sanctions in place today. The case of North Korea seems ripe for investigation. Despite the several rounds of severe sanctions imposed on the hermit kingdom to persuade it to abandon its nuclear program, the North Koreans continue to survive, buoyed by sanction-busting Russian and Chinese firms61. Realistically, there is little the international community can do through economic coercion to convince the powerful, autarkic Chinese and Russian states to refrain from supporting sanction-busting firms. Perhaps, however, the Chinese and Russian firms themselves can be convinced of the errors of their ways by manipulating their incentive structures. Thankfully, the policymakers seem to have realized this earlier than academics. The U.S. treasury has imposed rounds of secondary sanctions not on states trading with the North Koreans, but on individual sanction-busting firms.62 These firms are now barred from interacting with U.S.-based firms, putting them at major competitive disadvantage. These secondary sanctions were installed only recently at the time this article was written, so it is too early to evaluate how effective they are, and anyhow they are beyond the scope of this paper. While it is encouraging that policymakers are turning towards non-state centric solutions, further research should apply the incentive/disincentive model to explain whether Russian and Chinese firms were actually dissuaded from sanction-busting.

References

Andreas, Peter. ""Criminalizing Consequences of Sanctions: Embargo Busting and Its Legacy."" International Studies Quarterly 49, no. 2 (2005): 335-60. doi:10.1111/j.0020-8833.2005.00347.x.

Baldwin, David A. Economic statecraft. Princeton, NJ: Princeton University Press, 1985.

Baldwin, David A. ""The Sanctions Debate and the Logic of Choice."" International Security 24, no. 3 (2000): 80-107. doi:10.1162/016228899560248.

Barbieri, K., O. M.g. Keshk, and B. M. Pollins. ""Trading Data: Evaluating our Assumptions and Coding Rules."" Conflict Management and Peace Science 26, no. 5 (2009): 471-91. doi:10.1177/0738894209343887.

Biglaiser, Glen, and David Lektzian. ""The Effect of Sanctions on U.S. Foreign Direct Investment."" International Organization 65, no. 03 (2011): 531-51. doi:10.1017/s0020818311000117.

Brooks, Risa A. ""Sanctions and Regime Type: What Works, and When?"" Security Studies 11, no. 4 (2002): 1-50. doi:10.1080/714005349.

Christopher, A. J. ""The pattern of Diplomatic Sanctions against South Africa 1948-1994."" GeoJournal 34, no. 4 (December 1994): 439-46.

Cohen, Andrew. ""Lonrho and Oil Sanctions against Rhodesia in the 1960s."" Journal of Southern African Studies 37, no. 4 (2011): 715-30. doi:10 .1080/03057070.2011.611286.

Higginson. Collective Violence and the Agrarian Origins of South African Apartheid, 1900-1948. Cambridge University Press, 2014.

Davis, Lance, and Stanley Engerman. ""History Lessons Sanctions: Neither War nor Peace."" Journal of Economic Perspectives 17, no. 2 (2003): 187-97. doi:10.1257/089533003765888502.

Drezner, Daniel W. ""How Smart are Smart Sanctions?"" International Studies Review 5, no. 1 (2003): 107-10. doi:10.1111/1521-9488.501014.

———. ""Sanctions Sometimes Smart: Targeted Sanctions in Theory and Practice."" International Studies Review 13, no. 1 (2011): 96-108. doi:10.1111/j.1468-2486.2010.01001.x.

Drury, A. Cooper. ""Reviewed Work: Manipulating the Market: Understanding Economic Sanctions, Institutional Change and the Political Unity of White Rhodesia by David M. Rowe."" The American Political Science Review 96, no. 4 (December 2002): 892-93.

Early, Bryan R. Busted sanctions: explaining why economic sanctions fail. Stanford, CA: Stanford University Press, 2015.

Galtung, Johan. ""On the Effects of International Economic Sanctions, With Examples from the Case of Rhodesia."" World Politics 19, no. 03 (1967): 378-416. doi:10.2307/2009785.

Hufbauer, Gary Clyde., Jeffrey J. Schott, and Kimberly Ann Elliott. Economic sanctions reconsidered. Washington, DC: Institute for International Economics, 1990.

Kastner, S. L. ""When Do Conflicting Political Relations Affect International Trade?"" Journal of Conflict Resolution 51, no. 4 (2007): 664-88 doi:10.1177/0022002707302804.

Kirshner, Jonathan. ""The microfoundations of economic sanctions."" Security Studies 6, no. 3 (1997): 32-64. doi:10.1080/09636419708429314.

———. ""Economic Sanctions: The State of the Art."" Security Studies 11, no. 4 (2002): 160-79.

Knight, Richard. ""Sanctions, Disinvestment, and U.S. corporations in South Africa."" In Sanctioning Apartheid. Africa World Press, 1990.

Lektzian, D., and G. Biglaiser. ""The effect of foreign direct investment on the use and success of US sanctions."" Conflict Management and Peace Science 31, no. 1 (2013): 70-93. doi:10.1177/0738894213501976.

Levy, Philip I. ""Sanctions on South Africa: What Did They Do?"" American Economic Review 89, no. 2 (1999): 415-20. doi:10.1257/aer.89.2.415.

Martin, Lisa L. ""Credibility, Costs, and Institutions: Cooperation on Economic Sanctions."" World Politics 45, no. 03 (1993): 406-32. doi:10.2307/2950724.

Minter, William, and Elizabeth Schmidt. ""When Sanctions Worked: The Case of Rhodesia Reexamined."" African Affairs 87, no. 347 (April 1988): 207-37.

Moreno, Elida. ""Panama detains Mossack Fonseca founders on corruption charges."" Reuters. February 11, 2017. Accessed November 15, 2017. https://www.reuters.com/article/us-panama-corruption-odebrecht/panama-detains-mossack-fonseca-founders-on-corruption-charges-idUSKBN15Q0TK.

Naylor, R. T. Economic warfare: sanctions, embargo busting, and their human cost. Boston: Northeastern University Press, 2001.

Naylor, R. T. Patriots and profiteers: economic warfare, embargo busting, and state-sponsored crime. Montreal: McGill-Queen's University Press, 2008.

Otusanya, Olatunde Julius. ""The role of multinational companies in tax evasion and tax avoidance: The case of Nigeria."" Critical Perspectives on Accounting 22, no. 3 (2011): 316-32. doi:10.1016/j.cpa.2010.10.005.

Pape, Robert A. ""Why Economic Sanctions Do Not Work."" International Security 22, no. 2 (1997): 90. doi:10.2307/2539368.

PAZZANI, MICHAEL J. CREATING A MEMORY OF CAUSAL RELATIONSHIPS: an integration of empirical and explanation-based... learning methods. S.l.: PSYCHOLOGY PRESS, 2016.

Quirós, Jorge E. ""La entrevista sobre ""Panama Papers"" que puso contra las cuerdas al presidente Varela."" TVN. October 22, 2016. Accessed November 15, 2017. https://www.tvn-2.com/nacionales/Varela-reaciona-cuestionado-Papeles-Panama-entrevista-Jenny-Perez-Alemania_0_4603789596.html.

Rodman, Kenneth A. ""Public and Private Sanctions against South Africa."" Political Science Quarterly 109, no. 2 (1994): 313. doi:10.2307/2152627.

Rodman, Kenneth A. ""Sanctions at bay? Hegemonic decline, multinational corporations, and U.S. economic sanctions since the pipeline case."" International Organization 49, no. 01 (1995): 105. doi:10.1017/ s0020818300001594.

Rodman, Kenneth Aaron. Sanctions beyond borders: multinational corporations and U.S. economic statecraft. Lanham, MD: Rowman & Littlefield Publishers, 2001.

Rowe, David M. ""Economic sanctions do work: Economic statecraft and the oil embargo of Rhodesia."" Security Studies 9, no. 1-2 (1999): 25487. doi:10.1080/09636419908429401.

Rowe, David M. Manipulating the market: understanding economic sanctions, institutional change, and the political unity of white Rhodesia. Ann Arbor: University of Michigan Press, 2001.

Samasuwo, Nhamo W. ""An assessment of the impact of economic sanctions on Rhodesia's cattle industry."" Historia 47, no. 2 (November 2002): 655-78.

Schmidt, Elizabeth. ""Review: Rhodesian Sanctions: The Long View."" Journal of Southern African Studies 29, no. 1 (March 2003): 311-12.

Stephenson, Glenn V. ""The Impact of International Economic Sanctions on the Internal Viability of Rhodesia."" Geographical Review 65, no. 3 (1975): 377. doi:10.2307/213536.

Sullivan, Leon. ""The Sullivan Principles."" Sullivan Principles. Accessed November 15, 2017. http://www.marshall.edu/revleonsullivan/principles.htm.

""The Anti-Apartheid Movement, Britain and South Africa: Anti-Apartheid Protest vs Real Politik."" African National Congress. Accessed November 15, 2017. http://www.anc.org.za/content/anti-apartheid-movementbritainand-south-africa-anti-apartheid-protest-vs-real-politik.

""US hits Chinese and Russian firms over North Korea."" BBC News. August 23, 2017. Accessed November 15, 2017. http://www.bbc.co.uk/news/world-us-canada-41018573.

Weida, Jason Collins. ""Reaching Multinational Corporations: A new model for drafting effective economic sanctions."" Vermont Law Review.

White, Luise. Unpopular sovereignty: Rhodesian independence and African decolonization. Chicago: The University of Chicago Press, 2015.

Worden, Nigel. The making of modern South Africa: conquest, apartheid, democracy. Hoboken, NJ: Wiley & Sons, 2012

Endnotes"	http://www.inquiriesjournal.com/articles/1724/mnc-decision-making-under-sanctions-south-africa-and-rhodesia	"By using an incentives/disincentives model to map the divergent behaviors of multinational corporations (MNCs) confronted by a sanctioned economy, I explain why some economic sanctions work better than others at achieving their desired political outcomes. When presented with the opportunity to ""run the blockade,"" MNCs are incentivized to sanction bust by the allure of higher profit through rent extraction. At the same time, MNCs are disincentivized to sanction bust by the penalties for breaking the sanction, but only if MNCs believe sanction busting operations is inconspicuous enough to avoid detection. If the incentives to sanction bust outweigh the disincentives not to, then MNCs will trade with sanctioned states, as was the case with Rhodesia. Since MNCs were crucial to both the Rhodesian and the South African economies – as it provided oil to the former and operated a significant minority of the firms in the latter – the decisions of MNCs to remain engaged in Rhodesia and to disengage from South Africa had a significant impact on the economic and political life of the two apartheid regimes. Hence, while many economic and political indicators identified by literature predicted that Rhodesia would have a shorter life expectancy under economic sanctions, Rhodesia defied all expectations and survived twice as long as South Africa.
Introduction
Since the end of the Cold War, economic sanctions have been used to discourage ""any actions of a targeted nation that the targeting nation or group of nations disagree with.""2 Indeed, in just the last 25 years, the U.N. Security Council has authorized 24 economic sanctions; prior to 1990, the U.N. authorized only two.3 Furthermore, ""between 1960 and 1990, most of the sanctions were imposed unilaterally, most frequently by the United States, but in the 1990s, a large fraction was imposed by intergovernmental coalitions.""4
Given the sudden popularity of economic sanctions as a policy instrument, many scholars have studied it. But despite the proliferation of literatures on economic sanctions, few studies have compared the impact of economic sanctions against Rhodesia and South Africa on the political resolution of Apartheid. This gap in literature is surprising, because a comparison of the two Apartheid regimes under sanction allows for a great ""most-similar cases"" research design, in which most exogenous variables that may affect the result are kept constant. Rhodesia and South Africa were both post–colonial Apartheid states with resource extraction economies. Both states received a comprehensive U.N. sanction regime that banned the trade of nearly all goods with nearly all nations. But Rhodesian Apartheid held out for 15 years under sanction, while South African Apartheid barely survived seven. Why is there such a big difference in the effectiveness of sanctions?
Conventional literature points to state-centric factors, like differences in political systems or macroeconomic structures. Such factors fail to answer the question. Political and macroeconomic divergences between the Apartheids states were either nonexistent, or predicted that Rhodesia – not South Africa – would be weakened more by sanctions.
Instead, the key to understanding the mystery must be sought in non–state actors: the MNC. Rather than differences in state-centric factors, the divergent decisions of MNCs to sanction bust in Rhodesia – but not in South Africa – decided the fates of the two Apartheid states. Rational and profit-maximizing actors, the MNCs made this crucial decision by weighing the incentives and disincentives of sanction busting in a process that considers profitability, punishment, and conspicuousness. On one hand, the prospects of capturing large rents from sanctioned economies and hence earning high profits gave MNCs an incentive to run the blockade. On the other hand, the penalties threatened for disobeying the sanction regime dissuades MNCs from sanction busting, but only if the MNCs were caught. Like people, MNCs will respect laws only if they are enforced, and laws are enforceable only if the offenses are discovered. MNCs will sanction bust if and only if the incentives to do so outweigh disincentives not to. The Rhodesia/South Africa story has broader implications: policymakers should consider not just state-centric factors, but also the micro economies that regulate the incentives and disincentives for sanction busting. Individual market actors, not the state, are the vehicle of global trade; sanction regimes that do not secure the cooperation of these actors are bound to fail.
This study will develop the argument in six sections. The following section will summarize the history of Rhodesia and South Africa before and during sanctions for unfamiliar readers. The third section will condense the literature on what determines sanction success, and why they cannot explain the Rhodesian and South African cases. The fourth section will describe the incentive/ disincentive model. The fifth section will apply the model to the Rhodesian and South African cases to explain the divergent sanctions outcomes. The last section will present the conclusions of this study and the implications of the incentive/ disincentive model for the larger sanctions debate.
A Brief History of Rhodesia, South Africa Before and During Sanctions
Rhodesia and South Africa were both British colonies with a long history of racial segregation that endowed political, economic, and social privileges to the white settler‒colonist minorities at the expense of the indigenous African majority. In both countries, whites had suffrage, Africans did not; Whites farmed expropriated land, Africans were expelled to agriculturally unproductive areas; whites enjoyed access to higher education, Africans did not. These racist policies, collectively referred to as Apartheid policies, would provoke international outcry and economic sanctions in the late 20th century.5
Until the mid–20th century, segregated, colonialist societies faced little international opposition: Singapore, Australia, the United States, and many other countries all denied suffrage and other key civil rights to people of non–Caucasian descent without inviting international condemnation. However, with the rise of the civil rights movement and the decolonization movements around the world in the late 50s and early 60s, international norms turned against states that continued to racially segregate their people. Because of this change in international opinion, Rhodesia and South Africa were both subjected to a U.N. authorized sanction regime from 1965-80 and 1986-93, respectively. In both cases, the U.N. sanctions were intended to force the two states to abandon their Apartheid policies and grant suffrage to their disenfranchised African majorities. The sanctions aimed to change the policy, not the regimes; that said, many who were targeted by the sanctions felt that Apartheid was such a crucial aspect of their regime, that granting suffrage to Africans would undermine it.
The outcome of the U.N. sanctions could not have been any more different. Rhodesian Apartheid, against all odds, survived for 15 years under economic sanctions. Rhodesia accepted power‒sharing with African party members only when the African guerilla fighters made their military situation untenable. If Rhodesian military situations were more favorable, they may have survived the sanction regime for a lot longer, perhaps even indefinitely. In contrast, South African Apartheid barely lasted eight years, after which it extended suffrage to Africans and released Nelson Mandela from prison. Because was no military threat to the South African government, the South African decision to end racist policies was primarily an outcome of economic pressure.
Explaining Sanction Success
One of the most common determinants of the sanction's effectiveness is the nature of the sanction–targeted state.6 This line of thinking makes sense; the same sanction package may cripple a small nation with little domestic manufacturing capacity to meet internal demand, but barely affect a large nation with the domestic industries to achieve autarky. Likewise, a sanction may be lethal to an export-oriented economy, but a mere annoyance to an economy sustained by domestic consumption.
Another equally intuitive exercise will examine the breadth and depth of the sanction package.7 The breadth of a sanction package is the proportion of the global market that would participate in the sanction. Obviously, if the United States restricts access to its market, then the sanction would be much more effective than if the United States did not. The depth of a sanction package is the number of commodities sanctioned. In 1983, Australia refused to sell uranium to France; while the sanction did some harm to the French nuclear program, it had little impact on the health of the macroeconomy.8 If Australia had restricted French access to its entire market, the French economy would have been more acutely hurt.
While these two intuitive variables of comparing the national context or the sanction package to determine divergent sanction outcomes offer easy ways out, they are unhelpful in explaining the outcomes in Rhodesia and South Africa: both the national context and the sanction package remained constant between the two countries, which means neither could explain the differences in outcomes.
Almost all the key national characteristics of sanctioned Rhodesia and South Africa were astonishingly similar. Both states had a large African majority dominated by a white minority, an Apartheid political system, an isolated diplomatic status, tenuous trade relations, a Southern African geography, an Afrikaan-British heritage, a British colonial history, and an agricultural and mining-based export economy. If any one of these factors were different, it could at least partially explain the sanctions outcome. But with these key national characteristics held largely constant, none of them can explain the drastically different sanctions outcomes.
Both Rhodesia and South Africa were placed under comprehensive Hurbauer, Economic Sanctions Reconsidered, 91. economic sanctions with nearly universal participation from all major and most medium powers. For both countries, the sanctions seriously inhibited trade, depreciated their currencies, limited access to international finance, cut off access to foreign aid, and froze foreign assets held by the target government and citizens.9, 10 And yet, the sanctions in Rhodesia and South Africa still produced significantly different political results.
If similar sanction packages were imposed on similar countries, why did one collapse much more quickly than the other? When neither national context nor sanction package can explain sanction outcomes, the literature turns to micro variables for answers. Consider the following metaphor: if two people with similar phenotypes drank the same amount of milk, why might only one of them suffer indigestion? The doctors would answer that despite the similarities in the general circumstances, there was a miniscule, unclear, but nevertheless very crucial difference between the two individuals, perhaps a slight genetic disposition that makes one lactose intolerant. In a similar fashion, if proponents of sanctions attempted to resolve the Rhodesia-South Africa paradox, they would have answered that despite the similarities between the two sanctions, there must have been micro but very significant differences that contributed to the divergent outcomes. Unfortunately, these micro variables are inconclusive in respect to explaining the South Africa/ Rhodesia case.
So what could have been a micro difference? One of the most widely known micro variables used to explain divergent sanction outcomes is the vulnerability of a state to economic sanctions relies on the target state's economic concentration: ""the more a country's economy depends on one product, and the more its exports consist of one product… the more vulnerable is the country.""11 It is also reliant on commodity concentration, or the degree of consumer tastes that are concentrated on one commodity, and geographic concentration, the degree to which the target's trade partners are concentrated on one country. Here again, the implications of this micro foundation are counter-intuitive. According to a composite vulnerability index that considers these three types of concentration, sanctioned Rhodesia was the fourth most vulnerable state in the world, after Mauritius, Trinidad, and Panama (Galtung, year)12. There is no comparable measure of South Africa's economic vulnerability in 1985, but a comparison of the sum of net export and import, a rough indicator of a given economy's reliance on trade, reveals that Rhodesia was significantly more concentrated than South Africa. The import-export sum figure for South Africa at the time of the sanction was 22% of GDP,13 while the same figure for Rhodesia was a staggering 66% of GDP.14 These figures indicate that Rhodesia was an exceptionally concentrated, vulnerable state compared to South Africa, and predict the earlier demise of the former.
Brooks claims the regime type of the target influences the effectiveness of a sanction -democracies are more vulnerable to comprehensive sanctions, which target the macro-economic health of the state, while autocracies are weak to targeted sanctions that attack the economic health of key stakeholders in the government.15 However, regime type is an inconclusive explanation in this case.
With several possible explanations failing to resolve the Rhodesia‒South Africa paradox, many scholars argued whether the sanctions in Rhodesia and South Africa were effective at all in years during and immediately following the sanction.16 Citing the persistent economic rigor of the sanctioned Apartheid states, ""sanction skeptics"" have concluded that sanctions were inconsequential. Their most damning evidence came from Rhodesia. Despite the severe embargo, in 1973, eight years into the sanction, Rhodesia's exports grew, GDP rose by seven percent,17 and domestic manufacturing doubled compared to presanction levels.18 The nation also underwent significant political integration as Rhodesians ""rallied to the flag.""19 In South Africa, despite significant movement to disinvest, MNCs maintained an indirect presence in the nation.20 Therefore, capital flight and disinvestment from sanctioned South Africa may not have been as economically devastating as the raw economic figures suggest.
Despite these arguments, the academic consensus today is that sanctions on South Africa and Rhodesia did play a role in forcing political change.21 Many of the claims of Rhodesian strengths, made before 1975, did not consider the numerous hidden weaknesses directly caused by the economic sanctions, such as the dwindling state Foreign Exchange reserves, which broke into daylight only in the final days of Rhodesia.22 Galtung's articles, written in 1974, admit: ""we do not know how (the sanction) will all end, and primary source material of a crucial nature is not yet available.""23 But while today's scholars agree sanctions did play a role in the demise of Rhodesia, they still have yet to consider the case of Rhodesia in a comparative context in relation to South Africa.
Limitations in the Conventional Literature
The conventional literature fails to explain the Rhodesia-South Africa paradox because it suffers from a consistent over-emphasis of the role of state actors on sanction outcome. Baldwin and many other foundational scholars of the sanctions literature frame economic sanctions tools of ""economic statecraft,"" without mentioning the impact of behaviors by non-state actors on sanctions effectiveness.24 Furthermore, sanction scholars tend to predict the effectiveness of sanctions by relying on macroeconomic or political microfoundations, such as export/import reliance and economic concentration, or regime type and political decision makers, which are all influenced by or a direct outcome of state behaviors.
The reliance of sanctions literature on the behavior of state actors as an explanation follows a general partiality of political science literature to state actors. However, this bias has resulted in an unjustified presumption in recent sanctions literature that the effectiveness of economic sanctions can easily be maximized, and its collateral damage minimized through ""smarter"" sanctions policy with improved political and macroeconomic parameter setting by policy makers. Adherents of ""smarter policy, smarter sanctions"" risk reaching a reductive conclusion that policy makers who are aware of the political/macroeconomic factors affecting sanction effectiveness will produce more effective sanction policies.25 This claim is not empirically supported according to Drezner: despite the frequent implementation of ""smart sanctions,"" sanctions have not become significantly more effective.
""Smarter"" sanctions, however, have not translated into better outcomes. This is because smarter sanctions ultimately suffer from a deficit in the entire economic sanctions literature in general. All previous scholarship on sanctions are incorrect in assuming that the success or failure of a sanctions policy is largely determined just by the decisions of policy makers and their macroeconomic and political parameter setting. They fail to account for the fact that sanction effectiveness is also determined by the decisions of the MNCs and their microeconomic parameter setting. If policymakers want to draft a truly ""smart"" sanction, they must not only consider the influence of state behavior on macroeconomic and political parameters, but also the impact of behaviors of non-state actors, namely the MNCs, on sanctions policy.
The Role of MNCs and Their Microeconomic Parameter Setting on Sanctions Outcome
I will construct a model to explain MNC behaviors. Of course, actual MNC decision making is much more involved, but the model has satisfactory explanatory power. In the next section, the model will be used to explain the outcomes of the sanctions placed on Rhodesia and South Africa.
Future Profitability, Conspicuity, Max Potential Punishment
MNCs are international corporate entities with a single incentive: net profit maximization.26 Assuming MNCs will always choose options that will produce the largest net profit, if MNCs achieve positive net profit from sanctionbusting, they will sanction bust; if not, they will disengage from the sanctioned economy. ""Profit Maximization,"" however, is an aggregate product derived from many incentives and disincentives. Hence, to determine whether a profit-maximizing MNC will sanction-bust or not, the incentives and disincentives that shape the corporate environment must be studied.
The primary incentives and disincentives at work when a profit-maximizing MNC decides to engage with or disengage from a sanctioned economy are future profitability, conspicuity, and max potential punishment. Future Profitability is a positive incentive. It correlates with the perceived gross economic gain a MNC expects to obtain from engaging in sanction-busting trade. The larger the expected profit from sanctions-busting, the stronger the incentive will be for MNCs to engage with sanctioned economies. Conversely, the smaller the expected profit, the weaker the incentive to sanction-bust. Conspicuity is a negative disincentive. It correlates with the expected visibility of the MNC's sanctions-busting operation from the perspective of the sanctioning government and the public at large. High conspicuity significantly discourages MNCs from engaging in sanctionsbusting trade because it is more likely to invite punishment from the sanctioning government and the public. Max Potential Punishment (or Max Punishment) is a negative disincentive. It correlates with the expected severity of the gravest punitive action conceivable for engaging in sanction busting trade inflicted onto the MNC by the government and the public. Obviously, the more severe the max potential punishment is, the stronger the disincentive is for MNCs to engage in sanction-busting. In constructing the model, we assume that these three incentive/ disincentives are determined by several key input variables, listed below.
1) Future Profitability.
The expected future profitability of a given sanction-busting trade is an output value that is derived from three input variables. One of these inputs is the general economic desirability of the sanctioned economy. This variable represents the key economic facts, (such as market size or production capacity) that MNCs consider when they decide whether to engage with a market, whether sanctioned or not. Unless the sanctioned economy has some economic value that makes it valuable to the MNCs, they would not trade with them even without sanctions. The other input variables are relevant only to sanctioned economies. One of these is bargaining power. MNCs must gauge how much additional bargaining power they have in negotiating with sanctioned economies. These economies are not in a position to take the global market price due to their limited access to the world market, and hence will have to take higher prices for imports and lower prices for exports. This is to the advantage of MNCs. The greater the bargaining power, the greater the future profitability. Significant bargaining power permits MNCs to act as a monopolistic price-setter that can undersell the sanctioned economy. The strength of an MNC's bargaining power is determined by the answer to three questions. (1) Are other MNCs participating in sanction-busting operations? If so, the other players will force the MNC to buy and sell at prices that are more competitive, reducing their bargaining power. (2) To what extent has the sanctioned economy achieved autarky? If the sanctioned economy has achieved a significant degree of autarky and can domestically produce the product supplied by the MNC, then the MNC will have to set prices competitive with domestic products. Whether autarky is possible or not would at least partly depend on the mobility and the alternative availability of the particular product provided by the MNC. If the product is highly mobile (eg. a consultant), then the product can be quickly pulled out of a sanctioned economy before there is enough time to establish autarky and produce sanctioned goods domestically. If the product is available from an alternative source, either from other MNCs or a domestic firm, then autarky is much easier. Lastly, there is (3), the expected duration of the sanction. This is a significant determinant of future profitability. If the MNC perceives the sanction to be a short-term affair, losses from hiccup in trade is far outweighed by the benefits of a sustained, long-term trade relationship.
2) Conspicuity.
Information that is visible to governments is not necessarily visible to the public, and vice versa. For example, state intelligence may be aware of MNC sanction-busting, but intentionally not inform the public. Conversely, certain private actors with privileged access to insider information of MNC activities, like investigative journalists, may have information that the government does not. Therefore, a more accurate model than the one used in this study would take into account this information asymmetry. This would be done by disaggregating conspicuity into the conspicuity of the sanction-busting trade from the sanctioning government, and the conspicuity of the same trade from the public. Unfortunately, disaggregating conspicuity into these two incentives is beyond the scope of this study. Lacking access to key government archives, it is impossible to know whether information not known by the public at the time of the sanction was withheld by the government as classified knowledge, or was just as unknown to the sanctioner government as to the public.
This methodological obstacle is circumvented by assuming that what information was known by the public was also known by the government, and vice versa. Therefore, in this model, the degree of awareness by either the public or the sanctioner government of the MNC sanction-busting operation is the single input used to derive the conspicuity disincentive. This assumption is not problematic for Rhodesia and South Africa. The most economically significant sanctioner governments of Rhodesia and South Africa, the United States and those of Western Europe, were liberal democracies, where information that was visible to the government was also visible to the public. Even if either the government or the public did come to acquire information that one of them was not aware of, the information barrier was low enough that information about the key sanctionbusting trades in the two countries (particularly oil trade in Rhodesia), and the FDI engagement of MNCs in South Africa) quickly diffused from one actor to another.
3) Max Potential Punishment.
The value we use for total Max Potential Punishment is derived from both the Max Potential Punishment sustainable from the sanctioner government, and the Max Potential Punishment sustainable from the public. If the sanction-busting activity is illegal, the punishment from the sanctioner government may come in the form of a fine, imprisonment, or other legal recourse.. If the sanction-busting activity is technically legal, it could come in the form of closing of loopholes, severance of government contract to MNCs, and other extra-judicial punitive measures. The most common and effective form of punitive action by the public is the boycott, where consumers refuse to buy the products of the offending MNC or otherwise engage in transaction with them. While it may seem wise to disaggregate public punishment and government punishment, this is not necessary.
MNCs anticipate their actual punishment for engaging in sanction-busting trade to be much smaller than, and rarely equal to, the Max Punishment threatened. Such optimism is not exclusive to MNC. It is not uncommon for criminal entities to perceive the expected punishment as much lower than what is threatened. For example, students perceive the actual punishment of smoking marijuana to be much less serious than the Max Punishment because they are confident they won't be caught. This expectation as to what extent the Max Punishment will be realized is entirely dependent on conspicuity. Similarly, the students in the previous example will expect more serious consequences if, instead of smoking at home, they smoke in front of the police. Likewise, MNCs perceive that higher degrees of conspicuity makes it more likely for the government and the public to impose the Maximum Punishment, and less conspicuity makes such punishment less likely.
The sanctioner government is more likely to retaliate against a conspicuous sanctions-breaking operation for two reasons. First, if the MNC's sanctions-busting operation is illegal, government agencies are more likely to uncover and punish conspicuous sanctions-busting trade. Second, even if the MNC's operation is still technically legal due to loopholes, the governments are still more likely to interpret conspicuous sanctions-busting activities as an affront to government authority.The governments will have strong incentives to either amend the sanction legislation, close the loophole, or even resort to extrajudicial measures to punish the ""legal"" trade. While a government might look the other way if an illegal transaction is done under the table, it may be forced to take retaliatory measures if the same trade is done in broad daylight, in open defiance to the authority of the state.27
The public is more likely to retaliate against conspicuous sanctionbusting operations because the principal method of retaliation used by the public against MNC, boycotting, requires widespread mobilization.28 Boycotting is a worthwhile weapon only if enough people participate. However, people would consider a boycott only if they are aware of the MNC's sanction-busting operations. The public is more likely to be aware of MNC operations that are visible; thus, a conspicuous sanction-busting trade is necessary to activate civil society in an effective retaliatory boycott.
The Incentive/Disincentives Model: MNC Decision Making for Sanctioned Economies
Now that all the inputs behind each of the incentive and disincentives have been described, it's time to present the way in which the incentive/disincentives of future profitability, conspicuity, and max punishment produce the final MNC decision. The corporate calculus is:
P − C*(MP) = D,
where: P = Future Profitability, C = [0,1] = Conspicuity, MP = Max Punishment, D = MNC Decision,
And: IF D > 0, THEN sanction-bust,
And: IF D < 0, THEN don't sanction-bust.
The economic benefit of sanction busting is abstracted as P. Hence, a larger P correlates with a larger D, as higher the future profitability of a trade, the greater the incentive for MNCs to sanction bust. Future profitability only accounts for the gross future profitability of sanction-busting. However, sanction busting comes with its own expected cost that will depress net profits. This expected political and social cost of sanction busting is modeled by C*(MP). A larger MP correlates with a smaller D, as the more severe the maximum potential punishment is for sanctionbusting, the more MNCs are discouraged from engaging in such operations. However, the severity of the punishment imparts any meaningful weight on the MNC decision making only if sanction-busting trade is conspicuous enough for the government and the public to inflict all, or part of, the maximum possible punishment onto the MNC. In other words, no matter how severe the potential punishment is for sanction-busting, MNCs will only be discouraged if sanctionbusting is flagrant enough to invite punishment. Thus, the extent to which the max potential punishment disincentivizes sanction-busting depends on conspicuity, a variable which ranges from 0 to 1 in this model. If Conspicuity is 0, the sanctionbusting transaction is invisible and hence the MNC decision making is unaffected by the empty threat of punishment. If Conspicuity is 1, sanction-busting is visible and the MNC expects the full weight of the threatened punishment.
Figure 1: The MNC incentive/disincentive model
Maximum Possible Punishment from the public, and Maximum Possible Punishment from the sanctioner government were integrated into one incentive function. This abstraction was made possible by our assumption that the visibility and conspicuity from the perspective of the government and the public is the same:
IF: P − C public *(MP public ) − C government *(MP government ) = D,
AND C = C public = C government , MP = MP public + MP government ,
THEN: P − C*(MP public ) − C*(MP government ) = P − C*(MP public + MP government ) = P − C*(MP) = D.
Now that the incentive/disincentive model for MNCs has been established, let us apply it to South Africa and Rhodesia. In each case study, I will first establish that the decisions of MNCs to engage or disengage, Rhodesia for the former and South Africa for the latter, weakened the state's economic health and its capacity to reject the sanctioner demand to end apartheid. This shows that MNC decision did affect the outcome of the sanction. Second, I will trace the decision-making process of key MNCs where I find the measure of the input variables, derive the incentive/disincentive from the inputs and add the incentive/ disincentive to derive the final decision of the MNCs and its impact on the sanctioned economy.
MNC Decision Making in South Africa and Rhodesia
South Africa
The economic and political impact of MNC behavior on the end of Apartheid
Faced with the prospects of an imminent sanction, many key MNCs chose to disengage from the South African economy rather than participate in sanction-busting trade. Their decisions destroyed the South African economy. The most visible damage to the South African economy came from capital flight. Foreign capital fled Praetoria for safer investment options,29 leading to higher South African interest rates. In 1985, the sudden capital flight created a liquidity crisis,30 forcing the government to call a unilateral memorandum on all public debts. The memorandum made banks grow even more wary of investing in Praetoria, causing a confidence crisis for all South African debts, public and private. As a result, the prime interest rate in South Africa shot up to record rates, from nine percent in 1982 to 25% in 1985.31 The rates did normalize at a lower figure as the initial shock wore off, but investors remained cautious. The prime interest rate floated between 15% and 25% throughout the sanction. Less visible in the short term but just as corrosive in the long term was the withdrawal of FDI by MNC from the sanctioned state. Two hundred MNCs with FDIs in South Africa chose to disinvest32, selling off their physical assets to South Africans. Because of disinvestment, about 5 billion Rand of FDI was lost.33
Disinvestment harmed South African industries in two ways. First, it cut off local production facilities, previously owned by MNC subsidiaries, from the MNC's global supply and distribution network, leading to drastic reduction in corporate profits, wage, and unemployment. Second, because of disinvestment, the South African economy lost the practical and technical knowhow of the MNCs that was now integral to maintain South African competitiveness. The MNC decision to disinvest reduced South African access to global supply/distribution chains and MNC knowhow, which in turn paralyzed productivity growth from 1985 to 1993.34 This stagnation was caused by the sanction, as the economy recovered its high productivity growth after the sanction was lifted.
The combined impact of capital flight and disinvestment intensified the economic crisis. When capital fled, international interest rates rose, and foreign exchange became scarce, the South African economy required a competitive export market to acquire foreign currency if it was to have any chance of staying afloat. However, disinvestment reduced South African productivity, hamstringing international competitiveness and reducing its ability to earn foreign currency. The negative spiral was reflected in the meager GDP growth rate during the sanction. During the duration of the sanction from 1985 to 1993, GDP stagnated, growing only by 20 billion. In comparison, the South African economy grew 60 billion from 1975 to 1985, and 130 billion from 1995 to 2005.35
The economic downturn caused by MNC decisions created political pressure to end the sanction as median voters, most of whom suffered financially, steered the political tone of the government to one of reconciliation.36 The most apparent impact felt by the average South African came in the form of more expensive imports. Import prices inflated due to the depreciation of the Rand caused by capital flight. As a result, the South Africans were forced abstain from luxury, sometimes necessities, that were imported. The South African current account surplus grew in this period despite weaker exports, because reduced imports more than offset declines in import (South African Reserve Bank)37. As Rodman notes, the South African economy was not in a state of total collapse. However, for a people used to easy credits and no shortage of goods, the MNC's disengagement undermined the economic security that until then they had taken for granted. This created an imperative for policy change. The disinvestment of many famous MNCs also created a psychological imperative amongst decision makers to change direction. The simultaneous rejection of South African apartheid by private actors through disinvestment and by public actors through withholding of diplomatic recognition reinforced the perceived isolation of South Africa and undermined the legitimacy of Apartheid.38
One caveat to this narrative is that, since white-only suffrage excluded black and colored Africans from voting, the ""median voter"" of Apartheid South Africa was by no means synonymous with the ""median citizen."" In fact, the ""median voter,"" who was most likely white and affluent, and the ""median citizen,"" who was most likely black and poor, had different experiences in sanctioned South Africa. As in other historical examples of sanctioning, the population that the sanction was supposed to help suffered the brunt of the economic devastation; ""the median citizen""– the many Africans under Apartheid – experienced even more drastic economic collapse under the sanction regime than did white voters.39 For instance, in companies formerly operated by MNCs before disinvestment, internal corporate regulations that barred discriminatory practices were repealed once each MNC administration was replaced by domestic management. The change in administration exposed the African employees of these companies to even greater discrimination and economic damage.40 Given the severe, short-term economic devastation experienced by Africans, which dwarfed the inconveniences suffered by white voters, if the Africans did have electoral power, there would probably have been an even greater political pressure to end apartheid. If the ""median voter"" did include Africans, Apartheid may have collapsed even sooner under sanctions; but again, there would be no Apartheid if Africans had the vote.
Incentive/Disincentive Causal Mechanisms behind MNC behavior in South Africa
MNCs decided to disengage from South Africa because continued engagement with South Africa would have entailed low future profitability, high conspicuity, and high Max Punishment. There was a weak positive economic incentive and a strong negative political and social disincentive to sanction-bust, leading to a decision to disengage. The disincentive from political and social sanctions exceeded incentives from the expected profit MNCs could gain by sustained engagement. As these incentives/disincentives are derived from several input factors, I will examine each of these inputs to determine how MNCs establish their incentives/disincentives.
1) Low Future Profitability.
MNCs expected low future profitability from sanction-busting in South Africa due to expectations of low economic desirability of trading with South Africa, weak bargaining power in sanction-busting negotiation, and a long period of sanction. First, the South African economy was not a very desirable market for MNCs. For MNCs with FDI assets in South Africa, the revenue collected from operations in South Africa often amounted only to about two percent of total revenue, and thus was not essential for overall company future profitability.41 For international banks, historically South Africa's primary value had been a stable African investment option with higher interest rates than in the developed world. However, after MNCs with FDI assets retreated, destabilizing the economy in the process, the primary merit of South African investment – its stability – eroded, making South Africa a less desirable investment option. Second, the MNCs expected weak bargaining power from negotiation, so they were skeptical they could maintain competitive prices or profits because the MNCs would have to compete with competitive prices from budding but growing domestic manufacturers. In addition, numerous MNCs were engaged with pre-sanction South Africa; hence, in the ""fog of war"" where MNCs (like all market actors) are unaware of the decisions of their competitors, an MNC could not rule out the possibility that some of their competitors may also decide to sanction-bust, reducing their own bargaining power in undercutting South African prices. Finally, with the historical knowledge that the only other example of a comprehensive, U.N.-led sanction against another Apartheid African state, Rhodesia, was a protracted affair lasting 15 years despite early expectation that it would be over within the year, it would be expected that most MNCs probably anticipated the sanction against South Africa to be a long-term affair. Expectations for a longterm sanction reduced the need to maintain trade contact with South Africa in case commercial relations were normalized again soon. These input factors totaled to a weak, positive future profitability incentive to sanction-bust.
2) High Conspicuity.
MNCs perceived that any sanction-busting activity would be conspicuous and subject to a serious maximum potential punishment. This expectation was more a result of heightened public awareness than of government authority. Even before the sanctions were formally instated, several NGOs had dedicated themselves to exposing MNCs with engagements in South Africa. In the early 1960s, a decade before the mandatory U.N. sanctions were authorized, the Anti-Apartheid Movement (AAM) organized civilian boycotts of South African goods by naming and shaming companies engaged with South Africa.42The AAM fizzled out in the late 1960s, but other NGOs continued the fight. Several campus student organizations petitioned their universities to disinvest from MNCs engaged in trade with South Africa, often naming specific corporations that the universities were invested in.43 Because of this activism by an activated civil society, MNCs expected high visibility of sanction-busting operations in South Africa. This is not to say that the local black South Africans exercised little agency in combatting the Apartheid regime ruling over them. If anything, the civilian boycott in the West gained critical mass only after the world witnessed the intense resistance of black South Africans to the institution of the South African Constitution of 1983, which reaffirmed racial segregation.44 That said, external actors (inspired to action by the resistance of black South African internal actors) played the predominant role in shaping MNC behaviors.
3) High Maximum Potential Punishment.
NGOs also had a deep well of support they could readily mobilize for boycotts and public campaigns, raising the maximum punishment that the public could inflict upon MNCs. In 1971, the U.S.-based Interfaith Center on Corporate Responsibility (ICCR) pressured corporations operating in South Africa to adopt the Sullivan principles – a seven-point program that demanded non-segregated workplaces and ""equal and fair opportunities for all employees"" regardless of race.45 Since it was illegal under South African law to abide by the Sullivan principles, the ICCR pressured companies to leave South Africa. Those that did not leave South Africa faced massive consumer and investor boycotts organized by the ICCR. Furthermore, the widespread student protests meant that 155 university institutions had disinvested from MNCs trading with South Africans by 1988. As such, NGOs saw that they would receive a severe punishment in the form of disinvestment by universities, who are key institutional investors, if they continued to sanctionbust.46 The NGOs also significantly raised the maximum punishment that the sanctioner government could impose.by pressuring municipal, local, and state governments to rescind lucrative government contracts with MNCs engaged with South Africa. Combined, these conditions created the threat of high maximum potential punishment by both the public and the government. Furthermore, because of the visibility of sanction-busting operations, the MNCs expected the full extent of the maximum punishment if they did engage with South Africa. As a result, a powerful negative disincentive, fueled by high conspicuity and high maximum punishment, overrode the weak positive incentive and led to the MNC decision to disengage.
Rhodesia
The economic and political impact of MNCs behavior on Rhodesian apartheid
Although MNCs were pervasive in the Rhodesian economy, they still had an indispensable role in the Rhodesian economy that was equally important, if asymmetrical, to their functions in the South African economy because they provided all the Rhodesian energy requirement. Lacking any domestic energy sources, namely oil, Rhodesia was 100% dependent on oil imports, all of which were provided by oil giant MNCs.47 Rhodesia's dependency on oil MNCs was exacerbated by its landlocked geography, which forced the country to rely on pipelines to transport oil from the South East African coast, all of which were owned by the MNCs.48
The mediumto long-term decision of oil giant MNCs to stay engaged in Rhodesia was crucial to the country's economic health. Numerous commentators of the time noted that Rhodesia's economy would have collapsed very quickly without its oil imports.49 The continued oil imports permitted the rapid industrial expansion that was crucial for Rhodesia to skirt the worst effects of the sanction. Such an industrial expansion helped in two ways. First, an intensive import substitution industrialization program supplied Rhodesia with goods no longer imported under the sanction.50 Second, the domestic manufacturing expansion compensated for the economic atrophy of the former cash cows of Rhodesia under the sanction, such as the tobacco industry.51 The macroeconomic health of the country was maintained by shifting the source of growth away from such atrophied industries hit by the sanction to domestic manufacturing that supplied internal consumption. This industrial expansion was possible only through the continued supply of oil by MNCs. So, without oil imports, there would have been no industrial expansion, and the Rhodesian economy would have collapsed quickly due to shortage of imported goods and the collapse of source of income industries without another growth source to replace it.52 In addition to the industrial expansion, the ready availability of oil from MNCs softened the upward price pressure of oil per gallon. Cheaper oil was essential to Rhodesia, which was suffering from a foreign currency shortage, as expensive oil would have quickly depleted Rhodesian foreign currency reserves. 53
Rhodesia's ability to resist demands to abandon Apartheid was directly tied to the economic ramifications of the MNCs' continued engagement. The expansion of domestic manufacturing permitted by the oil import provided workers in sectors weakened by the sanction with alternative employment.54 Furthermore, the growing domestic manufacturing industry – and the economic health of the economy it helped maintain – also bolstered civilian morale. Secondly, Rhodesia could spend its foreign exchange on other vital economic concerns, as it did not need to pay a premium for smuggled oil. For example, without surplus cheap credit, Rhodesian agricultural subsidies for sanctioned tobacco goods would have quickly become unsustainable. The same foreign currency was also crucial to acquire weaponries needed to fight the guerilla war against Black nationalists.55 Finally, the abundance of oil meant the life of the median Rhodesian voter was largely unaffected. Rhodesians could still drive cars, as long as they carpooled every so often. If anything, what little oil rationing there was inspired a ""rally-to-the-flag"" effect on national unity that far outweighed the negative impact of the minor inconvenience.56 However, if oil shortages and the drop in white living standards were far more severe, Rhodesians could be expected to have behaved differently, perhaps demanding that the government cave to sanction demands.
Incentive/Disincentive Causal Mechanisms behind MNCs behavior
Although the MNC decision in the mediumto long-term was to sanction-bust in Rhodesia, this was not initially the case. At the very beginning of the sanction, incentives/disincentives motivated oil giant MNCs to disengage from the sanctioned economy. The oil giant MNCs initially cooperated with the British government, the primary sanctioner government, and agreed to stop oil shipments to Rhodesia. The MNCs chose not to sanction-bust as they expected only moderate future profitability in comparison to high conspicuity and high maximum punishment; the disincentives outweighed the incentives. However, overtime, these incentives/ disincentives changed, until they settled in a pattern of low future profitability, low conspicuity, and low Maximum Punishment. The new constellation of incentives/disincentives then encouraged MNCs to change their decisions from disengagement to engagement as although the low future profitability meant there was only a weak incentive, the low conspicuity and maximum punishment meant the negative disincentive to sanction-bust was even more marginal. Tracing MNC behaviors in Rhodesia in terms of the incentive/ disincentive model highlights that when incentives/disincentives change over time, MNC decisions will shift with the incentives.
1a) Initially, moderate future profitability.
At the beginning of the sanction, the constellation of key input variables meant oil giant MNCs expected moderate future profitability from the sanction. Given how small the Rhodesian demand for oil was, the Rhodesian market had limited economic desirability. The MNCs also expected medium bargaining power. The Rhodesians had no domestic oil industries to compete with which the MNCs, giving MNCs monopolistic control over price controls. Furthermore, since there were only five oil giant MNCs operating in Rhodesia, the withdrawal of any one competitor would significantly lower competitive pressure. In light of the optimistic statements about the quick capitulation of the Apartheid regime by the government and the public, the MNCs did not expect sanctions to last long. In total, the input variables of low general economic desirability, medium bargaining power, and short expected duration of sanction, resulted in the MNC expectation of moderate future profitability from sanction-busting.
1b) Over time, weak future profitability.
However, as the sanction progressed, the future profitability incentive begun to change. The sanction, which everyone claimed would be over very quickly, dragged on for years. Hence, the MNCs now expected a sanction with a longer duration. With lower expectation that trade relations would be normalized anytime soon, the MNCs felt less compelled to maintain trade contact. This change in this input meant its output, the future profitability incentive, changed: now, there was a weak future profitability incentive to continue trade.
2a) Initially, high conspicuity.
At the beginning of the sanction, the MNCs expected high conspicuity from engaging in sanction-busting because they expected visibility from the sanctioner government of United Kingdom. The British government declared that the British navy would enforce the sanction by identifying any ships carrying oil to ports that were conventionally used to transport oil to Rhodesia.57 Once the ships were identified, they could easily be tracked to their patron oil giant MNC; hence, the high conspicuity of MNC sanction busting.
3a) Initially, high maximum potential punishment.
When Joanne V, an oil tanker, defied the British blockade and sailed into Beira, a Mozambique port used to transport oil to Rhodesia, the British government – sanctioned by a U.N. resolution to use force if necessary to stop the oil trade – declared that any ships that unloaded oil destined to Rhodesia would be seized.58 The threat was taken seriously by MNCs: Joanne V left the Mozambique port without unloading its oil.59 Attempts by MNCs to break the blockade by using an oil pipeline of significant symbolic value as the connection between Rhodesia and South Africa were also shut down by the British government. Most MNCs saw the highly conspicuous, punishable nature of trading with Rhodesia to be a disincentive that far outweighed the paltry future profitability incentive and disengaged from Rhodesia.
2b) Over time, low conspicuity.
However, as time went on, the MNCs increasingly perceived low conspicuity since South African state practices changed the incentive structure. MNCs could deliver oil anonymously to Rhodesia as South Africa offered to purchase oil from MNCs and then sell it to Rhodesia. This permitted MNCs to claim plausible deniability – how could MNCs ""know,"" and so take responsibility for, what the South Africans were going to do with the oil once it was sold? In addition, the public attention to the sanction-busting operations was diverted from the sanction-busting MNCs to the South African regime that permitted it; thus, the increased South African notoriety caused reduced MNC conspicuity.
3b) Over time, low maximum potential punishment.
As for maximum punishment, South African state practice again changed the incentive structure. By threatening to impose a mutually painful countersanction onto the British economy, the South African regime prohibited the British from blockading a few Mozambique ports that were used to supply oil to both Rhodesia and South Africa. The official reason given for this prohibition by the South African government was that a blockade of the Mozambique ports would threaten South African access to oil. As a result, cargo ships transporting oil to these ports, whether with South African or Rhodesian contracts, were immune to British reprisals. Under the pretext of ""protecting national interest in protecting South African access to oil,"" the South Africans protected the Rhodesian oil trade. The MNCs saw that the British were spooked by the South African protests and, unwilling to risk a potential South African countersanction, would not enforce a comprehensive blockade of all ports.60 Because of South African state practice, the incentive structure changed, and given low future profitability, low conspicuity, and marginal maximum punishment, MNCs changed their commercial orientations and sanction-busted on behalf of Rhodesia for the rest of the sanction duration.
Figure 2: Summary of the MNC behaviors in sanctioned South Africa and Rhodesia
""Low"" implies negligible amounts that only marginally tips the balance of the equation. ""Moderate"" implies a considerable amount that can affect MNC decision-making unless it is overdetermined by a ""high"" value, which has the potential of single handedly determining the outcome.
Conclusion
The behaviors of MNC operating under sanctioned regimes offer insights into what make sanctions effective. Truly effective sanctions must consider the corporate calculus of MNCs, and shape the incentive/ disincentives driving corporate behavior to one of compliance with the law. The most important implication of the incentive/disincentive model is that makers of sanction policy should not only focus on threatening the MNCs, but also increasing their conspicuity in sanction-busting operations. Policymakers should prioritize raising the conspicuity of sanction-busting activities just as much as designing punishments for misbehavior; after all, the most severe punishments are paper tigers unless they are enforceable, and punishments are enforceable if and only if they can be detected in the first place. Furthermore, policymakers must be aware that, as observed in Rhodesia, third-party states can easily manipulate the MNC incentive structure to promote sanction-busting. South African intervention in the sanction structure for Rhodesia ultimately shaped MNC behavior in a way that limits sanction effectiveness. Policymakers must plan for another ""South Africa"" contingency by preparing to counter such attempts by third party states to undermine sanction effectiveness.
There are a few caveats to this model. First, the increased public awareness of MNC sanction-busting was assumed to encourage the public to enforce punishments. This may not necessarily the case if the public is indifferent to, or perhaps even hostile to, the government's sanction policy. If the public does not perceive the MNC sanction-busting as deplorable, they will not be motivated to punish the MNC for it. Second, the conspicuity of MNC behavior from the standpoints of the government and the public was assumed to be the same. This was not too much of an issue in these two cases, where the sanctioner regimes were liberal democracies where information was, in fact, shared between the public and government. This may not always be the case, especially if the sanctioner regime is not liberal democratic. In the case of the OPEC oil embargo, for example, autocratic sanctioner regimes may not readily share what they know with their citizens.
There is yet to be a method of determining – beyond qualitative terms – the amount of clout MNCs have in a sanctioned economy. In South Africa, the sheer number of MNCs implied heavy MNC clout, but clout does not necessarily require numbers. In Rhodesia, while fewer MNCs were operating, the MNC oil monopoly in the Rhodesian market made them punch far above their weight in terms of importance to the national economy. The more MNCs control crucial resources, the more impact they have on the national economy.
In this model, a linear relationship was assumed between conspicuity and the actual punishment sustained by the corporations. This may not necessarily be the case. The government and the public may tolerate MNC misdemeanors to a point and only retaliate if the MNCs overstep a critical line. Future research should clarify whether the relationship between conspicuity and actual punishment is actually linear or, if fact, incremental, where actual punishment remains unchanged for increasing conspicuity until a point, at which the actual punishment shoots up.
Lastly, this model assumed that a negative profit from sanction-busting behavior would encourage MNCs to disengage from the sanctioned economy. This is not always so. If a MNC loses even more money by disengaging from a sanctioned economy, it may prefer remaining engaged with a sanctioned economy, even if it operates at a net loss.
Despite its limitations, the incentive/disincentive model has large implications for the sanction literature. I hope this paper is the much needed intervention in sanction literature to distance it away from state-centric solutions. Perhaps further research can investigate the impact of other nonstate actors on sanction effectiveness. In particular, the impact of NGOs on sanction effectiveness seems to be a natural next step. I tangentially referred to the indirect role of NGOs in determining sanction effectiveness by exploring how NGO activities affected the incentive structures of MNCs, but NGOs deserve a much more thought-out investigation on how they affect sanction effectiveness in their own right.
The incentive/disincentive model also can hopefully assist analysts in evaluating the non-state mechanisms that determine the effectiveness of sanctions in place today. The case of North Korea seems ripe for investigation. Despite the several rounds of severe sanctions imposed on the hermit kingdom to persuade it to abandon its nuclear program, the North Koreans continue to survive, buoyed by sanction-busting Russian and Chinese firms61. Realistically, there is little the international community can do through economic coercion to convince the powerful, autarkic Chinese and Russian states to refrain from supporting sanction-busting firms. Perhaps, however, the Chinese and Russian firms themselves can be convinced of the errors of their ways by manipulating their incentive structures. Thankfully, the policymakers seem to have realized this earlier than academics. The U.S. treasury has imposed rounds of secondary sanctions not on states trading with the North Koreans, but on individual sanction-busting firms.62 These firms are now barred from interacting with U.S.-based firms, putting them at major competitive disadvantage. These secondary sanctions were installed only recently at the time this article was written, so it is too early to evaluate how effective they are, and anyhow they are beyond the scope of this paper. While it is encouraging that policymakers are turning towards non-state centric solutions, further research should apply the incentive/disincentive model to explain whether Russian and Chinese firms were actually dissuaded from sanction-busting.
References
Andreas, Peter. ""Criminalizing Consequences of Sanctions: Embargo Busting and Its Legacy."" International Studies Quarterly 49, no. 2 (2005): 335-60. doi:10.1111/j.0020-8833.2005.00347.x.
Baldwin, David A. Economic statecraft. Princeton, NJ: Princeton University Press, 1985.
Baldwin, David A. ""The Sanctions Debate and the Logic of Choice."" International Security 24, no. 3 (2000): 80-107. doi:10.1162/016228899560248.
Barbieri, K., O. M.g. Keshk, and B. M. Pollins. ""Trading Data: Evaluating our Assumptions and Coding Rules."" Conflict Management and Peace Science 26, no. 5 (2009): 471-91. doi:10.1177/0738894209343887.
Biglaiser, Glen, and David Lektzian. ""The Effect of Sanctions on U.S. Foreign Direct Investment."" International Organization 65, no. 03 (2011): 531-51. doi:10.1017/s0020818311000117.
Brooks, Risa A. ""Sanctions and Regime Type: What Works, and When?"" Security Studies 11, no. 4 (2002): 1-50. doi:10.1080/714005349.
Christopher, A. J. ""The pattern of Diplomatic Sanctions against South Africa 1948-1994."" GeoJournal 34, no. 4 (December 1994): 439-46.
Cohen, Andrew. ""Lonrho and Oil Sanctions against Rhodesia in the 1960s."" Journal of Southern African Studies 37, no. 4 (2011): 715-30. doi:10 .1080/03057070.2011.611286.
Higginson. Collective Violence and the Agrarian Origins of South African Apartheid, 1900-1948. Cambridge University Press, 2014.
Davis, Lance, and Stanley Engerman. ""History Lessons Sanctions: Neither War nor Peace."" Journal of Economic Perspectives 17, no. 2 (2003): 187-97. doi:10.1257/089533003765888502.
Drezner, Daniel W. ""How Smart are Smart Sanctions?"" International Studies Review 5, no. 1 (2003): 107-10. doi:10.1111/1521-9488.501014.
———. ""Sanctions Sometimes Smart: Targeted Sanctions in Theory and Practice."" International Studies Review 13, no. 1 (2011): 96-108. doi:10.1111/j.1468-2486.2010.01001.x.
Drury, A. Cooper. ""Reviewed Work: Manipulating the Market: Understanding Economic Sanctions, Institutional Change and the Political Unity of White Rhodesia by David M. Rowe."" The American Political Science Review 96, no. 4 (December 2002): 892-93.
Early, Bryan R. Busted sanctions: explaining why economic sanctions fail. Stanford, CA: Stanford University Press, 2015.
Galtung, Johan. ""On the Effects of International Economic Sanctions, With Examples from the Case of Rhodesia."" World Politics 19, no. 03 (1967): 378-416. doi:10.2307/2009785.
Hufbauer, Gary Clyde., Jeffrey J. Schott, and Kimberly Ann Elliott. Economic sanctions reconsidered. Washington, DC: Institute for International Economics, 1990.
Kastner, S. L. ""When Do Conflicting Political Relations Affect International Trade?"" Journal of Conflict Resolution 51, no. 4 (2007): 664-88 doi:10.1177/0022002707302804.
Kirshner, Jonathan. ""The microfoundations of economic sanctions."" Security Studies 6, no. 3 (1997): 32-64. doi:10.1080/09636419708429314.
———. ""Economic Sanctions: The State of the Art."" Security Studies 11, no. 4 (2002): 160-79.
Knight, Richard. ""Sanctions, Disinvestment, and U.S. corporations in South Africa."" In Sanctioning Apartheid. Africa World Press, 1990.
Lektzian, D., and G. Biglaiser. ""The effect of foreign direct investment on the use and success of US sanctions."" Conflict Management and Peace Science 31, no. 1 (2013): 70-93. doi:10.1177/0738894213501976.
Levy, Philip I. ""Sanctions on South Africa: What Did They Do?"" American Economic Review 89, no. 2 (1999): 415-20. doi:10.1257/aer.89.2.415.
Martin, Lisa L. ""Credibility, Costs, and Institutions: Cooperation on Economic Sanctions."" World Politics 45, no. 03 (1993): 406-32. doi:10.2307/2950724.
Minter, William, and Elizabeth Schmidt. ""When Sanctions Worked: The Case of Rhodesia Reexamined."" African Affairs 87, no. 347 (April 1988): 207-37.
Moreno, Elida. ""Panama detains Mossack Fonseca founders on corruption charges."" Reuters. February 11, 2017. Accessed November 15, 2017. https://www.reuters.com/article/us-panama-corruption-odebrecht/panama-detains-mossack-fonseca-founders-on-corruption-charges-idUSKBN15Q0TK.
Naylor, R. T. Economic warfare: sanctions, embargo busting, and their human cost. Boston: Northeastern University Press, 2001.
Naylor, R. T. Patriots and profiteers: economic warfare, embargo busting, and state-sponsored crime. Montreal: McGill-Queen's University Press, 2008.
Otusanya, Olatunde Julius. ""The role of multinational companies in tax evasion and tax avoidance: The case of Nigeria."" Critical Perspectives on Accounting 22, no. 3 (2011): 316-32. doi:10.1016/j.cpa.2010.10.005.
Pape, Robert A. ""Why Economic Sanctions Do Not Work."" International Security 22, no. 2 (1997): 90. doi:10.2307/2539368.
PAZZANI, MICHAEL J. CREATING A MEMORY OF CAUSAL RELATIONSHIPS: an integration of empirical and explanation-based... learning methods. S.l.: PSYCHOLOGY PRESS, 2016.
Quirós, Jorge E. ""La entrevista sobre ""Panama Papers"" que puso contra las cuerdas al presidente Varela."" TVN. October 22, 2016. Accessed November 15, 2017. https://www.tvn-2.com/nacionales/Varela-reaciona-cuestionado-Papeles-Panama-entrevista-Jenny-Perez-Alemania_0_4603789596.html.
Rodman, Kenneth A. ""Public and Private Sanctions against South Africa."" Political Science Quarterly 109, no. 2 (1994): 313. doi:10.2307/2152627.
Rodman, Kenneth A. ""Sanctions at bay? Hegemonic decline, multinational corporations, and U.S. economic sanctions since the pipeline case."" International Organization 49, no. 01 (1995): 105. doi:10.1017/ s0020818300001594.
Rodman, Kenneth Aaron. Sanctions beyond borders: multinational corporations and U.S. economic statecraft. Lanham, MD: Rowman & Littlefield Publishers, 2001.
Rowe, David M. ""Economic sanctions do work: Economic statecraft and the oil embargo of Rhodesia."" Security Studies 9, no. 1-2 (1999): 25487. doi:10.1080/09636419908429401.
Rowe, David M. Manipulating the market: understanding economic sanctions, institutional change, and the political unity of white Rhodesia. Ann Arbor: University of Michigan Press, 2001.
Samasuwo, Nhamo W. ""An assessment of the impact of economic sanctions on Rhodesia's cattle industry."" Historia 47, no. 2 (November 2002): 655-78.
Schmidt, Elizabeth. ""Review: Rhodesian Sanctions: The Long View."" Journal of Southern African Studies 29, no. 1 (March 2003): 311-12.
Stephenson, Glenn V. ""The Impact of International Economic Sanctions on the Internal Viability of Rhodesia."" Geographical Review 65, no. 3 (1975): 377. doi:10.2307/213536.
Sullivan, Leon. ""The Sullivan Principles."" Sullivan Principles. Accessed November 15, 2017. http://www.marshall.edu/revleonsullivan/principles.htm.
""The Anti-Apartheid Movement, Britain and South Africa: Anti-Apartheid Protest vs Real Politik."" African National Congress. Accessed November 15, 2017. http://www.anc.org.za/content/anti-apartheid-movementbritainand-south-africa-anti-apartheid-protest-vs-real-politik.
""US hits Chinese and Russian firms over North Korea."" BBC News. August 23, 2017. Accessed November 15, 2017. http://www.bbc.co.uk/news/world-us-canada-41018573.
Weida, Jason Collins. ""Reaching Multinational Corporations: A new model for drafting effective economic sanctions."" Vermont Law Review.
White, Luise. Unpopular sovereignty: Rhodesian independence and African decolonization. Chicago: The University of Chicago Press, 2015.
Worden, Nigel. The making of modern South Africa: conquest, apartheid, democracy. Hoboken, NJ: Wiley & Sons, 2012
Endnotes"	67688
economy	['Bedicks']	2017-05-22 00:00:00	"Abstract In 2009, Brazil was in the path to become a superpower. Immune to the economic crises of 2008, the country's economy benefitted from the commodity boom, achieving a growth rate of 7.5 per cent in 2010, when Rousseff was elected. A few years later, nonetheless, Brazil's boom turned into an economic bust. In 2014, the largest corruption scandal in its history denounced the involvement of major politicians, including then-President Rousseff, in schemes of money laundering. In this essay, I analyze the impacts of such scandals and Rousseff's impeachment on the Brazilian economy. I argue that these two events contributed indirectly to Brazil's economic instability, as they shone a light on larger structural problems such as unemployment and high public expenditures. Looking at the future, I conclude by discussing the challenges that face Temer's administration.

Introduction

On October 8, 2016, I was sitting at the Wilson Center listening to Brazil's current finance minister, Mr. Henrique Meirelles, suggest that the ongoing financial crisis in Brazil is as severe, if not worse, than the Great Depression. As I looked around the room, everyone's faces looked puzzled, worried, hopeless. The accumulation of lower commodity prices, corruption, impeachment, and inflation present in the last four years have shaken the Brazilian economy severely. Prices have skyrocketed, investors have fled, unemployment increased, and stagnation has arrived. In this paper, I analyze two specific factors that contributed to the chaotic economic situation in Brazil: the corruption scandals involving the state-owned company Petrobras and the subsequent impeachment of President Dilma Rousseff. I argue that these two events indirectly contributed to the economic situation of the country as they magnified the political instability in Brazil. To support my argument, I start by analyzing the political rise of Dilma Rousseff. Then I discuss the momentum she received in 2010 from the economic boom and the Lula years. After giving a background on Rousseff's debut as President of the Republic, I provide a detailed explanation of the Car Wash Operation since its start in 2009. Lastly, I analyze the political and economic consequences that the corruption scandal brought to Brazil, including Rousseff's impeachment. I conclude by discussing the challenges that face Temer's administration.

The Rise of Dilma Rousseff

Dilma Rousseff's debut as a Brazilian politician did not start in the twenty-first century as many would argue. Much earlier than that, in 1964, sixteen-year-old Rousseff served as a militant for the VAR-Palmares, an armed revolutionary organization that fought against the military dictatorship in Brazil. This organization, influenced by Debray's Revolution in the Revolution and Fidel Castro's success in Cuba, found an answer for Brazil in Marxism. However, in a repressive far-right military regime, VAR-Palmares lacked political rights. In 1970, Rousseff was arrested in São Paulo under charges of subversion. Condemned to six years in prison, Dilma lost her political rights. Her body still carries scars from the torture she experienced at the Tiradentes prison (Amaral, 2011).

Once released from prison, Rousseff met Leonel Brizola, an iconic figure in Brazil's political history, with whom she founded the Democratic Labour Party, or PDT (G1, 2016). Her involvement with the PDT coined her official debut in a legally recognized political organization. Through the PDT, Rousseff was elected Porto Alegre's Municipal Finance Secretary between 1985 and 1988. In 1989, she became director general of Porto Alegre's City Council under the administration of Alceu Collares. In 1991, Collares appointed Rousseff as the president of the Foundation of Economics and Statistics, where she served until being elected the state's Secretary of Energy, Mines and Communications in 1993 (Amaral, 2011).

A few years later, Rousseff's trajectory with the Worker's Party (PT) would begin. In 1998, she helped PT-candidate for state governor Olivio Dutra to win an election in Rio Grande do Sul. By then, Rousseff's de facto affiliation had turned from the PDT to the PT – a fact confirmed later that year when she left PDT after the party decided to break with the PT. In 2001, Dilma's preference for the Worker's Party went viral. In 2003, President Luiz Inácio Lula da Silva appointed Rousseff as his Minister of Mining and Energy (Amaral, 2011). In the same year, Rousseff became the President of the Board of the Directors of Brazil's largest oil and state-owned enterprise (SOE), Petrobras. She presided over the SOE's board for seven years, resigning in 2010 to run for Brazil's presidency (Tavares, 2016).

Two years after being nominated Minister of Mining and Energy, Rousseff had proven to be a firm, competent politician who could handle Brasilia quite well. Lula, impressed by her skills and desperate to find a replacement for his Chief of Staff Jose Dirceu, who was caught in the mensalão, the largest corruption scandal of Brazilian political history until then, fell on her. In Rousseff, he found a friend and the fuel needed to reset Brasilia's engine. In 2005, Lula appointed Dilma as his newest Chief of Staff. He trusted her not only to manage Brazil's ministries, but also two of his administration's major programs. The first, ""Minha Casa, Minha Vida,"" (my house, my life) promoted affordable housing for those living in poverty in Brazil. The second, ""Programa de Aceleração de Crescimento"" (growth acceleration program, also known as PAC), provided an economic stimulus package with investment projects in Brazil. This program was particularly responsible for increasing Rousseff's reputation, as Lula proudly entitled her the ""mother of the PAC"" (Amaral, 2011).

As Chief of Staff, ""mother of the PAC,"" and Lula's protégé, Rousseff became the Worker's Party strongest candidate for the 2010 elections. Running her campaign on the motto ""for Brazil to keep changing,"" Rousseff won the runoff with 56.95% of the votes (Savarese and Bencke, 2016).

Rousseff Gains Momentum

The 2010 elections served as a referendum on Lula's administration. The election of Dilma Rousseff confirmed the public approval of the programs and policies implemented by the Worker's Party. It also confirmed Brazil's oblivion toward corruption and the people's excitement towards the years to come. Brazilians were proud to have elected their first female President, but most importantly, they were eager to continue benefitting from a strong economy.

Brazil's economy reached a 24-year peak in 2010. The economy grew 7.5 per cent, unemployment kept low at 5 per cent (see figures 3 and 5), and the Brazilian Real was at its strongest rates since the implementation of the Real Plan (see appendix, Fig. I). In addition, as Besta points out, 2010 "" saw the highest increase in monthly incomes of Brazilians at 1490.61 Brazilian real ($893.4), up 19 percent compared with levels in 2003. Per capita income was up 5.5 percent in 2010 compared with 2009"" (2011). These numbers, subsequents of other factors like commodity prices boom and the discovery of Brazil's pre-salt, convinced Brazil's new middle-class, children of the cash transfer program Bolsa Familia, that reelecting the Worker's Party would serve them well. Rousseff's reelection thus served as a referendum on the Lula years and an engine to catalyze Brazil's potential overseas.

Both the media and academia shared their excitement to see Brazil's future, as they wrote about the country as the 21st century superpower. In the end of 2009, The Economist cover announced ""Brazil Takes Off"" (The Economist, 2009). In 2010, Peter Hakim published his book entitled ""Brazil on the Rise"" (Hakim, 2010) and Forbes released an article suggesting ""Brazil's Economy Catches Its Breath"" (Delgado, 2010). Following this route of excitement, Dauvergne and Farias wrote a well-known piece called ""The rise of Brazil as a Global Development Power"" in 2012 (Dauvergne and Farias, 2012).

These, among other publications, reflected the projections created for Brazil's economic path: a path to be of success, development, growth, power, independence, and global influence. The first years of Rousseff's administration were filled with momentum to see Brazil step into the role of global leader. However, the end of 2013 served as presage to a large group of politicians, including Lula and Rousseff, that the years to come would not be so glorious. The economy had stagnated with an annual growth of 0.9 per cent (see figure 3) and the June 2013 protests demonstrated the population's dissatisfaction with the Worker's Party administration. With more than 2 million people taking over the streets, they protested against rampant increases in bus fares, inflation, corruption, and lack of investment in education and infrastructure. By the end of the year, the media that had responded so positively to previous PT years started to question Brazil's direction. One year away from the World Cup, the international press doubted whether Brazil was ready to host the global event (Vanegas, 2013). In September 2013, The Economist asked ""Has Brazil Blown It?"" Investment companies, such as Morgan Stanley, published reports suggesting that Brazil's golden economy was ""a tale from the emerging world"" (The Economist, 2013).

2014 and The Car Wash Operation

2014 arrived with many surprises. For some, 2014 was the year of excitement as Brazil would host the World Cup. For others, 2014 was the year of despair. On one side, the Public Prosecutor's Office (MP) and the Federal Police (PF) discovered what has become the largest corruption scandal in the history of Brazil. On the other side, the stagnation of the economy and the discontent with the government brought a sense of hopelessness that spread among Brazilians across all socioeconomic classes.

In 2014, an expansive investigation performed by the Public Prosecutor's Office was disclosed. Initiated in 2009, the Car Wash Operation, named after a network of gas stations and car washes that to managed illicit resources pertaining to a criminal organization in Brazil, investigated a Federal Deputy from the state of Paraná, accused of money laundering (MPF, 2016). The MP also investigated Alberto Youssef, a black market dollar dealer who was already familiar to the Federal Police. In 2013, telephone interceptions monitored conversations among several dollar dealers in order to obtain further information on the criminal organizations charged with money laundering (MPF, 2016). In July of that year, the Police learned about Youssef's car donation to a former Director of Petrobras, Paulo Roberto Costa (MPF, 2016). This discovery led the Police to suspect that these criminal organizations' network for money laundering was larger than ever thought. The investigations did not only involve politicians who had previously been involved in this kind of corruption scandals, but they also involved businessmen and contractor companies across the country.

As the Federal Police, along with the Public Prosecutor's Office, denounced the dimension and severity of the Car Wash Operation, the economy reacted in despair. The first phase of the Car Wash investigations happened in March 2014. In this phase, the Police along with the MP arrested 17 people, including dollar dealer Alberto Youssef and former Director of Petrobras, Paulo Roberto da Costa (MPF, 2016). Since then, political and economic turmoil arose in Brazil as the Operation would discover more people involved every single day. In April 2014, the scandal had escalated so deeply that a Parliamentary Inquiry Commission (CPI) on Petrobras was installed in the Senate to address issues pertaining solely to the corruption revealed in the Car Wash Operation (G1, 2016). In June, already in its fourth phase, the investigations revealed that most of the money laundered had been deposited into bank accounts in fiscal havens, such as Monaco and Switzerland. Among the owners of these accounts was Paulo Roberto da Costa (Borges, 2016). When invited to testify about the issue, nonetheless, Costa became known as a whistleblower. Speaking under a plea bargain, Costa explained how the laundering schemes worked. In addition, as he denounced major players in these schemes, he cited the involvement of President Dilma Rousseff (Borges, 2016; MPF, 2016). By September 2014, the Car Wash Operation confirmed that part of the money laundering was illegally used to fund 2010 political campaigns from major parties, including Rousseff's PT and VP Michel Temer's PMDB (Borges, 2016). By the end of the year, the Operation had reached the seventh phase, arrested 39 people, and investigated four major Brazilian contractor companies: OAS, Odebrecht, Camargo Correa, and Queiroz Galvão (Borges, 2016).

Even though 2014 came to an end, the political and economic turmoil present in Brazil continued to worsen. The economic indicators were not showing an optimistic scenario. In 2014, the economy grew only 0.1 percent while inflation reached 6.41 percent (see figures 2 and 3). Talks about a recession dominated the news, while frustration dominated people's conversations about the government. Mistrusting the Worker's Party administration, Brazilians feared that the 6.8 percent unemployment rate would skyrocket (Borges, 2016). In addition, as the dollar continued to quickly appreciate, they feared that the bust after the boom had arrived. The same media outlets that had partaken in Brazil's exhilaration around 2010 now expressed their woes. The Economist published ""Why Brazil Needs Change"" (The Economist, 2014) followed by ""Brazil In a Quagmire"" (The Economist, 2015). The Independent article said ""It's Chaos in Brazil but don't panic"" (Herbert, 2014) while Bloomberg announced early in 2015 ""The Betrayal of Brazil"" (Smith et al, 2015).

2015 and Instability Rises

Despite the Car Wash Operation and the frustration brought with it, the corruption scandal did not compromise the Worker's Party leadership. In November 2014, Dilma Rousseff was re-elected president after a runoff against the Brazilian Social Democracy Party (PSDB) candidate Aécio Neves with 51.6% of the votes (G1, 2014). Even though the Car Wash Operation did not compromise the Worker's Party ability to re-elect Rousseff as President in 2014, the scandal and the economic despair dominating Brazil at the time alarmed Brasilia that instability was on the rise. Early in January 2015, then President of Petrobras, Graça Foster, suggested that the company had lost R$ 88,6 billion due to the Car Wash Scandal (G1, 2015). In February, a former manager of operations of Petrobras revealed that the Worker's Party had received somewhere between US$ 150 to US$ 200 million in one bribery contract (MPF, 2016). A few months later, as the investigation reached its twelfth phase, Folha de São Paulo, a renowned newspaper in Brazil, published an interview with Ricardo Pessoa, a contractor CEO who claimed that his company had donated R$ 7.5 million to Dilma Rousseff's reelection campaign (MPF, 2016).

A famous saying in Brazil says that those who search for something, will eventually find it. This proverb was reiterated throughout all of the MP's investigations in 2015. The more the Car Wash Operation searched for corruption schemes, the more names it found and the more complex the network of money laundering actors was revealed to be. In 2015 alone, the investigation had fourteen phases, arresting more than forty people, condemning more than twelve, and recovering more than R$870 million (G1, 2016). Through these operations, the presidents of contractor companies such as Odebrecht were arrested. Politicians such as former Presidents of Brazil Fernando Collor and Luiz Inácio Lula da Silva, then-Presidents of the House and of the Senate, Eduardo Cunha and Renan Calheiros, respectively, and former Chief of Staff José Dirceu were denounced for being involved in major money laundering schemes.

If anything, 2015 was the year that shook Brasilia. Brazil's political ""crème de la crème"" was shaken. Most parties were involved; every politician was pointing fingers. The German newspaper Die Zeit compared the political intrigues in Brazil to those in House of Cards (Fischermann, 2016). In addition to the political turmoil, economic instability also emerged. Economic indicators demonstrated that the economy had contracted by 3.8 per cent in 2015, while inflation had escalated to 10.67 per cent (see figures 3 and 4). The Brazilian Real had been severely depreciated as the exchange rate against the US dollar appreciated to 3.95943 (see appendix, Fig. I). Unemployment reached 9 per cent, the highest level in the last four years (see figure 5). Likewise, the federal debt also broke the administration's record reaching almost R$ 2 trillion (see appendix, Fig. III). In tandem, the political and economic instability that dominated Brazil in 2015 set the tone for drastic changes in the year to come.

2016 and Rousseff's Fall

The economic and political instability present in Rousseff's second term extended to 2016 as well – so much so that her presidency was compromised. The Car Wash Operations continued to investigate major networks of politicians and contractor companies involved with Petrobras and its money laundering schemes. By the time this paper was written, The Public Prosecutor's Office, along with the Federal Police, has established 1,397 procedures, conveyed 654 searches and seizures, 77 preventive arrests, 92 temporary arrests, and 6 in flagrante delicto. The Operation has received 52 criminal charges against 254 people for crimes of corruption, drug-trafficking, formation of criminal organization, and money laundering (MPF, 2016). These crimes refer to briberies that sum up to approximately R$ 6.4 billion, of which R$3.1 billion are to be recovered. Up to December 2016, the Car Wash Operation has condemned 118 people, accounting for more than 1,256 years of prison in total (MPF, 2016).

Different from the previous two years, nonetheless, the findings on the Petrolão compromised the Worker's Party administration like never before. As major members of the Party were arrested and indicted of money laundering crimes, Brazilians grew disillusioned with their political leadership. Just like in June 2013, the population returned to the streets demanding Rousseff to leave her office. Economic mismanagement caused inflation to increase to 10.7 per cent, and unemployment to reach 11 per cent. Tired of corruption and frustrated with the lack of investments in the country, people demanded change.

In May 2016, the Senate opened the process of impeachment against Rousseff. According to jurists Janaina Paschoal, Hélio Bicudo, and Miguel Reale Jr., Rousseff had violated the Fiscal Responsibility Law (G1, 2016). They claimed she had authorized R$2.5 billion of additional expenses between July and August 2015. According to the Law, such authorization could not have happened since government expenditures did not match the fiscal goals for the year, especially since the administration was committed to increase its savings to pay the public debt (G1, 2016). Rousseff was also charged for borrowing money from other federal institutions such as the Central Bank and the Brazilian Development Bank (BNDES) to finance her government's social programs in 2015, as cash transfer Bolsa Familia and Plano Safra (G1, 2016). This is known as a ""fiscal pedal"" in Brazil, as the government seeks to disguise a breakdown in the public accounts. However, as the judges highlighted, ""fiscal pedals"" infringe the Fiscal Responsibility Law, which forbids the government from borrowing money from public banks that are under the Executive branch (G1, 2016).

With Rousseff temporarily removed from office for three months, Rousseff's VP, Michel Temer, was put in charge of the interim presidency. Forbes reported that for the first time since 2009, Brazil's current account went into surplus. The market reacted positively with an appreciation of the Brazilian Real (Rapoza, 2016). As Temer appointed his cabinet, he carefully selected conservative technocrats who would take Brazil to a route much different from that during the PT years. So much so that into two weeks after Dilma's temporary leave, Temer's Finance Minister Henrique Meirelles already proposed cuts of more than 2 per cent of the GDP (Rapoza, 2016).

During the interim months, political instability was followed by acute polarization. On one side, scholars, mainstream civilians, politicians, and the press would argue that it was time for Dilma and her crew to go (The Economist, 2016). On the other side, people would argue that Rousseff's impeachment was a political coup orchestrated by the Brazil's Rightist elite. Despite disagreements, the Senate voted 61 to 20 to impeach Rousseff, convicting her of infringing the Fiscal Responsibility Law. Then-interim head of state, Mr. Michel Temer took over Brasilia, where he will preside until 2018 (Romero, 2016).

Impacts on the Economy

Measuring the economic impacts of the Car Wash Operations as well as Rousseff's impeachment is not an easy task. An anonymous officer at the Brazilian Embassy in Washington DC suggested that the consequences brought by the economic and political instability in Brazil could by no means be quantified. For her, ""the corruption scandal in Brazil removed the cork from the bottle. It served to shine light on larger structural obstacles that the country already faced both economically and politically."" While I agree with the officer's argument that Brazil's de facto struggle refers to structural issues rather than corruption alone, I disagree that the economic impacts of the Car Wash Operation, as well as the impeachment, cannot be quantified. In this section, I will address the economic indicators that I personally monitored at the Embassy of Brazil, as well as some scholarly arguments, as an endeavor to measure the impact of both events on the Brazilian economy.

The Car Wash Operation contributed to the economic and political instability present in Brazil, but it was not the protagonist factor. The Operation, with the help of the press, was able to reveal the expansive network of corrupted politicians across Brazil. Such revelation did not give room for people's political complacency. For the first time, a corruption scandal compromised political parties and their members. In this way, the corruption scandal contributed to the political instability in the country. In addition, the scandal also played a role impacting the economic instability in Brazil. As corruption schemes and political instability were revealed to the public, investments decreased severely. Compromising Brazil's credibility, the Car Wash Operation and the impeachment spillovers detracted foreign investment. As Figure 1 shows, from 2014 to 2016, FDI decreased by almost US$20 billion. Moreover, domestic investments also decreased. The continuous increase in the interest rate has prevented investors from borrowing money (see appendix, fig. IV), and the involvement of major contractor companies in the corruption scandals have left no one to work on big infrastructure projects in the country. According to consultancy firm Tendências, domestic investments shrunk 6 per cent in 2016 alone (Ribeiro and Cortez, 2016).

Figure 1: Inflow of FDI in Brazil from 1994 to Present in US$ billions Source: World Bank Data

Besides investments, the Car Wash Operations as well as the impeachment impacted Brazil's GDP. Figures 1 and 2 demonstrate how the economy shrunk since 2013. Figure 2 in particular show how the economy contracted, thus taking the country into a severe recession. Alessandra Ribeiro, from Tendências, has argued that the Petrolão is a factor that potentialized the contraction of the GDP. For her, two out the 3.8 per cent decline in economic growth relates back to the scandals (Ribeiro and Cortez, 2016).

Figure 2: Brazil's Gross Domestic Product from 2000 to 2015 in current US$ millions Source: BACEN

Figure 3: Brazil's economic growth from 2000 to 2016 Source: BACEN

A decline in economic activity came in tandem with a rise in inflation as well as in the unemployment rate. In this case, the impacts of the two events analyzed might be indirectly related, nonetheless still present. The mismanagement of the economy has caused inflation to reach almost 11 per cent in 2015. This inflation, followed by the lack of investments, and the decline in economic activity may have decreased the number of jobs available in the market. Figures 4 and 5 show the price fluctuation as well as the unemployment rate in the last years. For Bruno Lavieri, ""blaming the Car Wash Operation [for unemployment and stagnation] is like blaming the doctor for finding his patient's disease."" However, as GO Associates have argued, this indirect impact of the investigations have costed more than 2 million jobs in two years due to the lack of infrastructure projects with Petrobras and contractor companies (UOL, 2016). Petrobras alone composed 13 per cent of the economic activities in Brazil (Petrobras, 2016). According to Getúlio Vargas Foundation, ""the decrease in Petrobras' activities due to the Car Wash Operations could take away R$7 billion from the economy, lead to the loss of more than 1 million vacancies and a R$ 5.7 billion fall in the collection of taxes by the Union, states and municipalities in 2015"" (UOL, 2016).

Figure 4: Inflation levels in Brazil from 1995 to 2016. Source: IBGE

Figure 5: Unemployment rate of persons 14 years or older from 2012 to present in Brazil. Source: IBGE

Although the impeachment and the corruption scandal did not singlehandedly cause Brazil's economic recession, they shone a light on major existent structural problems such as corruption, fiscal pedals, lack of investments, and poor allocation of resources. Moreover, both events indirectly contributed, even though did not cause, the economic crisis. The corruption scandal influenced the decline in economic activities and lack of investments in the country, which subsequently increased the unemployment rate. In addition, inflation became rampant and political instability remained. With that, government expenditures increased and private savings decreased significantly, as Figure 6 reveals.

Figure 6: Composition of Brazil's GDP in terms of private consumption, current account balance, government expenditure, and private savings. Source: World Bank Data

Conclusion

With Michel Temer's administration comes new challenges. Foreign and domestic investors remain skeptical of Brazil's economic and political situation. Unemployment and inflation remains high, investments remain low, and the Brazilian Real continues depreciated. Since his inauguration, Temer dedicated most of his time and effort to get Bill 241 approved in Congress. This Bill proposes to reduce public spending and to balance the public accounts by freezing government expenditures in the next twenty years (Alessi, 2016). However, getting Bill 241 approved is not everything. President Temer will have to address several other factors such as Social Security and Tax reforms, unemployment rate, social 49 agenda, and Congress polarization to be able to recover from the economic and political instability that the Car Wash Operations shone a light to.

Author

Flávia Bedicks is a student of Economics and International Studies. She graduates in May of 2017. College of Arts & Sciences (CAS) and School of International Service (SIS), American University. Email: fb4631a@american.edu

References

""A Lava Jato Em Números – Caso Lava Jato"". 2016. Ministério Público Federal. http://lavajato.mpf.mp.br/atuacao-na-1a-instancia/resultados/a-lava-jato-em-numeros-1.

Alessi, Gil. 2016. ""Entenda O Que É A PEC 241 E Como Ela Pode Afetar Sua Vida"". El País. http://brasil.elpais.com/brasil/2016/10/10/politica/1476125574_221053.html.

Amaral, Ricardo Batista. 2011. A Vida Quer É Coragem. 1st ed. Rio de Janeiro: Primeira Pessoa.

Banco Central do Brasil (BACEN). 2016. Indicadores Econômicos. Brasília: Banco Central do Brasil.

Besta, Shankar. 2011. ""Brazil Unemployment Drops To Record Low In 2010"". International Business Times. http://www.ibtimes.com/brazil-unemployment-drops-record-low-2010-260209.

Borges, João. 2016. ""Linha Do Tempo – Lava-Jato"". Cbn.Globoradio.Globo.Com. http://cbn.globoradio.globo.com/grandescoberturas/operacao-lava-jato/2016/02/23/LINHA-DOTEMPOLAVA-JATO.htm.

""Brazil In A Quagmire"". 2015. The Economist. http://www.economist.com/news/leaders/21645181latin-americas-erstwhile-star-its-worst-mess-early-1990s-quagmire.

""Brazil Takes Off"". 2009. The Economist. http://www.economist.com/node/14845197.

""Brazil / U.S. Foreign Exchange Rate"". 2016. FRED. https://fred.stlouisfed.org/series/DEXBZUS.

""Cálculo Havia Apontado Perda De R$ 88,6 Bilhões, Segundo Graça Foster"". 2015. Globo G1. http://g1.globo.com/economia/negocios/noticia/2015/01/calculos-havia-apontado-perda-de-r886-bilhoes-segundo-graca-foster.html.

Dauvergne, Peter, and Déborah BL Farias. 2012. ""The rise of brazil as a global development power."" Third World Quarterly 33 (5): 903-17.

Delgado, Bertrand. 2010. ""Brazil's Economy Catches Its Breath"". Forbes. http://www.forbes.com/2010/09/08/brazil-economy-gdp-opinions-columnists-doctor-doom.html.

""Dilma É Reeleita Presidente E Amplia Para 16 Anos Ciclo Do PT No Poder"". 2014. Globo G1. http://g1.globo.com/politica/eleicoes/2014/noticia/2014/10/dilma-e-reeleita-presidente-eampliapara-16-anos-ciclo-do-pt-no-poder.html.

""Dívida Pública Federal, STN"". 2016. Tesouro Nacional: Ministério da Fazenda. http://www.tesouro.fazenda.gov.br/divida-publica-federal.

Fischermann, Thomas. 2016. ""Brasilien: Intrigen Wie Bei ""House Of Cards"""". Zeit. http://www.zeit.de/politik/ausland/2016-03/brasilien-opposition-dilma-roussef-luiz-inacio-luladasilva.

Hakim, Peter. 2010. ""Brazil on the rise: The challenges and choices of an emerging global power."" Politica Externa 19 (1).

""Has Brazil Blown It?"". 2013. The Economist. http://www.economist.com/news/leaders/21586833-stagnant-economy-bloated-state-andmass-protests-mean-dilmarousseffmust-change-course-has.

Herbert, Ian. 2014. ""World Cup 2014: It's Chaos In Brazil – But Don't Panic"". The Independent. http://www.independent.co.uk/sport/football/international/world-cup-2014-itschaosin-brazil-but-dont-panic-9210299.html.

""IBGE: Instituto Brasileiro De Geografia E Estatística"". 2016. Ibge.Gov.Br. http://www.ibge.gov.br/home/estatistica/indicadores/precos/inpc_ipca/ipca-inpc_201610_3.shtm.

""Linha Do Tempo Da Lava Jato"". 2016. G1. http://especiais.g1.globo.com/politica/2015/lavajato/linha-do-tempo-da-lava-jato/.

""Linha Do Tempo – Caso Lava Jato"". 2016. Ministério Publico Federal. http://lavajato.mpf.mp.br/atuacao-no-stj-e-no-stf/linha-do-tempo/todas-noticias.

""Lula Está Certo Ao Dizer Que Lava Jato Afeta Economia? Analistas Avaliam"". 2016. UOL Economia. http://economia.uol.com.br/noticias/redacao/2016/04/03/lula-esta-certo-ao-dizerquelava-jato-afeta-economia-analistas-avaliam.htm.

""Participação Do Setor De Petróleo E Gás Chega A 13% Do PIB Brasileiro"". 2016. Petrobras. http://www.petrobras.com/pt/magazine/post/participacao-do-setor-de-petroleo-egaschega-a-13-do-pib-brasileiro.htm.

""Processo De Impeachment É Aberto, E Dilma É Afastada Por Até 180 Dias"". 2016. Globo G1. http://g1.globo.com/politica/processo-de-impeachment-de-dilma/noticia/2016/05/processo-de-impeachment-e-aberto-e-dilma-e-afastada-por-ate-180-dias.html.

Rapoza, Kenneth. 2016. ""Global Investors Put Brazil's Interim President Michel Temer On Notice"". Forbes. http://www.forbes.com/sites/kenrapoza/2016/06/02/global-investors-putbrazilsinterim-president-michel-temer-on-notice/2/#7797e89e5f36.

""Relembre A Trajetória Política De Dilma Rousseff"". 2016. G1 Política. http://g1.globo.com/politica/processo-de-impeachment-de-dilma/noticia/2016/05/relembre-trajetoriapoliticade-dilma-rousseff.html.

Ribeiro, Alessandra and Rafael Cortez. 2016. ""Riscos Para A Recuperação"". Tendências Consultoria. http://www.tendencias.com.br/news.cgi?id=114.

Romero, Simon. 2016. ""Dilma Rousseff Is Ousted As Brazil'S President In Impeachment Vote"". The New York Times. http://www.nytimes.com/2016/09/01/world/americas/brazil-dilmarousseffimpeached-removed-president.html.

Savarese, Maurício and Carlos Bencke Bencke. 2016. ""Dilma É Eleita Primeira Mulher Presidente Do Brasil"". UOL. http://eleicoes.uol.com.br/2010/ultimas-noticias/2010/10/31/dilma-eeleitaprimeira-presiDente-mulher.jhtm.

Smith, Michael, Sabrina Valle, and Blake Schmidt. 2015. ""The Betrayal Of Brazil"". Bloomberg. http://www.bloomberg.com/news/features/2015-05-08/brazil-s-massive-corruption-scandalhasbitterness-replacing-hope.

""Tales From The Emerging World"". 2013. Morgan Stanley. https://www.morganstanley.com/public/Tales_from_the_Emerging_World_Fragile_Five.pdf.

Tavares, Monica. 2016. ""Dilma Deixa Conselho Da Petrobras"". O Globo. http://oglobo.globo.com/economia/dilma-deixa-conselho-da-petrobras-3036516.

""Time To Go"". 2016. The Economist. http://www.economist.com/news/leaders/21695391-tarnishedpresidentshould-now-resign-time-go.

Vanegas, Maria. 2013. ""Is Brazil Ready To Host The World Cup? A Look At South America's Most Stable Economy"". Latin Times. http://www.latintimes.com/brazil-ready-host-world-cuplooksouth-americas-most-stable-economy-133187.

""Why Brazil Needs Change"". 2014. The Economist. http://www.economist.com/news/leaders/21625780-voters-should-ditch-dilma-rousseff-and-elect-cio-neves-why-brazilneedschange.

""World Bank Indicators, Brazil"". 2016. World Bank Data. http://data.worldbank.org/country/brazil.

Appendix

Figure I: Price of US dollars in Brazilian Reais from 2011 to Present. Source: FRED

Figure II: Brazil's Public Debt in R$ millions from 2002 to 2016. Source: Tesouro Nacional

Figure III: Brazil's Public Debt as a Percent of its GDP from 2002 to Present. Source: Tesouro Nacional

Figure IV: Brazil's monthly Interest Rate levels from 2014 to 2016. Source: BACEN

Glossary"	http://www.inquiriesjournal.com/articles/1633/how-have-corruption-scandals-and-president-roussefs-impeachment-in-brazil-impacted-its-economy	"In 2009, Brazil was in the path to become a superpower. Immune to the economic crises of 2008, the country's economy benefitted from the commodity boom, achieving a growth rate of 7.5 per cent in 2010, when Rousseff was elected. A few years later, nonetheless, Brazil's boom turned into an economic bust. In 2014, the largest corruption scandal in its history denounced the involvement of major politicians, including then-President Rousseff, in schemes of money laundering. In this essay, I analyze the impacts of such scandals and Rousseff's impeachment on the Brazilian economy. I argue that these two events contributed indirectly to Brazil's economic instability, as they shone a light on larger structural problems such as unemployment and high public expenditures. Looking at the future, I conclude by discussing the challenges that face Temer's administration.
Introduction
On October 8, 2016, I was sitting at the Wilson Center listening to Brazil's current finance minister, Mr. Henrique Meirelles, suggest that the ongoing financial crisis in Brazil is as severe, if not worse, than the Great Depression. As I looked around the room, everyone's faces looked puzzled, worried, hopeless. The accumulation of lower commodity prices, corruption, impeachment, and inflation present in the last four years have shaken the Brazilian economy severely. Prices have skyrocketed, investors have fled, unemployment increased, and stagnation has arrived. In this paper, I analyze two specific factors that contributed to the chaotic economic situation in Brazil: the corruption scandals involving the state-owned company Petrobras and the subsequent impeachment of President Dilma Rousseff. I argue that these two events indirectly contributed to the economic situation of the country as they magnified the political instability in Brazil. To support my argument, I start by analyzing the political rise of Dilma Rousseff. Then I discuss the momentum she received in 2010 from the economic boom and the Lula years. After giving a background on Rousseff's debut as President of the Republic, I provide a detailed explanation of the Car Wash Operation since its start in 2009. Lastly, I analyze the political and economic consequences that the corruption scandal brought to Brazil, including Rousseff's impeachment. I conclude by discussing the challenges that face Temer's administration.
The Rise of Dilma Rousseff
Dilma Rousseff's debut as a Brazilian politician did not start in the twenty-first century as many would argue. Much earlier than that, in 1964, sixteen-year-old Rousseff served as a militant for the VAR-Palmares, an armed revolutionary organization that fought against the military dictatorship in Brazil. This organization, influenced by Debray's Revolution in the Revolution and Fidel Castro's success in Cuba, found an answer for Brazil in Marxism. However, in a repressive far-right military regime, VAR-Palmares lacked political rights. In 1970, Rousseff was arrested in São Paulo under charges of subversion. Condemned to six years in prison, Dilma lost her political rights. Her body still carries scars from the torture she experienced at the Tiradentes prison (Amaral, 2011).
Once released from prison, Rousseff met Leonel Brizola, an iconic figure in Brazil's political history, with whom she founded the Democratic Labour Party, or PDT (G1, 2016). Her involvement with the PDT coined her official debut in a legally recognized political organization. Through the PDT, Rousseff was elected Porto Alegre's Municipal Finance Secretary between 1985 and 1988. In 1989, she became director general of Porto Alegre's City Council under the administration of Alceu Collares. In 1991, Collares appointed Rousseff as the president of the Foundation of Economics and Statistics, where she served until being elected the state's Secretary of Energy, Mines and Communications in 1993 (Amaral, 2011).
A few years later, Rousseff's trajectory with the Worker's Party (PT) would begin. In 1998, she helped PT-candidate for state governor Olivio Dutra to win an election in Rio Grande do Sul. By then, Rousseff's de facto affiliation had turned from the PDT to the PT – a fact confirmed later that year when she left PDT after the party decided to break with the PT. In 2001, Dilma's preference for the Worker's Party went viral. In 2003, President Luiz Inácio Lula da Silva appointed Rousseff as his Minister of Mining and Energy (Amaral, 2011). In the same year, Rousseff became the President of the Board of the Directors of Brazil's largest oil and state-owned enterprise (SOE), Petrobras. She presided over the SOE's board for seven years, resigning in 2010 to run for Brazil's presidency (Tavares, 2016).
Two years after being nominated Minister of Mining and Energy, Rousseff had proven to be a firm, competent politician who could handle Brasilia quite well. Lula, impressed by her skills and desperate to find a replacement for his Chief of Staff Jose Dirceu, who was caught in the mensalão, the largest corruption scandal of Brazilian political history until then, fell on her. In Rousseff, he found a friend and the fuel needed to reset Brasilia's engine. In 2005, Lula appointed Dilma as his newest Chief of Staff. He trusted her not only to manage Brazil's ministries, but also two of his administration's major programs. The first, ""Minha Casa, Minha Vida,"" (my house, my life) promoted affordable housing for those living in poverty in Brazil. The second, ""Programa de Aceleração de Crescimento"" (growth acceleration program, also known as PAC), provided an economic stimulus package with investment projects in Brazil. This program was particularly responsible for increasing Rousseff's reputation, as Lula proudly entitled her the ""mother of the PAC"" (Amaral, 2011).
As Chief of Staff, ""mother of the PAC,"" and Lula's protégé, Rousseff became the Worker's Party strongest candidate for the 2010 elections. Running her campaign on the motto ""for Brazil to keep changing,"" Rousseff won the runoff with 56.95% of the votes (Savarese and Bencke, 2016).
Rousseff Gains Momentum
The 2010 elections served as a referendum on Lula's administration. The election of Dilma Rousseff confirmed the public approval of the programs and policies implemented by the Worker's Party. It also confirmed Brazil's oblivion toward corruption and the people's excitement towards the years to come. Brazilians were proud to have elected their first female President, but most importantly, they were eager to continue benefitting from a strong economy.
Brazil's economy reached a 24-year peak in 2010. The economy grew 7.5 per cent, unemployment kept low at 5 per cent (see figures 3 and 5), and the Brazilian Real was at its strongest rates since the implementation of the Real Plan (see appendix, Fig. I). In addition, as Besta points out, 2010 "" saw the highest increase in monthly incomes of Brazilians at 1490.61 Brazilian real ($893.4), up 19 percent compared with levels in 2003. Per capita income was up 5.5 percent in 2010 compared with 2009"" (2011). These numbers, subsequents of other factors like commodity prices boom and the discovery of Brazil's pre-salt, convinced Brazil's new middle-class, children of the cash transfer program Bolsa Familia, that reelecting the Worker's Party would serve them well. Rousseff's reelection thus served as a referendum on the Lula years and an engine to catalyze Brazil's potential overseas.
Both the media and academia shared their excitement to see Brazil's future, as they wrote about the country as the 21st century superpower. In the end of 2009, The Economist cover announced ""Brazil Takes Off"" (The Economist, 2009). In 2010, Peter Hakim published his book entitled ""Brazil on the Rise"" (Hakim, 2010) and Forbes released an article suggesting ""Brazil's Economy Catches Its Breath"" (Delgado, 2010). Following this route of excitement, Dauvergne and Farias wrote a well-known piece called ""The rise of Brazil as a Global Development Power"" in 2012 (Dauvergne and Farias, 2012).
These, among other publications, reflected the projections created for Brazil's economic path: a path to be of success, development, growth, power, independence, and global influence. The first years of Rousseff's administration were filled with momentum to see Brazil step into the role of global leader. However, the end of 2013 served as presage to a large group of politicians, including Lula and Rousseff, that the years to come would not be so glorious. The economy had stagnated with an annual growth of 0.9 per cent (see figure 3) and the June 2013 protests demonstrated the population's dissatisfaction with the Worker's Party administration. With more than 2 million people taking over the streets, they protested against rampant increases in bus fares, inflation, corruption, and lack of investment in education and infrastructure. By the end of the year, the media that had responded so positively to previous PT years started to question Brazil's direction. One year away from the World Cup, the international press doubted whether Brazil was ready to host the global event (Vanegas, 2013). In September 2013, The Economist asked ""Has Brazil Blown It?"" Investment companies, such as Morgan Stanley, published reports suggesting that Brazil's golden economy was ""a tale from the emerging world"" (The Economist, 2013).
2014 and The Car Wash Operation
2014 arrived with many surprises. For some, 2014 was the year of excitement as Brazil would host the World Cup. For others, 2014 was the year of despair. On one side, the Public Prosecutor's Office (MP) and the Federal Police (PF) discovered what has become the largest corruption scandal in the history of Brazil. On the other side, the stagnation of the economy and the discontent with the government brought a sense of hopelessness that spread among Brazilians across all socioeconomic classes.
In 2014, an expansive investigation performed by the Public Prosecutor's Office was disclosed. Initiated in 2009, the Car Wash Operation, named after a network of gas stations and car washes that to managed illicit resources pertaining to a criminal organization in Brazil, investigated a Federal Deputy from the state of Paraná, accused of money laundering (MPF, 2016). The MP also investigated Alberto Youssef, a black market dollar dealer who was already familiar to the Federal Police. In 2013, telephone interceptions monitored conversations among several dollar dealers in order to obtain further information on the criminal organizations charged with money laundering (MPF, 2016). In July of that year, the Police learned about Youssef's car donation to a former Director of Petrobras, Paulo Roberto Costa (MPF, 2016). This discovery led the Police to suspect that these criminal organizations' network for money laundering was larger than ever thought. The investigations did not only involve politicians who had previously been involved in this kind of corruption scandals, but they also involved businessmen and contractor companies across the country.
As the Federal Police, along with the Public Prosecutor's Office, denounced the dimension and severity of the Car Wash Operation, the economy reacted in despair. The first phase of the Car Wash investigations happened in March 2014. In this phase, the Police along with the MP arrested 17 people, including dollar dealer Alberto Youssef and former Director of Petrobras, Paulo Roberto da Costa (MPF, 2016). Since then, political and economic turmoil arose in Brazil as the Operation would discover more people involved every single day. In April 2014, the scandal had escalated so deeply that a Parliamentary Inquiry Commission (CPI) on Petrobras was installed in the Senate to address issues pertaining solely to the corruption revealed in the Car Wash Operation (G1, 2016). In June, already in its fourth phase, the investigations revealed that most of the money laundered had been deposited into bank accounts in fiscal havens, such as Monaco and Switzerland. Among the owners of these accounts was Paulo Roberto da Costa (Borges, 2016). When invited to testify about the issue, nonetheless, Costa became known as a whistleblower. Speaking under a plea bargain, Costa explained how the laundering schemes worked. In addition, as he denounced major players in these schemes, he cited the involvement of President Dilma Rousseff (Borges, 2016; MPF, 2016). By September 2014, the Car Wash Operation confirmed that part of the money laundering was illegally used to fund 2010 political campaigns from major parties, including Rousseff's PT and VP Michel Temer's PMDB (Borges, 2016). By the end of the year, the Operation had reached the seventh phase, arrested 39 people, and investigated four major Brazilian contractor companies: OAS, Odebrecht, Camargo Correa, and Queiroz Galvão (Borges, 2016).
Even though 2014 came to an end, the political and economic turmoil present in Brazil continued to worsen. The economic indicators were not showing an optimistic scenario. In 2014, the economy grew only 0.1 percent while inflation reached 6.41 percent (see figures 2 and 3). Talks about a recession dominated the news, while frustration dominated people's conversations about the government. Mistrusting the Worker's Party administration, Brazilians feared that the 6.8 percent unemployment rate would skyrocket (Borges, 2016). In addition, as the dollar continued to quickly appreciate, they feared that the bust after the boom had arrived. The same media outlets that had partaken in Brazil's exhilaration around 2010 now expressed their woes. The Economist published ""Why Brazil Needs Change"" (The Economist, 2014) followed by ""Brazil In a Quagmire"" (The Economist, 2015). The Independent article said ""It's Chaos in Brazil but don't panic"" (Herbert, 2014) while Bloomberg announced early in 2015 ""The Betrayal of Brazil"" (Smith et al, 2015).
2015 and Instability Rises
Despite the Car Wash Operation and the frustration brought with it, the corruption scandal did not compromise the Worker's Party leadership. In November 2014, Dilma Rousseff was re-elected president after a runoff against the Brazilian Social Democracy Party (PSDB) candidate Aécio Neves with 51.6% of the votes (G1, 2014). Even though the Car Wash Operation did not compromise the Worker's Party ability to re-elect Rousseff as President in 2014, the scandal and the economic despair dominating Brazil at the time alarmed Brasilia that instability was on the rise. Early in January 2015, then President of Petrobras, Graça Foster, suggested that the company had lost R$ 88,6 billion due to the Car Wash Scandal (G1, 2015). In February, a former manager of operations of Petrobras revealed that the Worker's Party had received somewhere between US$ 150 to US$ 200 million in one bribery contract (MPF, 2016). A few months later, as the investigation reached its twelfth phase, Folha de São Paulo, a renowned newspaper in Brazil, published an interview with Ricardo Pessoa, a contractor CEO who claimed that his company had donated R$ 7.5 million to Dilma Rousseff's reelection campaign (MPF, 2016).
A famous saying in Brazil says that those who search for something, will eventually find it. This proverb was reiterated throughout all of the MP's investigations in 2015. The more the Car Wash Operation searched for corruption schemes, the more names it found and the more complex the network of money laundering actors was revealed to be. In 2015 alone, the investigation had fourteen phases, arresting more than forty people, condemning more than twelve, and recovering more than R$870 million (G1, 2016). Through these operations, the presidents of contractor companies such as Odebrecht were arrested. Politicians such as former Presidents of Brazil Fernando Collor and Luiz Inácio Lula da Silva, then-Presidents of the House and of the Senate, Eduardo Cunha and Renan Calheiros, respectively, and former Chief of Staff José Dirceu were denounced for being involved in major money laundering schemes.
If anything, 2015 was the year that shook Brasilia. Brazil's political ""crème de la crème"" was shaken. Most parties were involved; every politician was pointing fingers. The German newspaper Die Zeit compared the political intrigues in Brazil to those in House of Cards (Fischermann, 2016). In addition to the political turmoil, economic instability also emerged. Economic indicators demonstrated that the economy had contracted by 3.8 per cent in 2015, while inflation had escalated to 10.67 per cent (see figures 3 and 4). The Brazilian Real had been severely depreciated as the exchange rate against the US dollar appreciated to 3.95943 (see appendix, Fig. I). Unemployment reached 9 per cent, the highest level in the last four years (see figure 5). Likewise, the federal debt also broke the administration's record reaching almost R$ 2 trillion (see appendix, Fig. III). In tandem, the political and economic instability that dominated Brazil in 2015 set the tone for drastic changes in the year to come.
2016 and Rousseff's Fall
The economic and political instability present in Rousseff's second term extended to 2016 as well – so much so that her presidency was compromised. The Car Wash Operations continued to investigate major networks of politicians and contractor companies involved with Petrobras and its money laundering schemes. By the time this paper was written, The Public Prosecutor's Office, along with the Federal Police, has established 1,397 procedures, conveyed 654 searches and seizures, 77 preventive arrests, 92 temporary arrests, and 6 in flagrante delicto. The Operation has received 52 criminal charges against 254 people for crimes of corruption, drug-trafficking, formation of criminal organization, and money laundering (MPF, 2016). These crimes refer to briberies that sum up to approximately R$ 6.4 billion, of which R$3.1 billion are to be recovered. Up to December 2016, the Car Wash Operation has condemned 118 people, accounting for more than 1,256 years of prison in total (MPF, 2016).
Different from the previous two years, nonetheless, the findings on the Petrolão compromised the Worker's Party administration like never before. As major members of the Party were arrested and indicted of money laundering crimes, Brazilians grew disillusioned with their political leadership. Just like in June 2013, the population returned to the streets demanding Rousseff to leave her office. Economic mismanagement caused inflation to increase to 10.7 per cent, and unemployment to reach 11 per cent. Tired of corruption and frustrated with the lack of investments in the country, people demanded change.
In May 2016, the Senate opened the process of impeachment against Rousseff. According to jurists Janaina Paschoal, Hélio Bicudo, and Miguel Reale Jr., Rousseff had violated the Fiscal Responsibility Law (G1, 2016). They claimed she had authorized R$2.5 billion of additional expenses between July and August 2015. According to the Law, such authorization could not have happened since government expenditures did not match the fiscal goals for the year, especially since the administration was committed to increase its savings to pay the public debt (G1, 2016). Rousseff was also charged for borrowing money from other federal institutions such as the Central Bank and the Brazilian Development Bank (BNDES) to finance her government's social programs in 2015, as cash transfer Bolsa Familia and Plano Safra (G1, 2016). This is known as a ""fiscal pedal"" in Brazil, as the government seeks to disguise a breakdown in the public accounts. However, as the judges highlighted, ""fiscal pedals"" infringe the Fiscal Responsibility Law, which forbids the government from borrowing money from public banks that are under the Executive branch (G1, 2016).
With Rousseff temporarily removed from office for three months, Rousseff's VP, Michel Temer, was put in charge of the interim presidency. Forbes reported that for the first time since 2009, Brazil's current account went into surplus. The market reacted positively with an appreciation of the Brazilian Real (Rapoza, 2016). As Temer appointed his cabinet, he carefully selected conservative technocrats who would take Brazil to a route much different from that during the PT years. So much so that into two weeks after Dilma's temporary leave, Temer's Finance Minister Henrique Meirelles already proposed cuts of more than 2 per cent of the GDP (Rapoza, 2016).
During the interim months, political instability was followed by acute polarization. On one side, scholars, mainstream civilians, politicians, and the press would argue that it was time for Dilma and her crew to go (The Economist, 2016). On the other side, people would argue that Rousseff's impeachment was a political coup orchestrated by the Brazil's Rightist elite. Despite disagreements, the Senate voted 61 to 20 to impeach Rousseff, convicting her of infringing the Fiscal Responsibility Law. Then-interim head of state, Mr. Michel Temer took over Brasilia, where he will preside until 2018 (Romero, 2016).
Impacts on the Economy
Measuring the economic impacts of the Car Wash Operations as well as Rousseff's impeachment is not an easy task. An anonymous officer at the Brazilian Embassy in Washington DC suggested that the consequences brought by the economic and political instability in Brazil could by no means be quantified. For her, ""the corruption scandal in Brazil removed the cork from the bottle. It served to shine light on larger structural obstacles that the country already faced both economically and politically."" While I agree with the officer's argument that Brazil's de facto struggle refers to structural issues rather than corruption alone, I disagree that the economic impacts of the Car Wash Operation, as well as the impeachment, cannot be quantified. In this section, I will address the economic indicators that I personally monitored at the Embassy of Brazil, as well as some scholarly arguments, as an endeavor to measure the impact of both events on the Brazilian economy.
The Car Wash Operation contributed to the economic and political instability present in Brazil, but it was not the protagonist factor. The Operation, with the help of the press, was able to reveal the expansive network of corrupted politicians across Brazil. Such revelation did not give room for people's political complacency. For the first time, a corruption scandal compromised political parties and their members. In this way, the corruption scandal contributed to the political instability in the country. In addition, the scandal also played a role impacting the economic instability in Brazil. As corruption schemes and political instability were revealed to the public, investments decreased severely. Compromising Brazil's credibility, the Car Wash Operation and the impeachment spillovers detracted foreign investment. As Figure 1 shows, from 2014 to 2016, FDI decreased by almost US$20 billion. Moreover, domestic investments also decreased. The continuous increase in the interest rate has prevented investors from borrowing money (see appendix, fig. IV), and the involvement of major contractor companies in the corruption scandals have left no one to work on big infrastructure projects in the country. According to consultancy firm Tendências, domestic investments shrunk 6 per cent in 2016 alone (Ribeiro and Cortez, 2016).
Figure 1: Inflow of FDI in Brazil from 1994 to Present in US$ billions Source: World Bank Data
Besides investments, the Car Wash Operations as well as the impeachment impacted Brazil's GDP. Figures 1 and 2 demonstrate how the economy shrunk since 2013. Figure 2 in particular show how the economy contracted, thus taking the country into a severe recession. Alessandra Ribeiro, from Tendências, has argued that the Petrolão is a factor that potentialized the contraction of the GDP. For her, two out the 3.8 per cent decline in economic growth relates back to the scandals (Ribeiro and Cortez, 2016).
Figure 2: Brazil's Gross Domestic Product from 2000 to 2015 in current US$ millions Source: BACEN
Figure 3: Brazil's economic growth from 2000 to 2016 Source: BACEN
A decline in economic activity came in tandem with a rise in inflation as well as in the unemployment rate. In this case, the impacts of the two events analyzed might be indirectly related, nonetheless still present. The mismanagement of the economy has caused inflation to reach almost 11 per cent in 2015. This inflation, followed by the lack of investments, and the decline in economic activity may have decreased the number of jobs available in the market. Figures 4 and 5 show the price fluctuation as well as the unemployment rate in the last years. For Bruno Lavieri, ""blaming the Car Wash Operation [for unemployment and stagnation] is like blaming the doctor for finding his patient's disease."" However, as GO Associates have argued, this indirect impact of the investigations have costed more than 2 million jobs in two years due to the lack of infrastructure projects with Petrobras and contractor companies (UOL, 2016). Petrobras alone composed 13 per cent of the economic activities in Brazil (Petrobras, 2016). According to Getúlio Vargas Foundation, ""the decrease in Petrobras' activities due to the Car Wash Operations could take away R$7 billion from the economy, lead to the loss of more than 1 million vacancies and a R$ 5.7 billion fall in the collection of taxes by the Union, states and municipalities in 2015"" (UOL, 2016).
Figure 4: Inflation levels in Brazil from 1995 to 2016. Source: IBGE
Figure 5: Unemployment rate of persons 14 years or older from 2012 to present in Brazil. Source: IBGE
Although the impeachment and the corruption scandal did not singlehandedly cause Brazil's economic recession, they shone a light on major existent structural problems such as corruption, fiscal pedals, lack of investments, and poor allocation of resources. Moreover, both events indirectly contributed, even though did not cause, the economic crisis. The corruption scandal influenced the decline in economic activities and lack of investments in the country, which subsequently increased the unemployment rate. In addition, inflation became rampant and political instability remained. With that, government expenditures increased and private savings decreased significantly, as Figure 6 reveals.
Figure 6: Composition of Brazil's GDP in terms of private consumption, current account balance, government expenditure, and private savings. Source: World Bank Data
Conclusion
With Michel Temer's administration comes new challenges. Foreign and domestic investors remain skeptical of Brazil's economic and political situation. Unemployment and inflation remains high, investments remain low, and the Brazilian Real continues depreciated. Since his inauguration, Temer dedicated most of his time and effort to get Bill 241 approved in Congress. This Bill proposes to reduce public spending and to balance the public accounts by freezing government expenditures in the next twenty years (Alessi, 2016). However, getting Bill 241 approved is not everything. President Temer will have to address several other factors such as Social Security and Tax reforms, unemployment rate, social 49 agenda, and Congress polarization to be able to recover from the economic and political instability that the Car Wash Operations shone a light to.
Author
Flávia Bedicks is a student of Economics and International Studies. She graduates in May of 2017. College of Arts & Sciences (CAS) and School of International Service (SIS), American University. Email: fb4631a@american.edu
References
""A Lava Jato Em Números – Caso Lava Jato"". 2016. Ministério Público Federal. http://lavajato.mpf.mp.br/atuacao-na-1a-instancia/resultados/a-lava-jato-em-numeros-1.
Alessi, Gil. 2016. ""Entenda O Que É A PEC 241 E Como Ela Pode Afetar Sua Vida"". El País. http://brasil.elpais.com/brasil/2016/10/10/politica/1476125574_221053.html.
Amaral, Ricardo Batista. 2011. A Vida Quer É Coragem. 1st ed. Rio de Janeiro: Primeira Pessoa.
Banco Central do Brasil (BACEN). 2016. Indicadores Econômicos. Brasília: Banco Central do Brasil.
Besta, Shankar. 2011. ""Brazil Unemployment Drops To Record Low In 2010"". International Business Times. http://www.ibtimes.com/brazil-unemployment-drops-record-low-2010-260209.
Borges, João. 2016. ""Linha Do Tempo – Lava-Jato"". Cbn.Globoradio.Globo.Com. http://cbn.globoradio.globo.com/grandescoberturas/operacao-lava-jato/2016/02/23/LINHA-DOTEMPOLAVA-JATO.htm.
""Brazil In A Quagmire"". 2015. The Economist. http://www.economist.com/news/leaders/21645181latin-americas-erstwhile-star-its-worst-mess-early-1990s-quagmire.
""Brazil Takes Off"". 2009. The Economist. http://www.economist.com/node/14845197.
""Brazil / U.S. Foreign Exchange Rate"". 2016. FRED. https://fred.stlouisfed.org/series/DEXBZUS.
""Cálculo Havia Apontado Perda De R$ 88,6 Bilhões, Segundo Graça Foster"". 2015. Globo G1. http://g1.globo.com/economia/negocios/noticia/2015/01/calculos-havia-apontado-perda-de-r886-bilhoes-segundo-graca-foster.html.
Dauvergne, Peter, and Déborah BL Farias. 2012. ""The rise of brazil as a global development power."" Third World Quarterly 33 (5): 903-17.
Delgado, Bertrand. 2010. ""Brazil's Economy Catches Its Breath"". Forbes. http://www.forbes.com/2010/09/08/brazil-economy-gdp-opinions-columnists-doctor-doom.html.
""Dilma É Reeleita Presidente E Amplia Para 16 Anos Ciclo Do PT No Poder"". 2014. Globo G1. http://g1.globo.com/politica/eleicoes/2014/noticia/2014/10/dilma-e-reeleita-presidente-eampliapara-16-anos-ciclo-do-pt-no-poder.html.
""Dívida Pública Federal, STN"". 2016. Tesouro Nacional: Ministério da Fazenda. http://www.tesouro.fazenda.gov.br/divida-publica-federal.
Fischermann, Thomas. 2016. ""Brasilien: Intrigen Wie Bei ""House Of Cards"""". Zeit. http://www.zeit.de/politik/ausland/2016-03/brasilien-opposition-dilma-roussef-luiz-inacio-luladasilva.
Hakim, Peter. 2010. ""Brazil on the rise: The challenges and choices of an emerging global power."" Politica Externa 19 (1).
""Has Brazil Blown It?"". 2013. The Economist. http://www.economist.com/news/leaders/21586833-stagnant-economy-bloated-state-andmass-protests-mean-dilmarousseffmust-change-course-has.
Herbert, Ian. 2014. ""World Cup 2014: It's Chaos In Brazil – But Don't Panic"". The Independent. http://www.independent.co.uk/sport/football/international/world-cup-2014-itschaosin-brazil-but-dont-panic-9210299.html.
""IBGE: Instituto Brasileiro De Geografia E Estatística"". 2016. Ibge.Gov.Br. http://www.ibge.gov.br/home/estatistica/indicadores/precos/inpc_ipca/ipca-inpc_201610_3.shtm.
""Linha Do Tempo Da Lava Jato"". 2016. G1. http://especiais.g1.globo.com/politica/2015/lavajato/linha-do-tempo-da-lava-jato/.
""Linha Do Tempo – Caso Lava Jato"". 2016. Ministério Publico Federal. http://lavajato.mpf.mp.br/atuacao-no-stj-e-no-stf/linha-do-tempo/todas-noticias.
""Lula Está Certo Ao Dizer Que Lava Jato Afeta Economia? Analistas Avaliam"". 2016. UOL Economia. http://economia.uol.com.br/noticias/redacao/2016/04/03/lula-esta-certo-ao-dizerquelava-jato-afeta-economia-analistas-avaliam.htm.
""Participação Do Setor De Petróleo E Gás Chega A 13% Do PIB Brasileiro"". 2016. Petrobras. http://www.petrobras.com/pt/magazine/post/participacao-do-setor-de-petroleo-egaschega-a-13-do-pib-brasileiro.htm.
""Processo De Impeachment É Aberto, E Dilma É Afastada Por Até 180 Dias"". 2016. Globo G1. http://g1.globo.com/politica/processo-de-impeachment-de-dilma/noticia/2016/05/processo-de-impeachment-e-aberto-e-dilma-e-afastada-por-ate-180-dias.html.
Rapoza, Kenneth. 2016. ""Global Investors Put Brazil's Interim President Michel Temer On Notice"". Forbes. http://www.forbes.com/sites/kenrapoza/2016/06/02/global-investors-putbrazilsinterim-president-michel-temer-on-notice/2/#7797e89e5f36.
""Relembre A Trajetória Política De Dilma Rousseff"". 2016. G1 Política. http://g1.globo.com/politica/processo-de-impeachment-de-dilma/noticia/2016/05/relembre-trajetoriapoliticade-dilma-rousseff.html.
Ribeiro, Alessandra and Rafael Cortez. 2016. ""Riscos Para A Recuperação"". Tendências Consultoria. http://www.tendencias.com.br/news.cgi?id=114.
Romero, Simon. 2016. ""Dilma Rousseff Is Ousted As Brazil'S President In Impeachment Vote"". The New York Times. http://www.nytimes.com/2016/09/01/world/americas/brazil-dilmarousseffimpeached-removed-president.html.
Savarese, Maurício and Carlos Bencke Bencke. 2016. ""Dilma É Eleita Primeira Mulher Presidente Do Brasil"". UOL. http://eleicoes.uol.com.br/2010/ultimas-noticias/2010/10/31/dilma-eeleitaprimeira-presiDente-mulher.jhtm.
Smith, Michael, Sabrina Valle, and Blake Schmidt. 2015. ""The Betrayal Of Brazil"". Bloomberg. http://www.bloomberg.com/news/features/2015-05-08/brazil-s-massive-corruption-scandalhasbitterness-replacing-hope.
""Tales From The Emerging World"". 2013. Morgan Stanley. https://www.morganstanley.com/public/Tales_from_the_Emerging_World_Fragile_Five.pdf.
Tavares, Monica. 2016. ""Dilma Deixa Conselho Da Petrobras"". O Globo. http://oglobo.globo.com/economia/dilma-deixa-conselho-da-petrobras-3036516.
""Time To Go"". 2016. The Economist. http://www.economist.com/news/leaders/21695391-tarnishedpresidentshould-now-resign-time-go.
Vanegas, Maria. 2013. ""Is Brazil Ready To Host The World Cup? A Look At South America's Most Stable Economy"". Latin Times. http://www.latintimes.com/brazil-ready-host-world-cuplooksouth-americas-most-stable-economy-133187.
""Why Brazil Needs Change"". 2014. The Economist. http://www.economist.com/news/leaders/21625780-voters-should-ditch-dilma-rousseff-and-elect-cio-neves-why-brazilneedschange.
""World Bank Indicators, Brazil"". 2016. World Bank Data. http://data.worldbank.org/country/brazil.
Appendix
Figure I: Price of US dollars in Brazilian Reais from 2011 to Present. Source: FRED
Figure II: Brazil's Public Debt in R$ millions from 2002 to 2016. Source: Tesouro Nacional
Figure III: Brazil's Public Debt as a Percent of its GDP from 2002 to Present. Source: Tesouro Nacional
Figure IV: Brazil's monthly Interest Rate levels from 2014 to 2016. Source: BACEN
Glossary"	34094
economy	['Tan', 'Aik Seng']	2016-05-22 00:00:00	"Introduction

Economic regionalism has been an observable phenomenon worldwide. Many countries around the world pursue some degree of economic integration with neighbouring countries, in the hopes of capitalizing on the benefits of such an arrangement. At the turn of the 21st century, there already existed various regional economic institutions, including the highly integrated Eurozone in Europe and the East African Community (EAC) for continental Africa. This is in addition to a proliferation of bilateral free trade agreements (FTAs) between countries, which can be regional or cross regional in nature. Many Asian countries, including South Korea, China and Singapore, have also been active participants in these economic processes, with many FTAs signed intra and inter-region. The Association of Southeast Asian Nations (ASEAN),2 recognizing the preponderant benefits that can be derived from trade ties, have been among the most vocal advocates of economic regionalism, an institution that goes beyond bilateral FTAs. Unlike bilateral FTAs, regional institutions include more than just two parties, thereby enabling more countries to benefit from trade together. Against this backdrop, the ASEAN Economic Community (AEC) is currently being implemented incrementally by all member states, according to a chronological roadmap. The AEC is expected to deliver substantial economic gains to all member countries, including increased access to regionally produced goods, better allocation of capital resources and overall improvement of economic well-being to the people in ASEAN. Yet, the creation of the AEC did not come without its obstacles. Much effort was – and continues to be channeled towards coordination, alignment of interests and resolution of conflicts. As is encountered even in the Eurozone and the EAC mentioned earlier, all regional economic institutions have to overcome various establishment and maintenance problems that can hinder the efficacy and effectiveness of such institutions. Thus far, the AEC appears to be coping with these barriers well. If it can do so successfully, the economic benefits accrued to member countries will be considerable.

Economic integration provides promise of improving societal welfare through market mechanisms, but also closer inter-state ties and improved intra-regional peace and stability. Much scholarship has been devoted towards exploring the political difficulties that undergird economic integration. Tensions from historical or ideological issues have been attributed as problems that hinder cooperation.3 Therefore, a common line of argumentation could hold that resolving these political problems must occur prior to cooperation between these states. However, evidence on this count is inconclusive. For example, the United States (U.S) and China were able to set aside ideological differences, despite decades of tension, in favour of cooperating against the Soviet Union. This implies that the tangible benefits from economic integration could provide the motivation to isolating or resolving major political differences. If there are clear material benefits at stake, as in the Soviet threat of the 1970s, there are reasonable grounds to posit that states may be willing to pursue self-interest before considering less immediate and non-threatening problems, such as ideology. As this paper will show, the AEC is a powerful force promoting peace in the Southeast Asian region. Greater labour mobility within this region, coupled with the material gains from collective economic growth are major stabilizers that mitigate unnecessary conflict. In other words, national behaviour increasingly adapts to ensure that economic development is prioritized. If a government cannot guarantee its people security in their basic material needs, then there can be no foundation to pursuing higher-level, abstract goals such as ideological legitimization.

Assuming the logic of economic integration holds true, Northeast Asia stands to profit immensely if the region creates a similarly structured economic institution. As such, this paper argues that Northeast Asia's ability to increase economic integration's viability can be greatly enhanced by understanding how ASEAN has approached the creation of the AEC. There are both economic and political-security benefits which can greatly enhance the attractiveness of creating a ""Northeast Asian Economic Community"" (NAEC). In the first section of this paper, I perform a case study of ASEAN to better understand the actual and potential benefits of the AEC's creation, followed by a distillation of lessons that can be learnt, pertaining to how major barriers to successful economic integration have been and are still being -mitigated. The second section will then proceed to examine the benefits of forming the NAEC and suggest how, by way of the ASEAN experience, significant barriers to economic integration can be overcome to increase the possibility of successfully creating the envisaged NAEC. The final part of this paper will articulate the possibility of a more encompassing East Asian economic community, contingent on the success of the AEC and NAEC.

I. The ASEAN Way to Economic Integration

Historically, Southeast Asia was a major node in the international trade network, exposed to major movements of goods between different nations around the world. Following post-colonialism, however, many member countries turned to import-substitution industrialization in order to develop their economies. In recent decades, more countries have increasingly embraced the idea of free trade, persuaded by the benefits that it can provide. The ASEAN Economic Community (AEC) was conceptualised in 2003, following the meeting of ASEAN headsofstate in Indonesia, and is part of a larger concept of regional integration.4 The AEC aims to create a common economic platform that will facilitate the participation and trade of ASEAN member countries. Its main pillars are: 1) a single market and production base; 2) a competitive economic region; 3) equitable economic development; and 4) integration into the global economy.5 With the integration of the AEC, the total value of the economic market is estimated at US$2.3 trillion.6 As a large market with considerable promise, the AEC is an attractive region for robust economic growth. It is in this regard that many countries have been convinced to participate in the AEC, as the tangible and rational benefits accrued to each country will go a long way in promoting the economic health and societal welfare of their respective countries.

How the AEC benefits its member countries

First, the AEC provides countries with incentives to align their production bases according to their comparative advantages. For many Southeast Asian countries, most countries retain a large agrarian base, with only a few having successfully transitioned to a more capital-intensive economy.7 For example, Singapore and Malaysia are the leading economies in the region and both have considerable infrastructure tailored to meet the needs of a more knowledge-based, high-value economy. This is aligned with their comparative advantages, especially so for Singapore. Public universities, such as the National University of Singapore and Nanyang Technological University, have consistently been recognized for academic excellence in various world rankings, while other technical and vocational training institutes provide Singaporeans with the needed skills that employers from MNCs look for.8,9 As a result, Singapore in general has emerged as a strong and competitive economy. On the other hand, the Philippines, with a large agrarian base, is presently much poorer than Singapore. Without specialization, it will be difficult for the Philippines to achieve economies of scale via large-scale efficient production of goods according to their comparative advantage in primary products, such as agricultural goods.10 The AEC aims to address this by encouraging economies to re-align according to their strengths. In doing so, other trading partners within the AEC will benefit from lower prices, thereby enhancing consumer welfare.

Second, the AEC effectively creates an integrated market for 611 million consumers.11 A large consumer base will be advantageous to many firms located within member countries, as their product reach will be greater than before. Where successful integration without significant trade barriers prevails, social welfare improves as raw material and final product prices are not unnecessarily distorted. It is little wonder, therefore, that many countries are receptive of the AEC since this will increase both domestic consumption and export expenditure, enhancing the interest of their respective business sectors. Consumers in the AEC will be exposed to even more product choice, variety and nature at a higher level of quantity and quality. Already, benefits have been apparent. For example, Jollibee, a fast food restaurant chain from the Philippines, has successfully expanded its operations into nine of the ten ASEAN nations as a result of lower trade and investment barriers. Today, its operations are worth a total of US$866 million.12 The presence of a large market is also effective in attracting foreign direct investment from other parts of the world. This sentiment was captured in a survey by the US Chamber of Commerce, which polled and discovered that 54% of American firms have plans that involve expanding into ASEAN.13 Such investments can go a long way in strengthening the regional economy even further, thereby enhancing the overall returns to integration. Specifically, more jobs can be created and economic development achieved. Effectively, consumers will benefit alongside business and every country is moved along in this cycle of economic prosperity.

Third, the AEC's success can promote greater regional peace and ameliorate security-related challenges. Economic competition has often been implicated in political conflicts among states (Le Billon, 2004). The presence of the AEC provides all states with positive security externalities, specifically, a buffer against unnecessary conflict caused by economic fundamentals. Under conditions of shared economic growth and prioritized economic cooperation, the benefits of a successful AEC will be enjoyed by all ASEAN states. Fostering an economic community can also provide additional benefits to regional peace, following economic liberalism. According to economic interdependence theory, states that form a network of cross-border cutting economic ties will be more disposed towards peace and less towards conflict. If the theory is true, the presence of the AEC will be a major driver of regional peace in ASEAN. While no studies have been conducted in ASEAN yet, economic interdependence theory tentatively has some support from research conducted by Russett and Oneal, who established the presence of a positive relationship between trade and peace.14 This is salient, as ASEAN leaders such as Singapore's Prime Minister Lee Hsien Loong have identified growing nationalism as a threat to regional stability.15 Integration in the economic sphere, in turn, can serve as a viable launch pad for better and more stable political and social relations across the region. Building a stable and powerful AEC will be advantageous to promoting peace in Southeast Asia, a goal that is desirable in the context of greater political conflict among East Asian states. This being achieved, peace in Southeast Asia will be a good base to enhance peace in the greater East Asia region.

Lessons from the ASEAN Experience

Following the above, it is apparent that economic cooperation is desirable as it provides economies access to a larger market, fosters economic competitiveness and carries positive security externalities. Together, these attractive ""push"" factors encourage states to seek out such an arrangement. Against this backdrop, there is cautious optimism that Northeast Asia can achieve similar results, if a similar economic community can be mooted. Should major economies such as China, Taiwan, Japan and South Korea come together to form a coherent economic multi-lateral institution, there is a high chance that these countries can enjoy the economic benefits that AEC member states will share. Yet, such communities are not naturally formed, owing to various barriers that can discourage countries from associating into an economic community. This section will enumerate three major difficulties associated with national-level economic organizations. Every multi-member organization is prone to conflict among constituent members owing to different priorities, norms and domestic needs. ASEAN is no stranger to these issues. Specifically, the issues under discussion are: 1) the dumping of excess produced goods in other countries' markets; 2) the extent of protectionism that hinders full economic integration; and 3) undesirable labour-side effects caused by labour mobility.

Dumping

Countries able to manufacture cheaper goods tend to export such goods overseas upon saturation of the local market, leading to greater competition and declining profitability of recipient country firms. This condition, known as dumping, is a recurrent problem found in economic zones with members being in different stages of economic development.16 Assuming free trade and a country X with a strong comparative advantage in good A, compared to countries Y and Z, the latter two countries will experience an influx of good A from X, leading to an outcompeting of domestic producers in Y and Z who lack the level of efficiency needed to provide similar goods at more competitive rates. This has been a problem faced in various regions, whereby the movement of goods according to comparative advantage has meant a decrease in earnings from domestic industries, who are unable to compete with more efficient producers from other countries. As a result, this phenomenon of dumping causes domestic producers to be squeezed out of the production market, leading to negative effects on the economy's production capacity as a whole. Of greater salience in politics, the marginalization of local sectors in business can foment greater resentment and demands placed on the government to protect their interests better. Should the government heed this call and respond in favour of their businesses, the resultant situation can be detrimental to the overall fabric of the regional outfit writ-large.

In ASEAN, this problem of dumping has been averted by negotiating and re-aligning comparative advantages while embracing the overall enhancing effects on consumer surplus. The ASEAN institution leverages greatly on such conciliatory negotiation mechanisms that aim to produce consensus among member states.17 This minimizes any consequent disagreements that can impede successful policy implementation. According to an ANZ Bank analyst, the ASEAN countries will likely structure their economies to match their resource endowments, with Singapore and Malaysia developing ""into service and financial hubs for the region"" while Indonesia, the Philippines, Thailand and Vietnam move into middle-end manufacturing, while Myanmar, Laos and Cambodia, will leverage on its physical resources for lower-value manufacturing.18 This indicates that no particular industry will see inefficient and large-scale production of any goods or services. Instead, countries largely avoid the narrow concentration of industrial efforts on similar products and focus on their niche areas of production. Hence, this reduces any surplus production needing to be ""dumped"" in regional markets.

Protectionism

Economic integration can be greatly hindered when member countries refuse to abide by agreed upon regulations permitting movement of goods and services across borders. This is commonly done by raising barriers to mobility, a practice officially known as protectionism. Traditional protectionist measures include tariffs and quotas on foreign imports, while modern protectionist measures typically involve regulations on product content requirements.19 Countries may adopt protectionism so as to protect their domestic industries from more efficient foreign competitors, which may have repercussions on domestic employment. Another reason for protectionism may be to pander to business groups with powerful influence and ties with politicians.20 Such businesses understandably do not wish to see an erosion of profits, which competition brings about generally. Still, despite such justifications, protectionist measures militate against the creation of an integrated and accessible economic community. These barriers prevent free flow of goods and services from one country to another, contradicting the basic tenet of economic integration.

ASEAN countries have tackled this problem by collaboratively building consensus about the disavowing of protectionism. This was embodied most clearly in the communique released after the ASEAN Summit in 2015, whereby ASEAN affirmed its commitment to eliminating trade tariffs, of which nearly 96% have been eliminated.21 Incrementally, every country has been reducing restrictions on competition in protected sectors such as banking, which is a major source of revenue for economies. The commitment to protectionism can be seen in agreements signed within the AEC, including the ASEAN Comprehensive Investment Agreement (ACIA). The ACIA institutionalizes each country's commitment to liberalize their business sectors and this has ""eased restrictions to cross-border trade.""22 This is supplemented by the ASEAN Trade in Goods Agreement, a ""legal framework to realize the free flow of goods within the AEC."" Consequently, much progress has been made in the AEC, whereby more than ""70% of intra-regional trade"" now has zero tariffs across 80 sub-sectors.23 With more time given, the rate of further liberalization is expected to increase given the continued commitment of political leaders to the AEC's cause.

Undesirable labor effects

Labor movement can be both a source of benefit and conflict to the receiving society, which may not always embrace foreigner inflows. This poses a formidable obstacle to any plans for full-scale economic integration, since any failure to permit free labour flow can go against a fundamental tenet of a shared economic community. In theory, free labour flow is beneficial because it can correct any asymmetry in the labour market by ensuring that workers move to where they are in demand from places where they are currently unemployed due to undersupply of jobs.24 Furthermore, these workers are likely to have received education and training in their home countries prior to movement to other countries, being a big advantage to the latter. Such prior training greatly reduces the need to equip these workers with basic skills, a source of cost savings for the receiving country. However, labour factors, despite their advantages for the receiving economy, interact with socio-cultural cleavages at the same time. Locals may perceive foreigners to be taking their jobs, fomenting greater resentment as a result. How have affected ASEAN countries dealt with this? To provide a more detailed analysis, here this paper examines a single case, Singapore, in detail.

Being an advanced economy, Singapore is attractive to many foreign workers around the world. This ranges from lower-value employment such as in construction and the service industry, of which most workers tend to be from neighbouring ASEAN countries and South Asia, to high-value employment in sectors such as banking and Information Technology (IT).25 Such workers can come from any part of the world, with the critical distinction being their professional qualifications and experience. Due to rapid development, alignment with comparative advantage, and a small labour force, Singapore has relied on foreigners to fill gaps in the various sectors, with most of them found in the lower-value sectors.26 The reason for this is that, since Singapore's only valuable resource endowment is human capital, such capital must be upgraded to be of higher value in order to sustain the economy's development.27 As such, labour shortages are found more in the sectors of construction and ancillary public services such as cleaning.

However, Singaporean society has not taken well to such measures and has displayed occasional resistance to the foreign labour policy of the government. This has sometimes translated into strong pressure on elected politicians to manage the policy better by slowing down the rate of uptake. As society's support is essential to any public policy, the state is unable to ignore public opinion. To this end, Singapore has been more careful and calibrated in its approach to foreign worker inflows. There are categories of employment that require different skill sets, and are open to different nationalities.28 In labour intensive sectors, the government does not appear to be intervening extensively, since such labour flows are transient and do not significantly pose problems to society. For the professional sectors, depending on deemed merit and worthiness, foreigners are eligible for Permanent Resident status or Singaporean citizenship itself. It is in this domain that discernible change is apparent, with majority beneficiaries being other ASEAN countries, while those from South Asia and other regions are in the minority. In 2013 for example, 55.3% of new PRs were from ASEAN countries while 34.6% came from other Asian countries such as India and China.29

On the societal frontier, the government has become more active in its drive to clarify the misconceptions related to foreign workers taking jobs away from locals, backed up with actual policies to lend such messages more credence. For example, companies must list openings for certain jobs one month prior to opening them up to foreigners.30 In addition, the number of foreign workers allowed for hire by a company is determined by a pre-set ratio of local to foreign labour.31 This means that firms must hire local workers before they can be allowed to hire foreigners. Together, these policies aim to assuage public concerns by showing that the government is monitoring labour inflows and ensuring no undesirable side effects on society writ large. This is a strategy that dovetails with larger AEC goals. Singapore has continued to privilege AEC partners in its foreign labour policies, as seen in the above proxy indicator of majority ASEAN PR and citizenship grants, supporting the AEC's eventual ideal of free labour mobility. In this regard, there are positive steps being taken to ensure that, despite societal resistance, Singapore's cooperation and compliance with AEC goals are not compromised, but have been managed in order to ensure smooth policy development and acceptance.Continued on Next Page »"	http://www.inquiriesjournal.com/articles/1512/the-asean-experience-northeast-asia-and-beyond-free-trade-and-economic-integration	"Introduction
Economic regionalism has been an observable phenomenon worldwide. Many countries around the world pursue some degree of economic integration with neighbouring countries, in the hopes of capitalizing on the benefits of such an arrangement. At the turn of the 21st century, there already existed various regional economic institutions, including the highly integrated Eurozone in Europe and the East African Community (EAC) for continental Africa. This is in addition to a proliferation of bilateral free trade agreements (FTAs) between countries, which can be regional or cross regional in nature. Many Asian countries, including South Korea, China and Singapore, have also been active participants in these economic processes, with many FTAs signed intra and inter-region. The Association of Southeast Asian Nations (ASEAN),2 recognizing the preponderant benefits that can be derived from trade ties, have been among the most vocal advocates of economic regionalism, an institution that goes beyond bilateral FTAs. Unlike bilateral FTAs, regional institutions include more than just two parties, thereby enabling more countries to benefit from trade together. Against this backdrop, the ASEAN Economic Community (AEC) is currently being implemented incrementally by all member states, according to a chronological roadmap. The AEC is expected to deliver substantial economic gains to all member countries, including increased access to regionally produced goods, better allocation of capital resources and overall improvement of economic well-being to the people in ASEAN. Yet, the creation of the AEC did not come without its obstacles. Much effort was – and continues to be channeled towards coordination, alignment of interests and resolution of conflicts. As is encountered even in the Eurozone and the EAC mentioned earlier, all regional economic institutions have to overcome various establishment and maintenance problems that can hinder the efficacy and effectiveness of such institutions. Thus far, the AEC appears to be coping with these barriers well. If it can do so successfully, the economic benefits accrued to member countries will be considerable.
Economic integration provides promise of improving societal welfare through market mechanisms, but also closer inter-state ties and improved intra-regional peace and stability. Much scholarship has been devoted towards exploring the political difficulties that undergird economic integration. Tensions from historical or ideological issues have been attributed as problems that hinder cooperation.3 Therefore, a common line of argumentation could hold that resolving these political problems must occur prior to cooperation between these states. However, evidence on this count is inconclusive. For example, the United States (U.S) and China were able to set aside ideological differences, despite decades of tension, in favour of cooperating against the Soviet Union. This implies that the tangible benefits from economic integration could provide the motivation to isolating or resolving major political differences. If there are clear material benefits at stake, as in the Soviet threat of the 1970s, there are reasonable grounds to posit that states may be willing to pursue self-interest before considering less immediate and non-threatening problems, such as ideology. As this paper will show, the AEC is a powerful force promoting peace in the Southeast Asian region. Greater labour mobility within this region, coupled with the material gains from collective economic growth are major stabilizers that mitigate unnecessary conflict. In other words, national behaviour increasingly adapts to ensure that economic development is prioritized. If a government cannot guarantee its people security in their basic material needs, then there can be no foundation to pursuing higher-level, abstract goals such as ideological legitimization.
Assuming the logic of economic integration holds true, Northeast Asia stands to profit immensely if the region creates a similarly structured economic institution. As such, this paper argues that Northeast Asia's ability to increase economic integration's viability can be greatly enhanced by understanding how ASEAN has approached the creation of the AEC. There are both economic and political-security benefits which can greatly enhance the attractiveness of creating a ""Northeast Asian Economic Community"" (NAEC). In the first section of this paper, I perform a case study of ASEAN to better understand the actual and potential benefits of the AEC's creation, followed by a distillation of lessons that can be learnt, pertaining to how major barriers to successful economic integration have been and are still being -mitigated. The second section will then proceed to examine the benefits of forming the NAEC and suggest how, by way of the ASEAN experience, significant barriers to economic integration can be overcome to increase the possibility of successfully creating the envisaged NAEC. The final part of this paper will articulate the possibility of a more encompassing East Asian economic community, contingent on the success of the AEC and NAEC.
I. The ASEAN Way to Economic Integration
Historically, Southeast Asia was a major node in the international trade network, exposed to major movements of goods between different nations around the world. Following post-colonialism, however, many member countries turned to import-substitution industrialization in order to develop their economies. In recent decades, more countries have increasingly embraced the idea of free trade, persuaded by the benefits that it can provide. The ASEAN Economic Community (AEC) was conceptualised in 2003, following the meeting of ASEAN headsofstate in Indonesia, and is part of a larger concept of regional integration.4 The AEC aims to create a common economic platform that will facilitate the participation and trade of ASEAN member countries. Its main pillars are: 1) a single market and production base; 2) a competitive economic region; 3) equitable economic development; and 4) integration into the global economy.5 With the integration of the AEC, the total value of the economic market is estimated at US$2.3 trillion.6 As a large market with considerable promise, the AEC is an attractive region for robust economic growth. It is in this regard that many countries have been convinced to participate in the AEC, as the tangible and rational benefits accrued to each country will go a long way in promoting the economic health and societal welfare of their respective countries.
How the AEC benefits its member countries
First, the AEC provides countries with incentives to align their production bases according to their comparative advantages. For many Southeast Asian countries, most countries retain a large agrarian base, with only a few having successfully transitioned to a more capital-intensive economy.7 For example, Singapore and Malaysia are the leading economies in the region and both have considerable infrastructure tailored to meet the needs of a more knowledge-based, high-value economy. This is aligned with their comparative advantages, especially so for Singapore. Public universities, such as the National University of Singapore and Nanyang Technological University, have consistently been recognized for academic excellence in various world rankings, while other technical and vocational training institutes provide Singaporeans with the needed skills that employers from MNCs look for.8,9 As a result, Singapore in general has emerged as a strong and competitive economy. On the other hand, the Philippines, with a large agrarian base, is presently much poorer than Singapore. Without specialization, it will be difficult for the Philippines to achieve economies of scale via large-scale efficient production of goods according to their comparative advantage in primary products, such as agricultural goods.10 The AEC aims to address this by encouraging economies to re-align according to their strengths. In doing so, other trading partners within the AEC will benefit from lower prices, thereby enhancing consumer welfare.
Second, the AEC effectively creates an integrated market for 611 million consumers.11 A large consumer base will be advantageous to many firms located within member countries, as their product reach will be greater than before. Where successful integration without significant trade barriers prevails, social welfare improves as raw material and final product prices are not unnecessarily distorted. It is little wonder, therefore, that many countries are receptive of the AEC since this will increase both domestic consumption and export expenditure, enhancing the interest of their respective business sectors. Consumers in the AEC will be exposed to even more product choice, variety and nature at a higher level of quantity and quality. Already, benefits have been apparent. For example, Jollibee, a fast food restaurant chain from the Philippines, has successfully expanded its operations into nine of the ten ASEAN nations as a result of lower trade and investment barriers. Today, its operations are worth a total of US$866 million.12 The presence of a large market is also effective in attracting foreign direct investment from other parts of the world. This sentiment was captured in a survey by the US Chamber of Commerce, which polled and discovered that 54% of American firms have plans that involve expanding into ASEAN.13 Such investments can go a long way in strengthening the regional economy even further, thereby enhancing the overall returns to integration. Specifically, more jobs can be created and economic development achieved. Effectively, consumers will benefit alongside business and every country is moved along in this cycle of economic prosperity.
Third, the AEC's success can promote greater regional peace and ameliorate security-related challenges. Economic competition has often been implicated in political conflicts among states (Le Billon, 2004). The presence of the AEC provides all states with positive security externalities, specifically, a buffer against unnecessary conflict caused by economic fundamentals. Under conditions of shared economic growth and prioritized economic cooperation, the benefits of a successful AEC will be enjoyed by all ASEAN states. Fostering an economic community can also provide additional benefits to regional peace, following economic liberalism. According to economic interdependence theory, states that form a network of cross-border cutting economic ties will be more disposed towards peace and less towards conflict. If the theory is true, the presence of the AEC will be a major driver of regional peace in ASEAN. While no studies have been conducted in ASEAN yet, economic interdependence theory tentatively has some support from research conducted by Russett and Oneal, who established the presence of a positive relationship between trade and peace.14 This is salient, as ASEAN leaders such as Singapore's Prime Minister Lee Hsien Loong have identified growing nationalism as a threat to regional stability.15 Integration in the economic sphere, in turn, can serve as a viable launch pad for better and more stable political and social relations across the region. Building a stable and powerful AEC will be advantageous to promoting peace in Southeast Asia, a goal that is desirable in the context of greater political conflict among East Asian states. This being achieved, peace in Southeast Asia will be a good base to enhance peace in the greater East Asia region.
Lessons from the ASEAN Experience
Following the above, it is apparent that economic cooperation is desirable as it provides economies access to a larger market, fosters economic competitiveness and carries positive security externalities. Together, these attractive ""push"" factors encourage states to seek out such an arrangement. Against this backdrop, there is cautious optimism that Northeast Asia can achieve similar results, if a similar economic community can be mooted. Should major economies such as China, Taiwan, Japan and South Korea come together to form a coherent economic multi-lateral institution, there is a high chance that these countries can enjoy the economic benefits that AEC member states will share. Yet, such communities are not naturally formed, owing to various barriers that can discourage countries from associating into an economic community. This section will enumerate three major difficulties associated with national-level economic organizations. Every multi-member organization is prone to conflict among constituent members owing to different priorities, norms and domestic needs. ASEAN is no stranger to these issues. Specifically, the issues under discussion are: 1) the dumping of excess produced goods in other countries' markets; 2) the extent of protectionism that hinders full economic integration; and 3) undesirable labour-side effects caused by labour mobility.
Dumping
Countries able to manufacture cheaper goods tend to export such goods overseas upon saturation of the local market, leading to greater competition and declining profitability of recipient country firms. This condition, known as dumping, is a recurrent problem found in economic zones with members being in different stages of economic development.16 Assuming free trade and a country X with a strong comparative advantage in good A, compared to countries Y and Z, the latter two countries will experience an influx of good A from X, leading to an outcompeting of domestic producers in Y and Z who lack the level of efficiency needed to provide similar goods at more competitive rates. This has been a problem faced in various regions, whereby the movement of goods according to comparative advantage has meant a decrease in earnings from domestic industries, who are unable to compete with more efficient producers from other countries. As a result, this phenomenon of dumping causes domestic producers to be squeezed out of the production market, leading to negative effects on the economy's production capacity as a whole. Of greater salience in politics, the marginalization of local sectors in business can foment greater resentment and demands placed on the government to protect their interests better. Should the government heed this call and respond in favour of their businesses, the resultant situation can be detrimental to the overall fabric of the regional outfit writ-large.
In ASEAN, this problem of dumping has been averted by negotiating and re-aligning comparative advantages while embracing the overall enhancing effects on consumer surplus. The ASEAN institution leverages greatly on such conciliatory negotiation mechanisms that aim to produce consensus among member states.17 This minimizes any consequent disagreements that can impede successful policy implementation. According to an ANZ Bank analyst, the ASEAN countries will likely structure their economies to match their resource endowments, with Singapore and Malaysia developing ""into service and financial hubs for the region"" while Indonesia, the Philippines, Thailand and Vietnam move into middle-end manufacturing, while Myanmar, Laos and Cambodia, will leverage on its physical resources for lower-value manufacturing.18 This indicates that no particular industry will see inefficient and large-scale production of any goods or services. Instead, countries largely avoid the narrow concentration of industrial efforts on similar products and focus on their niche areas of production. Hence, this reduces any surplus production needing to be ""dumped"" in regional markets.
Protectionism
Economic integration can be greatly hindered when member countries refuse to abide by agreed upon regulations permitting movement of goods and services across borders. This is commonly done by raising barriers to mobility, a practice officially known as protectionism. Traditional protectionist measures include tariffs and quotas on foreign imports, while modern protectionist measures typically involve regulations on product content requirements.19 Countries may adopt protectionism so as to protect their domestic industries from more efficient foreign competitors, which may have repercussions on domestic employment. Another reason for protectionism may be to pander to business groups with powerful influence and ties with politicians.20 Such businesses understandably do not wish to see an erosion of profits, which competition brings about generally. Still, despite such justifications, protectionist measures militate against the creation of an integrated and accessible economic community. These barriers prevent free flow of goods and services from one country to another, contradicting the basic tenet of economic integration.
ASEAN countries have tackled this problem by collaboratively building consensus about the disavowing of protectionism. This was embodied most clearly in the communique released after the ASEAN Summit in 2015, whereby ASEAN affirmed its commitment to eliminating trade tariffs, of which nearly 96% have been eliminated.21 Incrementally, every country has been reducing restrictions on competition in protected sectors such as banking, which is a major source of revenue for economies. The commitment to protectionism can be seen in agreements signed within the AEC, including the ASEAN Comprehensive Investment Agreement (ACIA). The ACIA institutionalizes each country's commitment to liberalize their business sectors and this has ""eased restrictions to cross-border trade.""22 This is supplemented by the ASEAN Trade in Goods Agreement, a ""legal framework to realize the free flow of goods within the AEC."" Consequently, much progress has been made in the AEC, whereby more than ""70% of intra-regional trade"" now has zero tariffs across 80 sub-sectors.23 With more time given, the rate of further liberalization is expected to increase given the continued commitment of political leaders to the AEC's cause.
Undesirable labor effects
Labor movement can be both a source of benefit and conflict to the receiving society, which may not always embrace foreigner inflows. This poses a formidable obstacle to any plans for full-scale economic integration, since any failure to permit free labour flow can go against a fundamental tenet of a shared economic community. In theory, free labour flow is beneficial because it can correct any asymmetry in the labour market by ensuring that workers move to where they are in demand from places where they are currently unemployed due to undersupply of jobs.24 Furthermore, these workers are likely to have received education and training in their home countries prior to movement to other countries, being a big advantage to the latter. Such prior training greatly reduces the need to equip these workers with basic skills, a source of cost savings for the receiving country. However, labour factors, despite their advantages for the receiving economy, interact with socio-cultural cleavages at the same time. Locals may perceive foreigners to be taking their jobs, fomenting greater resentment as a result. How have affected ASEAN countries dealt with this? To provide a more detailed analysis, here this paper examines a single case, Singapore, in detail.
Being an advanced economy, Singapore is attractive to many foreign workers around the world. This ranges from lower-value employment such as in construction and the service industry, of which most workers tend to be from neighbouring ASEAN countries and South Asia, to high-value employment in sectors such as banking and Information Technology (IT).25 Such workers can come from any part of the world, with the critical distinction being their professional qualifications and experience. Due to rapid development, alignment with comparative advantage, and a small labour force, Singapore has relied on foreigners to fill gaps in the various sectors, with most of them found in the lower-value sectors.26 The reason for this is that, since Singapore's only valuable resource endowment is human capital, such capital must be upgraded to be of higher value in order to sustain the economy's development.27 As such, labour shortages are found more in the sectors of construction and ancillary public services such as cleaning.
However, Singaporean society has not taken well to such measures and has displayed occasional resistance to the foreign labour policy of the government. This has sometimes translated into strong pressure on elected politicians to manage the policy better by slowing down the rate of uptake. As society's support is essential to any public policy, the state is unable to ignore public opinion. To this end, Singapore has been more careful and calibrated in its approach to foreign worker inflows. There are categories of employment that require different skill sets, and are open to different nationalities.28 In labour intensive sectors, the government does not appear to be intervening extensively, since such labour flows are transient and do not significantly pose problems to society. For the professional sectors, depending on deemed merit and worthiness, foreigners are eligible for Permanent Resident status or Singaporean citizenship itself. It is in this domain that discernible change is apparent, with majority beneficiaries being other ASEAN countries, while those from South Asia and other regions are in the minority. In 2013 for example, 55.3% of new PRs were from ASEAN countries while 34.6% came from other Asian countries such as India and China.29
On the societal frontier, the government has become more active in its drive to clarify the misconceptions related to foreign workers taking jobs away from locals, backed up with actual policies to lend such messages more credence. For example, companies must list openings for certain jobs one month prior to opening them up to foreigners.30 In addition, the number of foreign workers allowed for hire by a company is determined by a pre-set ratio of local to foreign labour.31 This means that firms must hire local workers before they can be allowed to hire foreigners. Together, these policies aim to assuage public concerns by showing that the government is monitoring labour inflows and ensuring no undesirable side effects on society writ large. This is a strategy that dovetails with larger AEC goals. Singapore has continued to privilege AEC partners in its foreign labour policies, as seen in the above proxy indicator of majority ASEAN PR and citizenship grants, supporting the AEC's eventual ideal of free labour mobility. In this regard, there are positive steps being taken to ensure that, despite societal resistance, Singapore's cooperation and compliance with AEC goals are not compromised, but have been managed in order to ensure smooth policy development and acceptance.Continued on Next Page »"	22950
covid19	['Marie', 'Deepak', 'Robert', 'Anthony', 'Tom', 'Marcel', 'David', 'Pavel', 'William', 'Already A Subscriber']		"The rapid approval and subsequent global rollout of vaccines against SARS-CoV-2 has inevitably resulted in reports of adverse events after receipt of the vaccine. Most adverse events reported thus far have been associated with risks similar to background risks in the general population and have not raised concerns. However, for very rare events with complex diagnostic algorithms, the background risks may be difficult to measure or interpret.

We report the detection of anti-PF4 antibodies, unrelated to the use of heparin therapy, in a mostly young, generally healthy cohort of patients presenting with acute atypical thrombosis, primarily involving the cerebral veins, and concurrent thrombocytopenia. All the patients had d-dimer levels at presentation that were much higher than would be expected in patients with acute venous thromboembolism13 and are typically seen in patients with cancer.14 The very conservative d-dimer cutoff used in our algorithm, of 4000 FEU, was chosen to ensure that cases were not missed and were considered for further testing, because there is a possibility that a spectrum of severity in this syndrome could be missed otherwise. In all the patients, manifestations occurred 6 to 24 days after the administration of the first dose of the ChAdOx1 nCoV-19 vaccine.

HIT is a progressive thrombotic condition that can cause both venous and arterial thrombosis, typically 5 to 14 days after exposure to heparin. It is more common in female patients, particularly those who receive unfractionated heparin during cardiac surgery, as well as in patients who receive heparin after surgery, especially cardiac and orthopedic procedures.15 Diagnosis is confirmed by the presence of anti-PF4 antibodies.16,17

Data describing the rare detection of pathologic anti-PF4 antibodies unrelated to the use of heparin therapy are limited.18,19 Furthermore, the analysis of anti-PF4 antibodies appears to be specific to the given assay. In our study, confirmation of ELISA results for anti-PF4 antibodies was undertaken with the use of a functional HIT assay. The clinical features of this vaccine-induced syndrome are more typical of those seen in patients with HIT who have early reexposure to heparin, including severe thrombocytopenia, aggressive thrombosis, and disseminated intravascular coagulation.20

The risk of thrombocytopenia and the risk of venous thromboembolism after vaccination against SARS-CoV-2 do not appear to be higher than the background risks in the general population, a finding consistent with the rare and sporadic nature of this syndrome. Furthermore, headaches, fevers, and muscle aches have occurred after vaccination for 48 to 72 hours in some patients. The events reported in this study appear to be rare, and until further analysis is performed, it is difficult to predict who may be affected. The symptoms developed more than 5 days after the first vaccine dose, reflecting an immunologic pattern similar to that of HIT.

We have identified a novel mechanism and pathophysiological basis that prompts careful consideration of treatment. Avoidance of platelet transfusions is critical, because such treatment would provide a substrate for further antibody-mediated platelet activation and coagulopathy. The exact nature of these pathologic antibodies has not been characterized, but they appear to be of the IgG subtype, and platelet activation can be completely abrogated with an excess of heparin, as seen in classic HIT. Identification of the mechanism through which the vaccine could trigger the formation of these pathologic antibodies would require further study. An understanding of the precise pathophysiological mechanism may allow for more targeted therapeutic interventions.

Although evidence does not yet suggest that the use of heparin will exacerbate this condition, pending further data, we would recommend considering anticoagulation with the use of a nonheparin anticoagulant agent, such as argatroban, danaparoid, fondaparinux, or direct oral anticoagulants. Intravenous immune globulin (IVIG) has been used successfully in the treatment of patients with “spontaneous” autoimmune HIT, which is the closest comparison to this vaccine-induced syndrome, and IVIG would be expected to have direct antibody-mediated toxic effects.21-23 Plasma exchange with plasma rather than albumin could also be effective in temporarily reducing levels of pathologic antibodies and providing some correction of the coagulopathy in terms of the hypofibrinogenemia.

A suggested algorithm for identification of vaccine-induced thrombosis and thrombocytopenia is presented and can be adapted as we generate further information. The combination of thrombosis and an apparent consumptive coagulopathy poses a dilemma with respect to the benefits and risks associated with aggressive anticoagulation. This dilemma is especially relevant in patients with cerebral venous thrombosis, in whom bleeding could be catastrophic but withholding anticoagulation could be equally harmful. It is unclear whether delaying aggressive anticoagulation until after initial disease control with IVIG or plasma exchange is warranted, but mortality among patients with cerebral venous thrombosis appears to be higher than expected, so early treatment decisions are likely to be critical. There is no evidence that heparin alternatives are required; however, in view of the similarity of this syndrome to conventional HIT, alternatives could be considered until further data are available.

In all cases reported to date, this syndrome of thrombocytopenia and venous thrombosis appears to be triggered by receipt of the first dose of the ChAdOx1 nCoV-19 vaccine. Although there have been a few reports of patients with symptoms consistent with this clinical syndrome after the receipt of other vaccines against SARS-CoV-2, none have yet been confirmed to fulfill the diagnostic criteria, specifically the presence of thrombocytopenia, thrombosis, a very high d-dimer level, and a low or normal fibrinogen level. Furthermore, in Israel, where two doses of the BNT162b2 vaccine (Pfizer–BioNTech) have been provided to more than 4 million people, no cases of this rare syndrome have been reported. Although natural SARS-CoV-2 infection has been associated with thromboembolic phenomena, those events differ from the specific syndrome described in this study.

The risk of Covid-19 remains a serious public health consideration worldwide, and vaccination against SARS-CoV-2 provides critical protection.24 There is a substantial risk of ascertainment bias when associating adverse clinical events with vaccination; however, the syndrome described in this study has a combination of clinical and laboratory features that is exceptional and has not been previously observed by any of the authors who are specialist hematologists or neurologists. Ongoing data collection and studies could help to establish whether and how the development of pathologic platelet-activating anti-PF4 antibodies, unrelated to the use of heparin therapy, could be associated with vaccination against SARS-CoV-2."	https://www.nejm.org/doi/full/10.1056/NEJMoa2105385?query=featured_coronavirus	"The rapid approval and subsequent global rollout of vaccines against SARS-CoV-2 has inevitably resulted in reports of adverse events after receipt of the vaccine. Most adverse events reported thus far have been associated with risks similar to background risks in the general population and have not raised concerns. However, for very rare events with complex diagnostic algorithms, the background risks may be difficult to measure or interpret.
We report the detection of anti-PF4 antibodies, unrelated to the use of heparin therapy, in a mostly young, generally healthy cohort of patients presenting with acute atypical thrombosis, primarily involving the cerebral veins, and concurrent thrombocytopenia. All the patients had d-dimer levels at presentation that were much higher than would be expected in patients with acute venous thromboembolism13 and are typically seen in patients with cancer.14 The very conservative d-dimer cutoff used in our algorithm, of 4000 FEU, was chosen to ensure that cases were not missed and were considered for further testing, because there is a possibility that a spectrum of severity in this syndrome could be missed otherwise. In all the patients, manifestations occurred 6 to 24 days after the administration of the first dose of the ChAdOx1 nCoV-19 vaccine.
HIT is a progressive thrombotic condition that can cause both venous and arterial thrombosis, typically 5 to 14 days after exposure to heparin. It is more common in female patients, particularly those who receive unfractionated heparin during cardiac surgery, as well as in patients who receive heparin after surgery, especially cardiac and orthopedic procedures.15 Diagnosis is confirmed by the presence of anti-PF4 antibodies.16,17
Data describing the rare detection of pathologic anti-PF4 antibodies unrelated to the use of heparin therapy are limited.18,19 Furthermore, the analysis of anti-PF4 antibodies appears to be specific to the given assay. In our study, confirmation of ELISA results for anti-PF4 antibodies was undertaken with the use of a functional HIT assay. The clinical features of this vaccine-induced syndrome are more typical of those seen in patients with HIT who have early reexposure to heparin, including severe thrombocytopenia, aggressive thrombosis, and disseminated intravascular coagulation.20
The risk of thrombocytopenia and the risk of venous thromboembolism after vaccination against SARS-CoV-2 do not appear to be higher than the background risks in the general population, a finding consistent with the rare and sporadic nature of this syndrome. Furthermore, headaches, fevers, and muscle aches have occurred after vaccination for 48 to 72 hours in some patients. The events reported in this study appear to be rare, and until further analysis is performed, it is difficult to predict who may be affected. The symptoms developed more than 5 days after the first vaccine dose, reflecting an immunologic pattern similar to that of HIT.
We have identified a novel mechanism and pathophysiological basis that prompts careful consideration of treatment. Avoidance of platelet transfusions is critical, because such treatment would provide a substrate for further antibody-mediated platelet activation and coagulopathy. The exact nature of these pathologic antibodies has not been characterized, but they appear to be of the IgG subtype, and platelet activation can be completely abrogated with an excess of heparin, as seen in classic HIT. Identification of the mechanism through which the vaccine could trigger the formation of these pathologic antibodies would require further study. An understanding of the precise pathophysiological mechanism may allow for more targeted therapeutic interventions.
Although evidence does not yet suggest that the use of heparin will exacerbate this condition, pending further data, we would recommend considering anticoagulation with the use of a nonheparin anticoagulant agent, such as argatroban, danaparoid, fondaparinux, or direct oral anticoagulants. Intravenous immune globulin (IVIG) has been used successfully in the treatment of patients with “spontaneous” autoimmune HIT, which is the closest comparison to this vaccine-induced syndrome, and IVIG would be expected to have direct antibody-mediated toxic effects.21-23 Plasma exchange with plasma rather than albumin could also be effective in temporarily reducing levels of pathologic antibodies and providing some correction of the coagulopathy in terms of the hypofibrinogenemia.
A suggested algorithm for identification of vaccine-induced thrombosis and thrombocytopenia is presented and can be adapted as we generate further information. The combination of thrombosis and an apparent consumptive coagulopathy poses a dilemma with respect to the benefits and risks associated with aggressive anticoagulation. This dilemma is especially relevant in patients with cerebral venous thrombosis, in whom bleeding could be catastrophic but withholding anticoagulation could be equally harmful. It is unclear whether delaying aggressive anticoagulation until after initial disease control with IVIG or plasma exchange is warranted, but mortality among patients with cerebral venous thrombosis appears to be higher than expected, so early treatment decisions are likely to be critical. There is no evidence that heparin alternatives are required; however, in view of the similarity of this syndrome to conventional HIT, alternatives could be considered until further data are available.
In all cases reported to date, this syndrome of thrombocytopenia and venous thrombosis appears to be triggered by receipt of the first dose of the ChAdOx1 nCoV-19 vaccine. Although there have been a few reports of patients with symptoms consistent with this clinical syndrome after the receipt of other vaccines against SARS-CoV-2, none have yet been confirmed to fulfill the diagnostic criteria, specifically the presence of thrombocytopenia, thrombosis, a very high d-dimer level, and a low or normal fibrinogen level. Furthermore, in Israel, where two doses of the BNT162b2 vaccine (Pfizer–BioNTech) have been provided to more than 4 million people, no cases of this rare syndrome have been reported. Although natural SARS-CoV-2 infection has been associated with thromboembolic phenomena, those events differ from the specific syndrome described in this study.
The risk of Covid-19 remains a serious public health consideration worldwide, and vaccination against SARS-CoV-2 provides critical protection.24 There is a substantial risk of ascertainment bias when associating adverse clinical events with vaccination; however, the syndrome described in this study has a combination of clinical and laboratory features that is exceptional and has not been previously observed by any of the authors who are specialist hematologists or neurologists. Ongoing data collection and studies could help to establish whether and how the development of pathologic platelet-activating anti-PF4 antibodies, unrelated to the use of heparin therapy, could be associated with vaccination against SARS-CoV-2."	7102
covid19	['Elizabeth M.', 'Xiaofei', 'Carolyn', 'Richard A.', 'Stefan', 'Vincent', 'Already A Subscriber', 'E.M. White Et Al.', 'C. Vayne Et Al.', 'J. O Connell Et Al.']		"To the Editor:

Since the deployment of the messenger RNA (mRNA) vaccines against severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)1,2 in nursing homes nationwide starting in mid-December 2020, aggregate public data have shown decreases in the incidence of cases of SARS-CoV-2 infection and related deaths.3 However, there have been minimal individual-level data available for understanding vaccine effectiveness in nursing home residents, who were absent from the clinical trials and who often have reduced immune responses.4 Using electronic health record data from Genesis HealthCare, a large long-term care provider in the United States, we report the incidence of SARS-CoV-2 infection among vaccinated residents and unvaccinated residents of 280 nursing homes across 21 states.

From immunization records, we identified residents who had received at least one dose of mRNA vaccine as of February 15, 2021; those who had received both doses by February 15, 2021; and those who were present at their facility on the day of the first vaccination clinic but who were not vaccinated as of March 31, 2021. We identified incident SARS-CoV-2 infections through March 31, 2021, on the basis of polymerase-chain-reaction assay and antigen-test records. Residents were tested every 3 to 7 days when there were confirmed cases in their facility and were tested if they had any new symptoms or potential exposure. Residents who had been infected in the 90 days before the study window were excluded. We counted incident infections after receipt of each dose among vaccinated residents and after the date of the first vaccination clinic among unvaccinated residents. Nurses assessed residents daily and documented new symptoms in structured change-in-condition notes. From these notes, we deemed residents to be symptomatic if SARS-CoV-2–related symptoms developed during the period from 5 days before to 14 days after a positive test. Detailed methods are described in the Supplementary Appendix, available with the full text of this letter at NEJM.org.

The sample included 18,242 residents who received at least one dose of mRNA vaccine; 14,669 residents (80.4%) received the Pfizer–BioNTech vaccine, and 3573 (19.6%) received the Moderna vaccine. Of these 18,242 residents, 13,048 also received the second dose of vaccine. A total of 3990 residents were unvaccinated. Table S1 in the Supplementary Appendix summarizes the characteristics of the residents.

Table 1. Table 1. Incident SARS-CoV-2 Infection among Nursing Home Residents According to Vaccination Status.

The incidence of infection decreased over time among both vaccinated residents and unvaccinated residents (Table 1). After receipt of the first vaccine dose, there were 822 incident cases (4.5% of vaccinated residents) within 0 to 14 days and 250 cases (1.4%) at 15 to 28 days. Among the 13,048 residents who received both doses of vaccine, there were 130 incident cases (1.0% of vaccinated residents) within 0 to 14 days after receipt of the second dose and 38 cases (0.3%) after 14 days (which included 19 cases occurring 15 to 21 days after receipt of the second dose) (Fig. S1). Among unvaccinated residents, incident cases decreased from 173 cases (4.3% of unvaccinated residents) within 0 to 14 days after the first vaccination clinic to 12 cases (0.3%) at more than 42 days after the clinic.

Across all the study groups, most infections were asymptomatic, and the incidence of both asymptomatic and symptomatic infections decreased. Nursing homes that were located in counties with the highest incidence of SARS-CoV-2 infection had the most incident cases but still had large decreases (Table S2). We observed inconsistent patterns in the incidence of infection among residents relative to rates of vaccination among staff members (Table S3).

These findings show the real-world effectiveness of the mRNA vaccines in reducing the incidence of asymptomatic and symptomatic SARS-CoV-2 infections in a vulnerable nursing home population. Our observation of a reduced incidence of infection among unvaccinated residents suggests that robust vaccine coverage among residents and staff, together with the continued use of face masks and other infection-control measures, is likely to afford protection for small numbers of unvaccinated residents in congregate settings. Still, the continued observation of incident cases after vaccination highlights the critical need for ongoing vaccination programs and surveillance testing in nursing homes to mitigate future outbreaks.

Elizabeth M. White, Ph.D., A.P.R.N.

Xiaofei Yang, Sc.M.

Brown University School of Public Health, Providence, RI

[email protected]

Carolyn Blackman, M.D.

Richard A. Feifer, M.D., M.P.H.

Genesis HealthCare, Kennett Square, PA

Stefan Gravenstein, M.D., M.P.H.

Alpert Medical School of Brown University, Providence, RI

Vincent Mor, Ph.D.

Brown University School of Public Health, Providence, RI

Supported by grants (3P01AG027296-11S1 and U54063546-S5, to Dr. Mor) from the National Institute on Aging. Disclosure forms provided by the authors are available with the full text of this letter at NEJM.org.

This letter was published on May 19, 2021, at NEJM.org."	https://www.nejm.org/doi/full/10.1056/NEJMc2104849?query=featured_coronavirus	"To the Editor:
Since the deployment of the messenger RNA (mRNA) vaccines against severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)1,2 in nursing homes nationwide starting in mid-December 2020, aggregate public data have shown decreases in the incidence of cases of SARS-CoV-2 infection and related deaths.3 However, there have been minimal individual-level data available for understanding vaccine effectiveness in nursing home residents, who were absent from the clinical trials and who often have reduced immune responses.4 Using electronic health record data from Genesis HealthCare, a large long-term care provider in the United States, we report the incidence of SARS-CoV-2 infection among vaccinated residents and unvaccinated residents of 280 nursing homes across 21 states.
From immunization records, we identified residents who had received at least one dose of mRNA vaccine as of February 15, 2021; those who had received both doses by February 15, 2021; and those who were present at their facility on the day of the first vaccination clinic but who were not vaccinated as of March 31, 2021. We identified incident SARS-CoV-2 infections through March 31, 2021, on the basis of polymerase-chain-reaction assay and antigen-test records. Residents were tested every 3 to 7 days when there were confirmed cases in their facility and were tested if they had any new symptoms or potential exposure. Residents who had been infected in the 90 days before the study window were excluded. We counted incident infections after receipt of each dose among vaccinated residents and after the date of the first vaccination clinic among unvaccinated residents. Nurses assessed residents daily and documented new symptoms in structured change-in-condition notes. From these notes, we deemed residents to be symptomatic if SARS-CoV-2–related symptoms developed during the period from 5 days before to 14 days after a positive test. Detailed methods are described in the Supplementary Appendix, available with the full text of this letter at NEJM.org.
The sample included 18,242 residents who received at least one dose of mRNA vaccine; 14,669 residents (80.4%) received the Pfizer–BioNTech vaccine, and 3573 (19.6%) received the Moderna vaccine. Of these 18,242 residents, 13,048 also received the second dose of vaccine. A total of 3990 residents were unvaccinated. Table S1 in the Supplementary Appendix summarizes the characteristics of the residents.
Table 1. Table 1. Incident SARS-CoV-2 Infection among Nursing Home Residents According to Vaccination Status.
The incidence of infection decreased over time among both vaccinated residents and unvaccinated residents (Table 1). After receipt of the first vaccine dose, there were 822 incident cases (4.5% of vaccinated residents) within 0 to 14 days and 250 cases (1.4%) at 15 to 28 days. Among the 13,048 residents who received both doses of vaccine, there were 130 incident cases (1.0% of vaccinated residents) within 0 to 14 days after receipt of the second dose and 38 cases (0.3%) after 14 days (which included 19 cases occurring 15 to 21 days after receipt of the second dose) (Fig. S1). Among unvaccinated residents, incident cases decreased from 173 cases (4.3% of unvaccinated residents) within 0 to 14 days after the first vaccination clinic to 12 cases (0.3%) at more than 42 days after the clinic.
Across all the study groups, most infections were asymptomatic, and the incidence of both asymptomatic and symptomatic infections decreased. Nursing homes that were located in counties with the highest incidence of SARS-CoV-2 infection had the most incident cases but still had large decreases (Table S2). We observed inconsistent patterns in the incidence of infection among residents relative to rates of vaccination among staff members (Table S3).
These findings show the real-world effectiveness of the mRNA vaccines in reducing the incidence of asymptomatic and symptomatic SARS-CoV-2 infections in a vulnerable nursing home population. Our observation of a reduced incidence of infection among unvaccinated residents suggests that robust vaccine coverage among residents and staff, together with the continued use of face masks and other infection-control measures, is likely to afford protection for small numbers of unvaccinated residents in congregate settings. Still, the continued observation of incident cases after vaccination highlights the critical need for ongoing vaccination programs and surveillance testing in nursing homes to mitigate future outbreaks.
Elizabeth M. White, Ph.D., A.P.R.N.
Xiaofei Yang, Sc.M.
Brown University School of Public Health, Providence, RI
[email protected]
Carolyn Blackman, M.D.
Richard A. Feifer, M.D., M.P.H.
Genesis HealthCare, Kennett Square, PA
Stefan Gravenstein, M.D., M.P.H.
Alpert Medical School of Brown University, Providence, RI
Vincent Mor, Ph.D.
Brown University School of Public Health, Providence, RI
Supported by grants (3P01AG027296-11S1 and U54063546-S5, to Dr. Mor) from the National Institute on Aging. Disclosure forms provided by the authors are available with the full text of this letter at NEJM.org.
This letter was published on May 19, 2021, at NEJM.org."	5198
covid19	['James', 'O Connell', 'Derek T.', 'O Keeffe', 'Already A Subscriber', 'E.M. White Et Al.', 'C. Vayne Et Al.', 'J. O Connell Et Al.', 'S. Martin Et Al.', 'V.P. Werth Et Al.']		"Outbreaks of coronavirus disease 2019 (Covid-19) emerged in the United States and in European countries in February 2020. Urgent action was called for, since experts estimated that 30 to 70% of people in these Western countries could become infected — a frightening projection at a time when the Covid-19 mortality rate was estimated to be substantially higher than we now know it to be. In March 2020, Michael Ryan, executive director of the Health Emergencies Program of the World Health Organization (WHO), implored countries to act, noting that when it comes to epidemic response, “speed trumps perfection” but “the greatest error is not to move.” At the time, the only tools for containing Covid-19 were social distancing, testing, case isolation, and contact tracing.

Contact tracing is a crucial public health practice that has been a part of epidemic responses for centuries. From the bubonic plague, to smallpox and tuberculosis, to HIV, the fate of public health has relied on our ability to identify people who have been in contact with infected people. In the case of Covid-19, however, the short time between symptom onset in the infector and that in the infected and the virus’s propensity for asymptomatic transmission posed challenges for contact tracing. Recall bias, the inability to identify contacts who are unknown to the infected person, and a shortage of trained contact tracers were additional challenges. There was an urgent need to augment the scale and rapidity of contact tracing to identify everyone who had been exposed to Covid-19.

Twenty-first–century digital technology had the potential to enable this escalation. Modeling studies suggested that if digital contact-tracing apps were combined with other mitigation measures, Covid-19 epidemics could be slowed and theoretically even ended.1 Lessons can be learned from the deployment of digital technologies to augment contact tracing during this pandemic.

The most basic measures of the effectiveness of a pandemic response are case numbers and deaths. By these measures, South Korea’s response during its first wave of Covid-19 was highly successful. Having experienced the Middle East respiratory syndrome (MERS) in 2015, the South Korean people and their political leaders understood the need for early recognition of the pandemic threat and a corresponding robust response. They successfully integrated a rapidly scaled diagnostic capacity and contact-tracing system with effective isolation and quarantine measures.

A key part of the South Korean contact-tracing system was digital contact-tracing technology. Legislative changes in South Korea arising from the MERS outbreak gave health authorities a legal foundation for using geolocation data for contact tracing from the very outset of their epidemic. Global positioning system (GPS) data from cell phones were used to create a centralized database of the movements of people with Covid-19 that was accessible online. The Corona 100m app used these data to warn users when they were near a location visited by an infected person. This intervention interfered with the privacy, data protection, and civil liberties of infected people, but it aimed to disrupt transmission chains to protect society’s most vulnerable members.

In most Western countries, no such effort to enhance contact tracing using automation was implemented early in the epidemic. Without prior experience in responding to epidemics in this way, many leaders and citizens found it inconceivable that personal privacy and data protection rights could be ceded to health protection. Yet the fact that many people in Western countries already permit collection of geolocation data by other apps that provide little personal benefit suggests that the resistance to doing so for health protection, while well intended, may have been misguided.2

Automation using geolocation tracking allowed teams of epidemiologic investigators in South Korea to trace not only contacts but also the setting in which contact occurred up to 14 days before symptom onset or diagnosis. This information allowed them to gain a greater understanding of the settings in which SARS-CoV-2 transmission was occurring and to implement more targeted health protection measures in response. In contrast, traditional contact-tracing systems in most Western countries had the capacity to identify and notify only people who had come into contact with an infected person within 48 hours before symptom onset or diagnosis. This digital limitation perhaps contributed to the first wave of Covid-19 in Western countries that outpaced the epidemic in South Korea. By the end of their first epidemic wave in April 2020, South Korea had reported 10,423 infections and only 204 deaths — a remarkable achievement given the population size of just over 50 million. In contrast, European countries saw more than 2.1 million cases and 180,000 deaths by the end of their first wave in June.

Digital contact tracing is not a perfect intervention, given the risks to privacy, personal data, and false positive or false negative characterization of contact status. However, as in a Swiss cheese model, imperfect interventions can work together to curb epidemics. South Korea’s deployment of digital technology to augment contact tracing was an example of speed trumping perfection, whereas Europe made the greatest error described by Ryan of the WHO: not to move. Having learned from this experience, Europeans may be much more amenable to sharing location data for contact tracing in health emergencies.3 It should help in combating the next pandemic that the balance between preserving privacy and preserving life has changed during this pandemic.

As the first epidemic wave came to an end and the imminent threat of further loss of life eased, geolocation-based digital contact-tracing systems and their interference with personal privacy and data protection rights became less palatable. They became the subject of intense scrutiny in countries that used them, including South Korea and also Norway and Israel. In a pandemic that had the potential to last several years, many Western countries recognized the need for trustworthy, transparent, privacy-preserving digital contact-tracing technologies that were acceptable to Western populations.

A Selection of Digital Contact-Tracing Systems in the United States and Europe

Following the example of Singapore’s Bluetooth Low Energy (BLE) digital contact-tracing app TraceTogether, Germany, Ireland, and the United Kingdom, among others, set out to develop their own systems, which had varying uptake by target populations (see table).4 Western countries tended to favor a decentralized, privacy-preserving protocol for contact tracing — meaning that rather than being sent to central government servers, the data collected stay on the user’s device, are encrypted, and are automatically deleted after 14 days.4 By the end of 2020, there were at least 65 BLE–enabled digital contact-tracing systems worldwide, including 26 in the United States.4

Although it was never believed that these systems alone would end Covid-19 epidemics,1 evidence is emerging that they have been beneficial in identifying higher numbers of contacts per case than has traditional contact tracing, increasing the number of people with Covid-19 who have entered quarantine, shortening the time to quarantine by 1 to 2 days, and possibly preventing large numbers of infections thanks to downstream effects of augmented contact tracing.5

Challenges remain, however. Integration of digital contact-tracing technologies with existing test-and-trace systems appears to be an important determinant of their utility.5 More importantly, digital contact-tracing technologies must be made accessible, particularly to people with limited access to smartphone technology, those with limited digital health literacy, speakers of languages other than a country’s primary language, and migrant communities. Increasing accessibility is important not only for maximizing uptake, but also for ensuring that all members of society can benefit equitably from digital advances in contact tracing. If these challenges can be overcome, Western countries will have gained a trustworthy, privacy-preserving, accessible tool to use during the next pandemic to enhance contact-tracing capacity and control disease spread until elimination is achieved."	https://www.nejm.org/doi/full/10.1056/NEJMp2102256?query=featured_coronavirus	"Outbreaks of coronavirus disease 2019 (Covid-19) emerged in the United States and in European countries in February 2020. Urgent action was called for, since experts estimated that 30 to 70% of people in these Western countries could become infected — a frightening projection at a time when the Covid-19 mortality rate was estimated to be substantially higher than we now know it to be. In March 2020, Michael Ryan, executive director of the Health Emergencies Program of the World Health Organization (WHO), implored countries to act, noting that when it comes to epidemic response, “speed trumps perfection” but “the greatest error is not to move.” At the time, the only tools for containing Covid-19 were social distancing, testing, case isolation, and contact tracing.
Contact tracing is a crucial public health practice that has been a part of epidemic responses for centuries. From the bubonic plague, to smallpox and tuberculosis, to HIV, the fate of public health has relied on our ability to identify people who have been in contact with infected people. In the case of Covid-19, however, the short time between symptom onset in the infector and that in the infected and the virus’s propensity for asymptomatic transmission posed challenges for contact tracing. Recall bias, the inability to identify contacts who are unknown to the infected person, and a shortage of trained contact tracers were additional challenges. There was an urgent need to augment the scale and rapidity of contact tracing to identify everyone who had been exposed to Covid-19.
Twenty-first–century digital technology had the potential to enable this escalation. Modeling studies suggested that if digital contact-tracing apps were combined with other mitigation measures, Covid-19 epidemics could be slowed and theoretically even ended.1 Lessons can be learned from the deployment of digital technologies to augment contact tracing during this pandemic.
The most basic measures of the effectiveness of a pandemic response are case numbers and deaths. By these measures, South Korea’s response during its first wave of Covid-19 was highly successful. Having experienced the Middle East respiratory syndrome (MERS) in 2015, the South Korean people and their political leaders understood the need for early recognition of the pandemic threat and a corresponding robust response. They successfully integrated a rapidly scaled diagnostic capacity and contact-tracing system with effective isolation and quarantine measures.
A key part of the South Korean contact-tracing system was digital contact-tracing technology. Legislative changes in South Korea arising from the MERS outbreak gave health authorities a legal foundation for using geolocation data for contact tracing from the very outset of their epidemic. Global positioning system (GPS) data from cell phones were used to create a centralized database of the movements of people with Covid-19 that was accessible online. The Corona 100m app used these data to warn users when they were near a location visited by an infected person. This intervention interfered with the privacy, data protection, and civil liberties of infected people, but it aimed to disrupt transmission chains to protect society’s most vulnerable members.
In most Western countries, no such effort to enhance contact tracing using automation was implemented early in the epidemic. Without prior experience in responding to epidemics in this way, many leaders and citizens found it inconceivable that personal privacy and data protection rights could be ceded to health protection. Yet the fact that many people in Western countries already permit collection of geolocation data by other apps that provide little personal benefit suggests that the resistance to doing so for health protection, while well intended, may have been misguided.2
Automation using geolocation tracking allowed teams of epidemiologic investigators in South Korea to trace not only contacts but also the setting in which contact occurred up to 14 days before symptom onset or diagnosis. This information allowed them to gain a greater understanding of the settings in which SARS-CoV-2 transmission was occurring and to implement more targeted health protection measures in response. In contrast, traditional contact-tracing systems in most Western countries had the capacity to identify and notify only people who had come into contact with an infected person within 48 hours before symptom onset or diagnosis. This digital limitation perhaps contributed to the first wave of Covid-19 in Western countries that outpaced the epidemic in South Korea. By the end of their first epidemic wave in April 2020, South Korea had reported 10,423 infections and only 204 deaths — a remarkable achievement given the population size of just over 50 million. In contrast, European countries saw more than 2.1 million cases and 180,000 deaths by the end of their first wave in June.
Digital contact tracing is not a perfect intervention, given the risks to privacy, personal data, and false positive or false negative characterization of contact status. However, as in a Swiss cheese model, imperfect interventions can work together to curb epidemics. South Korea’s deployment of digital technology to augment contact tracing was an example of speed trumping perfection, whereas Europe made the greatest error described by Ryan of the WHO: not to move. Having learned from this experience, Europeans may be much more amenable to sharing location data for contact tracing in health emergencies.3 It should help in combating the next pandemic that the balance between preserving privacy and preserving life has changed during this pandemic.
As the first epidemic wave came to an end and the imminent threat of further loss of life eased, geolocation-based digital contact-tracing systems and their interference with personal privacy and data protection rights became less palatable. They became the subject of intense scrutiny in countries that used them, including South Korea and also Norway and Israel. In a pandemic that had the potential to last several years, many Western countries recognized the need for trustworthy, transparent, privacy-preserving digital contact-tracing technologies that were acceptable to Western populations.
A Selection of Digital Contact-Tracing Systems in the United States and Europe
Following the example of Singapore’s Bluetooth Low Energy (BLE) digital contact-tracing app TraceTogether, Germany, Ireland, and the United Kingdom, among others, set out to develop their own systems, which had varying uptake by target populations (see table).4 Western countries tended to favor a decentralized, privacy-preserving protocol for contact tracing — meaning that rather than being sent to central government servers, the data collected stay on the user’s device, are encrypted, and are automatically deleted after 14 days.4 By the end of 2020, there were at least 65 BLE–enabled digital contact-tracing systems worldwide, including 26 in the United States.4
Although it was never believed that these systems alone would end Covid-19 epidemics,1 evidence is emerging that they have been beneficial in identifying higher numbers of contacts per case than has traditional contact tracing, increasing the number of people with Covid-19 who have entered quarantine, shortening the time to quarantine by 1 to 2 days, and possibly preventing large numbers of infections thanks to downstream effects of augmented contact tracing.5
Challenges remain, however. Integration of digital contact-tracing technologies with existing test-and-trace systems appears to be an important determinant of their utility.5 More importantly, digital contact-tracing technologies must be made accessible, particularly to people with limited access to smartphone technology, those with limited digital health literacy, speakers of languages other than a country’s primary language, and migrant communities. Increasing accessibility is important not only for maximizing uptake, but also for ensuring that all members of society can benefit equitably from digital advances in contact tracing. If these challenges can be overcome, Western countries will have gained a trustworthy, privacy-preserving, accessible tool to use during the next pandemic to enhance contact-tracing capacity and control disease spread until elimination is achieved."	8411
covid19	['Ezgi', 'Caryn', 'Yuhki', 'Nathalie E.', 'Marissa', 'Erin G.', 'Dennis J.', 'Justin', 'Frauke', 'Christian']		"Specimen Collection and Processing

Beginning in the fall of 2020, all employees and students at the Rockefeller University campus (approximately 1400 persons) were tested at least weekly with a saliva-based PCR test developed in the Darnell Clinical Laboratory Improvement Amendments–Clinical Laboratory Evaluation Program laboratory (approval number, PFI-9216) and approved for clinical use by a New York State emergency use authorization. Protocols for the collection of saliva samples for clinical SARS-CoV-2 testing were reviewed by the institutional review board at Rockefeller University and were deemed not to be research involving human subjects. Institutional review board–approved written informed consent for the analysis of antibody titers was obtained from Patient 1, and the study was conducted in accordance with International Council for Harmonisation Good Clinical Practice guidelines.

In accordance with New York State regulations regarding eligibility, 417 employees who had received a second dose of either the BNT162b2 (Pfizer–BioNTech) or mRNA-1273 (Moderna) vaccine at least 2 weeks previously were tested between January 21 and March 17, 2021, and weekly testing continued thereafter. The demographic characteristics of these 417 persons and of 1491 unvaccinated persons tested in parallel at Rockefeller University during the same period are shown in Table S1 of the Supplementary Appendix, available with the full text of this article at NEJM.org.

The employees and students were instructed to provide a saliva sample in a medicine cup and transfer 300 μl into a vial containing 300 μl of Darnell Rockefeller University Laboratory (DRUL) buffer (5 M of guanidine thiocyanate, 0.5% sarkosyl, and 300 mM of sodium acetate [pH 5.5]).2 Samples were processed on the Thermo KingFisher Apex system for rapid RNA purification, and complementary DNA (cDNA) was amplified with the use of TaqPath 1-Step RT-qPCR (reverse-transcriptase quantitative PCR) Master Mix (Thermo Fisher Scientific) and multiplexed primers and probes that were validated under a Food and Drug Administration emergency use authorization (Table S2) with the 7500 Fast Dx Real-Time PCR detection system (Applied Biosystems). Samples were considered to be interpretable if the housekeeping control (RNase P) cycle threshold (Ct) was less than 40, and viral RNA was considered to be detected with both viral primers and probes (N1 and N2, detecting two regions of the nucleocapsid [N] gene of SARS-CoV-2) at a Ct of less than 40.

Viral Load Calculation

We calculated the viral load per milliliter of saliva using chemically inactivated SARS-CoV-2 (ZeptoMetrix) spiked into saliva at various dilutions. Extractions and RT-PCR were performed as described previously to determine the corresponding Ct values for each dilution (Fig. S1).

Targeted Sequencing

Reverse transcription of RNA samples was performed with the iScript mix (Bio-Rad) according to the manufacturer’s instructions. PCR amplification of cDNA was performed with the use of two primer sets (primer set 1: forward primer 1 [CCAGATGATTTTACAGGCTGC] and reverse primer 1 [CTACTGATGTCTTGGTCATAGAC]; primer set 2: forward primer 2 [CTTGTTTTATTGCCACTAGTC] and reverse primer 1). PCR products were then extracted from gel and sent to Genewiz for Sanger sequencing.

Neutralization Assay

Neutralization assays with pseudotyped replication defective human immunodeficiency virus type 1 modified with SARS-CoV-2 spike protein were performed as previously described.3 Mean serum neutralizing antibody titers (50% neutralization testing [NT 50 ]) were calculated as an average of three independent experiments, each performed with the use of technical duplicates, and statistical significance was determined with the two-tailed Mann–Whitney test.

Whole Viral RNA Genome Sequencing

Total RNA was extracted as described above, and a meta-transcriptomic library was constructed for paired-end (150-bp reads) sequencing with an Illumina MiSeq platform. Libraries were prepared with the SureSelect XT HS2 DNA System (Agilent Technologies) and Community Design Pan Human Coronavirus Panel (Agilent Technologies) according to the manufacturer’s instructions. FASTQ files (a text-based format for storing both a biologic sequence and its corresponding quality scores) were trimmed with Agilent Genomics NextGen Toolkit (AGeNT) software (version 2.0.5) and used for downstream analysis. The SARS-CoV-2 genome was assembled with MEGAHIT with default parameters, and the longest sequence (30,005 nucleotides) was analyzed with Nextclade software (https://clades.nextstrain.org/) in order to assign the clade and call mutations. Detected mutations were confirmed by aligning RNA sequencing reads on the reference genome sequence of SARS-CoV-2 (GenBank number, NC_045512) with the Burrows–Wheeler Aligner (BWA-MEM).

Patient Histories

Patient 1 was a healthy 51-year-old woman with no risk factors for severe Covid-19 who received the first dose of mRNA-1273 vaccine on January 21, 2021, and the second dose on February 19. She had adhered strictly to routine precautions. Ten hours after she received the second vaccine dose, flulike muscle aches developed. These symptoms resolved the following day. On March 10 (19 days after she received the second vaccine dose), a sore throat, congestion, and headache developed, and she tested positive for SARS-CoV-2 RNA at Rockefeller University later that day. On March 11, she lost her sense of smell. Her symptoms gradually resolved over a 1-week period.

Patient 2 was a healthy 65-year-old woman with no risk factors for severe Covid-19 who received the first dose of BNT162b2 vaccine on January 19 and the second dose on February 9. Pain that developed in the inoculated arm lasted for 2 days. On March 3, her unvaccinated partner tested positive for SARS-CoV-2, and on March 16, fatigue, sinus congestion, and a headache developed in Patient 2. On March 17, she felt worse and tested positive for SARS-CoV-2 RNA, 36 days after completing vaccination. Her symptoms plateaued and began to resolve on March 20."	https://www.nejm.org/doi/full/10.1056/NEJMoa2105000?query=featured_coronavirus	"Specimen Collection and Processing
Beginning in the fall of 2020, all employees and students at the Rockefeller University campus (approximately 1400 persons) were tested at least weekly with a saliva-based PCR test developed in the Darnell Clinical Laboratory Improvement Amendments–Clinical Laboratory Evaluation Program laboratory (approval number, PFI-9216) and approved for clinical use by a New York State emergency use authorization. Protocols for the collection of saliva samples for clinical SARS-CoV-2 testing were reviewed by the institutional review board at Rockefeller University and were deemed not to be research involving human subjects. Institutional review board–approved written informed consent for the analysis of antibody titers was obtained from Patient 1, and the study was conducted in accordance with International Council for Harmonisation Good Clinical Practice guidelines.
In accordance with New York State regulations regarding eligibility, 417 employees who had received a second dose of either the BNT162b2 (Pfizer–BioNTech) or mRNA-1273 (Moderna) vaccine at least 2 weeks previously were tested between January 21 and March 17, 2021, and weekly testing continued thereafter. The demographic characteristics of these 417 persons and of 1491 unvaccinated persons tested in parallel at Rockefeller University during the same period are shown in Table S1 of the Supplementary Appendix, available with the full text of this article at NEJM.org.
The employees and students were instructed to provide a saliva sample in a medicine cup and transfer 300 μl into a vial containing 300 μl of Darnell Rockefeller University Laboratory (DRUL) buffer (5 M of guanidine thiocyanate, 0.5% sarkosyl, and 300 mM of sodium acetate [pH 5.5]).2 Samples were processed on the Thermo KingFisher Apex system for rapid RNA purification, and complementary DNA (cDNA) was amplified with the use of TaqPath 1-Step RT-qPCR (reverse-transcriptase quantitative PCR) Master Mix (Thermo Fisher Scientific) and multiplexed primers and probes that were validated under a Food and Drug Administration emergency use authorization (Table S2) with the 7500 Fast Dx Real-Time PCR detection system (Applied Biosystems). Samples were considered to be interpretable if the housekeeping control (RNase P) cycle threshold (Ct) was less than 40, and viral RNA was considered to be detected with both viral primers and probes (N1 and N2, detecting two regions of the nucleocapsid [N] gene of SARS-CoV-2) at a Ct of less than 40.
Viral Load Calculation
We calculated the viral load per milliliter of saliva using chemically inactivated SARS-CoV-2 (ZeptoMetrix) spiked into saliva at various dilutions. Extractions and RT-PCR were performed as described previously to determine the corresponding Ct values for each dilution (Fig. S1).
Targeted Sequencing
Reverse transcription of RNA samples was performed with the iScript mix (Bio-Rad) according to the manufacturer’s instructions. PCR amplification of cDNA was performed with the use of two primer sets (primer set 1: forward primer 1 [CCAGATGATTTTACAGGCTGC] and reverse primer 1 [CTACTGATGTCTTGGTCATAGAC]; primer set 2: forward primer 2 [CTTGTTTTATTGCCACTAGTC] and reverse primer 1). PCR products were then extracted from gel and sent to Genewiz for Sanger sequencing.
Neutralization Assay
Neutralization assays with pseudotyped replication defective human immunodeficiency virus type 1 modified with SARS-CoV-2 spike protein were performed as previously described.3 Mean serum neutralizing antibody titers (50% neutralization testing [NT 50 ]) were calculated as an average of three independent experiments, each performed with the use of technical duplicates, and statistical significance was determined with the two-tailed Mann–Whitney test.
Whole Viral RNA Genome Sequencing
Total RNA was extracted as described above, and a meta-transcriptomic library was constructed for paired-end (150-bp reads) sequencing with an Illumina MiSeq platform. Libraries were prepared with the SureSelect XT HS2 DNA System (Agilent Technologies) and Community Design Pan Human Coronavirus Panel (Agilent Technologies) according to the manufacturer’s instructions. FASTQ files (a text-based format for storing both a biologic sequence and its corresponding quality scores) were trimmed with Agilent Genomics NextGen Toolkit (AGeNT) software (version 2.0.5) and used for downstream analysis. The SARS-CoV-2 genome was assembled with MEGAHIT with default parameters, and the longest sequence (30,005 nucleotides) was analyzed with Nextclade software (https://clades.nextstrain.org/) in order to assign the clade and call mutations. Detected mutations were confirmed by aligning RNA sequencing reads on the reference genome sequence of SARS-CoV-2 (GenBank number, NC_045512) with the Burrows–Wheeler Aligner (BWA-MEM).
Patient Histories
Patient 1 was a healthy 51-year-old woman with no risk factors for severe Covid-19 who received the first dose of mRNA-1273 vaccine on January 21, 2021, and the second dose on February 19. She had adhered strictly to routine precautions. Ten hours after she received the second vaccine dose, flulike muscle aches developed. These symptoms resolved the following day. On March 10 (19 days after she received the second vaccine dose), a sore throat, congestion, and headache developed, and she tested positive for SARS-CoV-2 RNA at Rockefeller University later that day. On March 11, she lost her sense of smell. Her symptoms gradually resolved over a 1-week period.
Patient 2 was a healthy 65-year-old woman with no risk factors for severe Covid-19 who received the first dose of BNT162b2 vaccine on January 19 and the second dose on February 9. Pain that developed in the inoculated arm lasted for 2 days. On March 3, her unvaccinated partner tested positive for SARS-CoV-2, and on March 16, fatigue, sinus congestion, and a headache developed in Patient 2. On March 17, she felt worse and tested positive for SARS-CoV-2 RNA, 36 days after completing vaccination. Her symptoms plateaued and began to resolve on March 20."	6077
covid19	['Gabriele', 'Chiara', 'Claudia', 'Gianni', 'Gori Savellini', 'Simonetta', 'Giovanni B.', 'M. Grazia', 'Already A Subscriber', 'E.M. White Et Al.']		"To the Editor:

Whether or not persons who have already been infected with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) should be vaccinated is unclear. Only a few studies have shown that vaccinees who were previously infected with SARS-CoV-2 had a significantly higher antibody response than previously uninfected vaccinees.1-4 In an observational cohort study, we enrolled 100 health care workers, including 38 (9 men and 29 women) with a documented history of SARS-CoV-2 infection (mean duration between infection and vaccination, 111 days). The mean age of these previously infected participants was 35.1 years (95% confidence interval [CI], 31.7 to 38.6). Our study also included 62 participants (25 men and 37 women) who had not been previously infected. The mean age of those participants was 44.7 years (95% CI, 41.0 to 47.6).

Both groups of participants received the messenger RNA vaccine BNT162b2 (Pfizer–BioNTech). Serum samples were obtained from the previously infected participants 10 days after the administration of the first dose and from the previously uninfected participants 10 days after the administration of the second dose. Thereafter, all the participants were screened for the presence of specific anti–SARS-CoV-2 spike IgG by means of a chemiluminescence microparticle immunoassay.

Figure 1. Figure 1. Immune Response in Participants with or without Previous SARS-CoV-2 Infection. Shown are titers of circulating severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) anti-spike IgG antibodies (Panel A) and neutralizing SARS-CoV-2 anti-spike IgG antibodies (Panel B) in serum samples obtained from previously infected participants after they received a single dose of vaccine and in samples obtained from previously uninfected participants after they received a second dose of vaccine. Differences in circulating (Panel C) and neutralizing (Panel D) IgG antibodies in samples obtained from previously infected participants were evaluated according to the duration from natural infection to vaccination (1 to 2 months, >2 months to 3 months, or >3 months). In each box-and-whisker plot, the horizontal line represents the median, the top and bottom of the box the interquartile range, and the whiskers the minimum and maximum values. GMT denotes geometric mean titer.

No significant difference in circulating anti-spike IgG antibody titers was observed between the samples from previously infected participants (mean level, 20,120 arbitrary units per milliliter; 95% CI, 16,400 to 23,800) and those from previously uninfected participants (mean level, 22,639 arbitrary units per milliliter; 95% CI, 19,400 to 25,900) (median levels are shown in Figure 1A). Circulating anti-spike IgG antibodies were not detected in only one previously infected participant; that participant did not have an antibody response to natural infection with SARS-CoV-2.

The same serum samples were also analyzed for the presence of specific anti–SARS-CoV-2 neutralizing antibodies. We observed a difference in levels of neutralizing antibodies between samples from the previously infected participants (geometric mean titer, 569; 95% CI, 467 to 670) and those from the previously uninfected participants (geometric mean titer, 118; 95% CI, 85 to 152) (P<0.001) (median levels are shown in Figure 1B). No substantial differences were noted between the titers from the previously infected and the previously uninfected participants according to age (Fig. S1 in the Supplementary Appendix, available with the full text of this letter at NEJM.org) or sex (data not shown).

The previously infected participants were categorized into three groups according to the time that had elapsed from infection to vaccination: 1 to 2 months (8 participants), more than 2 months to 3 months (17 participants), and more than 3 months (12 participants). The previously infected patient in whom circulating anti-spike IgG antibodies were not detected was not included in this categorization. The circulating IgG mean titers differed between the group vaccinated at 1 to 2 months and the group vaccinated at more than 2 months to 3 months after natural infection (mean level, 15,837 arbitrary units per milliliter [95% CI, 11,265 to 20,410] vs. 21,450 arbitrary units per milliliter [95% CI, 15,377 to 27,523]) (median levels are shown in Figure 1C); however, because the number of participants was limited, a real distinction cannot be made. No further significant difference was observed between the group of participants vaccinated at more than 2 months to 3 months and the group of those vaccinated more than 3 months after infection (mean level, 21,090 arbitrary units per milliliter [95% CI, 14,702 to 27,477]).

The differences among the three groups were more evident with respect to levels of neutralizing antibodies, with geometric mean titers ranging from 437 (95% CI, 231 to 643) in participants vaccinated 1 to 2 months after infection to 559 (95% CI, 389 to 730) in those vaccinated more than 2 months to 3 months after infection to 694 (95% CI, 565 to 823) in those vaccinated more than 3 months after infection (median levels are shown in Figure 1D). Although these findings indicate that the booster response was more efficacious when the vaccine was administered more than 3 months after infection, not enough information is available to draw a definitive conclusion.

The most remarkable finding of this study was the significantly lower neutralizing antibody titer after administration of a second dose of vaccine in previously uninfected patients than the titer after only a single dose of vaccine in previously infected participants. It is unclear how the neutralizing antibody titers influence the ability of the host to transmit the virus. These findings provide evidence that after the administration of a single dose of vaccine, the humoral response against SARS-CoV-2 in persons with a history of SARS-CoV-2 infection is greater than the response in previously uninfected participants who have received a second dose.

Gabriele Anichini, M.S.

Chiara Terrosi, M.S.

Claudia Gandolfo, Ph.D.

Gianni Gori Savellini, Ph.D.

University of Siena, Siena, Italy

Simonetta Fabrizi, M.D.

Giovanni B. Miceli, M.D.

Santa Maria alle Scotte University Hospital, Siena, Italy

M. Grazia Cusi, Ph.D.

University of Siena, Siena, Italy

[email protected]

Disclosure forms provided by the authors are available with the full text of this letter at NEJM.org.

This letter was published on April 14, 2021, at NEJM.org."	https://www.nejm.org/doi/full/10.1056/NEJMc2103825?query=featured_coronavirus	"To the Editor:
Whether or not persons who have already been infected with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) should be vaccinated is unclear. Only a few studies have shown that vaccinees who were previously infected with SARS-CoV-2 had a significantly higher antibody response than previously uninfected vaccinees.1-4 In an observational cohort study, we enrolled 100 health care workers, including 38 (9 men and 29 women) with a documented history of SARS-CoV-2 infection (mean duration between infection and vaccination, 111 days). The mean age of these previously infected participants was 35.1 years (95% confidence interval [CI], 31.7 to 38.6). Our study also included 62 participants (25 men and 37 women) who had not been previously infected. The mean age of those participants was 44.7 years (95% CI, 41.0 to 47.6).
Both groups of participants received the messenger RNA vaccine BNT162b2 (Pfizer–BioNTech). Serum samples were obtained from the previously infected participants 10 days after the administration of the first dose and from the previously uninfected participants 10 days after the administration of the second dose. Thereafter, all the participants were screened for the presence of specific anti–SARS-CoV-2 spike IgG by means of a chemiluminescence microparticle immunoassay.
Figure 1. Figure 1. Immune Response in Participants with or without Previous SARS-CoV-2 Infection. Shown are titers of circulating severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) anti-spike IgG antibodies (Panel A) and neutralizing SARS-CoV-2 anti-spike IgG antibodies (Panel B) in serum samples obtained from previously infected participants after they received a single dose of vaccine and in samples obtained from previously uninfected participants after they received a second dose of vaccine. Differences in circulating (Panel C) and neutralizing (Panel D) IgG antibodies in samples obtained from previously infected participants were evaluated according to the duration from natural infection to vaccination (1 to 2 months, >2 months to 3 months, or >3 months). In each box-and-whisker plot, the horizontal line represents the median, the top and bottom of the box the interquartile range, and the whiskers the minimum and maximum values. GMT denotes geometric mean titer.
No significant difference in circulating anti-spike IgG antibody titers was observed between the samples from previously infected participants (mean level, 20,120 arbitrary units per milliliter; 95% CI, 16,400 to 23,800) and those from previously uninfected participants (mean level, 22,639 arbitrary units per milliliter; 95% CI, 19,400 to 25,900) (median levels are shown in Figure 1A). Circulating anti-spike IgG antibodies were not detected in only one previously infected participant; that participant did not have an antibody response to natural infection with SARS-CoV-2.
The same serum samples were also analyzed for the presence of specific anti–SARS-CoV-2 neutralizing antibodies. We observed a difference in levels of neutralizing antibodies between samples from the previously infected participants (geometric mean titer, 569; 95% CI, 467 to 670) and those from the previously uninfected participants (geometric mean titer, 118; 95% CI, 85 to 152) (P<0.001) (median levels are shown in Figure 1B). No substantial differences were noted between the titers from the previously infected and the previously uninfected participants according to age (Fig. S1 in the Supplementary Appendix, available with the full text of this letter at NEJM.org) or sex (data not shown).
The previously infected participants were categorized into three groups according to the time that had elapsed from infection to vaccination: 1 to 2 months (8 participants), more than 2 months to 3 months (17 participants), and more than 3 months (12 participants). The previously infected patient in whom circulating anti-spike IgG antibodies were not detected was not included in this categorization. The circulating IgG mean titers differed between the group vaccinated at 1 to 2 months and the group vaccinated at more than 2 months to 3 months after natural infection (mean level, 15,837 arbitrary units per milliliter [95% CI, 11,265 to 20,410] vs. 21,450 arbitrary units per milliliter [95% CI, 15,377 to 27,523]) (median levels are shown in Figure 1C); however, because the number of participants was limited, a real distinction cannot be made. No further significant difference was observed between the group of participants vaccinated at more than 2 months to 3 months and the group of those vaccinated more than 3 months after infection (mean level, 21,090 arbitrary units per milliliter [95% CI, 14,702 to 27,477]).
The differences among the three groups were more evident with respect to levels of neutralizing antibodies, with geometric mean titers ranging from 437 (95% CI, 231 to 643) in participants vaccinated 1 to 2 months after infection to 559 (95% CI, 389 to 730) in those vaccinated more than 2 months to 3 months after infection to 694 (95% CI, 565 to 823) in those vaccinated more than 3 months after infection (median levels are shown in Figure 1D). Although these findings indicate that the booster response was more efficacious when the vaccine was administered more than 3 months after infection, not enough information is available to draw a definitive conclusion.
The most remarkable finding of this study was the significantly lower neutralizing antibody titer after administration of a second dose of vaccine in previously uninfected patients than the titer after only a single dose of vaccine in previously infected participants. It is unclear how the neutralizing antibody titers influence the ability of the host to transmit the virus. These findings provide evidence that after the administration of a single dose of vaccine, the humoral response against SARS-CoV-2 in persons with a history of SARS-CoV-2 infection is greater than the response in previously uninfected participants who have received a second dose.
Gabriele Anichini, M.S.
Chiara Terrosi, M.S.
Claudia Gandolfo, Ph.D.
Gianni Gori Savellini, Ph.D.
University of Siena, Siena, Italy
Simonetta Fabrizi, M.D.
Giovanni B. Miceli, M.D.
Santa Maria alle Scotte University Hospital, Siena, Italy
M. Grazia Cusi, Ph.D.
University of Siena, Siena, Italy
[email protected]
Disclosure forms provided by the authors are available with the full text of this letter at NEJM.org.
This letter was published on April 14, 2021, at NEJM.org."	6519
covid19	['Sheila F.', 'Denise', 'O Donnell', 'Nicole E.', 'Philippa C.', 'Alison', 'Stephanie B.', 'Brian D.', 'Stuart', 'Tim']		"In this longitudinal cohort study, the presence of anti-spike antibodies was associated with a substantially reduced risk of PCR-confirmed SARS-CoV-2 infection over 31 weeks of follow-up. No symptomatic infections and only two PCR-positive results in asymptomatic health care workers were seen in those with anti-spike antibodies, which suggests that previous infection resulting in antibodies to SARS-CoV-2 is associated with protection from reinfection for most people for at least 6 months. Evidence of postinfection immunity was also seen when anti-nucleocapsid IgG or the combination of anti-nucleocapsid and anti-spike IgG was used as a marker of previous infection.

The incidence of SARS-CoV-2 infection was inversely associated with baseline anti-spike and anti-nucleocapsid antibody titers, including titers below the positive threshold for both assays, such that workers with high “negative” titers were relatively protected from infection. In addition to the 24 seronegative health care workers with a previous positive PCR test, it is likely that other health care workers with baseline titers below assay thresholds, which were set to ensure high specificity,23 had been previously infected with SARS-CoV-2 and had low peak postinfection titers or rising or waning responses at testing.5

Two of the three seropositive health care workers who had subsequent PCR-positive tests had discordant baseline antibody results, a finding that highlights the imperfect nature of antibody assays as markers of previous infection. Neither worker had a PCR-confirmed primary SARS-CoV-2 infection. Subsequent symptomatic infection developed in one worker, and both workers had subsequent dual antibody seroconversion. It is plausible that one or both had false positive baseline antibody results (e.g., from immunoassay interference27). The health care worker in whom both anti-spike and anti-nucleocapsid antibodies were detected had previously had PCR-confirmed SARS-CoV-2 infection; the subsequent PCR-positive result with a low viral load was not confirmed on repeat testing and was not associated with a change in IgG response. These results could be consistent with a reexposure to SARS-CoV-2 that did not lead to symptoms but could also plausibly have arisen from undetected laboratory error; although contemporaneous retesting of the PCR-positive sample was not undertaken, samples tested 2 and 4 days later were both negative. If the PCR-positive result is incorrect, the incidence rate ratio for PCR positivity if anti-spike IgG–seropositive would fall to 0.05. We detected and did not include in our analysis a presumed false positive PCR test in a fourth seropositive health care worker.

Owing to the low number of reinfections in seropositive health care workers, we cannot say whether past seroconversion or current antibody levels determine protection from infection or define which characteristics are associated with reinfection. Similarly, we cannot say whether protection is conferred through the antibodies we measured or through T-cell immunity, which we did not assess. It was not possible to use sequencing to compare primary and subsequent infections, since only one of the three seropositive health care workers with a subsequent PCR-positive test had PCR-confirmed primary infection and that worker’s original sample was not stored. Our study was relatively short, with up to 31 weeks of follow-up. Ongoing follow-up is needed in this and other cohorts, including the use of markers of both humoral and cellular immunity to SARS-CoV-2, to assess the magnitude and duration of protection from reinfection, symptomatic disease, and hospitalization or death and the effect of protection on transmission.

Health care workers were enrolled in a voluntary testing program with a flexible follow-up schedule, which led to different attendance frequencies. Although health care workers were offered asymptomatic PCR testing every 2 weeks, the workers attended less frequently than that (mean, once every 10 to 13 weeks). Therefore, asymptomatic infection is likely to have been underascertained. In addition, as staff were told their antibody results, “outcome ascertainment bias” occurred, with seropositive staff attending asymptomatic screening less frequently. However, a sensitivity analysis suggests that the differing attendance rates did not substantially alter our findings. Staff were told to follow guidance on social distancing and use of personal protective equipment and to attend testing if Covid-19 symptoms developed, even if the worker had been previously PCR- or antibody-positive. This is reflected in the similar rates of testing of symptomatic seropositive and seronegative health care workers.

Some health care workers were lost to follow-up after terminating employment at our hospitals; this was likely to have occurred at similar rates in seropositive and seronegative staff. Not all PCR-positive results from government symptomatic testing sites were communicated to the hospital. This is a study of predominantly healthy adult health care workers 65 years of age or younger; further studies are needed to assess postinfection immunity in other populations, including children, older adults, and persons with coexisting conditions, including immunosuppression.

In this study, we found a substantially lower risk of reinfection with SARS-CoV-2 in the short term among health care workers with anti-spike antibodies and those with anti-nucleocapsid antibodies than among those who were seronegative."	https://www.nejm.org/doi/full/10.1056/NEJMoa2034545?query=featured_coronavirus	"In this longitudinal cohort study, the presence of anti-spike antibodies was associated with a substantially reduced risk of PCR-confirmed SARS-CoV-2 infection over 31 weeks of follow-up. No symptomatic infections and only two PCR-positive results in asymptomatic health care workers were seen in those with anti-spike antibodies, which suggests that previous infection resulting in antibodies to SARS-CoV-2 is associated with protection from reinfection for most people for at least 6 months. Evidence of postinfection immunity was also seen when anti-nucleocapsid IgG or the combination of anti-nucleocapsid and anti-spike IgG was used as a marker of previous infection.
The incidence of SARS-CoV-2 infection was inversely associated with baseline anti-spike and anti-nucleocapsid antibody titers, including titers below the positive threshold for both assays, such that workers with high “negative” titers were relatively protected from infection. In addition to the 24 seronegative health care workers with a previous positive PCR test, it is likely that other health care workers with baseline titers below assay thresholds, which were set to ensure high specificity,23 had been previously infected with SARS-CoV-2 and had low peak postinfection titers or rising or waning responses at testing.5
Two of the three seropositive health care workers who had subsequent PCR-positive tests had discordant baseline antibody results, a finding that highlights the imperfect nature of antibody assays as markers of previous infection. Neither worker had a PCR-confirmed primary SARS-CoV-2 infection. Subsequent symptomatic infection developed in one worker, and both workers had subsequent dual antibody seroconversion. It is plausible that one or both had false positive baseline antibody results (e.g., from immunoassay interference27). The health care worker in whom both anti-spike and anti-nucleocapsid antibodies were detected had previously had PCR-confirmed SARS-CoV-2 infection; the subsequent PCR-positive result with a low viral load was not confirmed on repeat testing and was not associated with a change in IgG response. These results could be consistent with a reexposure to SARS-CoV-2 that did not lead to symptoms but could also plausibly have arisen from undetected laboratory error; although contemporaneous retesting of the PCR-positive sample was not undertaken, samples tested 2 and 4 days later were both negative. If the PCR-positive result is incorrect, the incidence rate ratio for PCR positivity if anti-spike IgG–seropositive would fall to 0.05. We detected and did not include in our analysis a presumed false positive PCR test in a fourth seropositive health care worker.
Owing to the low number of reinfections in seropositive health care workers, we cannot say whether past seroconversion or current antibody levels determine protection from infection or define which characteristics are associated with reinfection. Similarly, we cannot say whether protection is conferred through the antibodies we measured or through T-cell immunity, which we did not assess. It was not possible to use sequencing to compare primary and subsequent infections, since only one of the three seropositive health care workers with a subsequent PCR-positive test had PCR-confirmed primary infection and that worker’s original sample was not stored. Our study was relatively short, with up to 31 weeks of follow-up. Ongoing follow-up is needed in this and other cohorts, including the use of markers of both humoral and cellular immunity to SARS-CoV-2, to assess the magnitude and duration of protection from reinfection, symptomatic disease, and hospitalization or death and the effect of protection on transmission.
Health care workers were enrolled in a voluntary testing program with a flexible follow-up schedule, which led to different attendance frequencies. Although health care workers were offered asymptomatic PCR testing every 2 weeks, the workers attended less frequently than that (mean, once every 10 to 13 weeks). Therefore, asymptomatic infection is likely to have been underascertained. In addition, as staff were told their antibody results, “outcome ascertainment bias” occurred, with seropositive staff attending asymptomatic screening less frequently. However, a sensitivity analysis suggests that the differing attendance rates did not substantially alter our findings. Staff were told to follow guidance on social distancing and use of personal protective equipment and to attend testing if Covid-19 symptoms developed, even if the worker had been previously PCR- or antibody-positive. This is reflected in the similar rates of testing of symptomatic seropositive and seronegative health care workers.
Some health care workers were lost to follow-up after terminating employment at our hospitals; this was likely to have occurred at similar rates in seropositive and seronegative staff. Not all PCR-positive results from government symptomatic testing sites were communicated to the hospital. This is a study of predominantly healthy adult health care workers 65 years of age or younger; further studies are needed to assess postinfection immunity in other populations, including children, older adults, and persons with coexisting conditions, including immunosuppression.
In this study, we found a substantially lower risk of reinfection with SARS-CoV-2 in the short term among health care workers with anti-spike antibodies and those with anti-nucleocapsid antibodies than among those who were seronegative."	5540
covid19	['Teresa', 'Ana S.', 'Sadaf', 'Adriana', 'Van De Guchte', 'Zenab', 'Ajay', 'Jayeeta', 'Harm', 'Van Bakel']		"To the Editor:

Detection of replication-competent severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the most reliable indicator of contagiousness.1 Although the duration of live-virus shedding is well-characterized in immunocompetent patients with coronavirus disease 19 (Covid-19), little is known about how long immunocompromised patients are contagious. Consequently, the Centers for Disease Control and Prevention (CDC) guidelines on transmission-based precautions for immunocompromised patients are based on limited data.2

Figure 1. Figure 1. Study Design and Genetic Variant Profiles of Sequenced SARS-CoV-2. Panel A shows the patient enrollment, respiratory sample collection (all nasopharyngeal samples except for one sputum sample), and testing design. The 𝙸 bar indicates serial collection of samples. RT-qPCR denotes quantitative reverse-transcriptase polymerase chain reaction. Panel B shows the genetic variant profiles of sequenced SARS-CoV-2 relative to the Wuhan-Hu-1 reference genome in 13 patients who had at least one cultured isolate (plus sign), more than one longitudinal sample sequenced, or both. The genomes shown were derived from the original respiratory samples. The mean (±SD) read depth was 2481±1558 reads per base. Assembled genomic regions are indicated by shaded areas colored according to patient, and gaps in coverage are indicated by white areas. Complete genome sequences from the 4 patients who did not have a corresponding culture or follow-up samples are not included. The replicase domains (ORF1ab 1 and 2, ORF3a, ORF6, ORF7a, and ORF8) and the S, M, and N regions of the reference genome are shown on the x axis. UTR denotes untranslated region.

In the current study, we used cell cultures to detect viable virus in serially collected respiratory samples (nasopharyngeal and sputum samples) obtained from 20 immunocompromised patients who had Covid-19 (Figure 1A). These patients included 18 recipients of hematopoietic stem-cell transplants or chimeric antigen receptor (CAR) T-cell therapy and 2 patients with lymphoma. Covid-19 was diagnosed between March 10 and April 20, 2020, with the use of a modified CDC nucleic acid amplification test. Live virus was isolated in Vero cells, and genetic variants were identified by whole-genome sequencing of nasopharyngeal and cultured specimens (see the Supplemental Methods section of the Supplementary Appendix, available with the full text of this letter at NEJM.org). The patients’ demographic characteristics, medical history, and clinical course of Covid-19 were abstracted from medical records (Table S1 in the Supplementary Appendix).

Of the 20 patients, 15 were receiving active treatment or chemotherapy. Eleven had severe Covid-19. A total of 78 samples were collected from the 20 patients; 57 samples were obtained in the time periods shown in Figure S1. Viral RNA was detected for up to 78 days after the onset of symptoms (interquartile range, 24 to 64 days). Viable virus was detected in 10 of 14 nasopharyngeal samples (71%) that were available from the first day of laboratory testing. Follow-up samples obtained from 5 patients (Patients MSK-3, MSK-4, MSK-6, MSK-8, and MSK-9) grew virus in culture for 8, 17, 25, 26, and 61 days after the onset of symptoms (Figure 1). The 3 patients with viable virus for more than 20 days had received allogeneic hematopoietic stem-cell transplants (2 patients) or CAR T-cell therapy (1 patient) within the previous 6 months and remained seronegative for antibodies to viral nucleoprotein; 2 of these patients had severe Covid-19 and received investigational treatments.

Whole-genome sequencing detected viral reads in all the samples and yielded more than 95% complete SARS-CoV-2 genomes for 37 of 57 nasopharyngeal samples obtained from 17 patients and all 18 cultured specimens (accession numbers, EPI_ISL_583426 to EPI_ISL_583480 [55 complete genomes]). Serial sample genomes were obtained for 11 patients, up to day 63 after the onset of symptoms. Each patient was infected by a distinct virus, and there were no major changes in the consensus sequences of the original serial specimens or cultured isolates (Figure 1B); these findings were consistent with persistent infection.

Patients with profound immunosuppression after undergoing hematopoietic stem-cell transplantation or receiving cellular therapies may shed viable SARS-CoV-2 for at least 2 months. The current guidelines for Covid-19 isolation precautions may need to be revised for immunocompromised patients.

Teresa Aydillo, Ph.D.

Ana S. Gonzalez-Reiche, Ph.D.

Sadaf Aslam, B.A.

Adriana van de Guchte, M.S.

Zenab Khan, B.S.

Ajay Obla, Ph.D.

Jayeeta Dutta, M.B.A.

Harm van Bakel, Ph.D.

Judith Aberg, M.D.

Adolfo García-Sastre, Ph.D.

Icahn School of Medicine at Mount Sinai, New York, NY

Gunjan Shah, M.D.

Tobias Hohl, M.D., Ph.D.

Genovefa Papanicolaou, M.D.

Miguel-Angel Perales, M.D.

Kent Sepkowitz, M.D.

N. Esther Babady, Ph.D.

Mini Kamboj, M.D.

Memorial Sloan Kettering Cancer Center, New York, NY

[email protected]

Supported by an award (P01 CA23766) and a National Institutes of Health (NIH)– National Cancer Institute Cancer Center Support Grant (P30 CA008748) from the NIH, a grant (to Drs. Hohl, Babady, and Kamboj) from the Jack and Dorothy Byrne Foundation, a contract from the National Institute of Allergy and Infectious Diseases (HHSN272201400008C) awarded to the Center for Research on Influenza Pathogenesis –(a Center of Excellence for Influenza Research and Surveillance), philanthropic donations from the JPB Foundation , a research grant (2020-215611 [5384]) from the Open Philanthropy Project, philanthropic donations (to Dr. García-Sastre) from Mount Sinai Philanthropy, awards (S10OD018522 and S10OD026880) from the NIH Office of Research Infrastructure Programs, and a Robin Chemers Neustein Postdoctoral Fellowship Award. (to Dr. Gonzalez-Reiche). Disclosure forms provided by the authors are available with the full text of this letter at NEJM.org.

The content of this letter is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.

This letter was published on December 1, 2020, at NEJM.org.

Drs. Babady and Kamboj contributed equally to this letter."	https://www.nejm.org/doi/full/10.1056/NEJMc2031670?query=featured_coronavirus	"To the Editor:
Detection of replication-competent severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the most reliable indicator of contagiousness.1 Although the duration of live-virus shedding is well-characterized in immunocompetent patients with coronavirus disease 19 (Covid-19), little is known about how long immunocompromised patients are contagious. Consequently, the Centers for Disease Control and Prevention (CDC) guidelines on transmission-based precautions for immunocompromised patients are based on limited data.2
Figure 1. Figure 1. Study Design and Genetic Variant Profiles of Sequenced SARS-CoV-2. Panel A shows the patient enrollment, respiratory sample collection (all nasopharyngeal samples except for one sputum sample), and testing design. The 𝙸 bar indicates serial collection of samples. RT-qPCR denotes quantitative reverse-transcriptase polymerase chain reaction. Panel B shows the genetic variant profiles of sequenced SARS-CoV-2 relative to the Wuhan-Hu-1 reference genome in 13 patients who had at least one cultured isolate (plus sign), more than one longitudinal sample sequenced, or both. The genomes shown were derived from the original respiratory samples. The mean (±SD) read depth was 2481±1558 reads per base. Assembled genomic regions are indicated by shaded areas colored according to patient, and gaps in coverage are indicated by white areas. Complete genome sequences from the 4 patients who did not have a corresponding culture or follow-up samples are not included. The replicase domains (ORF1ab 1 and 2, ORF3a, ORF6, ORF7a, and ORF8) and the S, M, and N regions of the reference genome are shown on the x axis. UTR denotes untranslated region.
In the current study, we used cell cultures to detect viable virus in serially collected respiratory samples (nasopharyngeal and sputum samples) obtained from 20 immunocompromised patients who had Covid-19 (Figure 1A). These patients included 18 recipients of hematopoietic stem-cell transplants or chimeric antigen receptor (CAR) T-cell therapy and 2 patients with lymphoma. Covid-19 was diagnosed between March 10 and April 20, 2020, with the use of a modified CDC nucleic acid amplification test. Live virus was isolated in Vero cells, and genetic variants were identified by whole-genome sequencing of nasopharyngeal and cultured specimens (see the Supplemental Methods section of the Supplementary Appendix, available with the full text of this letter at NEJM.org). The patients’ demographic characteristics, medical history, and clinical course of Covid-19 were abstracted from medical records (Table S1 in the Supplementary Appendix).
Of the 20 patients, 15 were receiving active treatment or chemotherapy. Eleven had severe Covid-19. A total of 78 samples were collected from the 20 patients; 57 samples were obtained in the time periods shown in Figure S1. Viral RNA was detected for up to 78 days after the onset of symptoms (interquartile range, 24 to 64 days). Viable virus was detected in 10 of 14 nasopharyngeal samples (71%) that were available from the first day of laboratory testing. Follow-up samples obtained from 5 patients (Patients MSK-3, MSK-4, MSK-6, MSK-8, and MSK-9) grew virus in culture for 8, 17, 25, 26, and 61 days after the onset of symptoms (Figure 1). The 3 patients with viable virus for more than 20 days had received allogeneic hematopoietic stem-cell transplants (2 patients) or CAR T-cell therapy (1 patient) within the previous 6 months and remained seronegative for antibodies to viral nucleoprotein; 2 of these patients had severe Covid-19 and received investigational treatments.
Whole-genome sequencing detected viral reads in all the samples and yielded more than 95% complete SARS-CoV-2 genomes for 37 of 57 nasopharyngeal samples obtained from 17 patients and all 18 cultured specimens (accession numbers, EPI_ISL_583426 to EPI_ISL_583480 [55 complete genomes]). Serial sample genomes were obtained for 11 patients, up to day 63 after the onset of symptoms. Each patient was infected by a distinct virus, and there were no major changes in the consensus sequences of the original serial specimens or cultured isolates (Figure 1B); these findings were consistent with persistent infection.
Patients with profound immunosuppression after undergoing hematopoietic stem-cell transplantation or receiving cellular therapies may shed viable SARS-CoV-2 for at least 2 months. The current guidelines for Covid-19 isolation precautions may need to be revised for immunocompromised patients.
Teresa Aydillo, Ph.D.
Ana S. Gonzalez-Reiche, Ph.D.
Sadaf Aslam, B.A.
Adriana van de Guchte, M.S.
Zenab Khan, B.S.
Ajay Obla, Ph.D.
Jayeeta Dutta, M.B.A.
Harm van Bakel, Ph.D.
Judith Aberg, M.D.
Adolfo García-Sastre, Ph.D.
Icahn School of Medicine at Mount Sinai, New York, NY
Gunjan Shah, M.D.
Tobias Hohl, M.D., Ph.D.
Genovefa Papanicolaou, M.D.
Miguel-Angel Perales, M.D.
Kent Sepkowitz, M.D.
N. Esther Babady, Ph.D.
Mini Kamboj, M.D.
Memorial Sloan Kettering Cancer Center, New York, NY
[email protected]
Supported by an award (P01 CA23766) and a National Institutes of Health (NIH)– National Cancer Institute Cancer Center Support Grant (P30 CA008748) from the NIH, a grant (to Drs. Hohl, Babady, and Kamboj) from the Jack and Dorothy Byrne Foundation, a contract from the National Institute of Allergy and Infectious Diseases (HHSN272201400008C) awarded to the Center for Research on Influenza Pathogenesis –(a Center of Excellence for Influenza Research and Surveillance), philanthropic donations from the JPB Foundation , a research grant (2020-215611 [5384]) from the Open Philanthropy Project, philanthropic donations (to Dr. García-Sastre) from Mount Sinai Philanthropy, awards (S10OD018522 and S10OD026880) from the NIH Office of Research Infrastructure Programs, and a Robin Chemers Neustein Postdoctoral Fellowship Award. (to Dr. Gonzalez-Reiche). Disclosure forms provided by the authors are available with the full text of this letter at NEJM.org.
The content of this letter is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.
This letter was published on December 1, 2020, at NEJM.org.
Drs. Babady and Kamboj contributed equally to this letter."	6278
covid19	['Paolo Ferdinando Bruno', 'Maria Cappuccilli', 'Alessandra Spazzoli', 'Matteo De Liberali', 'Brunilda Sejdiu', 'Marianna Napoli', 'Vera Minerva', 'Simona Semprini', 'Giorgio Dirani', 'Vittorio Sambri']	2021-05-22 00:00:00	"Abstract

Background/Aims: The coronavirus disease 2019 (CO­VID-19) pandemic is the major current health emergency worldwide, adding a significant burden also to the community of nephrologists for the management of their patients. Here, we analyzed the impact of COVID-19 infection in renal patients to assess the time to viral clearance, together with the production and persistence of IgG and IgM antibody response, in consideration of the altered immune capacity of this fragile population. Methods: Viral clearance and antibody kinetics were investigated in 49 renal patients recovered from COVID-19 infection: 7 of them with chronic decompensated renal failure, 31 under dialysis treatment, and 11 kidney transplant recipients. Results: The time span between the diagnosis of infection and recovery based on laboratory testing (2 negative nasopharyngeal swabs in consecutive days) was 31.7 ± 13.3 days. Three new positive cases were detected from 8 to 13 days following recovery. At the first serological determination after swab negativization, all the patients developed IgG and IgM antibodies. The semiquantitative analysis showed a progressive increase in IgG and a slow reduction in IgM. Discussion/Conclusion: In subjects with decompensated chronic kidney disease, under dialysis and in transplant recipients, viral clearance is lengthened compared to the general population. However, in spite of their common status of immunodepression, all of them were able to produce specific antibodies. These data might provide useful insights for monitoring and planning health-care activities in the weak category of patients with compromised renal function recovered from COVID-19.

© 2021 S. Karger AG, Basel

Introduction

The coronavirus disease 2019 (COVID-19) pandemic is a major clinical and social problem in all the countries. At the end of year 2020, the number of cases worldwide was 83,102,166, with 1,812,671 deaths and 58,897,317 recovered (https://www.worldometers.info/coronavirus/, browsed on December 31, 2020).

The impact on health care in the nephrological setting is significant. In analogy with previous infections by other coronaviruses [1], COVID-19 has shown a significant spread among patients with advanced chronic renal failure (stages 3b, 4, and 5 based on the Chronic Kidney Disease Epidemiology Collaboration [CKD-EPI] classification), those under chronic dialysis treatment and kidney transplant recipients [2, 3]. The infection is associated with poorer outcomes in subjects with impaired renal function than in the general population, mainly due to the presence of concurrent chronic diseases, in particular cardiovascular comorbidity [4]. Moreover, besides the well-known link of SARS-CoV-2 infection and respiratory distress syndrome, there are further potential mechanisms leading to systemic multiorgan failure. The reason lies in the wide distribution of angiotensin-converting enzyme 2 receptor, the gate of entrance of the virus. Thus, in case of a high viral load, the infection can spread through the angiotensin-converting enzyme 2 receptor to various target organs, including the kidney, heart, liver, brain, endothelium, gastrointestinal tract, immune cells, and erythrocytes (thus causing thromboembolism) [5-7].

The mortality rate reported in the national survey data of the Italian Society of Nephrology is 33.76% in patients receiving hemodialysis treatment and 24.77% in renal transplant recipients [8]. Akin to the general population, death risk is higher with increasing age. Current evidence supports individual inflammatory response as a key player in the development of the disease with its related complications. The immunodepression induced by the metabolic status of chronic kidney disease (CKD) and dialysis [2] or by post-transplant antirejection therapy [3] can influence the clinical course and the eradication of SARS-CoV-2 infection. To date, the role of advanced CKD or renal transplantation on viral clearance and on the pattern of IgM and IgG antibody response over time is still poorly understood [9-13].

A better awareness of the timing until SARS-CoV-2 negativization is particularly relevant for the possible management of organizational repercussions in different health-care settings. In hemodialysis units, the artificial replacement treatment is carried out until laboratory recovery (2 negative nasopharyngeal swabs in consecutive days) in dedicated areas [3, 7, 14]. Similarly, in transplanted patients, the regular follow-up monitoring is conditioned by the persistence of the infection. Studies related to the viral load over time can represent a useful decision-making tool in case of infection relapse after the first recovery. Moreover, a full comprehension of the pattern of antibody production and persistence might also provide valuable indications for prophylaxis choices, above all vaccination, also in view of the known poor response to vaccination observed in patients with advanced CKD.

This study was undertaken to analyze the impact of COVID-19 infection in patients with chronic decompensated renal failure, those under dialysis treatment, and renal transplant recipients. In particular, we investigated the time to achieve viral clearance (recovery based on 2 negative nasopharyngeal swabs in consecutive days), as well as the kinetics of IgG and IgM antibody response, in consideration of the altered cellular and humoral immune responses commonly found in these patients.

Materials and Methods

Patients

A retrospective multicenter observational study was performed in chronic nephropathic patients who experienced SARS-CoV-2 infection (nasopharyngeal swab positivity regardless of subjective symptoms). The observation period was from February to April 2020, with data collection in May 2020.

The analysis considered patients followed at the Unit of Nephrology and Dialysis of the local health authority of Romagna (Forlì-Cesena, Ravenna, and Rimini) and at Nephrology, Dialysis, and Renal Transplant Unit of the S. Orsola Malpighi University Hospital of Bologna, an area with a total reference population of 2.2 inhabitants. Specifically, renal patients who recovered from COVID-19 infection (2 consecutive negative swabs) with the following clinical and functional characteristics were included:

Advanced chronic renal failure (KDOQI stages 3B-5) in conservative therapy with worsening renal function during infection and subsequent need for artificial replacement support (group 1). Chronic dialysis treatment at the time of infection (group 2). Clinically stable renal transplant received at least 6 months prior to the beginning of the study (group 3).

The time span between the first positive swab and the laboratory negativization was evaluated in all the subjects who met the above-mentioned inclusion criteria. The following general and clinical parameters were recorded: age, gender, estimated glomerular filtration rate (eGFR) based on CKD-EPI creatinine-based equation [15], dialysis vintage for group 2, and age at transplant for group 3.

The COVID-19-related data collected for this analysis were onset of the symptoms, severity of lung involvement, therapy for SARS-CoV-2 infection, and modulation of immunosuppressive therapy (for transplanted patients). Ethics Committee approval and informed consent were waived due to the observational nature of the study. The study followed the principles of the Declaration of Helsinki. All individuals cannot be identified in the study, as they have been fully anonymized.

Laboratory Assays

Nasopharyngeal swabs were performed according to the guidelines of the Italian National Institute of Health (Istituto Superiore di Sanità – ISS) [16]. Biological specimens were manipulated using appropriate personal protective equipment and at biosafety level 2, based on the recommendations by the World Health Organization and the European Centre for Disease Prevention and Control [17].

In all the swabs received at the laboratory of our Microbiology Unit, the presence of SARS-CoV-2 was assessed using a commercially available one-step real-time PCR (Allplex 2019-nCoV assay, Seegene, Seoul, South Korea) in which, after RNA extraction, retrotranscription and PCR amplification are carried out consecutively in the same reaction tube. This is a qualitative analysis based on the Seegene MuDTTM technology (combining DPOTM and ­TOCETM), which allows identification of multiple Ct values for every pathogen in each channel of the real-time PCR instrument. The target genes for the identification of SARS-CoV-2 are E (a specific gene common to Sarbecoviruses), RdRp, and N gene both specific for SARS-CoV-2. Sample positivity was attributed in case of detection of at least one of the 2 specific genes. The samples were extracted using the automated Microlab STARlet IVD platform (Hamilton Italia, Agrate Brianza, MB, Italy) and amplified using CFX96 Real-time PCR Detection System (CFX Manager Software-IVD v1.6) (Bio-Rad Laboratories, Segrate, Milan, Italy). The sensitivity declared by the manufacturer is 100 copies/reaction [18].

From 3 to 7 days following nasopharyngeal swab negativization, SARS-CoV-2 IgM and IgG antibody serum levels were measured using a solid-phase ELISA (COVID-19 IgG Enzyme ImmunoAssay and COVID-19 IgM Enzyme ImmunoAssay, DIA PRO Diagnostic Bioprobes, Milan, Italy), in line with the regional plan on serological tests. The assay identifies antibody binding to 3 virus-specific immunodominant antigens: nucleocapsid, spike 1 protein, and spike 2 protein. The detection of IgM antibodies requires a pretreatment step to remove the rheumatoid factor, as previously described [19]. The diagnostic sensitivity is 98%. Serological semiquantitative determinations of IgG and IgM were carried out in a time span ranging from 3 to 8 days following negativization.

The IgG index was considered as negative if <0.9, uncertain (gray zone) if ranging between 0.9 and 1.1, and positive if >1.1. The IgM index was considered as negative if <0.9, uncertain (gray zone) if ranging between 0.9 and 1.1, and positive if >1.1. The C.I. for IgG index between 1.1 and 3.0 and for IgM index between 1.1 and 2.5 are quite low. These values were considered as weakly positive.

Statistical Analysis

Data are given as means ± standard deviation for continuous variables and percentage and absolute numbers for categorical variables. Comparisons between continuous variables were made through Student’s t test or the nonparametric Mann-Whitney U or ANOVA followed by Tukey t test, as appropriate. χ2 test was used to evaluate the categorical variables. A p value below 0.05 was considered as significant, and all the statistical analyses were performed using the statistical package for the social sciences (SPSSTM for Windows Software Package, version 9.0.1; Chicago, IL, USA).

Results

This observational study considered a total of 75 renal patients diagnosed for COVID-19 in the period between February and April 2020, 9 of them with chronic decompensated renal failure (group 1), 50 on hemodialysis treatment (group 2), and 16 with a kidney transplant (group 3). There were no cases of infection in patients under peritoneal dialysis.

The incidence of COVID-19 was 3.8% (50/1,303) in patients on chronic hemodialysis treatment and 3.5% (50/1,423) if calculated on the total number of patients on renal replacement therapy (hemodialysis + peritoneal dialysis). In transplanted patients, the incidence was 1.7% (16/917). It was not possible to evaluate the incidence in patients with chronic decompensated renal failure due to the unavailability of the number of patients with eGFR <45 mL/min in the reference population.

Forty-nine out of 75 patients recovered from the infection. Overall mortality was 34.6% (26/75), specifically 22.2% (2/9) in group 1, 38.0% (19/50) in group 2, and 31.2% (5/16) in group 3.

Table 1 describes the main general and clinical features of the 49 patients who recovered from COVID-19 divided into patients with chronic decompensated renal failure (group 1, n = 7), patients under chronic dialysis treatment at the time of infection (group 2, n = 31), and renal transplant recipients (group 3, n = 11). Gender distribution was comparable in 3 groups. On the other hand, pairwise post hoc comparisons revealed that kidney transplant recipients were significantly younger than the patients under hemodialysis (p < 0.05).

Table 1.

In the 7 patients of group 1, the baseline eGFR value was 31.1 ± 15.3 mL/min (CKD-EPI formula), then after COVID-19 infection, the worsening renal function required artificial replacement support for a period of 20.8 ± 7.6 days (range 11–32 days). In group 2, all the 31 patients were on hemodialysis treatment. In groups 1 and 2, the artificial replacement treatments were performed in dedicated rooms (isolated from the conventional Dialysis Center) with continuous or intermittent standard techniques, depending on the specific situation. In group 3, the 11 transplanted patients had a preinfection eGFR of 47.7 ± 25.4 mL/min, and they were predominantly male and younger than the other groups.

At the time of COVID-19 diagnosis, 8 patients (1 in group 1, 5 in group 2, and 2 in group 3) had respiratory symptoms (cough and upper respiratory tract inflammation) without related radiological lesions; 33 patients (6 in group 1, 18 in group 2, and 9 in group 3) had bilateral interstitial pneumonia. Only 8 patients, all of them in group 2, were asymptomatic.

Table 2 shows the reasons for performing the nasopharyngeal swab in the 3 groups, which were clinical (fever or respiratory disorders, even minor), close contact with a positive patient (only in groups 1 and 3), or sporadic cases (only 3 out of the 31 hemodialysis patients of group 2). After the diagnosis of COVID-19, all the patients were initially hospitalized (range of hospitalization length: 3–58 days), due to comorbidities related to chronic renal failure and/or immunodepression for the transplant recipients.

Table 2.

The treatment interventions were based on the experience of the center. During hospitalization, 5 patients, all of them in group 2, did not require oxygen therapy. In the remaining 44 patients, varying degrees of respiratory failure was reported: in 20 of them (3 in group 1, 13 in group 2, and 4 in group 3), low-flow oxygen therapy (<6 L/min) was delivered using nasal cannulas or simple face masks, 18 (2 in group 1, 12 in group 2, and 4 in group 3) were treated with noninvasive ventilation by continuous positive airway pressure, and/or high-flow nasal cannulas, and 6 (2 in group 1, 1 in group 2, and 3 in group 3) required tracheal intubation and invasive mechanical ventilation.

The majority of the patients (95%) received different therapy schemes over time based on antiviral agents (hydroxychloroquine, darunavir/ritonavir, or lopinavir/ritonavir) and/or with tocilizumab and azithromycin. The distribution of the different drug regimes in the 3 groups are depicted in Table 3.

Table 3.

In all the transplant recipients under triple-drug immunosuppression regimen, mycophenolate or mTOR were withdrawn; also in 3 patients, calcineurin inhibitors were suspended during COVID-19 infection. Conversely, steroid therapy was always maintained or even enhanced.

The overall timing until SARS-CoV-2 negativization (documented by 2 consecutive negative swabs) in the total population of 49 recovered subjects was 32.4 ± 12.3 days (median: 31 days; range 15–58 days). This time span to achieve viral clearance was 38.8 ± 13.3 days in group 1 (34 days; range 24–56 days), 30.8 ± 12.2 days in group 2 (median: 30 days; range 15–58 days), and 32.8 ± 11.6 days in group 3 (median of 30 days; range 17–50 days), with no significant intergroup differences (p = n.s., ANOVA test). Time to negativization did not show any significant correlation with clinical severity at onset, antiviral therapy, or immunosuppressive regimen (transplant group).

Three patients (1 in each group) experienced a relapse of COVID-19 infection after 8–13 days from the laboratory recovery. In all the 3 cases, the viral load was significantly lower than that found at the first infection. The following nasopharyngeal swabs showed persistent negativity.

Both IgG and IgM against SARS-CoV-2 were detected in all patients since the first measurement following PCR negativization (range 3–8 days). In the first serological samples, the sample/cutoff (S/CO) index for IgG ranged from 0.93 to 12.12 (mean 7.78) and S/CO index for IgM ranged from 0.2 to 9.4 (mean 1.08). The semiquantitative analysis of SARS-CoV-2 antibodies in the patients who underwent at least 2 serological tests revealed a progressive increase of IgG index up to 37.5% and a concurrent slow decline of IgM index up to 94%.

IgG remained positive up to 44 days after viral clearance. At the last available determination, IgM were found to be negative in 16/49 patients.

At the time of data collection, patients treated for COVID-19 infection, were in stable clinical conditions and no significant organ damage was reported, except 1 single patient with decompensated renal failure (group 1) who is still dependent on artificial replacement treatment. Other 6 patients of group 1 achieved a partial functional recovery with respect to preinfective period (as indicated by the mild eGFR reduction to 21.6 ± 9.0 mL/min). In transplanted patients, the eGFR was also moderately decreased compared to the preinfection period (40.6 ± 14.05 vs. 47.7 ± 25.4 mL/min, p = n.s.).

Discussion/Conclusion

SARS-CoV-2 infection is having a tremendous impact on national health systems worldwide. Our data confirm the increased susceptibility to COVID-19 of patients with advanced-stage renal failure [20], under chronic dialysis therapy [21, 22] and kidney transplant recipients [23] compared to the general population, as previously reported in Italy and other industrialized countries [8, 24, 25].

The elevated incidence observed especially in hemodialysis patients is mainly related to the frequent access to the hospital, to the duration of individual treatments, implying a prolonged contact with other patients and health-care personnel, and the necessity of transportation to reach the dialysis center [2]. In our experience, no CO­VID-19-positive case were found among patients under peritoneal dialysis, thus confirming the advantages related to a home treatment, as already highlighted by recent evidence [26, 27].

The overall mortality of SARS-CoV-2 infection in Italy was 11.6%; a worse outcome is reported in patients with chronic diseases and cardiovascular comorbidity [28]. Consistent with previous reports [24, 25], our population showed an elevated death rate (22.2% in patients with decompensated renal failure, 38.0% in dialysis patients, and 31.2% in transplant recipients), confirming the fragility of chronic nephropathic patients, in spite of the timely therapy against COVID-19 given in most of the cases and the reduction of immunosuppression in transplanted patients. Sadly, the work suffers from the heterogeneity of therapeutic intervention for COVID-19, mainly related to the period in which the study was conducted, the early stages of the pandemic outbreak in Italy, when a reference treatment scheme was not well defined yet.

There was also a different diagnostic approach to the infection in the 3 study groups; among hemodialysis patients, the diagnosis was occasional in 10% of them during the screening for procedure execution not related to COVID-19 (preparation for instrumental examinations and scheduled hospitalizations) and in 15% for reporting contacts with positive cases. In transplant recipients, managed mainly at home, nasopharyngeal swab sampling was always performed for clinical reasons (fever and respiratory symptoms of variable severity), but this might have led to an underestimation of the incidence of infection.

Our data highlighted a partial worsening of kidney function following COVID-19 infection in patients with advanced chronic renal failure and transplant recipients, but the short observation period prevents us to draw any firm conclusions on this point. The management of chronic nephropathic patients is complex and involves personalized pathways for those who experienced CO­VID-19. Here, we tried mainly to evaluate the expected time of nasopharyngeal swab negativization in patients who, for different reasons, have an impaired immune response capacity. Even if the evolution of the single case cannot be generalized, a better understanding of the average time span to achieve viral clearance and antibody response in these weak populations might provide useful references for the planning of health-care activities and the logistic needs for therapeutic interventions. Unsurprisingly, our renal patients showed a slower viral clearance (32.4 ± 12.3 days) than that found in the general population, even in the context of a wide heterogeneity [11, 12, 29]. Moreover, the time to negativization was also longer in the patients diagnosed for COVID-19 due to sporadic cases or due to contact with an ascertained positive case.

In our case history, the time to viral clearance was moderately longer in patients with metabolic decompensation during infection, but we did not find significant differences between 3 groups of patients. The relatively small sample size and the heterogeneity of therapeutic approaches did not allow identification of a correlation between the time to negativization and antiviral therapy or clinical presentation at onset. Similarly, in transplant patients, no correlation with the immunosuppression regimen was possible.

The overall message emerging from our data indicates a delayed viral clearance of SARS-CoV-2 and can be interpreted in view of the cellular and humoral immunodepression typical of this population [7, 30, 31]. Comparably to other coronaviruses, SARS-CoV-2 has been proven to induce a T-lymphocyte-mediated immune response, but this protective mechanism might be suppressed in patients with lymphopenia, including those under chronic hemodialysis and renal transplant recipients [32, 33].

A striking finding was the discovery of 3 reinfections in patients who, in a time span ranging between 8 and 13 days following laboratory recovery, showed PCR positivity. It must be specified that swab sampling was performed due to subjective symptoms (fever or high respiratory tract inflammation), not through systematic search, and this might have possibly underestimated the rate of reinfection. It is known that the nasopharyngeal swab test has some limitations: the sensitivity is about 80%, implying that in up to 20–30% the test can produce false negatives, mainly related to too early timing of the sampling or human error in collecting adequate material. This limited accuracy suggests some caution before the complete reintroduction of patients with previous infection into the open-space sections of hemodialysis or in transplant follow-up surgeries. Applying a principle of extreme prudence in order to protect other fragile patients, the ones recovered from COVID-19 should be managed in filter zones for a period of 15–20 days. The development of rapid tests and/or highly accurate assays able to define the presence of the virus and not only of parts of the genome may be helpful in the near future.

Serological tests represent a useful integration of nasopharyngeal swab data. In our experience, all the patients treated for the infection have developed an antibody response, with a progressive increase and subsequent stabilization of IgG associated with a slow reduction or disappearance of IgM. The interpretation of serological tests remains complex and must be integrated into the clinical context and with other laboratory investigations. The assay used for antibody detection has a good sensitivity and specificity. The progressive reduction/disappearing of IgM is in line with the physiological immunological response. The finding of a prolonged persistence of IgM at the end of the observation period in 67% of our population studied is not a criterion of infection in progress. These data should be confirmed in a larger population, a wider range of cases, also keeping in mind that the detection of IgM has a lower specificity (high false-positive rate) due to increased cross-reactivity with other coronaviruses [34, 35]. The serological study requires a prolonged monitoring to verify the antibody changes over time and their persistence. The titration of IgG antibodies to identify a threshold level that confers an effective protection against the disease may represent a valuable tool for the recognition of immune subjects and for the possible management of vaccination programs. These first data on our renal patients show an overall satisfying antibody response capacity.

However, this study does not analyze the prognostic factors for COVID-19 recovery, which is still a matter of debate in current research. But to the best of our knowledge, this is the first investigation focused on the time to reach viral clearance and the development of specific antibodies against SARS-CoV-2 in different groups of chronic nephropathic patients. From an epidemiological perspective, a systematic screening of this weak population is desirable, regardless of an acute symptomatology and/or a diagnosis of infection. The limitations of the study are related to its retrospective nature, based on the analysis of laboratory data, performed in a clinical context of a critical health emergency due to the peak of pandemic outbreak that did not allow a uniform assay schedule. In particular, it was impossible to carry out the serology testing at programmed time points referred to the onset of infection (first positive swab) and to the moment of viral clearance (2 consecutive negative swabs). Another weakness is represented by the small sample size and the relatively short observation period. Serological investigations were performed after nasopharyngeal swab negativization, and this does not consent a full evaluation of antibody kinetics since the initial stages of their production. Furthermore, since the observation period was February–April 2020 during the most dramatic stage of pandemic, our efforts at that time were focused on our nephropathic patients known to have an impaired immune response, and we did not include a control group of subjects hospitalized for COVID-19 with normal renal function.

The experience gained during the pandemic leads to some considerations on the structural reorganization of spaces and access/exit routes of the Nephrology and Hemodialysis Units. The importance of prophylaxis actions that must be constantly maintained remains unquestionable; obviously, the workload of the different structures is amplified, due to the strict necessity of adequate filter procedures.

In conclusion, COVID-19 infection is a significant clinical problem in nephropathic patients and in kidney transplant recipients. Our findings reveal a delayed viral clearance in this fragile population, while there is a satisfying ability to produce specific antibodies. In a context still characterized by many clinical and epidemiological critical points, not fully clarified yet, a better understanding of the immune response in patients with unpaired renal function can represent a useful basis for further prospective research and in the management of vaccination programs. In weak populations with a high risk of morbidity and mortality, it is necessary to implement all the preventive measures to minimize the risk of COVID-19 infection and to identify positive patients at an early stage.

Statement of Ethics

Ethics Committee approval and informed consent were waived due to the observational nature of the study. The study followed the principles of the Declaration of Helsinki.

Conflict of Interest Statement

The authors have no conflicts of interest to declare.

Funding Sources

No funding was received in support of the present work.

Author Contributions

P.F.B.: data collection and manuscript drafting; M.C.: statistical analysis, manuscript preparation, editing, and revision; A.S., M.D.L., B.S., M.N., and V.M.: patients’ management, sample, and data collection; S.S., G.D., and V.S.: microbiology testing and validation; A.B., A.R., and E.M.: main supervision on patients’ management, sample, and data collection; P.M., G.L.M, and G.M: conceptualization and main supervision.



This Article is part of

Explore all 295 articles

References

Author Contacts

Gaetano La Manna, gaetano.lamanna@unibo.it

Article / Publication Details

Received: November 09, 2020

Accepted: February 09, 2021

Published online: April 26, 2021

Issue release date: Published online first Number of Print Pages: 8

Number of Figures: 0

Number of Tables: 3 ISSN: 1660-8151 (Print)

eISSN: 2235-3186 (Online) For additional information: https://www.karger.com/NEF

Copyright / Drug Dosage / Disclaimer

Copyright: All rights reserved. No part of this publication may be translated into other languages, reproduced or utilized in any form or by any means, electronic or mechanical, including photocopying, recording, microcopying, or by any information storage and retrieval system, without permission in writing from the publisher.

Drug Dosage: The authors and the publisher have exerted every effort to ensure that drug selection and dosage set forth in this text are in accord with current recommendations and practice at the time of publication. However, in view of ongoing research, changes in government regulations, and the constant flow of information relating to drug therapy and drug reactions, the reader is urged to check the package insert for each drug for any changes in indications and dosage and for added warnings and precautions. This is particularly important when the recommended agent is a new and/or infrequently employed drug.

Disclaimer: The statements, opinions and data contained in this publication are solely those of the individual authors and contributors and not of the publishers and the editor(s). The appearance of advertisements or/and product references in the publication is not a warranty, endorsement, or approval of the products or services advertised or of their effectiveness, quality or safety. The publisher and the editor(s) disclaim responsibility for any injury to persons or property resulting from any ideas, methods, instructions or products referred to in the content or advertisements."	https://www.karger.com/Article/FullText/515128	"Background/Aims: The coronavirus disease 2019 (CO­VID-19) pandemic is the major current health emergency worldwide, adding a significant burden also to the community of nephrologists for the management of their patients. Here, we analyzed the impact of COVID-19 infection in renal patients to assess the time to viral clearance, together with the production and persistence of IgG and IgM antibody response, in consideration of the altered immune capacity of this fragile population. Methods: Viral clearance and antibody kinetics were investigated in 49 renal patients recovered from COVID-19 infection: 7 of them with chronic decompensated renal failure, 31 under dialysis treatment, and 11 kidney transplant recipients. Results: The time span between the diagnosis of infection and recovery based on laboratory testing (2 negative nasopharyngeal swabs in consecutive days) was 31.7 ± 13.3 days. Three new positive cases were detected from 8 to 13 days following recovery. At the first serological determination after swab negativization, all the patients developed IgG and IgM antibodies. The semiquantitative analysis showed a progressive increase in IgG and a slow reduction in IgM. Discussion/Conclusion: In subjects with decompensated chronic kidney disease, under dialysis and in transplant recipients, viral clearance is lengthened compared to the general population. However, in spite of their common status of immunodepression, all of them were able to produce specific antibodies. These data might provide useful insights for monitoring and planning health-care activities in the weak category of patients with compromised renal function recovered from COVID-19.
© 2021 S. Karger AG, Basel
Introduction
The coronavirus disease 2019 (COVID-19) pandemic is a major clinical and social problem in all the countries. At the end of year 2020, the number of cases worldwide was 83,102,166, with 1,812,671 deaths and 58,897,317 recovered (https://www.worldometers.info/coronavirus/, browsed on December 31, 2020).
The impact on health care in the nephrological setting is significant. In analogy with previous infections by other coronaviruses [1], COVID-19 has shown a significant spread among patients with advanced chronic renal failure (stages 3b, 4, and 5 based on the Chronic Kidney Disease Epidemiology Collaboration [CKD-EPI] classification), those under chronic dialysis treatment and kidney transplant recipients [2, 3]. The infection is associated with poorer outcomes in subjects with impaired renal function than in the general population, mainly due to the presence of concurrent chronic diseases, in particular cardiovascular comorbidity [4]. Moreover, besides the well-known link of SARS-CoV-2 infection and respiratory distress syndrome, there are further potential mechanisms leading to systemic multiorgan failure. The reason lies in the wide distribution of angiotensin-converting enzyme 2 receptor, the gate of entrance of the virus. Thus, in case of a high viral load, the infection can spread through the angiotensin-converting enzyme 2 receptor to various target organs, including the kidney, heart, liver, brain, endothelium, gastrointestinal tract, immune cells, and erythrocytes (thus causing thromboembolism) [5-7].
The mortality rate reported in the national survey data of the Italian Society of Nephrology is 33.76% in patients receiving hemodialysis treatment and 24.77% in renal transplant recipients [8]. Akin to the general population, death risk is higher with increasing age. Current evidence supports individual inflammatory response as a key player in the development of the disease with its related complications. The immunodepression induced by the metabolic status of chronic kidney disease (CKD) and dialysis [2] or by post-transplant antirejection therapy [3] can influence the clinical course and the eradication of SARS-CoV-2 infection. To date, the role of advanced CKD or renal transplantation on viral clearance and on the pattern of IgM and IgG antibody response over time is still poorly understood [9-13].
A better awareness of the timing until SARS-CoV-2 negativization is particularly relevant for the possible management of organizational repercussions in different health-care settings. In hemodialysis units, the artificial replacement treatment is carried out until laboratory recovery (2 negative nasopharyngeal swabs in consecutive days) in dedicated areas [3, 7, 14]. Similarly, in transplanted patients, the regular follow-up monitoring is conditioned by the persistence of the infection. Studies related to the viral load over time can represent a useful decision-making tool in case of infection relapse after the first recovery. Moreover, a full comprehension of the pattern of antibody production and persistence might also provide valuable indications for prophylaxis choices, above all vaccination, also in view of the known poor response to vaccination observed in patients with advanced CKD.
This study was undertaken to analyze the impact of COVID-19 infection in patients with chronic decompensated renal failure, those under dialysis treatment, and renal transplant recipients. In particular, we investigated the time to achieve viral clearance (recovery based on 2 negative nasopharyngeal swabs in consecutive days), as well as the kinetics of IgG and IgM antibody response, in consideration of the altered cellular and humoral immune responses commonly found in these patients.
Materials and Methods
Patients
A retrospective multicenter observational study was performed in chronic nephropathic patients who experienced SARS-CoV-2 infection (nasopharyngeal swab positivity regardless of subjective symptoms). The observation period was from February to April 2020, with data collection in May 2020.
The analysis considered patients followed at the Unit of Nephrology and Dialysis of the local health authority of Romagna (Forlì-Cesena, Ravenna, and Rimini) and at Nephrology, Dialysis, and Renal Transplant Unit of the S. Orsola Malpighi University Hospital of Bologna, an area with a total reference population of 2.2 inhabitants. Specifically, renal patients who recovered from COVID-19 infection (2 consecutive negative swabs) with the following clinical and functional characteristics were included:
Advanced chronic renal failure (KDOQI stages 3B-5) in conservative therapy with worsening renal function during infection and subsequent need for artificial replacement support (group 1). Chronic dialysis treatment at the time of infection (group 2). Clinically stable renal transplant received at least 6 months prior to the beginning of the study (group 3).
The time span between the first positive swab and the laboratory negativization was evaluated in all the subjects who met the above-mentioned inclusion criteria. The following general and clinical parameters were recorded: age, gender, estimated glomerular filtration rate (eGFR) based on CKD-EPI creatinine-based equation [15], dialysis vintage for group 2, and age at transplant for group 3.
The COVID-19-related data collected for this analysis were onset of the symptoms, severity of lung involvement, therapy for SARS-CoV-2 infection, and modulation of immunosuppressive therapy (for transplanted patients). Ethics Committee approval and informed consent were waived due to the observational nature of the study. The study followed the principles of the Declaration of Helsinki. All individuals cannot be identified in the study, as they have been fully anonymized.
Laboratory Assays
Nasopharyngeal swabs were performed according to the guidelines of the Italian National Institute of Health (Istituto Superiore di Sanità – ISS) [16]. Biological specimens were manipulated using appropriate personal protective equipment and at biosafety level 2, based on the recommendations by the World Health Organization and the European Centre for Disease Prevention and Control [17].
In all the swabs received at the laboratory of our Microbiology Unit, the presence of SARS-CoV-2 was assessed using a commercially available one-step real-time PCR (Allplex 2019-nCoV assay, Seegene, Seoul, South Korea) in which, after RNA extraction, retrotranscription and PCR amplification are carried out consecutively in the same reaction tube. This is a qualitative analysis based on the Seegene MuDTTM technology (combining DPOTM and ­TOCETM), which allows identification of multiple Ct values for every pathogen in each channel of the real-time PCR instrument. The target genes for the identification of SARS-CoV-2 are E (a specific gene common to Sarbecoviruses), RdRp, and N gene both specific for SARS-CoV-2. Sample positivity was attributed in case of detection of at least one of the 2 specific genes. The samples were extracted using the automated Microlab STARlet IVD platform (Hamilton Italia, Agrate Brianza, MB, Italy) and amplified using CFX96 Real-time PCR Detection System (CFX Manager Software-IVD v1.6) (Bio-Rad Laboratories, Segrate, Milan, Italy). The sensitivity declared by the manufacturer is 100 copies/reaction [18].
From 3 to 7 days following nasopharyngeal swab negativization, SARS-CoV-2 IgM and IgG antibody serum levels were measured using a solid-phase ELISA (COVID-19 IgG Enzyme ImmunoAssay and COVID-19 IgM Enzyme ImmunoAssay, DIA PRO Diagnostic Bioprobes, Milan, Italy), in line with the regional plan on serological tests. The assay identifies antibody binding to 3 virus-specific immunodominant antigens: nucleocapsid, spike 1 protein, and spike 2 protein. The detection of IgM antibodies requires a pretreatment step to remove the rheumatoid factor, as previously described [19]. The diagnostic sensitivity is 98%. Serological semiquantitative determinations of IgG and IgM were carried out in a time span ranging from 3 to 8 days following negativization.
The IgG index was considered as negative if <0.9, uncertain (gray zone) if ranging between 0.9 and 1.1, and positive if >1.1. The IgM index was considered as negative if <0.9, uncertain (gray zone) if ranging between 0.9 and 1.1, and positive if >1.1. The C.I. for IgG index between 1.1 and 3.0 and for IgM index between 1.1 and 2.5 are quite low. These values were considered as weakly positive.
Statistical Analysis
Data are given as means ± standard deviation for continuous variables and percentage and absolute numbers for categorical variables. Comparisons between continuous variables were made through Student’s t test or the nonparametric Mann-Whitney U or ANOVA followed by Tukey t test, as appropriate. χ2 test was used to evaluate the categorical variables. A p value below 0.05 was considered as significant, and all the statistical analyses were performed using the statistical package for the social sciences (SPSSTM for Windows Software Package, version 9.0.1; Chicago, IL, USA).
Results
This observational study considered a total of 75 renal patients diagnosed for COVID-19 in the period between February and April 2020, 9 of them with chronic decompensated renal failure (group 1), 50 on hemodialysis treatment (group 2), and 16 with a kidney transplant (group 3). There were no cases of infection in patients under peritoneal dialysis.
The incidence of COVID-19 was 3.8% (50/1,303) in patients on chronic hemodialysis treatment and 3.5% (50/1,423) if calculated on the total number of patients on renal replacement therapy (hemodialysis + peritoneal dialysis). In transplanted patients, the incidence was 1.7% (16/917). It was not possible to evaluate the incidence in patients with chronic decompensated renal failure due to the unavailability of the number of patients with eGFR <45 mL/min in the reference population.
Forty-nine out of 75 patients recovered from the infection. Overall mortality was 34.6% (26/75), specifically 22.2% (2/9) in group 1, 38.0% (19/50) in group 2, and 31.2% (5/16) in group 3.
Table 1 describes the main general and clinical features of the 49 patients who recovered from COVID-19 divided into patients with chronic decompensated renal failure (group 1, n = 7), patients under chronic dialysis treatment at the time of infection (group 2, n = 31), and renal transplant recipients (group 3, n = 11). Gender distribution was comparable in 3 groups. On the other hand, pairwise post hoc comparisons revealed that kidney transplant recipients were significantly younger than the patients under hemodialysis (p < 0.05).
Table 1.
In the 7 patients of group 1, the baseline eGFR value was 31.1 ± 15.3 mL/min (CKD-EPI formula), then after COVID-19 infection, the worsening renal function required artificial replacement support for a period of 20.8 ± 7.6 days (range 11–32 days). In group 2, all the 31 patients were on hemodialysis treatment. In groups 1 and 2, the artificial replacement treatments were performed in dedicated rooms (isolated from the conventional Dialysis Center) with continuous or intermittent standard techniques, depending on the specific situation. In group 3, the 11 transplanted patients had a preinfection eGFR of 47.7 ± 25.4 mL/min, and they were predominantly male and younger than the other groups.
At the time of COVID-19 diagnosis, 8 patients (1 in group 1, 5 in group 2, and 2 in group 3) had respiratory symptoms (cough and upper respiratory tract inflammation) without related radiological lesions; 33 patients (6 in group 1, 18 in group 2, and 9 in group 3) had bilateral interstitial pneumonia. Only 8 patients, all of them in group 2, were asymptomatic.
Table 2 shows the reasons for performing the nasopharyngeal swab in the 3 groups, which were clinical (fever or respiratory disorders, even minor), close contact with a positive patient (only in groups 1 and 3), or sporadic cases (only 3 out of the 31 hemodialysis patients of group 2). After the diagnosis of COVID-19, all the patients were initially hospitalized (range of hospitalization length: 3–58 days), due to comorbidities related to chronic renal failure and/or immunodepression for the transplant recipients.
Table 2.
The treatment interventions were based on the experience of the center. During hospitalization, 5 patients, all of them in group 2, did not require oxygen therapy. In the remaining 44 patients, varying degrees of respiratory failure was reported: in 20 of them (3 in group 1, 13 in group 2, and 4 in group 3), low-flow oxygen therapy (<6 L/min) was delivered using nasal cannulas or simple face masks, 18 (2 in group 1, 12 in group 2, and 4 in group 3) were treated with noninvasive ventilation by continuous positive airway pressure, and/or high-flow nasal cannulas, and 6 (2 in group 1, 1 in group 2, and 3 in group 3) required tracheal intubation and invasive mechanical ventilation.
The majority of the patients (95%) received different therapy schemes over time based on antiviral agents (hydroxychloroquine, darunavir/ritonavir, or lopinavir/ritonavir) and/or with tocilizumab and azithromycin. The distribution of the different drug regimes in the 3 groups are depicted in Table 3.
Table 3.
In all the transplant recipients under triple-drug immunosuppression regimen, mycophenolate or mTOR were withdrawn; also in 3 patients, calcineurin inhibitors were suspended during COVID-19 infection. Conversely, steroid therapy was always maintained or even enhanced.
The overall timing until SARS-CoV-2 negativization (documented by 2 consecutive negative swabs) in the total population of 49 recovered subjects was 32.4 ± 12.3 days (median: 31 days; range 15–58 days). This time span to achieve viral clearance was 38.8 ± 13.3 days in group 1 (34 days; range 24–56 days), 30.8 ± 12.2 days in group 2 (median: 30 days; range 15–58 days), and 32.8 ± 11.6 days in group 3 (median of 30 days; range 17–50 days), with no significant intergroup differences (p = n.s., ANOVA test). Time to negativization did not show any significant correlation with clinical severity at onset, antiviral therapy, or immunosuppressive regimen (transplant group).
Three patients (1 in each group) experienced a relapse of COVID-19 infection after 8–13 days from the laboratory recovery. In all the 3 cases, the viral load was significantly lower than that found at the first infection. The following nasopharyngeal swabs showed persistent negativity.
Both IgG and IgM against SARS-CoV-2 were detected in all patients since the first measurement following PCR negativization (range 3–8 days). In the first serological samples, the sample/cutoff (S/CO) index for IgG ranged from 0.93 to 12.12 (mean 7.78) and S/CO index for IgM ranged from 0.2 to 9.4 (mean 1.08). The semiquantitative analysis of SARS-CoV-2 antibodies in the patients who underwent at least 2 serological tests revealed a progressive increase of IgG index up to 37.5% and a concurrent slow decline of IgM index up to 94%.
IgG remained positive up to 44 days after viral clearance. At the last available determination, IgM were found to be negative in 16/49 patients.
At the time of data collection, patients treated for COVID-19 infection, were in stable clinical conditions and no significant organ damage was reported, except 1 single patient with decompensated renal failure (group 1) who is still dependent on artificial replacement treatment. Other 6 patients of group 1 achieved a partial functional recovery with respect to preinfective period (as indicated by the mild eGFR reduction to 21.6 ± 9.0 mL/min). In transplanted patients, the eGFR was also moderately decreased compared to the preinfection period (40.6 ± 14.05 vs. 47.7 ± 25.4 mL/min, p = n.s.).
Discussion/Conclusion
SARS-CoV-2 infection is having a tremendous impact on national health systems worldwide. Our data confirm the increased susceptibility to COVID-19 of patients with advanced-stage renal failure [20], under chronic dialysis therapy [21, 22] and kidney transplant recipients [23] compared to the general population, as previously reported in Italy and other industrialized countries [8, 24, 25].
The elevated incidence observed especially in hemodialysis patients is mainly related to the frequent access to the hospital, to the duration of individual treatments, implying a prolonged contact with other patients and health-care personnel, and the necessity of transportation to reach the dialysis center [2]. In our experience, no CO­VID-19-positive case were found among patients under peritoneal dialysis, thus confirming the advantages related to a home treatment, as already highlighted by recent evidence [26, 27].
The overall mortality of SARS-CoV-2 infection in Italy was 11.6%; a worse outcome is reported in patients with chronic diseases and cardiovascular comorbidity [28]. Consistent with previous reports [24, 25], our population showed an elevated death rate (22.2% in patients with decompensated renal failure, 38.0% in dialysis patients, and 31.2% in transplant recipients), confirming the fragility of chronic nephropathic patients, in spite of the timely therapy against COVID-19 given in most of the cases and the reduction of immunosuppression in transplanted patients. Sadly, the work suffers from the heterogeneity of therapeutic intervention for COVID-19, mainly related to the period in which the study was conducted, the early stages of the pandemic outbreak in Italy, when a reference treatment scheme was not well defined yet.
There was also a different diagnostic approach to the infection in the 3 study groups; among hemodialysis patients, the diagnosis was occasional in 10% of them during the screening for procedure execution not related to COVID-19 (preparation for instrumental examinations and scheduled hospitalizations) and in 15% for reporting contacts with positive cases. In transplant recipients, managed mainly at home, nasopharyngeal swab sampling was always performed for clinical reasons (fever and respiratory symptoms of variable severity), but this might have led to an underestimation of the incidence of infection.
Our data highlighted a partial worsening of kidney function following COVID-19 infection in patients with advanced chronic renal failure and transplant recipients, but the short observation period prevents us to draw any firm conclusions on this point. The management of chronic nephropathic patients is complex and involves personalized pathways for those who experienced CO­VID-19. Here, we tried mainly to evaluate the expected time of nasopharyngeal swab negativization in patients who, for different reasons, have an impaired immune response capacity. Even if the evolution of the single case cannot be generalized, a better understanding of the average time span to achieve viral clearance and antibody response in these weak populations might provide useful references for the planning of health-care activities and the logistic needs for therapeutic interventions. Unsurprisingly, our renal patients showed a slower viral clearance (32.4 ± 12.3 days) than that found in the general population, even in the context of a wide heterogeneity [11, 12, 29]. Moreover, the time to negativization was also longer in the patients diagnosed for COVID-19 due to sporadic cases or due to contact with an ascertained positive case.
In our case history, the time to viral clearance was moderately longer in patients with metabolic decompensation during infection, but we did not find significant differences between 3 groups of patients. The relatively small sample size and the heterogeneity of therapeutic approaches did not allow identification of a correlation between the time to negativization and antiviral therapy or clinical presentation at onset. Similarly, in transplant patients, no correlation with the immunosuppression regimen was possible.
The overall message emerging from our data indicates a delayed viral clearance of SARS-CoV-2 and can be interpreted in view of the cellular and humoral immunodepression typical of this population [7, 30, 31]. Comparably to other coronaviruses, SARS-CoV-2 has been proven to induce a T-lymphocyte-mediated immune response, but this protective mechanism might be suppressed in patients with lymphopenia, including those under chronic hemodialysis and renal transplant recipients [32, 33].
A striking finding was the discovery of 3 reinfections in patients who, in a time span ranging between 8 and 13 days following laboratory recovery, showed PCR positivity. It must be specified that swab sampling was performed due to subjective symptoms (fever or high respiratory tract inflammation), not through systematic search, and this might have possibly underestimated the rate of reinfection. It is known that the nasopharyngeal swab test has some limitations: the sensitivity is about 80%, implying that in up to 20–30% the test can produce false negatives, mainly related to too early timing of the sampling or human error in collecting adequate material. This limited accuracy suggests some caution before the complete reintroduction of patients with previous infection into the open-space sections of hemodialysis or in transplant follow-up surgeries. Applying a principle of extreme prudence in order to protect other fragile patients, the ones recovered from COVID-19 should be managed in filter zones for a period of 15–20 days. The development of rapid tests and/or highly accurate assays able to define the presence of the virus and not only of parts of the genome may be helpful in the near future.
Serological tests represent a useful integration of nasopharyngeal swab data. In our experience, all the patients treated for the infection have developed an antibody response, with a progressive increase and subsequent stabilization of IgG associated with a slow reduction or disappearance of IgM. The interpretation of serological tests remains complex and must be integrated into the clinical context and with other laboratory investigations. The assay used for antibody detection has a good sensitivity and specificity. The progressive reduction/disappearing of IgM is in line with the physiological immunological response. The finding of a prolonged persistence of IgM at the end of the observation period in 67% of our population studied is not a criterion of infection in progress. These data should be confirmed in a larger population, a wider range of cases, also keeping in mind that the detection of IgM has a lower specificity (high false-positive rate) due to increased cross-reactivity with other coronaviruses [34, 35]. The serological study requires a prolonged monitoring to verify the antibody changes over time and their persistence. The titration of IgG antibodies to identify a threshold level that confers an effective protection against the disease may represent a valuable tool for the recognition of immune subjects and for the possible management of vaccination programs. These first data on our renal patients show an overall satisfying antibody response capacity.
However, this study does not analyze the prognostic factors for COVID-19 recovery, which is still a matter of debate in current research. But to the best of our knowledge, this is the first investigation focused on the time to reach viral clearance and the development of specific antibodies against SARS-CoV-2 in different groups of chronic nephropathic patients. From an epidemiological perspective, a systematic screening of this weak population is desirable, regardless of an acute symptomatology and/or a diagnosis of infection. The limitations of the study are related to its retrospective nature, based on the analysis of laboratory data, performed in a clinical context of a critical health emergency due to the peak of pandemic outbreak that did not allow a uniform assay schedule. In particular, it was impossible to carry out the serology testing at programmed time points referred to the onset of infection (first positive swab) and to the moment of viral clearance (2 consecutive negative swabs). Another weakness is represented by the small sample size and the relatively short observation period. Serological investigations were performed after nasopharyngeal swab negativization, and this does not consent a full evaluation of antibody kinetics since the initial stages of their production. Furthermore, since the observation period was February–April 2020 during the most dramatic stage of pandemic, our efforts at that time were focused on our nephropathic patients known to have an impaired immune response, and we did not include a control group of subjects hospitalized for COVID-19 with normal renal function.
The experience gained during the pandemic leads to some considerations on the structural reorganization of spaces and access/exit routes of the Nephrology and Hemodialysis Units. The importance of prophylaxis actions that must be constantly maintained remains unquestionable; obviously, the workload of the different structures is amplified, due to the strict necessity of adequate filter procedures.
In conclusion, COVID-19 infection is a significant clinical problem in nephropathic patients and in kidney transplant recipients. Our findings reveal a delayed viral clearance in this fragile population, while there is a satisfying ability to produce specific antibodies. In a context still characterized by many clinical and epidemiological critical points, not fully clarified yet, a better understanding of the immune response in patients with unpaired renal function can represent a useful basis for further prospective research and in the management of vaccination programs. In weak populations with a high risk of morbidity and mortality, it is necessary to implement all the preventive measures to minimize the risk of COVID-19 infection and to identify positive patients at an early stage.
Statement of Ethics
Ethics Committee approval and informed consent were waived due to the observational nature of the study. The study followed the principles of the Declaration of Helsinki.
Conflict of Interest Statement
The authors have no conflicts of interest to declare.
Funding Sources
No funding was received in support of the present work.
Author Contributions
P.F.B.: data collection and manuscript drafting; M.C.: statistical analysis, manuscript preparation, editing, and revision; A.S., M.D.L., B.S., M.N., and V.M.: patients’ management, sample, and data collection; S.S., G.D., and V.S.: microbiology testing and validation; A.B., A.R., and E.M.: main supervision on patients’ management, sample, and data collection; P.M., G.L.M, and G.M: conceptualization and main supervision.
This Article is part of
Explore all 295 articles
References
Author Contacts
Gaetano La Manna, gaetano.lamanna@unibo.it
Article / Publication Details
Received: November 09, 2020
Accepted: February 09, 2021
Published online: April 26, 2021
Issue release date: Published online first Number of Print Pages: 8
Number of Figures: 0
Number of Tables: 3 ISSN: 1660-8151 (Print)
eISSN: 2235-3186 (Online) For additional information: https://www.karger.com/NEF
Copyright / Drug Dosage / Disclaimer
Copyright: All rights reserved. No part of this publication may be translated into other languages, reproduced or utilized in any form or by any means, electronic or mechanical, including photocopying, recording, microcopying, or by any information storage and retrieval system, without permission in writing from the publisher.
Drug Dosage: The authors and the publisher have exerted every effort to ensure that drug selection and dosage set forth in this text are in accord with current recommendations and practice at the time of publication. However, in view of ongoing research, changes in government regulations, and the constant flow of information relating to drug therapy and drug reactions, the reader is urged to check the package insert for each drug for any changes in indications and dosage and for added warnings and precautions. This is particularly important when the recommended agent is a new and/or infrequently employed drug.
Disclaimer: The statements, opinions and data contained in this publication are solely those of the individual authors and contributors and not of the publishers and the editor(s). The appearance of advertisements or/and product references in the publication is not a warranty, endorsement, or approval of the products or services advertised or of their effectiveness, quality or safety. The publisher and the editor(s) disclaim responsibility for any injury to persons or property resulting from any ideas, methods, instructions or products referred to in the content or advertisements."	30474
covid19	['Leonardo V. Riella', 'A B', 'Jamil R. Azzi', 'C D', 'Paolo Cravedi', 'Author Affiliations', 'Corresponding Author']	2021-05-22 00:00:00	"Abstract

Context: Chronic immunosuppression is associated with an increased risk of opportunistic infections. Although kidney transplant recipients with coronavirus disease 2019 (COVID-19) have higher mortality than the general population, data on their risk of infection with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) are unknown. Subject of Review: A recent single-center screening study from the UK (Transplantation. 2021 Jan 1;105(1):151–7) showed that 89 (10.4%) of 855 consecutive kidney transplant recipients tested positive for SARS-CoV-2 antibodies. Risk factors for infection included a nonwhite background, diabetes, and a history of allograft rejection. Risk factors for mortality in individuals who developed COVID-19 were older age and receiving steroids. Second Opinion: This study shows that the rate of SARS-CoV-2 infection in kidney transplant recipients is similar to the one observed in the general population in the same area (13%), indicating that transplant recipients are not at increased risk of COVID-19. However, the investigators raise the interesting point that since transplant individuals were advised to shelter earlier than the general population, they may be in fact more susceptible. This statement is hard to substantiate, but the identification of specific risk factors for infection and poor outcomes is crucial to tailor strategies to prevent spread of the infection. This is particularly important, considering that kidney transplant recipients may be at increased risk of prolonged viral spread and in-host viral mutations, making them not just a particularly fragile population for COVID-19 but also a potentially major source of further contagions.

© 2021 S. Karger AG, Basel

Kidney Transplant Recipients and SARS-CoV-2 Infection

A comment on “Willicombe et al.: Identification of patient characteristics associated with SARS-CoV-2 infection and outcome in kidney transplant patients using serological screening. Transplantation. 2021 Jan 1;105(1):151–7.”

Immunosuppression is a major risk factor for opportunistic infections [1]. Therefore, since the very beginning of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pandemic, kidney transplant recipients have been considered a fragile population at higher risk of infection and poor outcomes [2, 3]. Finding that the most common risk factors for mortality in coronavirus disease 2019 (COVID-19), such as older age, diabetes, and hypertension, are also common in transplanted individuals further supported this notion.

Initial series of hospitalized kidney transplant recipients with COVID-19 showed alarming rates of mortality, between 25 and 35% [4]. These rates are higher than in-hospital case-fatality rates reported in the general population (ranging between 10 and 20%) [5-7]. However, there are also series showing that solid organ recipients have similar COVID-19-related mortality rates than the general population [8]. This suggests that such increased risk might be mitigated, for instance, by early hospitalization and a greater ease of access to intensive care unit [8].

Progressive failure of most initially tested therapeutic approaches for COVID-19 further alarmed the transplant community that was progressively left without real therapeutic options. Effective prevention of COVID-19 infection in kidney transplant recipients, a population in need for frequent monitoring of graft function with blood draws, and often complex therapies has been therefore a challenging effort [9]. General measures for infectious disease prevention do apply to transplant patients, similar to the general population. However, identification of specific risk factors for infection may allow to strengthen rigor in selected patients, while avoiding excessive precautions in others, overall permitting a safe working and social activity.

In this context, we welcome the large serological screening by Willicombe et al. [10] providing new, important data on risk factors for SARS-CoV-2 infection in kidney transplant recipients. This study measured anti-SARS-CoV-2 antibody levels in 855 consecutive kidney transplant recipients out of a cohort of 1,250 patients followed at Imperial College Renal and Transplant Center, London, UK. Eighty-nine (10.4%) patients tested positive for SARS-CoV-2 antibodies. For comparison, the seroprevalence in London around the same period was 13%. This supports the concept that kidney transplant recipients are at similar risk for SARS-CoV-2 infection compared to the general population. Considering that transplant recipients were advised to shield in the UK months earlier than the general population and their generally more cautious attitude to limit exposure to infections, the authors of the study predicted a much lower exposure rate. However, the fact that family members may not always be as careful as the transplant recipients makes this conclusion hard to substantiate.

How Should We Define SARS-CoV-2 Infection in Transplant Recipients?

Ideally, viral PCR screening would be required to assess asymptomatic cases of infection. However, this approach is extremely demanding and would require serial tests to capture the infection in patients without symptoms. Antiviral antibody measurement seems therefore the most effective approach as antibodies are thought to persist in the circulation for months after the infection, even in the asymptomatic cases. However, antibody screening has its own limitations as it may underestimate the prevalence of individuals who experienced viral infection. In particular, kidney transplant recipients may not develop an effective antibody response, or anti-SARS-CoV-2 antibodies may disappear weeks after infection. In this study, only 10% of patients with positive RT-PCR results did not have detectable antibodies at 1 month after infection, but this number could increase in patients with asymptomatic disease or at later time points after infection. Therefore, the infection risk of transplant recipients may in fact be significantly higher than the one reported here.

The same UK group recently published in a separate article the comparisons between different anti-SARS-CoV-2 antibody assays in supposedly the same cohort of patients [11]. They performed a comparison between 3 serological assays and found that the 2 immunoassays that tested for the presence of antibodies against the RBD of the S protein were superior to the one they used for the detection of antibodies reactive for the NP antigen.

What Are the Risk Factors for SARS-CoV-2 Infection in Transplant Recipients?

In the UK study, risk factors for the development of anti-SARS-CoV-2 antibodies included a nonwhite background, a diagnosis of diabetes, and a history of allograft rejection. The association between ethnicity and diabetes with COVID-19 infection is consistent with risk factors for COVID-19 found in studies within the general population [12, 13] and with prior studies identifying common risk factors for posttransplant infections [14].

Less clear is the association between acute rejection and increased infection risk, which could be related in part to the greater cumulative exposure to immunosuppressive drugs after rejection. It is also possible that rejection episodes were a sign of poor adherence to immunosuppressive therapy and, possibly, worse compliance to recommended medical preventive strategies against the virus. The authors also emphasize that patients enrolled in this single-center study [10] received steroids only if they had prior rejection episodes or if they were using steroids before transplantation. Therefore, the rejection-associated risk could be driven by the use of steroids due to a prior episode, similar to the results of studies in other immunosuppressed populations, suggesting that glucocorticoid exposure increases the risk of severe COVID-19 disease [15]. In this respect, it will be important to identify specific risk factors for steroid toxicity that put people at risk for SARS-CoV-2 infection. With the limitations of a single-center, cross-sectional study, this report provides important information in the identification of kidney transplant patients in whom preventive strategies should be reinforced the most.

Is the Antiviral Immune Response Impaired in Transplanted Individuals?

While high rates of infections and poor outcomes in transplant patients suggest an impaired immune response, the few available immune phenotypic analyses [16] and anti-SARS-CoV-2 antibody measurements in infected individuals did not detect major abnormalities. However, anti-SARS-CoV-2 T-cell responses in transplanted patients have not been investigated extensively.

The anti-SARS-CoV-2 antibodies measured in the UK study were targeted against the RBD viral antigen [10]. There is evidence that anti-RBD antibodies may provide information on functional immunity, given reported correlations between RBD antibodies and neutralizing antibodies [17, 18]. However, ad hoc functional studies testing the viral-neutralizing capacity of the anti-SARS-CoV-2 antibodies produced by transplant recipients are needed. Understanding the antiviral response in kidney transplant recipients is key not just to interpret data of antibody screening studies but more importantly to manage immunosuppression during COVID-19 and for future vaccine strategies.

Should We Isolate Infected Transplant Recipients Longer Than Non-Immunosuppressed Individuals?

An immunosuppressive state may associate with higher risk of persistent infections and viral spread than people from the general population. It may also be associated with higher viral shedding. Recently, the cases of immunocompromised individuals with persistent (over 5 months) viral infection have been reported [5, 19]. Importantly, ex vivo studies suggest that the virus that is persistently shed by immunosuppressed individuals would still be able to establish productive infection in contacts upon transmission [19]. If confirmed also in transplanted individuals, this may suggest that longer quarantine periods and repeated negative tests may be needed for transplant recipients who recover from COVID-19. In particular, monitoring of genomic and subgenomic RNA might be required to exclude persistence of SARS-CoV-2 infection [19].

Importantly, an immunocompromised state has been shown to accelerate in-host SARS-CoV-2 viral evolution and formation of new viral variants [5, 19]. Potential factors contributing to the observed within-host evolution is prolonged infection and the compromised immune status of the host, possibly resulting in a different set of selective pressures compared with an immune-competent host. This could be particularly important based on recent data suggesting that a more sustained antibody response and greater CD4+ T-cell activation with increased somatic mutations on B cells are associated with shorter disease trajectories [20]. Mutations that alter RNA virus may affect SARS-CoV-2 virulence and transmissibility and facilitate global spread, similar to what happened with D614G mutation [21].

Preventing SARS-CoV-2 Infection in Transplant Recipients and How This Pandemic Has Changed Transplant Medicine

The ultimate strategy to effectively prevent COVID-19 is vaccination against SARS-CoV-2. Recently approved vaccines, including mRNA-based vaccines, have shown great promise in terms of safety/efficacy profile [22, 23]. Luckily, anti-SARS-CoV-2 vaccines are already being distributed worldwide, and transplant recipients have initiated vaccination in multiple locations with major variability in timing driven by government policies and availability of vaccines. Although data in transplant patients are limited, it is reasonable to think, based on data with other vaccines [24], that their immune responses will be protective, although the degree of protection and its duration are still unknown.

Strategies to prevent infection in transplant recipients are still important while vaccine is being delivered, in particular the use of masks and avoidance of crowded nonventilated spaces. The COVID-19 outbreak has seriously challenged the transplant practice worldwide. Innovative telehealth solutions have emerged allowing safe and efficient outpatient care, delineating the future transplant practice in the post-COVID-19 era. As the pandemic continues and new, more infectious variants of the virus emerge, identification of risk factors for infection may allow to risk stratify patients and tailor prevention strategies and prioritize vaccination in patients at highest risk.

The early decision to postpone nonurgent transplant programs has been accompanied by a drastic drop of organ procurement around the world. Such position has been questioned by the risk of mortality on the waiting list, and huge efforts have been made to settle efficient mitigating strategies to overcome these ethical issues. Ideal strategies still have to be defined, and risk/benefit assessment should be discussed on a case-by-case basis within the transplant team. Even more uncertain is the management of immunosuppression and how it affects the risk of infection, mortality, and, possibly, response to vaccination. This might be very important, as preliminary data among 436 transplant recipients that received either BNT162b2 vaccine (Pfizer-BioNTech) or the mRNA-1273 vaccine (Moderna) suggest that the antibody immunity may be lower than in the general population, as evident by only 17% detection of antibody against the spike antigen after first dose of the vaccine [25].

The need for remote monitoring of graft function has further highlighted the importance of noninvasive biomarkers of acute rejection. Despite the approval by the Food and Drug Administration of a few assays, this still largely represents an unmet need that should also be adequately addressed in the post-COVID-19 period of transplant medicine.

Optimal protection of kidney transplant recipients during COVID-19 outbreak is an evolving concept, and more studies are required to understand special needs of this fragile population. Through an extremely challenging path, this pandemic forced the transplant community to revisit strategies and priorities. Once the viral spread will be under control, these challenges may ultimately result in a better care of transplanted individuals.

Conflict of Interest Statement

The authors declare that they have no conflict of interest to disclose. P.C. is supported by NIH NIAID grant 3U01AI063594-17S1.



This Article is part of

Explore all 295 articles

References

Author Contacts

Paolo Cravedi, paolo.cravedi@mssm.edu

Article / Publication Details

Received: February 01, 2021

Accepted: February 11, 2021

Published online: March 31, 2021

Issue release date: May 2021 Number of Print Pages: 5

Number of Figures: 0

Number of Tables: 0 ISSN: 1660-8151 (Print)

eISSN: 2235-3186 (Online) For additional information: https://www.karger.com/NEF

Copyright / Drug Dosage / Disclaimer

Copyright: All rights reserved. No part of this publication may be translated into other languages, reproduced or utilized in any form or by any means, electronic or mechanical, including photocopying, recording, microcopying, or by any information storage and retrieval system, without permission in writing from the publisher.

Drug Dosage: The authors and the publisher have exerted every effort to ensure that drug selection and dosage set forth in this text are in accord with current recommendations and practice at the time of publication. However, in view of ongoing research, changes in government regulations, and the constant flow of information relating to drug therapy and drug reactions, the reader is urged to check the package insert for each drug for any changes in indications and dosage and for added warnings and precautions. This is particularly important when the recommended agent is a new and/or infrequently employed drug.

Disclaimer: The statements, opinions and data contained in this publication are solely those of the individual authors and contributors and not of the publishers and the editor(s). The appearance of advertisements or/and product references in the publication is not a warranty, endorsement, or approval of the products or services advertised or of their effectiveness, quality or safety. The publisher and the editor(s) disclaim responsibility for any injury to persons or property resulting from any ideas, methods, instructions or products referred to in the content or advertisements."	https://www.karger.com/Article/FullText/515165	"Context: Chronic immunosuppression is associated with an increased risk of opportunistic infections. Although kidney transplant recipients with coronavirus disease 2019 (COVID-19) have higher mortality than the general population, data on their risk of infection with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) are unknown. Subject of Review: A recent single-center screening study from the UK (Transplantation. 2021 Jan 1;105(1):151–7) showed that 89 (10.4%) of 855 consecutive kidney transplant recipients tested positive for SARS-CoV-2 antibodies. Risk factors for infection included a nonwhite background, diabetes, and a history of allograft rejection. Risk factors for mortality in individuals who developed COVID-19 were older age and receiving steroids. Second Opinion: This study shows that the rate of SARS-CoV-2 infection in kidney transplant recipients is similar to the one observed in the general population in the same area (13%), indicating that transplant recipients are not at increased risk of COVID-19. However, the investigators raise the interesting point that since transplant individuals were advised to shelter earlier than the general population, they may be in fact more susceptible. This statement is hard to substantiate, but the identification of specific risk factors for infection and poor outcomes is crucial to tailor strategies to prevent spread of the infection. This is particularly important, considering that kidney transplant recipients may be at increased risk of prolonged viral spread and in-host viral mutations, making them not just a particularly fragile population for COVID-19 but also a potentially major source of further contagions.
© 2021 S. Karger AG, Basel
Kidney Transplant Recipients and SARS-CoV-2 Infection
A comment on “Willicombe et al.: Identification of patient characteristics associated with SARS-CoV-2 infection and outcome in kidney transplant patients using serological screening. Transplantation. 2021 Jan 1;105(1):151–7.”
Immunosuppression is a major risk factor for opportunistic infections [1]. Therefore, since the very beginning of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pandemic, kidney transplant recipients have been considered a fragile population at higher risk of infection and poor outcomes [2, 3]. Finding that the most common risk factors for mortality in coronavirus disease 2019 (COVID-19), such as older age, diabetes, and hypertension, are also common in transplanted individuals further supported this notion.
Initial series of hospitalized kidney transplant recipients with COVID-19 showed alarming rates of mortality, between 25 and 35% [4]. These rates are higher than in-hospital case-fatality rates reported in the general population (ranging between 10 and 20%) [5-7]. However, there are also series showing that solid organ recipients have similar COVID-19-related mortality rates than the general population [8]. This suggests that such increased risk might be mitigated, for instance, by early hospitalization and a greater ease of access to intensive care unit [8].
Progressive failure of most initially tested therapeutic approaches for COVID-19 further alarmed the transplant community that was progressively left without real therapeutic options. Effective prevention of COVID-19 infection in kidney transplant recipients, a population in need for frequent monitoring of graft function with blood draws, and often complex therapies has been therefore a challenging effort [9]. General measures for infectious disease prevention do apply to transplant patients, similar to the general population. However, identification of specific risk factors for infection may allow to strengthen rigor in selected patients, while avoiding excessive precautions in others, overall permitting a safe working and social activity.
In this context, we welcome the large serological screening by Willicombe et al. [10] providing new, important data on risk factors for SARS-CoV-2 infection in kidney transplant recipients. This study measured anti-SARS-CoV-2 antibody levels in 855 consecutive kidney transplant recipients out of a cohort of 1,250 patients followed at Imperial College Renal and Transplant Center, London, UK. Eighty-nine (10.4%) patients tested positive for SARS-CoV-2 antibodies. For comparison, the seroprevalence in London around the same period was 13%. This supports the concept that kidney transplant recipients are at similar risk for SARS-CoV-2 infection compared to the general population. Considering that transplant recipients were advised to shield in the UK months earlier than the general population and their generally more cautious attitude to limit exposure to infections, the authors of the study predicted a much lower exposure rate. However, the fact that family members may not always be as careful as the transplant recipients makes this conclusion hard to substantiate.
How Should We Define SARS-CoV-2 Infection in Transplant Recipients?
Ideally, viral PCR screening would be required to assess asymptomatic cases of infection. However, this approach is extremely demanding and would require serial tests to capture the infection in patients without symptoms. Antiviral antibody measurement seems therefore the most effective approach as antibodies are thought to persist in the circulation for months after the infection, even in the asymptomatic cases. However, antibody screening has its own limitations as it may underestimate the prevalence of individuals who experienced viral infection. In particular, kidney transplant recipients may not develop an effective antibody response, or anti-SARS-CoV-2 antibodies may disappear weeks after infection. In this study, only 10% of patients with positive RT-PCR results did not have detectable antibodies at 1 month after infection, but this number could increase in patients with asymptomatic disease or at later time points after infection. Therefore, the infection risk of transplant recipients may in fact be significantly higher than the one reported here.
The same UK group recently published in a separate article the comparisons between different anti-SARS-CoV-2 antibody assays in supposedly the same cohort of patients [11]. They performed a comparison between 3 serological assays and found that the 2 immunoassays that tested for the presence of antibodies against the RBD of the S protein were superior to the one they used for the detection of antibodies reactive for the NP antigen.
What Are the Risk Factors for SARS-CoV-2 Infection in Transplant Recipients?
In the UK study, risk factors for the development of anti-SARS-CoV-2 antibodies included a nonwhite background, a diagnosis of diabetes, and a history of allograft rejection. The association between ethnicity and diabetes with COVID-19 infection is consistent with risk factors for COVID-19 found in studies within the general population [12, 13] and with prior studies identifying common risk factors for posttransplant infections [14].
Less clear is the association between acute rejection and increased infection risk, which could be related in part to the greater cumulative exposure to immunosuppressive drugs after rejection. It is also possible that rejection episodes were a sign of poor adherence to immunosuppressive therapy and, possibly, worse compliance to recommended medical preventive strategies against the virus. The authors also emphasize that patients enrolled in this single-center study [10] received steroids only if they had prior rejection episodes or if they were using steroids before transplantation. Therefore, the rejection-associated risk could be driven by the use of steroids due to a prior episode, similar to the results of studies in other immunosuppressed populations, suggesting that glucocorticoid exposure increases the risk of severe COVID-19 disease [15]. In this respect, it will be important to identify specific risk factors for steroid toxicity that put people at risk for SARS-CoV-2 infection. With the limitations of a single-center, cross-sectional study, this report provides important information in the identification of kidney transplant patients in whom preventive strategies should be reinforced the most.
Is the Antiviral Immune Response Impaired in Transplanted Individuals?
While high rates of infections and poor outcomes in transplant patients suggest an impaired immune response, the few available immune phenotypic analyses [16] and anti-SARS-CoV-2 antibody measurements in infected individuals did not detect major abnormalities. However, anti-SARS-CoV-2 T-cell responses in transplanted patients have not been investigated extensively.
The anti-SARS-CoV-2 antibodies measured in the UK study were targeted against the RBD viral antigen [10]. There is evidence that anti-RBD antibodies may provide information on functional immunity, given reported correlations between RBD antibodies and neutralizing antibodies [17, 18]. However, ad hoc functional studies testing the viral-neutralizing capacity of the anti-SARS-CoV-2 antibodies produced by transplant recipients are needed. Understanding the antiviral response in kidney transplant recipients is key not just to interpret data of antibody screening studies but more importantly to manage immunosuppression during COVID-19 and for future vaccine strategies.
Should We Isolate Infected Transplant Recipients Longer Than Non-Immunosuppressed Individuals?
An immunosuppressive state may associate with higher risk of persistent infections and viral spread than people from the general population. It may also be associated with higher viral shedding. Recently, the cases of immunocompromised individuals with persistent (over 5 months) viral infection have been reported [5, 19]. Importantly, ex vivo studies suggest that the virus that is persistently shed by immunosuppressed individuals would still be able to establish productive infection in contacts upon transmission [19]. If confirmed also in transplanted individuals, this may suggest that longer quarantine periods and repeated negative tests may be needed for transplant recipients who recover from COVID-19. In particular, monitoring of genomic and subgenomic RNA might be required to exclude persistence of SARS-CoV-2 infection [19].
Importantly, an immunocompromised state has been shown to accelerate in-host SARS-CoV-2 viral evolution and formation of new viral variants [5, 19]. Potential factors contributing to the observed within-host evolution is prolonged infection and the compromised immune status of the host, possibly resulting in a different set of selective pressures compared with an immune-competent host. This could be particularly important based on recent data suggesting that a more sustained antibody response and greater CD4+ T-cell activation with increased somatic mutations on B cells are associated with shorter disease trajectories [20]. Mutations that alter RNA virus may affect SARS-CoV-2 virulence and transmissibility and facilitate global spread, similar to what happened with D614G mutation [21].
Preventing SARS-CoV-2 Infection in Transplant Recipients and How This Pandemic Has Changed Transplant Medicine
The ultimate strategy to effectively prevent COVID-19 is vaccination against SARS-CoV-2. Recently approved vaccines, including mRNA-based vaccines, have shown great promise in terms of safety/efficacy profile [22, 23]. Luckily, anti-SARS-CoV-2 vaccines are already being distributed worldwide, and transplant recipients have initiated vaccination in multiple locations with major variability in timing driven by government policies and availability of vaccines. Although data in transplant patients are limited, it is reasonable to think, based on data with other vaccines [24], that their immune responses will be protective, although the degree of protection and its duration are still unknown.
Strategies to prevent infection in transplant recipients are still important while vaccine is being delivered, in particular the use of masks and avoidance of crowded nonventilated spaces. The COVID-19 outbreak has seriously challenged the transplant practice worldwide. Innovative telehealth solutions have emerged allowing safe and efficient outpatient care, delineating the future transplant practice in the post-COVID-19 era. As the pandemic continues and new, more infectious variants of the virus emerge, identification of risk factors for infection may allow to risk stratify patients and tailor prevention strategies and prioritize vaccination in patients at highest risk.
The early decision to postpone nonurgent transplant programs has been accompanied by a drastic drop of organ procurement around the world. Such position has been questioned by the risk of mortality on the waiting list, and huge efforts have been made to settle efficient mitigating strategies to overcome these ethical issues. Ideal strategies still have to be defined, and risk/benefit assessment should be discussed on a case-by-case basis within the transplant team. Even more uncertain is the management of immunosuppression and how it affects the risk of infection, mortality, and, possibly, response to vaccination. This might be very important, as preliminary data among 436 transplant recipients that received either BNT162b2 vaccine (Pfizer-BioNTech) or the mRNA-1273 vaccine (Moderna) suggest that the antibody immunity may be lower than in the general population, as evident by only 17% detection of antibody against the spike antigen after first dose of the vaccine [25].
The need for remote monitoring of graft function has further highlighted the importance of noninvasive biomarkers of acute rejection. Despite the approval by the Food and Drug Administration of a few assays, this still largely represents an unmet need that should also be adequately addressed in the post-COVID-19 period of transplant medicine.
Optimal protection of kidney transplant recipients during COVID-19 outbreak is an evolving concept, and more studies are required to understand special needs of this fragile population. Through an extremely challenging path, this pandemic forced the transplant community to revisit strategies and priorities. Once the viral spread will be under control, these challenges may ultimately result in a better care of transplanted individuals.
Conflict of Interest Statement
The authors declare that they have no conflict of interest to disclose. P.C. is supported by NIH NIAID grant 3U01AI063594-17S1.
This Article is part of
Explore all 295 articles
References
Author Contacts
Paolo Cravedi, paolo.cravedi@mssm.edu
Article / Publication Details
Received: February 01, 2021
Accepted: February 11, 2021
Published online: March 31, 2021
Issue release date: May 2021 Number of Print Pages: 5
Number of Figures: 0
Number of Tables: 0 ISSN: 1660-8151 (Print)
eISSN: 2235-3186 (Online) For additional information: https://www.karger.com/NEF
Copyright / Drug Dosage / Disclaimer
Copyright: All rights reserved. No part of this publication may be translated into other languages, reproduced or utilized in any form or by any means, electronic or mechanical, including photocopying, recording, microcopying, or by any information storage and retrieval system, without permission in writing from the publisher.
Drug Dosage: The authors and the publisher have exerted every effort to ensure that drug selection and dosage set forth in this text are in accord with current recommendations and practice at the time of publication. However, in view of ongoing research, changes in government regulations, and the constant flow of information relating to drug therapy and drug reactions, the reader is urged to check the package insert for each drug for any changes in indications and dosage and for added warnings and precautions. This is particularly important when the recommended agent is a new and/or infrequently employed drug.
Disclaimer: The statements, opinions and data contained in this publication are solely those of the individual authors and contributors and not of the publishers and the editor(s). The appearance of advertisements or/and product references in the publication is not a warranty, endorsement, or approval of the products or services advertised or of their effectiveness, quality or safety. The publisher and the editor(s) disclaim responsibility for any injury to persons or property resulting from any ideas, methods, instructions or products referred to in the content or advertisements."	16516
covid19	['Marco Ciotti', 'Https', 'Francesca Benedetti', 'Davide Zella', 'Silvia Angeletti', 'Massimo Ciccozzi', 'Sergio Bernardini', 'E F', 'Author Affiliations', 'Corresponding Author']		"Abstract

Background: Currently, a pandemic of coronavirus disease 2019 (COVID-19) caused by the novel coronavirus severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is underway, resulting in high morbidity and mortality across the globe. Summary: A prompt and effective diagnosis is crucial to identify infected individuals, to monitor the infection, to perform contact tracing, and to limit the spread of the virus. Since the announcement of this public health emergency, several diagnostic methods have been developed including molecular and serological assays, and more recently biosensors. Here, we present the use of these assays as well as their main technical features, advantages, and limits. Key Messages: The development of reliable diagnostic assays is crucial not only for a correct diagnosis and containment of COVID-19 pandemic, but also for the decision-making process that is behind the clinical decisions, eventually contributing to the improvement of patient management. Furthermore, with the advent of vaccine and therapeutic monoclonal antibodies against SARS-CoV-2, serological assays will be instrumental for the validation of these new therapeutic options.

© 2021 S. Karger AG, Basel

Introduction

An outbreak of unknown pneumonia was reported at the end of December 2019 in Wuhan, Hubei province, China. From the respiratory secretions of affected patients was isolated, a novel coronavirus whose genome analysis indicated belonging to the genus β-coronavirus, lineage B, subgenus Sarbecovirus. This seventh human coronavirus is related to some severe acute respiratory syndrome (SARS)-like coronaviruses detected in bats, but it is distinct from the SARS-CoV and MERS-CoV [1]. The new coronavirus was named SARS-CoV-2. Since its discovery, SARS-CoV-2 spread all over the world, and at the time of writing >85 million people are infected and almost 2 million died (January 3, 2021). Exposure to the virus may result in asymptomatic infection or development of symptoms that may range from mild upper respiratory tract symptoms to severe pneumonia with respiratory failure, hypercoagulation, hyperinflammatory manifestations, and eventually death because of multiple organ failure [2, 3]. In Italy, according to the report released by Istituto Superiore Sanità updated to December 2, 2020 (https://www.epicentro.iss.it/coronavirus/sars-cov-2-decessi-italia), the mean age of patients dying for SARS-CoV-2 infection is 80 years, most deaths occur in men, and 63.1% of deceased men positive to SARS-CoV-2 suffers from 3 or more comorbidities. Among these, the most common are hypertension (64.2%), type 2 diabetes (30.7%), and ischemic heart disease (30.7%). In order to contain the pandemic, an effective and rapid diagnosis of SARS-CoV-2 suspected infection is required. Several diagnostics methods have been developed in a short time frame since the beginning of coronavirus disease 2019 (COVID-19) pandemic including molecular and serological assays [4], and more recently very innovative methods such as biosensors which are currently under investigation and require validation [5, 6].

Currently, real-time PCR is the gold standard for SARS-CoV-2 testing or for confirming COVID-19 diagnosis. However, it requires skilled personnel and time-consuming laboratory procedures. For this reason, the development of innovative approaches such as biosensors is welcome. They could facilitate the control of outbreaks allowing for a diagnosis of infection at earlier stages thus reducing the rate of transmission, morbidity, and mortality. In this review, we address the use of these diagnostic assays, underlying their advantages and limits, and reporting on their main technical features.

SARS-CoV-2 Diagnostics

Real-Time PCR Assays

Several molecular assays have been validated and are currently available on the market for the diagnosis of SARS-CoV-2 infection. Validated specimen types include nasopharyngeal swab, nasopharyngeal aspirate, oropharyngeal swab, bronchoalveolar lavage, and sputum [7]. More recently, saliva has also been evaluated but awaits validation. Preliminary results indicate saliva as a promising biological specimen for diagnosis, monitoring, and infection control [8].

Suspected SARS-CoV-2 infections are confirmed after detection of unique and specific target regions within the viral genome. According to the PCR design, the specific target regions may include ORF1ab, RdRp, N, and S genes. In addition, some commercial assays include also the amplification of the common beta coronavirus E gene, which is amplified along with one or more specific target genes. In Table 1 are reported the molecular assays approved by the Italian Ministry of Health (0011715-03/04/2020-DGPRE-DGPRE-P).

Table 1.

In the USA, the Centers for Disease Control and Prevention (CDC) developed a real-time PCR protocol, which targets 2 regions of the N gene of SARS-CoV-2. The internal control is represented by the human RNase P gene that is detected both in clinical specimens and control samples (https://www.fda.gov/media/134922/download. Revision 3, March 30, 2020).

Real-time PCR assays represent the gold standard for the laboratory diagnosis of SARS-CoV-2 infection; however, false-negative results may occur. Several factors may be responsible for these incorrect results: quality of the specimen, viral load below the limit of detection (LOD) of the method, incorrect handling of the specimen, problems during shipment, timing of sampling (sample collected too early or too late during infection), and source of sample (upper or lower respiratory tract). In the initial phase of COVID-19 disease, upper respiratory tract sample can result in RT-PCR negative, while chest computed tomography images show the presence of pulmonary abnormalities consistent with viral pneumonia [9-11]. Repeat testing can increase the chance of detecting SARS-CoV-2 RNA [12].

Point of care (POC) molecular tests are designed to deliver results in <1 h using RT-PCR technology and are performed on individuals with suspected COVID-19. They do not require particularly trained personnel; hands-on time is minimal (∼1–2 min), and thier result interpretation is straightforward. POC assays may facilitate the management and triage of patients, and some of these platforms could be used outside hospital settings such as in nursing homes to screen the elderly population, which is at high risk of developing pneumonia with consequences often fatal. In Table 2 are listed some of the rapid molecular tests that can be used for qualitative detection of SARS-CoV-2 RNA in individuals with signs and symptoms of suspected COVID-19.

Table 2.

Although correlations have been done between viral load and severity of disease [13], Ct values cannot be used to assess disease’s severity or to monitor response to therapy yet. However, low Ct values, indicative of high viral loads, may be used to indicate transmissibility [14, 15]

SHERLOCK One-Pot and DETECTR Testing

SHERLOCK (Specific High-sensitivity Enzymatic Reporter unLOCKing) assay combines simplified viral RNA extraction with isothermal amplification and CRISPR (Clustered Regularly Interspaced Short Palindromic Repeats)-mediated detection. The assay was designed to detect the N gene of SARS-CoV-2 by optimized sets of LAMP (Loop-mediated isothermal AMPlification) primers and AapCas12b guide RNA. The assay is highly sensitive (33 copies/mL vs. 1,000 copies/mL of the CDC qRT-PCR), and the results can be read on a lateral-flow strip or fluorescence reader after 80 or 45 min, respectively [16]. Similarly, DETECRT (SARS-CoV-2 DNA Endonuclease-TargEted CRISPR Trans Reporter) assay performs reverse transcription and isothermal amplification of RNA extracted from nasopharyngeal or oropharyngeal swabs, followed by Cas12-based detection of E and N gene sequences. The N sequence targeted by DETECTR differs from that recognized by the CDC assay (N1 and N3 regions). The selected Cas12 guide RNAs allows the identification of SARS-CoV-2, bat SARS-like coronavirus and SARS-CoV in the E gene, whereas the N region is specific for SARS-CoV-2. The test is positive if both the E and N genes are detected or presumptive positive if either E or N gene is detected. The LOD of the assay is 10 copies/µL reaction versus 1 copy/µL of the CDC assay. The positive predictive agreement and negative predictive agreement of DETECTR assay versus CDC qRT-PCR are 95 and 100%, respectively. The turn-around time of the DETECTR assay from extraction to result is about 52 min for 1–8 samples extracted manually. Results are visualized by a fluorescent reader or lateral flow strip [17].

Serological Assays

Serological assays have been developed using as target the highly antigenic structural proteins spike (S) and nucleocapsid (N) of SARS-CoV-2. In most individuals, measurable antibodies develop within days or weeks from symptoms onset [18, 19], thus limiting the use of serological testing in the early phase of infection [20, 21]. However, serological assays are important for contact tracing, for identifying suspected cases who are PCR negative but show radiological findings suggestive of CO­VID-19 [9, 10], to identify asymptomatic carriers [22] or to determine the development of neutralizing antibodies in response to vaccination [23, 24]. Instead, doubts arise about the use of antibody tests for seroprevalence surveys for public health management reasons because the duration of circulating antibody is currently unknown [25].

Most of the available serological assays detect IgM and IgG antibodies although IgA antibodies play an important role in mucosal immunity. IgAs can be detected earlier than IgGs, and in atypical cases or in patients with repeated negative RT-PCR, IgA along with IgG may contribute to the diagnosis of SARS-CoV-2 infection [26].

Several commercial serological assays have been developed since the beginning of the COVID-19 pandemic such as ELISAs, chemiluminescence assays (CLIAs), and lateral flow assays. Because of the differences in the assay design, immunoglobulin classes detected (IgM, IgG, and IgA), SARS-CoV-2 antigen used (receptor binding domain [RBD], S protein, S1 subunit, N protein) or native inactivated SARS-CoV-2 [27], and specimen type (serum, plasma, whole blood, and finger-stick whole blood) results are often not comparable. Furthermore, it is important to verify the performance characteristics of the assays through a validation process that takes into account the analytical features of the assay as well as the clinical sensitivity of the test results [28].

Serological assays can be qualitative or semiquantitative and show different sensitivity and specificity toward the antibodies detected. In a recent study where was compared the performance of ELISA, CLIA, and ECLIA assays, it was found that overall, the ECLIA assay performed best showing an optimal sensitivity and specificity since the first days of infection suggesting that it could be considered a valid screening method. IgAs were detected earlier than IgMs by ELISA assay suggesting that IgA detection can be more useful than IgM in the early diagnosis of SARS-CoV-2 infection. Also, its level was higher than IgM over 20 days of observation [29]. A similar observation was already reported in literature [30]. Instead, IgGs were consistently detected after 10 days from symptoms onset in line with other studies [29, 31].

Magnetic chemiluminescence enzyme immunoassay is a double antibody sandwich immunoassay that uses the nucleoprotein and a peptide of the spike protein of SARS-CoV-2 as recombinant antigens for the detection of IgM and IgG against SARS-CoV-2. Using this assay, IgM and IgG were detected since the first days after symptoms onset, and by 17–19 days from symptoms onset, 100% of the study subjects were IgG positive. The subject’s IgM positive reached a peak of 94.1% after 20–22 days from symptoms onset. The median day of seroconversion for both IgM and IgG was 13 days post symptoms onset. Three types of seroconversion were observed: (i) synchronous seroconversion of IgM and IgG; (ii) IgM seroconversion earlier than IgG; and (iii) IgG seroconversion earlier than IgM. In some patients, IgM and IgG plateaued 6 days after the first positive determination. Furthermore, testing of the sera of individuals with negative RT-PCR and no symptoms who came in close contacts with COVID-19 patients showed that some of these individuals later tested IgM and/or IgG positive. These last cases demonstrate the importance of serology in identifying those suspected cases that are missed by molecular tests [32].

With the introduction of vaccination against SARS-CoV-2, it will be important to monitor the development of neutralizing antibodies against the virus and their duration. At the time of writing, the vaccine Comirnaty developed by Pfizer-BioNTech against SARS-CoV-2 has received both the Food and Drug Administration and European Medicine Agency approval (Pfizer-BioNTech COVID-19 Vaccine|FDA; European Medicine Agency recommends first COVID-19 vaccine for authorization in the EU|European Medicines Agency [europa.eu]), and the approval for the AIFA (Agenzia Italiana del Farmaco), and is being distributed in Italy. This vaccine demonstrated a 95% efficacy in clinical trial in people 16 years of age or older [33].

Recently, an immunoassay, Elecsys® Anti-SARS-CoV-2 S (Roche Diagnostics, Monza, Italy), has been released which is designed for the quantitative determination of antibodies to the SARS-CoV-2 spike RBD in human serum and plasma. The assay uses a recombinant RBD of the S antigen in a double-antigen sandwich assay format that favors detection of high-affinity antibodies against SARS-CoV-2. The assay aims at evaluating the adaptive humoral immune response to the S protein of SARS-CoV-2 [34]. This assay showed a clinical sensitivity of 98.8% (95% CI: 98.1–99.3%) when testing samples collected 14 days or later after confirmed SARS-CoV-2 diagnosis by PCR, and an analytical specificity of 100% (95% CI: 99.7–100%) when testing samples collected before October 2019, including samples from individuals with common cold symptoms, samples from individuals infected with 1 of the 4 common cold coronaviruses (HKU1, NL63, 229E, or OC43), and anti-MERS-CoV positive samples. The clinical specificity was 99.98% (95% CI: 99.91–100%), when testing pre-pandemic samples obtained from routine diagnostics and blood donors (Roche Diagnostics International Ltd, Rotkreuz, Switzerland).

Rapid Diagnostic Assays

There are 2 types of rapid diagnostic assays: the antigen detection assay and the antibody detection assay. Both tests can be used at the POC or near it and do not require special equipment or laboratory infrastructures. They are particularly useful in settings where more expensive equipment or reagents are not available or when it is necessary to reduce the pressure on the molecular biology laboratories [35].

The antigen detection assay is usually directed against the nucleocapsid protein of the SARS-CoV-2 which is produced during active infection. The assay is performed on nasopharyngeal swab samples that are released into a dedicated or universal transport medium. The result of the antigen detection assay may be influenced by several factors including the time of sample collection, the quality of sampling, the concentration of the virus in the collected sample, and the quality of the reagents. The sensitivity of the assay is low compared to molecular tests [36], and it is generally positive when the viral load is very high and the subject is very infectious, that is, in the first days of infection during the asymptomatic phase (1–2 days) and few days after symptoms onset (5–7 days). The antigen test is usually negative when the Ct value of the real-time PCR is over 30, while its sensitivity increases with Ct values below 25 [36]. Therefore, a negative antigen test result cannot exclude a SARS-CoV-2 infection, and cannot be used to provide guidance for quarantine decision. Nonetheless, rapid antigen tests may be useful in high prevalence settings, where a positive result most likely represents a true positive result or in the presence of asymptomatic carriers with high viral load where accelerates contact tracing [37]. Time to result is about 15 min.

Rapid antibody tests are easy to perform and can be used at the POC returning a result within 15–20 min. The test can be run on whole blood, finger-stick whole blood, serum, and plasma. Rapid antibody tests can be used to detect previous SARS-CoV-2 infection and should not be used for determining active infections in clinical care or for contact-tracing purposes. A positive result does not necessarily reflect the presence of neutralizing antibodies or protective immunity [38].

Rapid antibody tests are not useful for the early diagnosis of SARS-CoV-2 infection. Overall, antibody tests have an accuracy of 30% in the first week after symptoms onset, that increases to 70% in the second week, and to >90% after 3 weeks [25]. Sensitivities, specificity, and accuracy vary with the manufacturer. For instance, in a study performed in our laboratory, rapid tests were compared to CLIA assay. The following results were obtained: CLIA assay showed a sensitivity of 95% for IgG versus about 90% for the immunochromatographic tests, whereas the sensitivity for IgM was 91% for CLIA and ranged from 61.4 to 87.8% for the rapid tests. Specificity was 100% for all tests [39]. Therefore, these tests should undergo laboratory validation before use in outpatient clinics or as direct-to-consumer testing.

Biosensors in the Diagnosis of SARS-CoV-2 Infection

A rapid and accurate, easy to use, diagnostic system is important for controlling infection source and monitoring progression of disease. Recently, it has been developed an ultrasensitive electrochemical detection technology that uses calixarene functionalized graphene oxide for targeting SARS-CoV-2 RNA. Using a portable electrochemical smartphone, this technology can detect viral RNA without performing reverse transcription and amplification, but instead, it relies on a supersandwich-type recognition approach. The biosensor detected viral RNA from COVID-19 confirmed patients and recovery patients, and the detectable ratio was higher than RT-PCR suggesting a higher sensitivity. In this work, the authors reached a LOD of 200 copies/mL which is much lower than the LODs claimed by several commercial diagnostic RT-PCR assays. Given the high sensitivity and easy to use, the biosensor could be used as a point-of-care (POC) testing [5].

Another interesting approach is the use of a breath device for differentiating between COVID-19 patients and patients with other lung infections. The device is composed of a nanomaterial-based hybrid sensor array able to detect disease-specific biomarkers from exhaled breath [6]. In the case of SARS-CoV-2 infection, this biosensor showed a good capability of differentiating between COVID-19 patients, healthy controls, and patients with lung infections unrelated to COVID-19. Training and test set data showed 94 and 76% accuracy in differentiating patients from controls, and 90 and 95% accuracy in differentiating between COVID-19 patients and patients with other lung infections, respectively [6]. If the validity of this technology is confirmed by future studies, this biosensor may be used as screening tool in POC facilities.

Conclusions

Laboratory diagnostic assays are key for a proper management of COVID-19 patients and for limiting the spread of SARS-CoV-2. Molecular assays represent the gold standard for the diagnosis of suspected COVID-19 cases. Nonetheless, serological assays may be used in combination with molecular tests to improve diagnostic sensitivity and to identify asymptomatic individuals who tested PCR negative but are IgG or IgM positive; in particular close contacts of COVID-19 patients. With the introduction of the prophylactic vaccine against SARS-CoV-2, detection and quantification of neutralizing antibodies by serological assays will allow to verify the efficacy of the vaccine through the determination of the antibody response against the virus and to monitor the neutralizing antibody titer. Monitoring of the antibody titer is important to determine when a new dose of vaccine is needed. Finally, a new impulse in support of the battle against COVID-19 pandemic comes from new technologies such as biosensors that, if validated, might be used in POC facilities.

Conflict of Interest Statement

The authors have no conflicts of interest to declare.

Funding Sources

The authors did not receive any funding.

Author Contributions

Marco Ciotti: conception of the work, writing, revision, and final approval; Francesca Benedetti: PubMed search, revision, and final approval; Davide Zella: critical revision and final approval; Silvia Angeletti: critical revision and final approval; Massimo Ciccozzi: critical revision and final approval; Sergio Bernardini: conception of the work, critical revision, and final approval.



This Article is part of

Explore all 295 articles

References

Author Contacts

Marco Ciotti Virology Unit, Laboratory of Clinical Microbiology and Virology Polyclinic Tor Vergata Foundation Viale Oxford 81, IT–00133 Rome (Italy) marco.ciotti@ptvonline.it

Article / Publication Details

Received: January 06, 2021

Accepted: January 28, 2021

Published online: March 19, 2021

Number of Print Pages: 7

Number of Figures: 0

Number of Tables: 2 ISSN: 0009-3157 (Print)

eISSN: 1421-9794 (Online) For additional information: https://www.karger.com/CHE

Copyright / Drug Dosage / Disclaimer

Copyright: All rights reserved. No part of this publication may be translated into other languages, reproduced or utilized in any form or by any means, electronic or mechanical, including photocopying, recording, microcopying, or by any information storage and retrieval system, without permission in writing from the publisher.

Drug Dosage: The authors and the publisher have exerted every effort to ensure that drug selection and dosage set forth in this text are in accord with current recommendations and practice at the time of publication. However, in view of ongoing research, changes in government regulations, and the constant flow of information relating to drug therapy and drug reactions, the reader is urged to check the package insert for each drug for any changes in indications and dosage and for added warnings and precautions. This is particularly important when the recommended agent is a new and/or infrequently employed drug.

Disclaimer: The statements, opinions and data contained in this publication are solely those of the individual authors and contributors and not of the publishers and the editor(s). The appearance of advertisements or/and product references in the publication is not a warranty, endorsement, or approval of the products or services advertised or of their effectiveness, quality or safety. The publisher and the editor(s) disclaim responsibility for any injury to persons or property resulting from any ideas, methods, instructions or products referred to in the content or advertisements."	https://www.karger.com/Article/FullText/515343	"Background: Currently, a pandemic of coronavirus disease 2019 (COVID-19) caused by the novel coronavirus severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is underway, resulting in high morbidity and mortality across the globe. Summary: A prompt and effective diagnosis is crucial to identify infected individuals, to monitor the infection, to perform contact tracing, and to limit the spread of the virus. Since the announcement of this public health emergency, several diagnostic methods have been developed including molecular and serological assays, and more recently biosensors. Here, we present the use of these assays as well as their main technical features, advantages, and limits. Key Messages: The development of reliable diagnostic assays is crucial not only for a correct diagnosis and containment of COVID-19 pandemic, but also for the decision-making process that is behind the clinical decisions, eventually contributing to the improvement of patient management. Furthermore, with the advent of vaccine and therapeutic monoclonal antibodies against SARS-CoV-2, serological assays will be instrumental for the validation of these new therapeutic options.
© 2021 S. Karger AG, Basel
Introduction
An outbreak of unknown pneumonia was reported at the end of December 2019 in Wuhan, Hubei province, China. From the respiratory secretions of affected patients was isolated, a novel coronavirus whose genome analysis indicated belonging to the genus β-coronavirus, lineage B, subgenus Sarbecovirus. This seventh human coronavirus is related to some severe acute respiratory syndrome (SARS)-like coronaviruses detected in bats, but it is distinct from the SARS-CoV and MERS-CoV [1]. The new coronavirus was named SARS-CoV-2. Since its discovery, SARS-CoV-2 spread all over the world, and at the time of writing >85 million people are infected and almost 2 million died (January 3, 2021). Exposure to the virus may result in asymptomatic infection or development of symptoms that may range from mild upper respiratory tract symptoms to severe pneumonia with respiratory failure, hypercoagulation, hyperinflammatory manifestations, and eventually death because of multiple organ failure [2, 3]. In Italy, according to the report released by Istituto Superiore Sanità updated to December 2, 2020 (https://www.epicentro.iss.it/coronavirus/sars-cov-2-decessi-italia), the mean age of patients dying for SARS-CoV-2 infection is 80 years, most deaths occur in men, and 63.1% of deceased men positive to SARS-CoV-2 suffers from 3 or more comorbidities. Among these, the most common are hypertension (64.2%), type 2 diabetes (30.7%), and ischemic heart disease (30.7%). In order to contain the pandemic, an effective and rapid diagnosis of SARS-CoV-2 suspected infection is required. Several diagnostics methods have been developed in a short time frame since the beginning of coronavirus disease 2019 (COVID-19) pandemic including molecular and serological assays [4], and more recently very innovative methods such as biosensors which are currently under investigation and require validation [5, 6].
Currently, real-time PCR is the gold standard for SARS-CoV-2 testing or for confirming COVID-19 diagnosis. However, it requires skilled personnel and time-consuming laboratory procedures. For this reason, the development of innovative approaches such as biosensors is welcome. They could facilitate the control of outbreaks allowing for a diagnosis of infection at earlier stages thus reducing the rate of transmission, morbidity, and mortality. In this review, we address the use of these diagnostic assays, underlying their advantages and limits, and reporting on their main technical features.
SARS-CoV-2 Diagnostics
Real-Time PCR Assays
Several molecular assays have been validated and are currently available on the market for the diagnosis of SARS-CoV-2 infection. Validated specimen types include nasopharyngeal swab, nasopharyngeal aspirate, oropharyngeal swab, bronchoalveolar lavage, and sputum [7]. More recently, saliva has also been evaluated but awaits validation. Preliminary results indicate saliva as a promising biological specimen for diagnosis, monitoring, and infection control [8].
Suspected SARS-CoV-2 infections are confirmed after detection of unique and specific target regions within the viral genome. According to the PCR design, the specific target regions may include ORF1ab, RdRp, N, and S genes. In addition, some commercial assays include also the amplification of the common beta coronavirus E gene, which is amplified along with one or more specific target genes. In Table 1 are reported the molecular assays approved by the Italian Ministry of Health (0011715-03/04/2020-DGPRE-DGPRE-P).
Table 1.
In the USA, the Centers for Disease Control and Prevention (CDC) developed a real-time PCR protocol, which targets 2 regions of the N gene of SARS-CoV-2. The internal control is represented by the human RNase P gene that is detected both in clinical specimens and control samples (https://www.fda.gov/media/134922/download. Revision 3, March 30, 2020).
Real-time PCR assays represent the gold standard for the laboratory diagnosis of SARS-CoV-2 infection; however, false-negative results may occur. Several factors may be responsible for these incorrect results: quality of the specimen, viral load below the limit of detection (LOD) of the method, incorrect handling of the specimen, problems during shipment, timing of sampling (sample collected too early or too late during infection), and source of sample (upper or lower respiratory tract). In the initial phase of COVID-19 disease, upper respiratory tract sample can result in RT-PCR negative, while chest computed tomography images show the presence of pulmonary abnormalities consistent with viral pneumonia [9-11]. Repeat testing can increase the chance of detecting SARS-CoV-2 RNA [12].
Point of care (POC) molecular tests are designed to deliver results in <1 h using RT-PCR technology and are performed on individuals with suspected COVID-19. They do not require particularly trained personnel; hands-on time is minimal (∼1–2 min), and thier result interpretation is straightforward. POC assays may facilitate the management and triage of patients, and some of these platforms could be used outside hospital settings such as in nursing homes to screen the elderly population, which is at high risk of developing pneumonia with consequences often fatal. In Table 2 are listed some of the rapid molecular tests that can be used for qualitative detection of SARS-CoV-2 RNA in individuals with signs and symptoms of suspected COVID-19.
Table 2.
Although correlations have been done between viral load and severity of disease [13], Ct values cannot be used to assess disease’s severity or to monitor response to therapy yet. However, low Ct values, indicative of high viral loads, may be used to indicate transmissibility [14, 15]
SHERLOCK One-Pot and DETECTR Testing
SHERLOCK (Specific High-sensitivity Enzymatic Reporter unLOCKing) assay combines simplified viral RNA extraction with isothermal amplification and CRISPR (Clustered Regularly Interspaced Short Palindromic Repeats)-mediated detection. The assay was designed to detect the N gene of SARS-CoV-2 by optimized sets of LAMP (Loop-mediated isothermal AMPlification) primers and AapCas12b guide RNA. The assay is highly sensitive (33 copies/mL vs. 1,000 copies/mL of the CDC qRT-PCR), and the results can be read on a lateral-flow strip or fluorescence reader after 80 or 45 min, respectively [16]. Similarly, DETECRT (SARS-CoV-2 DNA Endonuclease-TargEted CRISPR Trans Reporter) assay performs reverse transcription and isothermal amplification of RNA extracted from nasopharyngeal or oropharyngeal swabs, followed by Cas12-based detection of E and N gene sequences. The N sequence targeted by DETECTR differs from that recognized by the CDC assay (N1 and N3 regions). The selected Cas12 guide RNAs allows the identification of SARS-CoV-2, bat SARS-like coronavirus and SARS-CoV in the E gene, whereas the N region is specific for SARS-CoV-2. The test is positive if both the E and N genes are detected or presumptive positive if either E or N gene is detected. The LOD of the assay is 10 copies/µL reaction versus 1 copy/µL of the CDC assay. The positive predictive agreement and negative predictive agreement of DETECTR assay versus CDC qRT-PCR are 95 and 100%, respectively. The turn-around time of the DETECTR assay from extraction to result is about 52 min for 1–8 samples extracted manually. Results are visualized by a fluorescent reader or lateral flow strip [17].
Serological Assays
Serological assays have been developed using as target the highly antigenic structural proteins spike (S) and nucleocapsid (N) of SARS-CoV-2. In most individuals, measurable antibodies develop within days or weeks from symptoms onset [18, 19], thus limiting the use of serological testing in the early phase of infection [20, 21]. However, serological assays are important for contact tracing, for identifying suspected cases who are PCR negative but show radiological findings suggestive of CO­VID-19 [9, 10], to identify asymptomatic carriers [22] or to determine the development of neutralizing antibodies in response to vaccination [23, 24]. Instead, doubts arise about the use of antibody tests for seroprevalence surveys for public health management reasons because the duration of circulating antibody is currently unknown [25].
Most of the available serological assays detect IgM and IgG antibodies although IgA antibodies play an important role in mucosal immunity. IgAs can be detected earlier than IgGs, and in atypical cases or in patients with repeated negative RT-PCR, IgA along with IgG may contribute to the diagnosis of SARS-CoV-2 infection [26].
Several commercial serological assays have been developed since the beginning of the COVID-19 pandemic such as ELISAs, chemiluminescence assays (CLIAs), and lateral flow assays. Because of the differences in the assay design, immunoglobulin classes detected (IgM, IgG, and IgA), SARS-CoV-2 antigen used (receptor binding domain [RBD], S protein, S1 subunit, N protein) or native inactivated SARS-CoV-2 [27], and specimen type (serum, plasma, whole blood, and finger-stick whole blood) results are often not comparable. Furthermore, it is important to verify the performance characteristics of the assays through a validation process that takes into account the analytical features of the assay as well as the clinical sensitivity of the test results [28].
Serological assays can be qualitative or semiquantitative and show different sensitivity and specificity toward the antibodies detected. In a recent study where was compared the performance of ELISA, CLIA, and ECLIA assays, it was found that overall, the ECLIA assay performed best showing an optimal sensitivity and specificity since the first days of infection suggesting that it could be considered a valid screening method. IgAs were detected earlier than IgMs by ELISA assay suggesting that IgA detection can be more useful than IgM in the early diagnosis of SARS-CoV-2 infection. Also, its level was higher than IgM over 20 days of observation [29]. A similar observation was already reported in literature [30]. Instead, IgGs were consistently detected after 10 days from symptoms onset in line with other studies [29, 31].
Magnetic chemiluminescence enzyme immunoassay is a double antibody sandwich immunoassay that uses the nucleoprotein and a peptide of the spike protein of SARS-CoV-2 as recombinant antigens for the detection of IgM and IgG against SARS-CoV-2. Using this assay, IgM and IgG were detected since the first days after symptoms onset, and by 17–19 days from symptoms onset, 100% of the study subjects were IgG positive. The subject’s IgM positive reached a peak of 94.1% after 20–22 days from symptoms onset. The median day of seroconversion for both IgM and IgG was 13 days post symptoms onset. Three types of seroconversion were observed: (i) synchronous seroconversion of IgM and IgG; (ii) IgM seroconversion earlier than IgG; and (iii) IgG seroconversion earlier than IgM. In some patients, IgM and IgG plateaued 6 days after the first positive determination. Furthermore, testing of the sera of individuals with negative RT-PCR and no symptoms who came in close contacts with COVID-19 patients showed that some of these individuals later tested IgM and/or IgG positive. These last cases demonstrate the importance of serology in identifying those suspected cases that are missed by molecular tests [32].
With the introduction of vaccination against SARS-CoV-2, it will be important to monitor the development of neutralizing antibodies against the virus and their duration. At the time of writing, the vaccine Comirnaty developed by Pfizer-BioNTech against SARS-CoV-2 has received both the Food and Drug Administration and European Medicine Agency approval (Pfizer-BioNTech COVID-19 Vaccine|FDA; European Medicine Agency recommends first COVID-19 vaccine for authorization in the EU|European Medicines Agency [europa.eu]), and the approval for the AIFA (Agenzia Italiana del Farmaco), and is being distributed in Italy. This vaccine demonstrated a 95% efficacy in clinical trial in people 16 years of age or older [33].
Recently, an immunoassay, Elecsys® Anti-SARS-CoV-2 S (Roche Diagnostics, Monza, Italy), has been released which is designed for the quantitative determination of antibodies to the SARS-CoV-2 spike RBD in human serum and plasma. The assay uses a recombinant RBD of the S antigen in a double-antigen sandwich assay format that favors detection of high-affinity antibodies against SARS-CoV-2. The assay aims at evaluating the adaptive humoral immune response to the S protein of SARS-CoV-2 [34]. This assay showed a clinical sensitivity of 98.8% (95% CI: 98.1–99.3%) when testing samples collected 14 days or later after confirmed SARS-CoV-2 diagnosis by PCR, and an analytical specificity of 100% (95% CI: 99.7–100%) when testing samples collected before October 2019, including samples from individuals with common cold symptoms, samples from individuals infected with 1 of the 4 common cold coronaviruses (HKU1, NL63, 229E, or OC43), and anti-MERS-CoV positive samples. The clinical specificity was 99.98% (95% CI: 99.91–100%), when testing pre-pandemic samples obtained from routine diagnostics and blood donors (Roche Diagnostics International Ltd, Rotkreuz, Switzerland).
Rapid Diagnostic Assays
There are 2 types of rapid diagnostic assays: the antigen detection assay and the antibody detection assay. Both tests can be used at the POC or near it and do not require special equipment or laboratory infrastructures. They are particularly useful in settings where more expensive equipment or reagents are not available or when it is necessary to reduce the pressure on the molecular biology laboratories [35].
The antigen detection assay is usually directed against the nucleocapsid protein of the SARS-CoV-2 which is produced during active infection. The assay is performed on nasopharyngeal swab samples that are released into a dedicated or universal transport medium. The result of the antigen detection assay may be influenced by several factors including the time of sample collection, the quality of sampling, the concentration of the virus in the collected sample, and the quality of the reagents. The sensitivity of the assay is low compared to molecular tests [36], and it is generally positive when the viral load is very high and the subject is very infectious, that is, in the first days of infection during the asymptomatic phase (1–2 days) and few days after symptoms onset (5–7 days). The antigen test is usually negative when the Ct value of the real-time PCR is over 30, while its sensitivity increases with Ct values below 25 [36]. Therefore, a negative antigen test result cannot exclude a SARS-CoV-2 infection, and cannot be used to provide guidance for quarantine decision. Nonetheless, rapid antigen tests may be useful in high prevalence settings, where a positive result most likely represents a true positive result or in the presence of asymptomatic carriers with high viral load where accelerates contact tracing [37]. Time to result is about 15 min.
Rapid antibody tests are easy to perform and can be used at the POC returning a result within 15–20 min. The test can be run on whole blood, finger-stick whole blood, serum, and plasma. Rapid antibody tests can be used to detect previous SARS-CoV-2 infection and should not be used for determining active infections in clinical care or for contact-tracing purposes. A positive result does not necessarily reflect the presence of neutralizing antibodies or protective immunity [38].
Rapid antibody tests are not useful for the early diagnosis of SARS-CoV-2 infection. Overall, antibody tests have an accuracy of 30% in the first week after symptoms onset, that increases to 70% in the second week, and to >90% after 3 weeks [25]. Sensitivities, specificity, and accuracy vary with the manufacturer. For instance, in a study performed in our laboratory, rapid tests were compared to CLIA assay. The following results were obtained: CLIA assay showed a sensitivity of 95% for IgG versus about 90% for the immunochromatographic tests, whereas the sensitivity for IgM was 91% for CLIA and ranged from 61.4 to 87.8% for the rapid tests. Specificity was 100% for all tests [39]. Therefore, these tests should undergo laboratory validation before use in outpatient clinics or as direct-to-consumer testing.
Biosensors in the Diagnosis of SARS-CoV-2 Infection
A rapid and accurate, easy to use, diagnostic system is important for controlling infection source and monitoring progression of disease. Recently, it has been developed an ultrasensitive electrochemical detection technology that uses calixarene functionalized graphene oxide for targeting SARS-CoV-2 RNA. Using a portable electrochemical smartphone, this technology can detect viral RNA without performing reverse transcription and amplification, but instead, it relies on a supersandwich-type recognition approach. The biosensor detected viral RNA from COVID-19 confirmed patients and recovery patients, and the detectable ratio was higher than RT-PCR suggesting a higher sensitivity. In this work, the authors reached a LOD of 200 copies/mL which is much lower than the LODs claimed by several commercial diagnostic RT-PCR assays. Given the high sensitivity and easy to use, the biosensor could be used as a point-of-care (POC) testing [5].
Another interesting approach is the use of a breath device for differentiating between COVID-19 patients and patients with other lung infections. The device is composed of a nanomaterial-based hybrid sensor array able to detect disease-specific biomarkers from exhaled breath [6]. In the case of SARS-CoV-2 infection, this biosensor showed a good capability of differentiating between COVID-19 patients, healthy controls, and patients with lung infections unrelated to COVID-19. Training and test set data showed 94 and 76% accuracy in differentiating patients from controls, and 90 and 95% accuracy in differentiating between COVID-19 patients and patients with other lung infections, respectively [6]. If the validity of this technology is confirmed by future studies, this biosensor may be used as screening tool in POC facilities.
Conclusions
Laboratory diagnostic assays are key for a proper management of COVID-19 patients and for limiting the spread of SARS-CoV-2. Molecular assays represent the gold standard for the diagnosis of suspected COVID-19 cases. Nonetheless, serological assays may be used in combination with molecular tests to improve diagnostic sensitivity and to identify asymptomatic individuals who tested PCR negative but are IgG or IgM positive; in particular close contacts of COVID-19 patients. With the introduction of the prophylactic vaccine against SARS-CoV-2, detection and quantification of neutralizing antibodies by serological assays will allow to verify the efficacy of the vaccine through the determination of the antibody response against the virus and to monitor the neutralizing antibody titer. Monitoring of the antibody titer is important to determine when a new dose of vaccine is needed. Finally, a new impulse in support of the battle against COVID-19 pandemic comes from new technologies such as biosensors that, if validated, might be used in POC facilities.
Conflict of Interest Statement
The authors have no conflicts of interest to declare.
Funding Sources
The authors did not receive any funding.
Author Contributions
Marco Ciotti: conception of the work, writing, revision, and final approval; Francesca Benedetti: PubMed search, revision, and final approval; Davide Zella: critical revision and final approval; Silvia Angeletti: critical revision and final approval; Massimo Ciccozzi: critical revision and final approval; Sergio Bernardini: conception of the work, critical revision, and final approval.
This Article is part of
Explore all 295 articles
References
Author Contacts
Marco Ciotti Virology Unit, Laboratory of Clinical Microbiology and Virology Polyclinic Tor Vergata Foundation Viale Oxford 81, IT–00133 Rome (Italy) marco.ciotti@ptvonline.it
Article / Publication Details
Received: January 06, 2021
Accepted: January 28, 2021
Published online: March 19, 2021
Number of Print Pages: 7
Number of Figures: 0
Number of Tables: 2 ISSN: 0009-3157 (Print)
eISSN: 1421-9794 (Online) For additional information: https://www.karger.com/CHE
Copyright / Drug Dosage / Disclaimer
Copyright: All rights reserved. No part of this publication may be translated into other languages, reproduced or utilized in any form or by any means, electronic or mechanical, including photocopying, recording, microcopying, or by any information storage and retrieval system, without permission in writing from the publisher.
Drug Dosage: The authors and the publisher have exerted every effort to ensure that drug selection and dosage set forth in this text are in accord with current recommendations and practice at the time of publication. However, in view of ongoing research, changes in government regulations, and the constant flow of information relating to drug therapy and drug reactions, the reader is urged to check the package insert for each drug for any changes in indications and dosage and for added warnings and precautions. This is particularly important when the recommended agent is a new and/or infrequently employed drug.
Disclaimer: The statements, opinions and data contained in this publication are solely those of the individual authors and contributors and not of the publishers and the editor(s). The appearance of advertisements or/and product references in the publication is not a warranty, endorsement, or approval of the products or services advertised or of their effectiveness, quality or safety. The publisher and the editor(s) disclaim responsibility for any injury to persons or property resulting from any ideas, methods, instructions or products referred to in the content or advertisements."	23171
geography	['National Geographic Society']	2011-01-21 00:00:00	"Geography is the study of places and the relationships between people and their environments. Geographers explore both the physical properties of Earth’s surface and the human societies spread across it. They also examine how human culture interacts with the natural environment, and the way that locations and places can have an impact on people. Geography seeks to understand where things are found, why they are there, and how they develop and change over time.



Ancient Geographers



The term ""geography"" comes to us from the ancient Greeks, who needed a word to describe the writings and maps that were helping them make sense of the world in which they lived. In Greek, geo means “earth” and -graphy means “to write.” Using geography, Greeks developed an understanding of where their homeland was located in relation to other places, what their own and other places were like, and how people and environments were distributed. These concerns have been central to geography ever since.



Of course, the Greeks were not the only people interested in geography. Throughout human history, most societies have sought to understand something about their place in the world, and the people and environments around them.



Indeed, mapmaking probably came even before writing in many places. But ancient Greek geographers were particularly influential. They developed very detailed maps of areas in and around Greece, including parts of Europe, Africa, and Asia. More importantly, they also raised questions about how and why different human and natural patterns came into being on Earth’s surface, and why variations existed from place to place. The effort to answer these questions about patterns and distribution led them to figure out that the world was round, to calculate Earth’s circumference, and to develop explanations of everything from the seasonal flooding of the Nile River to differences in population densities from place to place.



During the Middle Ages, geography ceased to be a major academic pursuit in Europe. Advances in geography were chiefly made by scientists of the Muslim world, based around the Arabian Peninsula and North Africa. Geographers of this Islamic Golden Age created the world’s first rectangular map based on a grid, a map system that is still familiar today. Islamic scholars also applied their study of people and places to agriculture, determining which crops and livestock were most suited to specific habitats or environments.



In addition to the advances in the Middle East, the Chinese empire in Asia also contributed immensely to geography. Until about 1500, China was the most prosperous civilization on Earth. The Chinese were scientifically advanced, especially in the field of astronomy. Around 1000, they also achieved one of the most important developments in the history of geography: They were the first to use the compass for navigational purposes. In the early 1400s, the explorer Cheng Ho embarked on seven voyages to the lands bordering the China Sea and the Indian Ocean, establishing China’s dominance throughout Southeast Asia.



Age of Discovery



Through the 13th-century travels of the Italian explorer Marco Polo, Europeans learned about the riches of China. Curiosity was awakened; a desire to trade with wealthy Asian cultures motivated a renewed interest in exploring the world. The period of time between the 15th and 17th centuries is known in the West as the Age of Exploration or the Age of Discovery.



With the dawn of the Age of Discovery, the study of geography regained popularity in Europe. The invention of the printing press in the mid-1400s helped spread geographic knowledge by making maps and charts widely available. Improvements in shipbuilding and navigation facilitated more exploring, greatly improving the accuracy of maps and geographic information.



Greater geographic understanding allowed European powers to extend their global influence. During the Age of Discovery, European nations established colonies around the world. Improved transportation, communication, and navigational technology allowed countries such as the United Kingdom to successfully govern colonies as far away as the Americas, Asia, Australia, and Africa.





Geography was not just a subject that made colonialism possible, however. It also helped people understand the planet on which they lived. Not surprisingly, geography became an important focus of study in schools and universities.



Geography also became an important part of other academic disciplines, such as chemistry, economics, and philosophy. In fact, every academic subject has some geographic connection. Chemists study where certain chemical elements, such as gold or silver, can be found. Economists examine which nations trade with other nations, and what resources are exchanged. Philosophers analyze the responsibility people have to take care of the Earth.



Emergence of Modern Geography



Some people have trouble understanding the complete scope of the discipline of geography because, unlike most other disciplines, geography is not defined by one particular topic. Instead, geography is concerned with many different topics—people, culture, politics, settlements, plants, landforms, and much more.



What distinguishes geography is that it approaches the study of diverse topics in a particular way (that is, from a particular perspective). Geography asks spatial questions—how and why things are distributed or arranged in particular ways on Earth’s surface. It looks at these different distributions and arrangements at many different scales. It also asks questions about how the interaction of different human and natural activities on Earth’s surface shape the characteristics of the world in which we live.



Geography seeks to understand where things are found and why they are present in those places; how things that are located in the same or distant places influence one another over time; and why places and the people who live in them develop and change in particular ways. Raising these questions is at the heart of the “geographic perspective.”



Exploration has long been an important part of geography. But exploration no longer simply means going to places that have not been visited before. It means documenting and trying to explain the variations that exist across the surface of Earth, as well as figuring out what those variations mean for the future.



The age-old practice of mapping still plays an important role in this type of exploration, but exploration can also be done by using images from satellites or gathering information from interviews. Discoveries can come by using computers to map and analyze the relationship among things in geographic space, or from piecing together the multiple forces, near and far, that shape the way individual places develop.



Applying a geographic perspective demonstrates geography’s concern not just with where things are, but with “the why of where”—a short, but useful definition of geography’s central focus.



The insights that have come from geographic research show the importance of asking “the why of where” questions. Geographic studies comparing physical characteristics of continents on either side of the Atlantic Ocean, for instance, gave rise to the idea that Earth’s surface is comprised of large, slowly moving plates—plate tectonics.



Studies of the geographic distribution of human settlements have shown how economic forces and modes of transport influence the location of towns and cities. For example, geographic analysis has pointed to the role of the U.S. Interstate Highway System and the rapid growth of car ownership in creating a boom in U.S. suburban growth after World War II. The geographic perspective helped show where Americans were moving, why they were moving there, and how their new living places affected their lives, their relationships with others, and their interactions with the environment.



Geographic analyses of the spread of diseases have pointed to the conditions that allow particular diseases to develop and spread. Dr. John Snow’s cholera map stands out as a classic example. When cholera broke out in London, England, in 1854, Snow represented the deaths per household on a street map. Using the map, he was able to trace the source of the outbreak to a water pump on the corner of Broad Street and Cambridge Street. The geographic perspective helped identify the source of the problem (the water from a specific pump) and allowed people to avoid the disease (avoiding water from that pump).





Investigations of the geographic impact of human activities have advanced understanding of the role of humans in transforming the surface of Earth, exposing the spatial extent of threats such as water pollution by manmade waste. For example, geographic study has shown that a large mass of tiny pieces of plastic currently floating in the Pacific Ocean is approximately the size of Texas. Satellite images and other geographic technology identified the so-called “Great Pacific Garbage Patch.”



These examples of different uses of the geographic perspective help explain why geographic study and research is important as we confront many 21st century challenges, including environmental pollution, poverty, hunger, and ethnic or political conflict.



Because the study of geography is so broad, the discipline is typically divided into specialties. At the broadest level, geography is divided into physical geography, human geography, geographic techniques, and regional geography.



Physical Geography



The natural environment is the primary concern of physical geographers, although many physical geographers also look at how humans have altered natural systems. Physical geographers study Earth’s seasons, climate, atmosphere, soil, streams, landforms, and oceans. Some disciplines within physical geography include geomorphology, glaciology, pedology, hydrology, climatology, biogeography, and oceanography.



Geomorphology is the study of landforms and the processes that shape them. Geomorphologists investigate the nature and impact of wind, ice, rivers, erosion, earthquakes, volcanoes, living things, and other forces that shape and change the surface of the Earth.



Glaciologists focus on the Earth’s ice fields and their impact on the planet’s climate. Glaciologists document the properties and distribution of glaciers and icebergs. Data collected by glaciologists has demonstrated the retreat of Arctic and Antarctic ice in the past century.



Pedologists study soil and how it is created, changed, and classified. Soil studies are used by a variety of professions, from farmers analyzing field fertility to engineers investigating the suitability of different areas for building heavy structures.



Hydrology is the study of Earth’s water: its properties, distribution, and effects. Hydrologists are especially concerned with the movement of water as it cycles from the ocean to the atmosphere, then back to Earth’s surface. Hydrologists study the water cycle through rainfall into streams, lakes, the soil, and underground aquifers. Hydrologists provide insights that are critical to building or removing dams, designing irrigation systems, monitoring water quality, tracking drought conditions, and predicting flood risk.



Climatologists study Earth’s climate system and its impact on Earth’s surface. For example, climatologists make predictions about El Nino, a cyclical weather phenomenon of warm surface temperatures in the Pacific Ocean. They analyze the dramatic worldwide climate changes caused by El Nino, such as flooding in Peru, drought in Australia, and, in the United States, the oddities of heavy Texas rains or an unseasonably warm Minnesota winter.



Biogeographers study the impact of the environment on the distribution of plants and animals. For example, a biogeographer might document all the places in the world inhabited by a certain spider species, and what those places have in common.



Oceanography, a related discipline of physical geography, focuses on the creatures and environments of the world’s oceans. Observation of ocean tides and currents constituted some of the first oceanographic investigations. For example, 18th-century mariners figured out the geography of the Gulf Stream, a massive current flowing like a river through the Atlantic Ocean. The discovery and tracking of the Gulf Stream helped communications and travel between Europe and the Americas.



Today, oceanographers conduct research on the impacts of water pollution, track tsunamis, design offshore oil rigs, investigate underwater eruptions of lava, and study all types of marine organisms from toxic algae to friendly dolphins.





Human Geography



Human geography is concerned with the distribution and networks of people and cultures on Earth’s surface. A human geographer might investigate the local, regional, and global impact of rising economic powers China and India, which represent 37 percent of the world’s people. They also might look at how consumers in China and India adjust to new technology and markets, and how markets respond to such a huge consumer base.



Human geographers also study how people use and alter their environments. When, for example, people allow their animals to overgraze a region, the soil erodes and grassland is transformed into desert. The impact of overgrazing on the landscape as well as agricultural production is an area of study for human geographers.



Finally, human geographers study how political, social, and economic systems are organized across geographical space. These include governments, religious organizations, and trade partnerships. The boundaries of these groups constantly change.



The main divisions within human geography reflect a concern with different types of human activities or ways of living. Some examples of human geography include urban geography, economic geography, cultural geography, political geography, social geography, and population geography. Human geographers who study geographic patterns and processes in past times are part of the subdiscipline of historical geography. Those who study how people understand maps and geographic space belong to a subdiscipline known as behavioral geography.



Many human geographers interested in the relationship between humans and the environment work in the subdisciplines of cultural geography and political geography.

Cultural geographers study how the natural environment influences the development of human culture, such as how the climate affects the agricultural practices of a region. Political geographers study the impact of political circumstances on interactions between people and their environment, as well as environmental conflicts, such as disputes over water rights.



Some human geographers focus on the connection between human health and geography. For example, health geographers create maps that track the location and spread of specific diseases. They analyze the geographic disparities of health-care access. They are very interested in the impact of the environment on human health, especially the effects of environmental hazards such as radiation, lead poisoning, or water pollution.



Geographic Techniques



Specialists in geographic techniques study the ways in which geographic processes can be analyzed and represented using different methods and technologies. Mapmaking, or cartography, is perhaps the most basic of these. Cartography has been instrumental to geography throughout the ages.



As early as 1500 BCE, Polynesian navigators in the Pacific Ocean used complex maps made of tiny sticks and shells that represented islands and ocean currents they would encounter on their voyages. Today, satellites placed into orbit by the U.S. Department of Defense communicate with receivers on the ground called global positioning system (GPS) units to instantly identify exact locations on Earth.



Today, almost the entire surface of Earth has been mapped with remarkable accuracy, and much of this information is available instantly on the internet. One of the most remarkable of these websites is Google Earth, which “lets you fly anywhere on Earth to view satellite imagery, maps, terrain, 3D buildings, from galaxies in outer space to the canyons of the ocean.” In essence, anyone can be a virtual Christopher Columbus from the comfort of home.



Technological developments during the past 100 years have given rise to a number of other specialties for scientists studying geographic techniques. The airplane made it possible to photograph land from above. Now, there are many satellites and other above-Earth vehicles that help geographers figure out what the surface of the planet looks like and how it is changing.





Geographers looking at what above-Earth cameras and sensors reveal are specialists in remote sensing. Pictures taken from space can be used to make maps, monitor ice melt, assess flood damage, track oil spills, predict weather, or perform endless other functions. For example, by comparing satellite photos taken from 1955 to 2007, scientists from the U.S. Geological Survey (USGS) discovered that the rate of coastal erosion along Alaska’s Beaufort Sea had doubled. Every year from 2002 to 2007, about 45 feet per year of coast, mostly icy permafrost, vanished into the sea.



Computerized systems that allow for precise calculations of how things are distributed and relate to one another have made the study of geographic information systems (GIS) an increasingly important specialty within geography. Geographic information systems are powerful databases that collect all types of information (maps, reports, statistics, satellite images, surveys, demographic data, and more) and link each piece of data to a geographic reference point, such as geographic coordinates. This data, called geospatial information, can be stored, analyzed, modeled, and manipulated in ways not possible before GIS computer technology existed.



The popularity and importance of GIS has given rise to a new science known as geographic information science (GISci). Geographic information scientists study patterns in nature as well as human development. They might study natural hazards, such as a fire that struck Los Angeles, California, in 2008. A map posted on the internet showed the real-time spread of the fire, along with information to help people make decisions about how to evacuate quickly. GIS can also illustrate human struggles from a geographic perspective, such as the interactive online map published by the New York Times in May 2009 that showed building foreclosure rates in various regions around the New York City area.



The enormous possibilities for producing computerized maps and diagrams that can help us understand environmental and social problems have made geographic visualization an increasingly important specialty within geography. This geospatial information is in high demand by just about every institution, from government agencies monitoring water quality to entrepreneurs deciding where to locate new businesses.



Regional Geography



Regional geographers take a somewhat different approach to specialization, directing their attention to the general geographic characteristics of a region. A regional geographer might specialize in African studies, observing and documenting the people, nations, rivers, mountains, deserts, weather, trade, and other attributes of the continent. There are different ways you can define a region. You can look at climate zones, cultural regions, or political regions. Often regional geographers have a physical or human geography specialty as well as a regional specialty.



Regional geographers may also study smaller regions, such as urban areas. A regional geographer may be interested in the way a city like Shanghai, China, is growing. They would study transportation, migration, housing, and language use, as well as the human impact on elements of the natural environment, such as the Huangpu River.



Whether geography is thought of as a discipline or as a basic feature of our world, developing an understanding of the subject is important. Some grasp of geography is essential as people seek to make sense of the world and understand their place in it. Thinking geographically helps people to be aware of the connections among and between places and to see how important events are shaped by where they take place. Finally, knowing something about geography enriches people’s lives—promoting curiosity about other people and places and an appreciation of the patterns, environments, and peoples that make up the endlessly fascinating, varied planet on which we live."	https://www.nationalgeographic.org/encyclopedia/geography/	"Geography is the study of places and the relationships between people and their environments. Geographers explore both the physical properties of Earth’s surface and the human societies spread across it. They also examine how human culture interacts with the natural environment, and the way that locations and places can have an impact on people. Geography seeks to understand where things are found, why they are there, and how they develop and change over time.
Ancient Geographers
The term ""geography"" comes to us from the ancient Greeks, who needed a word to describe the writings and maps that were helping them make sense of the world in which they lived. In Greek, geo means “earth” and -graphy means “to write.” Using geography, Greeks developed an understanding of where their homeland was located in relation to other places, what their own and other places were like, and how people and environments were distributed. These concerns have been central to geography ever since.
Of course, the Greeks were not the only people interested in geography. Throughout human history, most societies have sought to understand something about their place in the world, and the people and environments around them.
Indeed, mapmaking probably came even before writing in many places. But ancient Greek geographers were particularly influential. They developed very detailed maps of areas in and around Greece, including parts of Europe, Africa, and Asia. More importantly, they also raised questions about how and why different human and natural patterns came into being on Earth’s surface, and why variations existed from place to place. The effort to answer these questions about patterns and distribution led them to figure out that the world was round, to calculate Earth’s circumference, and to develop explanations of everything from the seasonal flooding of the Nile River to differences in population densities from place to place.
During the Middle Ages, geography ceased to be a major academic pursuit in Europe. Advances in geography were chiefly made by scientists of the Muslim world, based around the Arabian Peninsula and North Africa. Geographers of this Islamic Golden Age created the world’s first rectangular map based on a grid, a map system that is still familiar today. Islamic scholars also applied their study of people and places to agriculture, determining which crops and livestock were most suited to specific habitats or environments.
In addition to the advances in the Middle East, the Chinese empire in Asia also contributed immensely to geography. Until about 1500, China was the most prosperous civilization on Earth. The Chinese were scientifically advanced, especially in the field of astronomy. Around 1000, they also achieved one of the most important developments in the history of geography: They were the first to use the compass for navigational purposes. In the early 1400s, the explorer Cheng Ho embarked on seven voyages to the lands bordering the China Sea and the Indian Ocean, establishing China’s dominance throughout Southeast Asia.
Age of Discovery
Through the 13th-century travels of the Italian explorer Marco Polo, Europeans learned about the riches of China. Curiosity was awakened; a desire to trade with wealthy Asian cultures motivated a renewed interest in exploring the world. The period of time between the 15th and 17th centuries is known in the West as the Age of Exploration or the Age of Discovery.
With the dawn of the Age of Discovery, the study of geography regained popularity in Europe. The invention of the printing press in the mid-1400s helped spread geographic knowledge by making maps and charts widely available. Improvements in shipbuilding and navigation facilitated more exploring, greatly improving the accuracy of maps and geographic information.
Greater geographic understanding allowed European powers to extend their global influence. During the Age of Discovery, European nations established colonies around the world. Improved transportation, communication, and navigational technology allowed countries such as the United Kingdom to successfully govern colonies as far away as the Americas, Asia, Australia, and Africa.
Geography was not just a subject that made colonialism possible, however. It also helped people understand the planet on which they lived. Not surprisingly, geography became an important focus of study in schools and universities.
Geography also became an important part of other academic disciplines, such as chemistry, economics, and philosophy. In fact, every academic subject has some geographic connection. Chemists study where certain chemical elements, such as gold or silver, can be found. Economists examine which nations trade with other nations, and what resources are exchanged. Philosophers analyze the responsibility people have to take care of the Earth.
Emergence of Modern Geography
Some people have trouble understanding the complete scope of the discipline of geography because, unlike most other disciplines, geography is not defined by one particular topic. Instead, geography is concerned with many different topics—people, culture, politics, settlements, plants, landforms, and much more.
What distinguishes geography is that it approaches the study of diverse topics in a particular way (that is, from a particular perspective). Geography asks spatial questions—how and why things are distributed or arranged in particular ways on Earth’s surface. It looks at these different distributions and arrangements at many different scales. It also asks questions about how the interaction of different human and natural activities on Earth’s surface shape the characteristics of the world in which we live.
Geography seeks to understand where things are found and why they are present in those places; how things that are located in the same or distant places influence one another over time; and why places and the people who live in them develop and change in particular ways. Raising these questions is at the heart of the “geographic perspective.”
Exploration has long been an important part of geography. But exploration no longer simply means going to places that have not been visited before. It means documenting and trying to explain the variations that exist across the surface of Earth, as well as figuring out what those variations mean for the future.
The age-old practice of mapping still plays an important role in this type of exploration, but exploration can also be done by using images from satellites or gathering information from interviews. Discoveries can come by using computers to map and analyze the relationship among things in geographic space, or from piecing together the multiple forces, near and far, that shape the way individual places develop.
Applying a geographic perspective demonstrates geography’s concern not just with where things are, but with “the why of where”—a short, but useful definition of geography’s central focus.
The insights that have come from geographic research show the importance of asking “the why of where” questions. Geographic studies comparing physical characteristics of continents on either side of the Atlantic Ocean, for instance, gave rise to the idea that Earth’s surface is comprised of large, slowly moving plates—plate tectonics.
Studies of the geographic distribution of human settlements have shown how economic forces and modes of transport influence the location of towns and cities. For example, geographic analysis has pointed to the role of the U.S. Interstate Highway System and the rapid growth of car ownership in creating a boom in U.S. suburban growth after World War II. The geographic perspective helped show where Americans were moving, why they were moving there, and how their new living places affected their lives, their relationships with others, and their interactions with the environment.
Geographic analyses of the spread of diseases have pointed to the conditions that allow particular diseases to develop and spread. Dr. John Snow’s cholera map stands out as a classic example. When cholera broke out in London, England, in 1854, Snow represented the deaths per household on a street map. Using the map, he was able to trace the source of the outbreak to a water pump on the corner of Broad Street and Cambridge Street. The geographic perspective helped identify the source of the problem (the water from a specific pump) and allowed people to avoid the disease (avoiding water from that pump).
Investigations of the geographic impact of human activities have advanced understanding of the role of humans in transforming the surface of Earth, exposing the spatial extent of threats such as water pollution by manmade waste. For example, geographic study has shown that a large mass of tiny pieces of plastic currently floating in the Pacific Ocean is approximately the size of Texas. Satellite images and other geographic technology identified the so-called “Great Pacific Garbage Patch.”
These examples of different uses of the geographic perspective help explain why geographic study and research is important as we confront many 21st century challenges, including environmental pollution, poverty, hunger, and ethnic or political conflict.
Because the study of geography is so broad, the discipline is typically divided into specialties. At the broadest level, geography is divided into physical geography, human geography, geographic techniques, and regional geography.
Physical Geography
The natural environment is the primary concern of physical geographers, although many physical geographers also look at how humans have altered natural systems. Physical geographers study Earth’s seasons, climate, atmosphere, soil, streams, landforms, and oceans. Some disciplines within physical geography include geomorphology, glaciology, pedology, hydrology, climatology, biogeography, and oceanography.
Geomorphology is the study of landforms and the processes that shape them. Geomorphologists investigate the nature and impact of wind, ice, rivers, erosion, earthquakes, volcanoes, living things, and other forces that shape and change the surface of the Earth.
Glaciologists focus on the Earth’s ice fields and their impact on the planet’s climate. Glaciologists document the properties and distribution of glaciers and icebergs. Data collected by glaciologists has demonstrated the retreat of Arctic and Antarctic ice in the past century.
Pedologists study soil and how it is created, changed, and classified. Soil studies are used by a variety of professions, from farmers analyzing field fertility to engineers investigating the suitability of different areas for building heavy structures.
Hydrology is the study of Earth’s water: its properties, distribution, and effects. Hydrologists are especially concerned with the movement of water as it cycles from the ocean to the atmosphere, then back to Earth’s surface. Hydrologists study the water cycle through rainfall into streams, lakes, the soil, and underground aquifers. Hydrologists provide insights that are critical to building or removing dams, designing irrigation systems, monitoring water quality, tracking drought conditions, and predicting flood risk.
Climatologists study Earth’s climate system and its impact on Earth’s surface. For example, climatologists make predictions about El Nino, a cyclical weather phenomenon of warm surface temperatures in the Pacific Ocean. They analyze the dramatic worldwide climate changes caused by El Nino, such as flooding in Peru, drought in Australia, and, in the United States, the oddities of heavy Texas rains or an unseasonably warm Minnesota winter.
Biogeographers study the impact of the environment on the distribution of plants and animals. For example, a biogeographer might document all the places in the world inhabited by a certain spider species, and what those places have in common.
Oceanography, a related discipline of physical geography, focuses on the creatures and environments of the world’s oceans. Observation of ocean tides and currents constituted some of the first oceanographic investigations. For example, 18th-century mariners figured out the geography of the Gulf Stream, a massive current flowing like a river through the Atlantic Ocean. The discovery and tracking of the Gulf Stream helped communications and travel between Europe and the Americas.
Today, oceanographers conduct research on the impacts of water pollution, track tsunamis, design offshore oil rigs, investigate underwater eruptions of lava, and study all types of marine organisms from toxic algae to friendly dolphins.
Human Geography
Human geography is concerned with the distribution and networks of people and cultures on Earth’s surface. A human geographer might investigate the local, regional, and global impact of rising economic powers China and India, which represent 37 percent of the world’s people. They also might look at how consumers in China and India adjust to new technology and markets, and how markets respond to such a huge consumer base.
Human geographers also study how people use and alter their environments. When, for example, people allow their animals to overgraze a region, the soil erodes and grassland is transformed into desert. The impact of overgrazing on the landscape as well as agricultural production is an area of study for human geographers.
Finally, human geographers study how political, social, and economic systems are organized across geographical space. These include governments, religious organizations, and trade partnerships. The boundaries of these groups constantly change.
The main divisions within human geography reflect a concern with different types of human activities or ways of living. Some examples of human geography include urban geography, economic geography, cultural geography, political geography, social geography, and population geography. Human geographers who study geographic patterns and processes in past times are part of the subdiscipline of historical geography. Those who study how people understand maps and geographic space belong to a subdiscipline known as behavioral geography.
Many human geographers interested in the relationship between humans and the environment work in the subdisciplines of cultural geography and political geography.
Cultural geographers study how the natural environment influences the development of human culture, such as how the climate affects the agricultural practices of a region. Political geographers study the impact of political circumstances on interactions between people and their environment, as well as environmental conflicts, such as disputes over water rights.
Some human geographers focus on the connection between human health and geography. For example, health geographers create maps that track the location and spread of specific diseases. They analyze the geographic disparities of health-care access. They are very interested in the impact of the environment on human health, especially the effects of environmental hazards such as radiation, lead poisoning, or water pollution.
Geographic Techniques
Specialists in geographic techniques study the ways in which geographic processes can be analyzed and represented using different methods and technologies. Mapmaking, or cartography, is perhaps the most basic of these. Cartography has been instrumental to geography throughout the ages.
As early as 1500 BCE, Polynesian navigators in the Pacific Ocean used complex maps made of tiny sticks and shells that represented islands and ocean currents they would encounter on their voyages. Today, satellites placed into orbit by the U.S. Department of Defense communicate with receivers on the ground called global positioning system (GPS) units to instantly identify exact locations on Earth.
Today, almost the entire surface of Earth has been mapped with remarkable accuracy, and much of this information is available instantly on the internet. One of the most remarkable of these websites is Google Earth, which “lets you fly anywhere on Earth to view satellite imagery, maps, terrain, 3D buildings, from galaxies in outer space to the canyons of the ocean.” In essence, anyone can be a virtual Christopher Columbus from the comfort of home.
Technological developments during the past 100 years have given rise to a number of other specialties for scientists studying geographic techniques. The airplane made it possible to photograph land from above. Now, there are many satellites and other above-Earth vehicles that help geographers figure out what the surface of the planet looks like and how it is changing.
Geographers looking at what above-Earth cameras and sensors reveal are specialists in remote sensing. Pictures taken from space can be used to make maps, monitor ice melt, assess flood damage, track oil spills, predict weather, or perform endless other functions. For example, by comparing satellite photos taken from 1955 to 2007, scientists from the U.S. Geological Survey (USGS) discovered that the rate of coastal erosion along Alaska’s Beaufort Sea had doubled. Every year from 2002 to 2007, about 45 feet per year of coast, mostly icy permafrost, vanished into the sea.
Computerized systems that allow for precise calculations of how things are distributed and relate to one another have made the study of geographic information systems (GIS) an increasingly important specialty within geography. Geographic information systems are powerful databases that collect all types of information (maps, reports, statistics, satellite images, surveys, demographic data, and more) and link each piece of data to a geographic reference point, such as geographic coordinates. This data, called geospatial information, can be stored, analyzed, modeled, and manipulated in ways not possible before GIS computer technology existed.
The popularity and importance of GIS has given rise to a new science known as geographic information science (GISci). Geographic information scientists study patterns in nature as well as human development. They might study natural hazards, such as a fire that struck Los Angeles, California, in 2008. A map posted on the internet showed the real-time spread of the fire, along with information to help people make decisions about how to evacuate quickly. GIS can also illustrate human struggles from a geographic perspective, such as the interactive online map published by the New York Times in May 2009 that showed building foreclosure rates in various regions around the New York City area.
The enormous possibilities for producing computerized maps and diagrams that can help us understand environmental and social problems have made geographic visualization an increasingly important specialty within geography. This geospatial information is in high demand by just about every institution, from government agencies monitoring water quality to entrepreneurs deciding where to locate new businesses.
Regional Geography
Regional geographers take a somewhat different approach to specialization, directing their attention to the general geographic characteristics of a region. A regional geographer might specialize in African studies, observing and documenting the people, nations, rivers, mountains, deserts, weather, trade, and other attributes of the continent. There are different ways you can define a region. You can look at climate zones, cultural regions, or political regions. Often regional geographers have a physical or human geography specialty as well as a regional specialty.
Regional geographers may also study smaller regions, such as urban areas. A regional geographer may be interested in the way a city like Shanghai, China, is growing. They would study transportation, migration, housing, and language use, as well as the human impact on elements of the natural environment, such as the Huangpu River.
Whether geography is thought of as a discipline or as a basic feature of our world, developing an understanding of the subject is important. Some grasp of geography is essential as people seek to make sense of the world and understand their place in it. Thinking geographically helps people to be aware of the connections among and between places and to see how important events are shaped by where they take place. Finally, knowing something about geography enriches people’s lives—promoting curiosity about other people and places and an appreciation of the patterns, environments, and peoples that make up the endlessly fascinating, varied planet on which we live."	20590
geography	[]	2021-05-21 00:00:00	"A global survey of fossil pollen has discovered that the planet's vegetation is changing at least as quickly today as it did when the last ice sheets retreated around 10,000 years ago. Beginning some 3,000-to-4,000 years ago, Earth's plant communities began changing at an accelerating pace. Today, this pace rivals or exceeds the rapid turnover that took place as plants raced to colonize formerly frozen landscapes and adapt to a global climate that warmed by about 10 degrees Fahrenheit.

The research, published May 20 in Science, suggests that humanity's dominant influence on ecosystems that is so visible today has its origin in the earliest civilizations and the rise of agriculture, deforestation and other ways our species has influenced the landscape.

This work also suggests ecosystem rates of change will continue to accelerate over the coming decades, as modern climate change further adds to this long history of flux. And by showing that recent biodiversity trends are the start of a longer-term acceleration in ecosystem transformations, the new study provides context for other recent reports that global biodiversity changes have accelerated over the last century.

An international collaboration of scientists led the new analysis, which was powered by an innovative database for paleoecological data. The Neotoma Paleoecology Database is an open-access tool that gathers and curates data on past ecosystems from hundreds of scientists. Neotoma is chaired by University of Wisconsin-Madison professor of geography Jack Williams, who helped lead the new research.

The study authors analyzed more than 1,100 fossil pollen records from Neotoma, spanning all continents except Antarctica, to understand how plant ecosystems have changed since the end of the last ice age about 18,000 years ago, and how quickly this change occurred.

""At the end of the ice age, we had complete, biome-scale ecosystem conversions,"" says Williams, who also curates Neotoma's North American pollen database. ""And over the past few thousand years, we're at that scale again. It has changed that much. And these changes began earlier than we might have thought before.""

Fossil pollen provides an extremely sensitive measure of past plant communities. As pollen from surrounding plants falls into lakes, it settles in layers from oldest at the bottom to newest at the top. Scientists can extract sediment cores and conduct the painstaking work of identifying pollen and reconstructing plant ecosystems over thousands of years.

advertisement

Yet each sediment core only provides information about one place on Earth, so true global-scale analyses of past vegetation change require the amassing and curating of many such records. Neotoma has gathered thousands of such datapoints to help scientists uncover global trends. Researchers from the University of Bergen in Norway, UW-Madison, and Neotoma data stewards from around the world collaborated to perform the new analysis.

Using these pollen records, the team applied new statistical methods to better analyze how quickly plant communities have changed in the last 18,000 years.

They discovered that the rate of change initially peaked between 8,000 and 16,000 years ago, depending on the continent. These continental differences are likely caused by different timing and patterns of climate change linked to retreating glaciers, rising carbon dioxide concentrations in the atmosphere, changes in Earth's orbit, and changes in ocean and atmospheric circulation.

Ecosystems then stabilized until about 4,000 years ago. Then, the rate of change began a meteoric rise that continues today, when most plant ecosystems are changing at least as fast as they did at the peak of ice-age-induced flux.

""That was a surprising finding, because over the last few thousand years, not a whole lot was happening climatically, but the rates of ecosystem change were as big or bigger than anything we've seen from the last ice age to the present,"" says Williams.

advertisement

Although this analysis of pollen records was focused on detecting ecosystem changes, rather than formally determining causes, these recent ecosystem changes correlate with the beginning of intensive agriculture and the earliest cities and civilizations around the world.

Williams says that one intriguing feature of these analyses is that the early rise is so early worldwide, even though each continent had different trajectories of land use, agricultural development and urbanization.

Scientists have coined the term the Anthropocene to describe the modern geological period, when humans are the dominant influence on the world. ""And one of the questions has been, when did the Anthropocene begin?"" says Williams. ""This work suggests that 3,000 to 4,000 years ago, humans were already having an enormous impact on the world (and) that continues today.""

A sobering implication from this work, say the scientists, is that in the past, the periods of ecosystem transformations driven by climate change and those driven by land use were largely separate. But now, intensified land use continues, and the world is warming at an increasing rate due to the accumulation of greenhouse gases. As plant communities respond to the combination of direct human impacts and human-induced climate change, future rates of ecosystem transformation may break new records yet again.

This work was supported in part by the National Science Foundation (grants 1550707, 1550805, and 1948926)."	https://www.sciencedaily.com/releases/2021/05/210520145347.htm	"A global survey of fossil pollen has discovered that the planet's vegetation is changing at least as quickly today as it did when the last ice sheets retreated around 10,000 years ago. Beginning some 3,000-to-4,000 years ago, Earth's plant communities began changing at an accelerating pace. Today, this pace rivals or exceeds the rapid turnover that took place as plants raced to colonize formerly frozen landscapes and adapt to a global climate that warmed by about 10 degrees Fahrenheit.
The research, published May 20 in Science, suggests that humanity's dominant influence on ecosystems that is so visible today has its origin in the earliest civilizations and the rise of agriculture, deforestation and other ways our species has influenced the landscape.
This work also suggests ecosystem rates of change will continue to accelerate over the coming decades, as modern climate change further adds to this long history of flux. And by showing that recent biodiversity trends are the start of a longer-term acceleration in ecosystem transformations, the new study provides context for other recent reports that global biodiversity changes have accelerated over the last century.
An international collaboration of scientists led the new analysis, which was powered by an innovative database for paleoecological data. The Neotoma Paleoecology Database is an open-access tool that gathers and curates data on past ecosystems from hundreds of scientists. Neotoma is chaired by University of Wisconsin-Madison professor of geography Jack Williams, who helped lead the new research.
The study authors analyzed more than 1,100 fossil pollen records from Neotoma, spanning all continents except Antarctica, to understand how plant ecosystems have changed since the end of the last ice age about 18,000 years ago, and how quickly this change occurred.
""At the end of the ice age, we had complete, biome-scale ecosystem conversions,"" says Williams, who also curates Neotoma's North American pollen database. ""And over the past few thousand years, we're at that scale again. It has changed that much. And these changes began earlier than we might have thought before.""
Fossil pollen provides an extremely sensitive measure of past plant communities. As pollen from surrounding plants falls into lakes, it settles in layers from oldest at the bottom to newest at the top. Scientists can extract sediment cores and conduct the painstaking work of identifying pollen and reconstructing plant ecosystems over thousands of years.
advertisement
Yet each sediment core only provides information about one place on Earth, so true global-scale analyses of past vegetation change require the amassing and curating of many such records. Neotoma has gathered thousands of such datapoints to help scientists uncover global trends. Researchers from the University of Bergen in Norway, UW-Madison, and Neotoma data stewards from around the world collaborated to perform the new analysis.
Using these pollen records, the team applied new statistical methods to better analyze how quickly plant communities have changed in the last 18,000 years.
They discovered that the rate of change initially peaked between 8,000 and 16,000 years ago, depending on the continent. These continental differences are likely caused by different timing and patterns of climate change linked to retreating glaciers, rising carbon dioxide concentrations in the atmosphere, changes in Earth's orbit, and changes in ocean and atmospheric circulation.
Ecosystems then stabilized until about 4,000 years ago. Then, the rate of change began a meteoric rise that continues today, when most plant ecosystems are changing at least as fast as they did at the peak of ice-age-induced flux.
""That was a surprising finding, because over the last few thousand years, not a whole lot was happening climatically, but the rates of ecosystem change were as big or bigger than anything we've seen from the last ice age to the present,"" says Williams.
advertisement
Although this analysis of pollen records was focused on detecting ecosystem changes, rather than formally determining causes, these recent ecosystem changes correlate with the beginning of intensive agriculture and the earliest cities and civilizations around the world.
Williams says that one intriguing feature of these analyses is that the early rise is so early worldwide, even though each continent had different trajectories of land use, agricultural development and urbanization.
Scientists have coined the term the Anthropocene to describe the modern geological period, when humans are the dominant influence on the world. ""And one of the questions has been, when did the Anthropocene begin?"" says Williams. ""This work suggests that 3,000 to 4,000 years ago, humans were already having an enormous impact on the world (and) that continues today.""
A sobering implication from this work, say the scientists, is that in the past, the periods of ecosystem transformations driven by climate change and those driven by land use were largely separate. But now, intensified land use continues, and the world is warming at an increasing rate due to the accumulation of greenhouse gases. As plant communities respond to the combination of direct human impacts and human-induced climate change, future rates of ecosystem transformation may break new records yet again.
This work was supported in part by the National Science Foundation (grants 1550707, 1550805, and 1948926)."	5475
geography	[]	2021-05-21 00:00:00	"Greenhouse gases and aerosol pollution emitted by human activities are responsible for increases in the frequency, intensity and duration of droughts around the world, according to researchers at the University of California, Irvine.

In a study published recently in Nature Communications, scientists in UCI's Department of Civil & Environmental Engineering showed that over the past century, the likelihood of stronger and more long-lasting dry spells grew in the Americas, the Mediterranean, western and southern Africa and eastern Asia.

""There has always been natural variability in drought events around the world, but our research shows the clear human influence on drying, specifically from anthropogenic aerosols, carbon dioxide and other greenhouse gases,"" said lead author Felicia Chiang, who conducted the project as a UCI graduate student in civil & environmental engineering.

Chiang, who earned her Ph.D. in 2020 and is now a postdoctoral scholar at NASA's Goddard Institute for Space Studies in New York, said that her team's research demonstrated significant shifts in drought characteristics -- frequency, duration and intensity -- due to human influence, or what they call ""anthropogenic forcing.""

The researchers used the recently released Coupled Model Intercomparison Project Phase 6 platform to run climate simulations showing how the length and strength of droughts changes under various scenarios including ""natural-only"" and with the addition of greenhouse gas and aerosol emissions.

The modeling experiments under natural-only conditions did not show regional changes in drought characteristics from the late 19th to late 20th centuries, according to the study. But when the team accounted for anthropogenic greenhouse gas and aerosol contributions, statistically significant increases occurred in drought hotspots in southern Europe, Central and South America, western and southern Africa and eastern Asia.

The team found that in examining the anthropogenic forcings separately, greenhouse gases had a bigger impact in the Mediterranean, Central America, the Amazon and southern Africa, while anthropogenic aerosols played a larger role in Northern Hemisphere monsoonal and sub-arctic regions. Chiang said human-emitted aerosols are essentially particulate matter that are small enough to be suspended in the air. They can come from power plants, car exhaust and biomass burning (fires to clear land or to burn farm waste).

""Knowing where, how and why droughts have been worsening around the world is important, because these events directly and indirectly impact everything from wildlife habitats to agricultural production to our economy,"" said co-author Amir AghaKouchak, UCI professor of civil & environmental engineering and Earth system science. ""Lengthy dry spells can even hamper the energy sector through disruptions to solar thermal, geothermal and hydropower generation.""

Co-author Omid Mazdiyasni, who earned a Ph.D. in civil & environmental engineering at UCI in 2020 and is now a project scientist with the Los Angeles County Department of Public Works, said, ""To make matters worse, droughts can be accompanied by heat waves, and high heat and low moisture can increase wildfire risk, which is already significant in the western United States.""

Mazdiyasni said that while the research paints a gloomy picture of the unwanted impact of humans on the global environment, it points to a potential solution.

""If droughts over the past century have been worsened by human-sourced pollution, then there is a strong possibility that the problem can be mitigated by limiting those emissions,"" he said."	https://www.sciencedaily.com/releases/2021/05/210517124927.htm	"Greenhouse gases and aerosol pollution emitted by human activities are responsible for increases in the frequency, intensity and duration of droughts around the world, according to researchers at the University of California, Irvine.
In a study published recently in Nature Communications, scientists in UCI's Department of Civil & Environmental Engineering showed that over the past century, the likelihood of stronger and more long-lasting dry spells grew in the Americas, the Mediterranean, western and southern Africa and eastern Asia.
""There has always been natural variability in drought events around the world, but our research shows the clear human influence on drying, specifically from anthropogenic aerosols, carbon dioxide and other greenhouse gases,"" said lead author Felicia Chiang, who conducted the project as a UCI graduate student in civil & environmental engineering.
Chiang, who earned her Ph.D. in 2020 and is now a postdoctoral scholar at NASA's Goddard Institute for Space Studies in New York, said that her team's research demonstrated significant shifts in drought characteristics -- frequency, duration and intensity -- due to human influence, or what they call ""anthropogenic forcing.""
The researchers used the recently released Coupled Model Intercomparison Project Phase 6 platform to run climate simulations showing how the length and strength of droughts changes under various scenarios including ""natural-only"" and with the addition of greenhouse gas and aerosol emissions.
The modeling experiments under natural-only conditions did not show regional changes in drought characteristics from the late 19th to late 20th centuries, according to the study. But when the team accounted for anthropogenic greenhouse gas and aerosol contributions, statistically significant increases occurred in drought hotspots in southern Europe, Central and South America, western and southern Africa and eastern Asia.
The team found that in examining the anthropogenic forcings separately, greenhouse gases had a bigger impact in the Mediterranean, Central America, the Amazon and southern Africa, while anthropogenic aerosols played a larger role in Northern Hemisphere monsoonal and sub-arctic regions. Chiang said human-emitted aerosols are essentially particulate matter that are small enough to be suspended in the air. They can come from power plants, car exhaust and biomass burning (fires to clear land or to burn farm waste).
""Knowing where, how and why droughts have been worsening around the world is important, because these events directly and indirectly impact everything from wildlife habitats to agricultural production to our economy,"" said co-author Amir AghaKouchak, UCI professor of civil & environmental engineering and Earth system science. ""Lengthy dry spells can even hamper the energy sector through disruptions to solar thermal, geothermal and hydropower generation.""
Co-author Omid Mazdiyasni, who earned a Ph.D. in civil & environmental engineering at UCI in 2020 and is now a project scientist with the Los Angeles County Department of Public Works, said, ""To make matters worse, droughts can be accompanied by heat waves, and high heat and low moisture can increase wildfire risk, which is already significant in the western United States.""
Mazdiyasni said that while the research paints a gloomy picture of the unwanted impact of humans on the global environment, it points to a potential solution.
""If droughts over the past century have been worsened by human-sourced pollution, then there is a strong possibility that the problem can be mitigated by limiting those emissions,"" he said."	3633
geography	[]		"The study of the distribution of species and ecosystems in geographic space and through geological time

Biogeography is the study of the distribution of species and ecosystems in geographic space and through geological time. Organisms and biological communities often vary in a regular fashion along geographic gradients of latitude, elevation, isolation and habitat area.[1] Phytogeography is the branch of biogeography that studies the distribution of plants. Zoogeography is the branch that studies distribution of animals. Mycogeography is the branch that studies distribution of fungi, such as mushrooms.

Knowledge of spatial variation in the numbers and types of organisms is as vital to us today as it was to our early human ancestors, as we adapt to heterogeneous but geographically predictable environments. Biogeography is an integrative field of inquiry that unites concepts and information from ecology, evolutionary biology, taxonomy, geology, physical geography, palaeontology, and climatology.[2][3]

Modern biogeographic research combines information and ideas from many fields, from the physiological and ecological constraints on organismal dispersal to geological and climatological phenomena operating at global spatial scales and evolutionary time frames.

The short-term interactions within a habitat and species of organisms describe the ecological application of biogeography. Historical biogeography describes the long-term, evolutionary periods of time for broader classifications of organisms.[4] Early scientists, beginning with Carl Linnaeus, contributed to the development of biogeography as a science.

The scientific theory of biogeography grows out of the work of Alexander von Humboldt (1769–1859),[5] Francisco Jose de Caldas (1768-1816),[6] Hewett Cottrell Watson (1804–1881),[7] Alphonse de Candolle (1806–1893),[8] Alfred Russel Wallace (1823–1913),[9] Philip Lutley Sclater (1829–1913) and other biologists and explorers.[10]

Introduction [ edit ]

The patterns of species distribution across geographical areas can usually be explained through a combination of historical factors such as: speciation, extinction, continental drift, and glaciation. Through observing the geographic distribution of species, we can see associated variations in sea level, river routes, habitat, and river capture. Additionally, this science considers the geographic constraints of landmass areas and isolation, as well as the available ecosystem energy supplies.

Over periods of ecological changes, biogeography includes the study of plant and animal species in: their past and/or present living refugium habitat; their interim living sites; and/or their survival locales.[11] As writer David Quammen put it, ""...biogeography does more than ask Which species? and Where. It also asks Why? and, what is sometimes more crucial, Why not?.""[12]

Modern biogeography often employs the use of Geographic Information Systems (GIS), to understand the factors affecting organism distribution, and to predict future trends in organism distribution.[13] Often mathematical models and GIS are employed to solve ecological problems that have a spatial aspect to them.[14]

Biogeography is most keenly observed on the world's islands. These habitats are often much more manageable areas of study because they are more condensed than larger ecosystems on the mainland.[15] Islands are also ideal locations because they allow scientists to look at habitats that new invasive species have only recently colonized and can observe how they disperse throughout the island and change it. They can then apply their understanding to similar but more complex mainland habitats. Islands are very diverse in their biomes, ranging from the tropical to arctic climates. This diversity in habitat allows for a wide range of species study in different parts of the world.

One scientist who recognized the importance of these geographic locations was Charles Darwin, who remarked in his journal ""The Zoology of Archipelagoes will be well worth examination"".[15] Two chapters in On the Origin of Species were devoted to geographical distribution.

History [ edit ]

18th century [ edit ]

The first discoveries that contributed to the development of biogeography as a science began in the mid-18th century, as Europeans explored the world and described the biodiversity of life. During the 18th century most views on the world were shaped around religion and for many natural theologists, the bible. Carl Linnaeus, in the mid-18th century, initiated the ways to classify organisms through his exploration of undiscovered territories. When he noticed that species were not as perpetual as he believed, he developed the Mountain Explanation to explain the distribution of biodiversity; when Noah's ark landed on Mount Ararat and the waters receded, the animals dispersed throughout different elevations on the mountain. This showed different species in different climates proving species were not constant.[4] Linnaeus' findings set a basis for ecological biogeography. Through his strong beliefs in Christianity, he was inspired to classify the living world, which then gave way to additional accounts of secular views on geographical distribution.[10] He argued that the structure of an animal was very closely related to its physical surroundings. This was important to a George Louis Buffon's rival theory of distribution.[10]

The Theory of Island Biogeography and helped to start much of the research that has been done on this topic since the work of Watson and Wallace almost a century before Edward O. Wilson , a prominent biologist and conservationist, coauthoredand helped to start much of the research that has been done on this topic since the work of Watson and Wallace almost a century before

Closely after Linnaeus, Georges-Louis Leclerc, Comte de Buffon observed shifts in climate and how species spread across the globe as a result. He was the first to see different groups of organisms in different regions of the world. Buffon saw similarities between some regions which led him to believe that at one point continents were connected and then water separated them and caused differences in species. His hypotheses were described in his work, the 36 volume Histoire Naturelle, générale et particulière, in which he argued that varying geographical regions would have different forms of life. This was inspired by his observations comparing the Old and New World, as he determined distinct variations of species from the two regions. Buffon believed there was a single species creation event, and that different regions of the world were homes for varying species, which is an alternate view than that of Linnaeus. Buffon's law eventually became a principle of biogeography by explaining how similar environments were habitats for comparable types of organisms.[10] Buffon also studied fossils which led him to believe that the earth was over tens of thousands of years old, and that humans had not lived there long in comparison to the age of the earth.[4]

19th century [ edit ]

Following the period of exploration came the Age of Enlightenment in Europe, which attempted to explain the patterns of biodiversity observed by Buffon and Linnaeus. At the birth of the 19th century, Alexander von Humboldt, known as the ""founder of plant geography"",[4] developed the concept of physique generale to demonstrate the unity of science and how species fit together. As one of the first to contribute empirical data to the science of biogeography through his travel as an explorer, he observed differences in climate and vegetation. The earth was divided into regions which he defined as tropical, temperate, and arctic and within these regions there were similar forms of vegetation.[4] This ultimately enabled him to create the isotherm, which allowed scientists to see patterns of life within different climates.[4] He contributed his observations to findings of botanical geography by previous scientists, and sketched this description of both the biotic and abiotic features of the earth in his book, Cosmos.[10]

Augustin de Candolle contributed to the field of biogeography as he observed species competition and the several differences that influenced the discovery of the diversity of life. He was a Swiss botanist and created the first Laws of Botanical Nomenclature in his work, Prodromus.[16] He discussed plant distribution and his theories eventually had a great impact on Charles Darwin, who was inspired to consider species adaptations and evolution after learning about botanical geography. De Candolle was the first to describe the differences between the small-scale and large-scale distribution patterns of organisms around the globe.[10]

Several additional scientists contributed new theories to further develop the concept of biogeography. Charles Lyell developed the Theory of Uniformitarianism after studying fossils. This theory explained how the world was not created by one sole catastrophic event, but instead from numerous creation events and locations.[17] Uniformitarianism also introduced the idea that the Earth was actually significantly older than was previously accepted. Using this knowledge, Lyell concluded that it was possible for species to go extinct.[18] Since he noted that earth's climate changes, he realized that species distribution must also change accordingly. Lyell argued that climate changes complemented vegetation changes, thus connecting the environmental surroundings to varying species. This largely influenced Charles Darwin in his development of the theory of evolution.[10]

Charles Darwin was a natural theologist who studied around the world, and most importantly in the Galapagos Islands. Darwin introduced the idea of natural selection, as he theorized against previously accepted ideas that species were static or unchanging. His contributions to biogeography and the theory of evolution were different from those of other explorers of his time, because he developed a mechanism to describe the ways that species changed. His influential ideas include the development of theories regarding the struggle for existence and natural selection. Darwin's theories started a biological segment to biogeography and empirical studies, which enabled future scientists to develop ideas about the geographical distribution of organisms around the globe.[10]

Alfred Russel Wallace studied the distribution of flora and fauna in the Amazon Basin and the Malay Archipelago in the mid-19th century. His research was essential to the further development of biogeography, and he was later nicknamed the ""father of Biogeography"". Wallace conducted fieldwork researching the habits, breeding and migration tendencies, and feeding behavior of thousands of species. He studied butterfly and bird distributions in comparison to the presence or absence of geographical barriers. His observations led him to conclude that the number of organisms present in a community was dependent on the amount of food resources in the particular habitat.[10] Wallace believed species were dynamic by responding to biotic and abiotic factors. He and Philip Sclater saw biogeography as a source of support for the theory of evolution as they used Darwin's conclusion to explain how biogeography was similar to a record of species inheritance.[10] Key findings, such as the sharp difference in fauna either side of the Wallace Line, and the sharp difference that existed between North and South America prior to their relatively recent faunal interchange, can only be understood in this light. Otherwise, the field of biogeography would be seen as a purely descriptive one.[4]

Schematic distribution of fossils on Pangea according to Wegener

20th and 21st century [ edit ]

Distribution of four Permian and Triassic fossil groups used as biogeographic evidence for continental drift, and land bridging

Moving on to the 20th century, Alfred Wegener introduced the Theory of Continental Drift in 1912, though it was not widely accepted until the 1960s.[4] This theory was revolutionary because it changed the way that everyone thought about species and their distribution around the globe. The theory explained how continents were formerly joined together in one large landmass, Pangea, and slowly drifted apart due to the movement of the plates below Earth's surface. The evidence for this theory is in the geological similarities between varying locations around the globe, fossil comparisons from different continents, and the jigsaw puzzle shape of the landmasses on Earth. Though Wegener did not know the mechanism of this concept of Continental Drift, this contribution to the study of biogeography was significant in the way that it shed light on the importance of environmental and geographic similarities or differences as a result of climate and other pressures on the planet. Importantly, late in his career Wegener recognised that testing his theory required measurement of continental movement rather than inference from fossils species distributions.[19]

The publication of The Theory of Island Biogeography by Robert MacArthur and E.O. Wilson in 1967[20] showed that the species richness of an area could be predicted in terms of such factors as habitat area, immigration rate and extinction rate. This added to the long-standing interest in island biogeography. The application of island biogeography theory to habitat fragments spurred the development of the fields of conservation biology and landscape ecology.[21]

Classic biogeography has been expanded by the development of molecular systematics, creating a new discipline known as phylogeography. This development allowed scientists to test theories about the origin and dispersal of populations, such as island endemics. For example, while classic biogeographers were able to speculate about the origins of species in the Hawaiian Islands, phylogeography allows them to test theories of relatedness between these populations and putative source populations in Asia and North America.[15]

Biogeography continues as a point of study for many life sciences and geography students worldwide, however it may be under different broader titles within institutions such as ecology or evolutionary biology.

In recent years, one of the most important and consequential developments in biogeography has been to show how multiple organisms, including mammals like monkeys and reptiles like lizards, overcame barriers such as large oceans that many biogeographers formerly believed were impossible to cross.[22] See also Oceanic dispersal.

Biogeographic regions of Europe

Modern applications [ edit ]

Biogeography now incorporates many different fields including but not limited to physical geography, geology, botany and plant biology, zoology, general biology, and modelling. A biogeographer's main focus is on how the environment and humans affect the distribution of species as well as other manifestations of Life such as species or genetic diversity. Biogeography is being applied to biodiversity conservation and planning, projecting global environmental changes on species and biomes, projecting the spread of infectious diseases, invasive species, and for supporting planning for the establishment of crops. Technological evolving and advances have allowed for generating a whole suit of predictor variables for biogeographic analysis, including satellite imaging and processing of the Earth.[23] Two main types of satellite imaging that are important within modern biogeography are Global Production Efficiency Model (GLO-PEM) and Geographic Information Systems (GIS). GLO-PEM uses satellite-imaging gives ""repetitive, spatially contiguous, and time specific observations of vegetation"". These observations are on a global scale.[24] GIS can show certain processes on the earth's surface like whale locations, sea surface temperatures, and bathymetry.[25] Current scientists also use coral reefs to delve into the history of biogeography through the fossilized reefs.

Paleobiogeography [ edit ]

Paleobiogeography goes one step further to include paleogeographic data and considerations of plate tectonics. Using molecular analyses and corroborated by fossils, it has been possible to demonstrate that perching birds evolved first in the region of Australia or the adjacent Antarctic (which at that time lay somewhat further north and had a temperate climate). From there, they spread to the other Gondwanan continents and Southeast Asia – the part of Laurasia then closest to their origin of dispersal – in the late Paleogene, before achieving a global distribution in the early Neogene.[26] Not knowing that at the time of dispersal, the Indian Ocean was much narrower than it is today, and that South America was closer to the Antarctic, one would be hard pressed to explain the presence of many ""ancient"" lineages of perching birds in Africa, as well as the mainly South American distribution of the suboscines.

Paleobiogeography also helps constrain hypotheses on the timing of biogeographic events such as vicariance and geodispersal, and provides unique information on the formation of regional biotas. For example, data from species-level phylogenetic and biogeographic studies tell us that the Amazonian fish fauna accumulated in increments over a period of tens of millions of years, principally by means of allopatric speciation, and in an arena extending over most of the area of tropical South America (Albert & Reis 2011). In other words, unlike some of the well-known insular faunas (Galapagos finches, Hawaiian drosophilid flies, African rift lake cichlids), the species-rich Amazonian ichthyofauna is not the result of recent adaptive radiations.[27]

For freshwater organisms, landscapes are divided naturally into discrete drainage basins by watersheds, episodically isolated and reunited by erosional processes. In regions like the Amazon Basin (or more generally Greater Amazonia, the Amazon basin, Orinoco basin, and Guianas) with an exceptionally low (flat) topographic relief, the many waterways have had a highly reticulated history over geological time. In such a context, stream capture is an important factor affecting the evolution and distribution of freshwater organisms. Stream capture occurs when an upstream portion of one river drainage is diverted to the downstream portion of an adjacent basin. This can happen as a result of tectonic uplift (or subsidence), natural damming created by a landslide, or headward or lateral erosion of the watershed between adjacent basins.[27]

Concepts and fields [ edit ]

Biogeography is a synthetic science, related to geography, biology, soil science, geology, climatology, ecology and evolution.

Some fundamental concepts in biogeography include:

allopatric speciation – the splitting of a species by evolution of geographically isolated populations

evolution – change in genetic composition of a population

extinction – disappearance of a species

dispersal – movement of populations away from their point of origin, related to migration

endemic areas

geodispersal – the erosion of barriers to biotic dispersal and gene flow, that permit range expansion and the merging of previously isolated biotas

range and distribution

vicariance – the formation of barriers to biotic dispersal and gene flow, that tend to subdivide species and biotas, leading to speciation and extinction; vicariance biogeography is the field that studies these patterns

Comparative biogeography [ edit ]

The study of comparative biogeography can follow two main lines of investigation:[28]

Systematic biogeography, the study of biotic area relationships, their distribution, and hierarchical classification

Evolutionary biogeography, the proposal of evolutionary mechanisms responsible for organismal distributions. Possible mechanisms include widespread taxa disrupted by continental break-up or individual episodes of long-distance movement.

Biogeographic regionalisations [ edit ]

There are many types of biogeographic units used in biogeographic regionalisation schemes,[29][30][31] as there are many criteria (species composition, physiognomy, ecological aspects) and hierarchization schemes: biogeographic realms (or ecozones), bioregions (sensu stricto), ecoregions, zoogeographical regions, floristic regions, vegetation types, biomes, etc.

The terms biogeographic unit,[32] biogeographic area[33] or bioregion sensu lato,[34] can be used for these categories, regardless of rank.

In 2008, an International Code of Area Nomenclature was proposed for biogeography.[35][36]

See also [ edit ]

Notes and references [ edit ]

Further reading [ edit ]"	https://en.wikipedia.org/wiki/Biogeography	"The study of the distribution of species and ecosystems in geographic space and through geological time
Biogeography is the study of the distribution of species and ecosystems in geographic space and through geological time. Organisms and biological communities often vary in a regular fashion along geographic gradients of latitude, elevation, isolation and habitat area. Phytogeography is the branch of biogeography that studies the distribution of plants. Zoogeography is the branch that studies distribution of animals. Mycogeography is the branch that studies distribution of fungi, such as mushrooms.
Knowledge of spatial variation in the numbers and types of organisms is as vital to us today as it was to our early human ancestors, as we adapt to heterogeneous but geographically predictable environments. Biogeography is an integrative field of inquiry that unites concepts and information from ecology, evolutionary biology, taxonomy, geology, physical geography, palaeontology, and climatology.
Modern biogeographic research combines information and ideas from many fields, from the physiological and ecological constraints on organismal dispersal to geological and climatological phenomena operating at global spatial scales and evolutionary time frames.
The short-term interactions within a habitat and species of organisms describe the ecological application of biogeography. Historical biogeography describes the long-term, evolutionary periods of time for broader classifications of organisms. Early scientists, beginning with Carl Linnaeus, contributed to the development of biogeography as a science.
The scientific theory of biogeography grows out of the work of Alexander von Humboldt (1769–1859), Francisco Jose de Caldas (1768-1816), Hewett Cottrell Watson (1804–1881), Alphonse de Candolle (1806–1893), Alfred Russel Wallace (1823–1913), Philip Lutley Sclater (1829–1913) and other biologists and explorers.
Introduction 
The patterns of species distribution across geographical areas can usually be explained through a combination of historical factors such as: speciation, extinction, continental drift, and glaciation. Through observing the geographic distribution of species, we can see associated variations in sea level, river routes, habitat, and river capture. Additionally, this science considers the geographic constraints of landmass areas and isolation, as well as the available ecosystem energy supplies.
Over periods of ecological changes, biogeography includes the study of plant and animal species in: their past and/or present living refugium habitat; their interim living sites; and/or their survival locales. As writer David Quammen put it, ""...biogeography does more than ask Which species? and Where. It also asks Why? and, what is sometimes more crucial, Why not?.""
Modern biogeography often employs the use of Geographic Information Systems (GIS), to understand the factors affecting organism distribution, and to predict future trends in organism distribution. Often mathematical models and GIS are employed to solve ecological problems that have a spatial aspect to them.
Biogeography is most keenly observed on the world's islands. These habitats are often much more manageable areas of study because they are more condensed than larger ecosystems on the mainland. Islands are also ideal locations because they allow scientists to look at habitats that new invasive species have only recently colonized and can observe how they disperse throughout the island and change it. They can then apply their understanding to similar but more complex mainland habitats. Islands are very diverse in their biomes, ranging from the tropical to arctic climates. This diversity in habitat allows for a wide range of species study in different parts of the world.
One scientist who recognized the importance of these geographic locations was Charles Darwin, who remarked in his journal ""The Zoology of Archipelagoes will be well worth examination"". Two chapters in On the Origin of Species were devoted to geographical distribution.
History 
18th century 
The first discoveries that contributed to the development of biogeography as a science began in the mid-18th century, as Europeans explored the world and described the biodiversity of life. During the 18th century most views on the world were shaped around religion and for many natural theologists, the bible. Carl Linnaeus, in the mid-18th century, initiated the ways to classify organisms through his exploration of undiscovered territories. When he noticed that species were not as perpetual as he believed, he developed the Mountain Explanation to explain the distribution of biodiversity; when Noah's ark landed on Mount Ararat and the waters receded, the animals dispersed throughout different elevations on the mountain. This showed different species in different climates proving species were not constant. Linnaeus' findings set a basis for ecological biogeography. Through his strong beliefs in Christianity, he was inspired to classify the living world, which then gave way to additional accounts of secular views on geographical distribution. He argued that the structure of an animal was very closely related to its physical surroundings. This was important to a George Louis Buffon's rival theory of distribution.
The Theory of Island Biogeography and helped to start much of the research that has been done on this topic since the work of Watson and Wallace almost a century before Edward O. Wilson , a prominent biologist and conservationist, coauthoredand helped to start much of the research that has been done on this topic since the work of Watson and Wallace almost a century before
Closely after Linnaeus, Georges-Louis Leclerc, Comte de Buffon observed shifts in climate and how species spread across the globe as a result. He was the first to see different groups of organisms in different regions of the world. Buffon saw similarities between some regions which led him to believe that at one point continents were connected and then water separated them and caused differences in species. His hypotheses were described in his work, the 36 volume Histoire Naturelle, générale et particulière, in which he argued that varying geographical regions would have different forms of life. This was inspired by his observations comparing the Old and New World, as he determined distinct variations of species from the two regions. Buffon believed there was a single species creation event, and that different regions of the world were homes for varying species, which is an alternate view than that of Linnaeus. Buffon's law eventually became a principle of biogeography by explaining how similar environments were habitats for comparable types of organisms. Buffon also studied fossils which led him to believe that the earth was over tens of thousands of years old, and that humans had not lived there long in comparison to the age of the earth.
19th century 
Following the period of exploration came the Age of Enlightenment in Europe, which attempted to explain the patterns of biodiversity observed by Buffon and Linnaeus. At the birth of the 19th century, Alexander von Humboldt, known as the ""founder of plant geography"", developed the concept of physique generale to demonstrate the unity of science and how species fit together. As one of the first to contribute empirical data to the science of biogeography through his travel as an explorer, he observed differences in climate and vegetation. The earth was divided into regions which he defined as tropical, temperate, and arctic and within these regions there were similar forms of vegetation. This ultimately enabled him to create the isotherm, which allowed scientists to see patterns of life within different climates. He contributed his observations to findings of botanical geography by previous scientists, and sketched this description of both the biotic and abiotic features of the earth in his book, Cosmos.
Augustin de Candolle contributed to the field of biogeography as he observed species competition and the several differences that influenced the discovery of the diversity of life. He was a Swiss botanist and created the first Laws of Botanical Nomenclature in his work, Prodromus. He discussed plant distribution and his theories eventually had a great impact on Charles Darwin, who was inspired to consider species adaptations and evolution after learning about botanical geography. De Candolle was the first to describe the differences between the small-scale and large-scale distribution patterns of organisms around the globe.
Several additional scientists contributed new theories to further develop the concept of biogeography. Charles Lyell developed the Theory of Uniformitarianism after studying fossils. This theory explained how the world was not created by one sole catastrophic event, but instead from numerous creation events and locations. Uniformitarianism also introduced the idea that the Earth was actually significantly older than was previously accepted. Using this knowledge, Lyell concluded that it was possible for species to go extinct. Since he noted that earth's climate changes, he realized that species distribution must also change accordingly. Lyell argued that climate changes complemented vegetation changes, thus connecting the environmental surroundings to varying species. This largely influenced Charles Darwin in his development of the theory of evolution.
Charles Darwin was a natural theologist who studied around the world, and most importantly in the Galapagos Islands. Darwin introduced the idea of natural selection, as he theorized against previously accepted ideas that species were static or unchanging. His contributions to biogeography and the theory of evolution were different from those of other explorers of his time, because he developed a mechanism to describe the ways that species changed. His influential ideas include the development of theories regarding the struggle for existence and natural selection. Darwin's theories started a biological segment to biogeography and empirical studies, which enabled future scientists to develop ideas about the geographical distribution of organisms around the globe.
Alfred Russel Wallace studied the distribution of flora and fauna in the Amazon Basin and the Malay Archipelago in the mid-19th century. His research was essential to the further development of biogeography, and he was later nicknamed the ""father of Biogeography"". Wallace conducted fieldwork researching the habits, breeding and migration tendencies, and feeding behavior of thousands of species. He studied butterfly and bird distributions in comparison to the presence or absence of geographical barriers. His observations led him to conclude that the number of organisms present in a community was dependent on the amount of food resources in the particular habitat. Wallace believed species were dynamic by responding to biotic and abiotic factors. He and Philip Sclater saw biogeography as a source of support for the theory of evolution as they used Darwin's conclusion to explain how biogeography was similar to a record of species inheritance. Key findings, such as the sharp difference in fauna either side of the Wallace Line, and the sharp difference that existed between North and South America prior to their relatively recent faunal interchange, can only be understood in this light. Otherwise, the field of biogeography would be seen as a purely descriptive one.
Schematic distribution of fossils on Pangea according to Wegener
20th and 21st century 
Distribution of four Permian and Triassic fossil groups used as biogeographic evidence for continental drift, and land bridging
Moving on to the 20th century, Alfred Wegener introduced the Theory of Continental Drift in 1912, though it was not widely accepted until the 1960s. This theory was revolutionary because it changed the way that everyone thought about species and their distribution around the globe. The theory explained how continents were formerly joined together in one large landmass, Pangea, and slowly drifted apart due to the movement of the plates below Earth's surface. The evidence for this theory is in the geological similarities between varying locations around the globe, fossil comparisons from different continents, and the jigsaw puzzle shape of the landmasses on Earth. Though Wegener did not know the mechanism of this concept of Continental Drift, this contribution to the study of biogeography was significant in the way that it shed light on the importance of environmental and geographic similarities or differences as a result of climate and other pressures on the planet. Importantly, late in his career Wegener recognised that testing his theory required measurement of continental movement rather than inference from fossils species distributions.
The publication of The Theory of Island Biogeography by Robert MacArthur and E.O. Wilson in 1967 showed that the species richness of an area could be predicted in terms of such factors as habitat area, immigration rate and extinction rate. This added to the long-standing interest in island biogeography. The application of island biogeography theory to habitat fragments spurred the development of the fields of conservation biology and landscape ecology.
Classic biogeography has been expanded by the development of molecular systematics, creating a new discipline known as phylogeography. This development allowed scientists to test theories about the origin and dispersal of populations, such as island endemics. For example, while classic biogeographers were able to speculate about the origins of species in the Hawaiian Islands, phylogeography allows them to test theories of relatedness between these populations and putative source populations in Asia and North America.
Biogeography continues as a point of study for many life sciences and geography students worldwide, however it may be under different broader titles within institutions such as ecology or evolutionary biology.
In recent years, one of the most important and consequential developments in biogeography has been to show how multiple organisms, including mammals like monkeys and reptiles like lizards, overcame barriers such as large oceans that many biogeographers formerly believed were impossible to cross."	14394
geography	[]		"Scientific study of climate, defined as weather conditions averaged over a period of time

""Climate Research"" redirects here. For the journal of that name, see Climate Research (journal)

Climatology is the scientific study of the climate.

Climatology (from Greek κλίμα, klima, ""place, zone""; and -λογία, -logia) or climate science is the scientific study of climate, scientifically defined as weather conditions averaged over a period of time.[1] This modern field of study is regarded as a branch of the atmospheric sciences and a subfield of physical geography, which is one of the Earth sciences. Climatology now includes aspects of oceanography and biogeochemistry.

The main methods employed by climatologists are the analysis of observations and modelling the physical laws that determine the climate. The main topics of research are the study of climate variability, mechanisms of climate changes and modern climate change. Basic knowledge of climate can be used within shorter term weather forecasting, for instance about climatic cycles such as the El Niño–Southern Oscillation (ENSO), the Madden–Julian oscillation (MJO), the North Atlantic oscillation (NAO), the Arctic oscillation (AO), the Pacific decadal oscillation (PDO), and the Interdecadal Pacific Oscillation (IPO).

Climate models are used for a variety of purposes from study of the dynamics of the weather and climate system to projections of future climate. Weather is known as the condition of the atmosphere over a period of time, while climate has to do with the atmospheric condition over an extended to indefinite period of time.[2]

History [ edit ]

The Greeks began the formal study of climate; in fact the word climate is derived from the Greek word klima, meaning ""slope,"" referring to the slope or inclination of the Earth's axis. Arguably the most influential classic text on climate was On Airs, Water and Places[3] written by Hippocrates around 400 BCE. This work commented on the effect of climate on human health and cultural differences between Asia and Europe.[3] This idea that climate controls which countries excel depending on their climate, or climatic determinism, remained influential throughout history.[3] Chinese scientist Shen Kuo (1031–1095) inferred that climates naturally shifted over an enormous span of time, after observing petrified bamboos found underground near Yanzhou (modern day Yan'an, Shaanxi province), a dry-climate area unsuitable for the growth of bamboo.[4]

The invention of the thermometer and the barometer during the Scientific Revolution allowed for systematic recordkeeping, that began as early as 1640–1642 in England.[3] Early climate researchers include Edmund Halley, who published a map of the trade winds in 1686 after a voyage to the southern hemisphere. Benjamin Franklin (1706–1790) first mapped the course of the Gulf Stream for use in sending mail from the United States to Europe. Francis Galton (1822–1911) invented the term anticyclone.[5] Helmut Landsberg (1906–1985) fostered the use of statistical analysis in climatology, which led to its evolution into a physical science.

In the early 20th century, climatology was mostly focused on the description of regional climates. This descriptive climatology was mainly an applied science, giving farmers and other interested people statistics about what the normal weather was and how big chances were of extreme events.[6] To do this, climatologists had to define a climate normal, or an average of weather and weather extremes over a period of typically 30 years.

Around the middle of the 20th century, many assumptions in meteorology and climatology considered climate to be roughly constant. While scientists knew of past climate change such as the ice ages, the concept of climate as unchanging was useful in the development of a general theory of what determines climate. This started to change in the decades that followed, and while the history of climate change science started earlier, climate change only became one of the mean topics of study for climatologists in the seventies and onward.

Subfields [ edit ]

Map of the average temperature over 30 years. Data sets formed from the long-term average of historical weather parameters are sometimes called a ""climatology"".

Various subfields of climatology study different aspects of the climate. There are different categorizations of the fields in climatology. The American Meteorological Society for instance identifies descriptive climatology, scientific climatology and applied climatology as the three subcategories of climatology, a categorization based on the complexity and the purpose of the research.[9] Applied climatologists apply their expertise to different industries such as manufacturing and agriculture.

Paleoclimatology seeks to reconstruct and understand past climates by examining records such as ice cores and tree rings (dendroclimatology). Paleotempestology uses these same records to help determine hurricane frequency over millennia. Historical climatology is the study of climate as related to human history and thus focuses only on the last few thousand years.

Boundary-layer climatology is preoccupied with exchanges in water, energy and momentum near the surface.[11] Further identified subfields are physical climatology, dynamic climatology, tornado climatology, regional climatology, bioclimatology, and synoptic climatology. The study of the hydrological cycle over long time scales is sometimes called hydroclimatology, in particular when studying the effects of climate change on the water cycle.[9]

Methods [ edit ]

The study of contemporary climates incorporates meteorological data accumulated over many years, such as records of rainfall, temperature and atmospheric composition. Knowledge of the atmosphere and its dynamics is also embodied in models, either statistical or mathematical, which help by integrating different observations and testing how they fit together. Modeling is used for understanding past, present and potential future climates.

Climate research is made difficult by the large scale, long time periods, and complex processes which govern climate. Climate is governed by physical laws which can be expressed as differential equations. These equations are coupled and nonlinear, so that approximate solutions are obtained by using numerical methods to create global climate models. Climate is sometimes modeled as a stochastic process but this is generally accepted as an approximation to processes that are otherwise too complicated to analyze.

Climate data [ edit ]

The collection of long record of climate variables is essential for the study of climate. Climatology deals with the aggregate data that meteorology has collected.[12] Scientists use both direct and indirect observations of the climate, from Earth observing satellites and scientific instrumentation such as a global network of thermometers, to prehistoric ice extracted from glaciers.[13] As measuring technology changes over time, records of data cannot be compared directly. As cities are generally warmer than the surrounding areas, urbanization has made it necessary to constantly correct data for this urban heat island effect.

Models [ edit ]

Climate models use quantitative methods to simulate the interactions of the atmosphere, oceans, land surface, and ice. They are used for a variety of purposes from study of the dynamics of the weather and climate system to projections of future climate. All climate models balance, or very nearly balance, incoming energy as short wave (including visible) electromagnetic radiation to the earth with outgoing energy as long wave (infrared) electromagnetic radiation from the earth. Any unbalance results in a change in the average temperature of the earth. Most climate models include the radiative effects of greenhouse gases such as carbon dioxide. These models predict an upward trend in the surface temperatures, as well as a more rapid increase in temperature at higher latitudes.

Models can range from relatively simple to complex:

A simple radiant heat transfer model that treats the earth as a single point and averages outgoing energy

this can be expanded vertically (radiative-convective models), or horizontally

Coupled atmosphere–ocean–sea ice global climate models discretise and solve the full equations for mass and energy transfer and radiant exchange.

Earth system models further include the biosphere.

Topics of research [ edit ]

Topics that climatologists study fall roughly into three categories: climate variability, mechanisms of climate change and modern climate change.[15]

Climatological processes [ edit ]

Various factors impact the average state of the atmosphere at a particular location. For instance, midlatitudes will have a pronounced seasonal cycle in temperature whereas tropical regions show little variation in temperature over the year. Another major control in climate is continentality: the distance to major water bodies such as oceans. Oceans act as a moderating factor, so that land close to it has typically has mild winters and moderate summers. The atmosphere interacts with other spheres of the climate system, with winds generating ocean currents that transport heat around the globe.

Climate classification [ edit ]

Classification is an important aspect of many sciences as a tool of simplifying complicated processes. Different climate classifications have been developed over the centuries, with the first ones in Ancient Greece. How climates are classified depends on what the application is. A wind energy producer will require different information (wind) in the classification than somebody interested in agriculture, for who precipitation and temperature are more important. The most widely used classification, the Köppen climate classification, was developed in the late nineteenth century and is based on vegetation. It uses monthly temperature and precipitation data.

Climate variability [ edit ]

El Niño impacts

There are different modes of variability: recurring patterns of temperature or other climate variables. They are quantified with different indices. Much in the way the Dow Jones Industrial Average, which is based on the stock prices of 30 companies, is used to represent the fluctuations in the stock market as a whole, climate indices are used to represent the essential elements of climate. Climate indices are generally devised with the twin objectives of simplicity and completeness, and each index typically represents the status and timing of the climate factor it represents. By their very nature, indices are simple, and combine many details into a generalized, overall description of the atmosphere or ocean which can be used to characterize the factors which impact the global climate system.

El Niño–Southern Oscillation (ENSO) is a coupled ocean-atmosphere phenomenon in the Pacific Ocean responsible for most of the global variability in temperature, and has a cycle between two and seven years.[21] The North Atlantic oscillation is a mode of variability that is mainly contained to the lower atmosphere, the troposphere. The layer of atmosphere above, the stratosphere is also capable of creating its own variability, most importantly in the Madden–Julian oscillation (MJO), which has a cycle of approximately 30-60 days. The Interdecadal Pacific oscillation can create changes in the Pacific Ocean and lower atmosphere on decadal time scales.

Climatic change [ edit ]

Climate change occurs when changes in Earth's climate system result in new weather patterns that remain in place for an extended period of time. This length of time can be as short as a few decades to as long as millions of years. The climate system receives nearly all of its energy from the sun. The climate system also gives off energy to outer space. The balance of incoming and outgoing energy, and the passage of the energy through the climate system, determines Earth's energy budget. When the incoming energy is greater than the outgoing energy, earth's energy budget is positive and the climate system is warming. If more energy goes out, the energy budget is negative and earth experiences cooling. Climate change also influences the average sea level.

Modern climate change is driven by the human emissions of greenhouse gas from the burning of fossil fuel driving up global mean surface temperatures. Rising temperatures are only one aspect of modern climate change though, with includes observed changes in precipitation, storm tracks and cloudiness. Warmer temperatures are driving further changes in the climate system, such as the widespread melt of glaciers, sea level rise and shifts in flora and fauna.[22]

Differences with meteorology [ edit ]

In contrast to meteorology, which focuses on short term weather systems lasting up to a few weeks, climatology studies the frequency and trends of those systems. It studies the periodicity of weather events over years to millennia, as well as changes in long-term average weather patterns, in relation to atmospheric conditions. Climatologists study both the nature of climates – local, regional or global – and the natural or human-induced factors that cause climates to change. Climatology considers the past and can help predict future climate change.

Phenomena of climatological interest include the atmospheric boundary layer, circulation patterns, heat transfer (radiative, convective and latent), interactions between the atmosphere and the oceans and land surface (particularly vegetation, land use and topography), and the chemical and physical composition of the atmosphere.

Use in weather forecasting [ edit ]

A more complicated way of making a forecast, the analog technique requires remembering a previous weather event which is expected to be mimicked by an upcoming event. What makes it a difficult technique to use is that there is rarely a perfect analog for an event in the future.[23] Some call this type of forecasting pattern recognition, which remains a useful method of observing rainfall over data voids such as oceans with knowledge of how satellite imagery relates to precipitation rates over land,[24] as well as the forecasting of precipitation amounts and distribution in the future. A variation on this theme is used in medium range forecasting, which is known as teleconnections, when systems in other locations are used to help pin down the location of a system within the surrounding regime.[25] One method of using teleconnections are by using climate indices such as ENSO-related phenomena.[26]

See also [ edit ]

References [ edit ]

Books [ edit ]

Robinson, Peter J. Robinson; Henderson-Sellers, Ann (1999). Contemporary Climatology . Harlow, England: Pearson Prentice Hall. ISBN 0582276314 .

Rohli, Robert. V.; Vega, Anthony J. (2018). Climatology (fourth ed.). Jones & Bartlett Learning. ISBN 9781284126563 .

Rohli, Robert. V.; Vega, Anthony J. (2011). Climatology (second ed.). Jones & Bartlett Learning.

Wang, Shih-Yu; Gillies, Robert R., eds. (2012). Modern Climatology. Rijeka, Croatia: InTech. ISBN 978-953-51-0095-9 .

Further reading [ edit ]

Jenny Uglow, ""What the Weather Is"" (review of Sarah Dry, Waters of the World: The Story of the Scientists Who Unraveled the Mysteries of Our Oceans, Atmosphere, and Ice Sheets and Made the Planet Whole, University of Chicago Press, 2019, 332 pp.), The New York Review of Books, vol. LXVI, no. 20 (19 December 2019), pp. 56–58."	https://en.wikipedia.org/wiki/Climatology	"Scientific study of climate, defined as weather conditions averaged over a period of time
""Climate Research"" redirects here. For the journal of that name, see Climate Research (journal)
Climatology is the scientific study of the climate.
Climatology (from Greek κλίμα, klima, ""place, zone""; and -λογία, -logia) or climate science is the scientific study of climate, scientifically defined as weather conditions averaged over a period of time. This modern field of study is regarded as a branch of the atmospheric sciences and a subfield of physical geography, which is one of the Earth sciences. Climatology now includes aspects of oceanography and biogeochemistry.
The main methods employed by climatologists are the analysis of observations and modelling the physical laws that determine the climate. The main topics of research are the study of climate variability, mechanisms of climate changes and modern climate change. Basic knowledge of climate can be used within shorter term weather forecasting, for instance about climatic cycles such as the El Niño–Southern Oscillation (ENSO), the Madden–Julian oscillation (MJO), the North Atlantic oscillation (NAO), the Arctic oscillation (AO), the Pacific decadal oscillation (PDO), and the Interdecadal Pacific Oscillation (IPO).
Climate models are used for a variety of purposes from study of the dynamics of the weather and climate system to projections of future climate. Weather is known as the condition of the atmosphere over a period of time, while climate has to do with the atmospheric condition over an extended to indefinite period of time.
History 
The Greeks began the formal study of climate; in fact the word climate is derived from the Greek word klima, meaning ""slope,"" referring to the slope or inclination of the Earth's axis. Arguably the most influential classic text on climate was On Airs, Water and Places written by Hippocrates around 400 BCE. This work commented on the effect of climate on human health and cultural differences between Asia and Europe. This idea that climate controls which countries excel depending on their climate, or climatic determinism, remained influential throughout history. Chinese scientist Shen Kuo (1031–1095) inferred that climates naturally shifted over an enormous span of time, after observing petrified bamboos found underground near Yanzhou (modern day Yan'an, Shaanxi province), a dry-climate area unsuitable for the growth of bamboo.
The invention of the thermometer and the barometer during the Scientific Revolution allowed for systematic recordkeeping, that began as early as 1640–1642 in England. Early climate researchers include Edmund Halley, who published a map of the trade winds in 1686 after a voyage to the southern hemisphere. Benjamin Franklin (1706–1790) first mapped the course of the Gulf Stream for use in sending mail from the United States to Europe. Francis Galton (1822–1911) invented the term anticyclone. Helmut Landsberg (1906–1985) fostered the use of statistical analysis in climatology, which led to its evolution into a physical science.
In the early 20th century, climatology was mostly focused on the description of regional climates. This descriptive climatology was mainly an applied science, giving farmers and other interested people statistics about what the normal weather was and how big chances were of extreme events. To do this, climatologists had to define a climate normal, or an average of weather and weather extremes over a period of typically 30 years.
Around the middle of the 20th century, many assumptions in meteorology and climatology considered climate to be roughly constant. While scientists knew of past climate change such as the ice ages, the concept of climate as unchanging was useful in the development of a general theory of what determines climate. This started to change in the decades that followed, and while the history of climate change science started earlier, climate change only became one of the mean topics of study for climatologists in the seventies and onward.
Subfields 
Map of the average temperature over 30 years. Data sets formed from the long-term average of historical weather parameters are sometimes called a ""climatology"".
Various subfields of climatology study different aspects of the climate. There are different categorizations of the fields in climatology. The American Meteorological Society for instance identifies descriptive climatology, scientific climatology and applied climatology as the three subcategories of climatology, a categorization based on the complexity and the purpose of the research. Applied climatologists apply their expertise to different industries such as manufacturing and agriculture.
Paleoclimatology seeks to reconstruct and understand past climates by examining records such as ice cores and tree rings (dendroclimatology). Paleotempestology uses these same records to help determine hurricane frequency over millennia. Historical climatology is the study of climate as related to human history and thus focuses only on the last few thousand years.
Boundary-layer climatology is preoccupied with exchanges in water, energy and momentum near the surface. Further identified subfields are physical climatology, dynamic climatology, tornado climatology, regional climatology, bioclimatology, and synoptic climatology. The study of the hydrological cycle over long time scales is sometimes called hydroclimatology, in particular when studying the effects of climate change on the water cycle.
Methods 
The study of contemporary climates incorporates meteorological data accumulated over many years, such as records of rainfall, temperature and atmospheric composition. Knowledge of the atmosphere and its dynamics is also embodied in models, either statistical or mathematical, which help by integrating different observations and testing how they fit together. Modeling is used for understanding past, present and potential future climates.
Climate research is made difficult by the large scale, long time periods, and complex processes which govern climate. Climate is governed by physical laws which can be expressed as differential equations. These equations are coupled and nonlinear, so that approximate solutions are obtained by using numerical methods to create global climate models. Climate is sometimes modeled as a stochastic process but this is generally accepted as an approximation to processes that are otherwise too complicated to analyze.
Climate data 
The collection of long record of climate variables is essential for the study of climate. Climatology deals with the aggregate data that meteorology has collected. Scientists use both direct and indirect observations of the climate, from Earth observing satellites and scientific instrumentation such as a global network of thermometers, to prehistoric ice extracted from glaciers. As measuring technology changes over time, records of data cannot be compared directly. As cities are generally warmer than the surrounding areas, urbanization has made it necessary to constantly correct data for this urban heat island effect.
Models 
Climate models use quantitative methods to simulate the interactions of the atmosphere, oceans, land surface, and ice. They are used for a variety of purposes from study of the dynamics of the weather and climate system to projections of future climate. All climate models balance, or very nearly balance, incoming energy as short wave (including visible) electromagnetic radiation to the earth with outgoing energy as long wave (infrared) electromagnetic radiation from the earth. Any unbalance results in a change in the average temperature of the earth. Most climate models include the radiative effects of greenhouse gases such as carbon dioxide. These models predict an upward trend in the surface temperatures, as well as a more rapid increase in temperature at higher latitudes.
Models can range from relatively simple to complex:
A simple radiant heat transfer model that treats the earth as a single point and averages outgoing energy
this can be expanded vertically (radiative-convective models), or horizontally
Coupled atmosphere–ocean–sea ice global climate models discretise and solve the full equations for mass and energy transfer and radiant exchange.
Earth system models further include the biosphere.
Topics of research 
Topics that climatologists study fall roughly into three categories: climate variability, mechanisms of climate change and modern climate change.
Climatological processes 
Various factors impact the average state of the atmosphere at a particular location. For instance, midlatitudes will have a pronounced seasonal cycle in temperature whereas tropical regions show little variation in temperature over the year. Another major control in climate is continentality: the distance to major water bodies such as oceans. Oceans act as a moderating factor, so that land close to it has typically has mild winters and moderate summers. The atmosphere interacts with other spheres of the climate system, with winds generating ocean currents that transport heat around the globe.
Climate classification 
Classification is an important aspect of many sciences as a tool of simplifying complicated processes. Different climate classifications have been developed over the centuries, with the first ones in Ancient Greece. How climates are classified depends on what the application is. A wind energy producer will require different information (wind) in the classification than somebody interested in agriculture, for who precipitation and temperature are more important. The most widely used classification, the Köppen climate classification, was developed in the late nineteenth century and is based on vegetation. It uses monthly temperature and precipitation data.
Climate variability 
El Niño impacts
There are different modes of variability: recurring patterns of temperature or other climate variables. They are quantified with different indices. Much in the way the Dow Jones Industrial Average, which is based on the stock prices of 30 companies, is used to represent the fluctuations in the stock market as a whole, climate indices are used to represent the essential elements of climate. Climate indices are generally devised with the twin objectives of simplicity and completeness, and each index typically represents the status and timing of the climate factor it represents. By their very nature, indices are simple, and combine many details into a generalized, overall description of the atmosphere or ocean which can be used to characterize the factors which impact the global climate system.
El Niño–Southern Oscillation (ENSO) is a coupled ocean-atmosphere phenomenon in the Pacific Ocean responsible for most of the global variability in temperature, and has a cycle between two and seven years. The North Atlantic oscillation is a mode of variability that is mainly contained to the lower atmosphere, the troposphere. The layer of atmosphere above, the stratosphere is also capable of creating its own variability, most importantly in the Madden–Julian oscillation (MJO), which has a cycle of approximately 30-60 days. The Interdecadal Pacific oscillation can create changes in the Pacific Ocean and lower atmosphere on decadal time scales.
Climatic change 
Climate change occurs when changes in Earth's climate system result in new weather patterns that remain in place for an extended period of time. This length of time can be as short as a few decades to as long as millions of years. The climate system receives nearly all of its energy from the sun. The climate system also gives off energy to outer space. The balance of incoming and outgoing energy, and the passage of the energy through the climate system, determines Earth's energy budget. When the incoming energy is greater than the outgoing energy, earth's energy budget is positive and the climate system is warming. If more energy goes out, the energy budget is negative and earth experiences cooling. Climate change also influences the average sea level.
Modern climate change is driven by the human emissions of greenhouse gas from the burning of fossil fuel driving up global mean surface temperatures. Rising temperatures are only one aspect of modern climate change though, with includes observed changes in precipitation, storm tracks and cloudiness. Warmer temperatures are driving further changes in the climate system, such as the widespread melt of glaciers, sea level rise and shifts in flora and fauna.
Differences with meteorology 
In contrast to meteorology, which focuses on short term weather systems lasting up to a few weeks, climatology studies the frequency and trends of those systems. It studies the periodicity of weather events over years to millennia, as well as changes in long-term average weather patterns, in relation to atmospheric conditions. Climatologists study both the nature of climates – local, regional or global – and the natural or human-induced factors that cause climates to change. Climatology considers the past and can help predict future climate change.
Phenomena of climatological interest include the atmospheric boundary layer, circulation patterns, heat transfer (radiative, convective and latent), interactions between the atmosphere and the oceans and land surface (particularly vegetation, land use and topography), and the chemical and physical composition of the atmosphere.
Use in weather forecasting 
A more complicated way of making a forecast, the analog technique requires remembering a previous weather event which is expected to be mimicked by an upcoming event. What makes it a difficult technique to use is that there is rarely a perfect analog for an event in the future. Some call this type of forecasting pattern recognition, which remains a useful method of observing rainfall over data voids such as oceans with knowledge of how satellite imagery relates to precipitation rates over land, as well as the forecasting of precipitation amounts and distribution in the future. A variation on this theme is used in medium range forecasting, which is known as teleconnections, when systems in other locations are used to help pin down the location of a system within the surrounding regime. One method of using teleconnections are by using climate indices such as ENSO-related phenomena."	14405
geography	[]		"The integrated, quantitative, and interdisciplinary approach to the study of environmental systems.

For the TV series episode, see Environmental Science (Community)

Not to be confused with Environmental studies

""Environmental research"" redirects here. For the Elsevier journal, see Environmental Research

Environmental science is an interdisciplinary academic field that integrates physical, biological and information sciences (including ecology, biology, physics, chemistry, plant science, zoology, mineralogy, oceanography, limnology, soil science, geology and physical geography, and atmospheric science) to the study of the environment, and the solution of environmental problems. Environmental science emerged from the fields of natural history and medicine during the Enlightenment.[1] Today it provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems.[2]

Environmental studies incorporates more of the social sciences for understanding human relationships, perceptions and policies towards the environment. Environmental engineering focuses on design and technology for improving environmental quality in every aspect.

Environmental scientists study subjects like the understanding of earth processes, evaluating alternative energy systems, pollution control and mitigation, natural resource management, and the effects of global climate change. Environmental issues almost always include an interaction of physical, chemical, and biological processes. Environmental scientists bring a systems approach to the analysis of environmental problems. Key elements of an effective environmental scientist include the ability to relate space, and time relationships as well as quantitative analysis.

Environmental science came alive as a substantive, active field of scientific investigation in the 1960s and 1970s driven by (a) the need for a multi-disciplinary approach to analyze complex environmental problems, (b) the arrival of substantive environmental laws requiring specific environmental protocols of investigation and (c) the growing public awareness of a need for action in addressing environmental problems. Events that spurred this development included the publication of Rachel Carson's landmark environmental book Silent Spring[3] along with major environmental issues becoming very public, such as the 1969 Santa Barbara oil spill, and the Cuyahoga River of Cleveland, Ohio, ""catching fire"" (also in 1969), and helped increase the visibility of environmental issues and create this new field of study.

Terminology [ edit ]

In common usage, ""environmental science"" and ""ecology"" are often used interchangeably, but technically, ecology refers only to the study of organisms and their interactions with each other as well as how they interrelate with environment. Ecology could be considered a subset of environmental science, which also could involve purely chemical or public health issues (for example) ecologists would be unlikely to study. In practice, there are considerable similarities between the work of ecologists and other environmental scientists. There is substantial overlap between ecology and environmental science with the disciplines of fisheries, forestry, and wildlife.

Components [ edit ]

Blue Marble composite images generated by NASA in 2001 (left) and 2002 (right)

Atmospheric sciences [ edit ]

Atmospheric sciences focus on the Earth's atmosphere, with an emphasis upon its interrelation to other systems. Atmospheric sciences can include studies of meteorology, greenhouse gas phenomena, atmospheric dispersion modeling of airborne contaminants,[4][5] sound propagation phenomena related to noise pollution, and even light pollution.

Taking the example of the global warming phenomena, physicists create computer models of atmospheric circulation and infrared radiation transmission, chemists examine the inventory of atmospheric chemicals and their reactions, biologists analyze the plant and animal contributions to carbon dioxide fluxes, and specialists such as meteorologists and oceanographers add additional breadth in understanding the atmospheric dynamics.

Ecology [ edit ]

Biodiversity of a coral reef. Corals adapt and modify their environment by forming calcium carbonate skeletons. This provides growing conditions for future generations and forms a habitat for many other species.

As defined by the Ecological Society of America, ""Ecology is the study of the relationships between living organisms, including humans, and their physical environment; it seeks to understand the vital connections between plants and animals and the world around them.""[6] Ecologists might investigate the relationship between a population of organisms and some physical characteristic of their environment, such as concentration of a chemical; or they might investigate the interaction between two populations of different organisms through some symbiotic or competitive relationship. For example, an interdisciplinary analysis of an ecological system which is being impacted by one or more stressors might include several related environmental science fields. In an estuarine setting where a proposed industrial development could impact certain species by water and air pollution, biologists would describe the flora and fauna, chemists would analyze the transport of water pollutants to the marsh, physicists would calculate air pollution emissions and geologists would assist in understanding the marsh soils and bay muds.

Environmental chemistry [ edit ]

Environmental chemistry is the study of chemical alterations in the environment. Principal areas of study include soil contamination and water pollution. The topics of analysis include chemical degradation in the environment, multi-phase transport of chemicals (for example, evaporation of a solvent containing lake to yield solvent as an air pollutant), and chemical effects upon biota.

As an example study, consider the case of a leaking solvent tank which has entered the habitat soil of an endangered species of amphibian. As a method to resolve or understand the extent of soil contamination and subsurface transport of solvent, a computer model would be implemented. Chemists would then characterize the molecular bonding of the solvent to the specific soil type, and biologists would study the impacts upon soil arthropods, plants, and ultimately pond-dwelling organisms that are the food of the endangered amphibian.

Geosciences [ edit ]

Geosciences include environmental geology, environmental soil science, volcanic phenomena and evolution of the Earth's crust. In some classification systems this can also include hydrology, including oceanography.

As an example study, of soils erosion, calculations would be made of surface runoff by soil scientists. Fluvial geomorphologists would assist in examining sediment transport in overland flow. Physicists would contribute by assessing the changes in light transmission in the receiving waters. Biologists would analyze subsequent impacts to aquatic flora and fauna from increases in water turbidity.

Regulations driving the studies [ edit ]

Environmental science examines the effects of humans on nature, such as the Glen Canyon Dam in the United States

In the United States the National Environmental Policy Act (NEPA) of 1969 set forth requirements for analysis of federal government actions (such as highway construction projects and land management decisions) in terms of specific environmental criteria.[7] Numerous state laws have echoed these mandates, applying the principles to local-scale actions. The upshot has been an explosion of documentation and study of environmental consequences before the fact of development actions.[citation needed]

One can examine the specifics of environmental science by reading examples of Environmental Impact Statements prepared under NEPA such as: Wastewater treatment expansion options discharging into the San Diego/Tijuana Estuary, Expansion of the San Francisco International Airport, Development of the Houston, Metro Transportation system, Expansion of the metropolitan Boston MBTA transit system, and Construction of Interstate 66 through Arlington, Virginia.[citation needed]

In England and Wales the Environment Agency (EA),[8] formed in 1996, is a public body for protecting and improving the environment and enforces the regulations listed on the communities and local government site.[9] (formerly the office of the deputy prime minister). The agency was set up under the Environment Act 1995 as an independent body and works closely with UK Government to enforce the regulations.

See also [ edit ]

References [ edit ]"	https://en.wikipedia.org/wiki/Environmental_science	"The integrated, quantitative, and interdisciplinary approach to the study of environmental systems.
For the TV series episode, see Environmental Science (Community)
Not to be confused with Environmental studies
""Environmental research"" redirects here. For the Elsevier journal, see Environmental Research
Environmental science is an interdisciplinary academic field that integrates physical, biological and information sciences (including ecology, biology, physics, chemistry, plant science, zoology, mineralogy, oceanography, limnology, soil science, geology and physical geography, and atmospheric science) to the study of the environment, and the solution of environmental problems. Environmental science emerged from the fields of natural history and medicine during the Enlightenment. Today it provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems.
Environmental studies incorporates more of the social sciences for understanding human relationships, perceptions and policies towards the environment. Environmental engineering focuses on design and technology for improving environmental quality in every aspect.
Environmental scientists study subjects like the understanding of earth processes, evaluating alternative energy systems, pollution control and mitigation, natural resource management, and the effects of global climate change. Environmental issues almost always include an interaction of physical, chemical, and biological processes. Environmental scientists bring a systems approach to the analysis of environmental problems. Key elements of an effective environmental scientist include the ability to relate space, and time relationships as well as quantitative analysis.
Environmental science came alive as a substantive, active field of scientific investigation in the 1960s and 1970s driven by (a) the need for a multi-disciplinary approach to analyze complex environmental problems, (b) the arrival of substantive environmental laws requiring specific environmental protocols of investigation and (c) the growing public awareness of a need for action in addressing environmental problems. Events that spurred this development included the publication of Rachel Carson's landmark environmental book Silent Spring along with major environmental issues becoming very public, such as the 1969 Santa Barbara oil spill, and the Cuyahoga River of Cleveland, Ohio, ""catching fire"" (also in 1969), and helped increase the visibility of environmental issues and create this new field of study.
Terminology 
In common usage, ""environmental science"" and ""ecology"" are often used interchangeably, but technically, ecology refers only to the study of organisms and their interactions with each other as well as how they interrelate with environment. Ecology could be considered a subset of environmental science, which also could involve purely chemical or public health issues (for example) ecologists would be unlikely to study. In practice, there are considerable similarities between the work of ecologists and other environmental scientists. There is substantial overlap between ecology and environmental science with the disciplines of fisheries, forestry, and wildlife.
Components 
Blue Marble composite images generated by NASA in 2001 (left) and 2002 (right)
Atmospheric sciences 
Atmospheric sciences focus on the Earth's atmosphere, with an emphasis upon its interrelation to other systems. Atmospheric sciences can include studies of meteorology, greenhouse gas phenomena, atmospheric dispersion modeling of airborne contaminants, sound propagation phenomena related to noise pollution, and even light pollution.
Taking the example of the global warming phenomena, physicists create computer models of atmospheric circulation and infrared radiation transmission, chemists examine the inventory of atmospheric chemicals and their reactions, biologists analyze the plant and animal contributions to carbon dioxide fluxes, and specialists such as meteorologists and oceanographers add additional breadth in understanding the atmospheric dynamics.
Ecology 
Biodiversity of a coral reef. Corals adapt and modify their environment by forming calcium carbonate skeletons. This provides growing conditions for future generations and forms a habitat for many other species.
As defined by the Ecological Society of America, ""Ecology is the study of the relationships between living organisms, including humans, and their physical environment; it seeks to understand the vital connections between plants and animals and the world around them."" Ecologists might investigate the relationship between a population of organisms and some physical characteristic of their environment, such as concentration of a chemical; or they might investigate the interaction between two populations of different organisms through some symbiotic or competitive relationship. For example, an interdisciplinary analysis of an ecological system which is being impacted by one or more stressors might include several related environmental science fields. In an estuarine setting where a proposed industrial development could impact certain species by water and air pollution, biologists would describe the flora and fauna, chemists would analyze the transport of water pollutants to the marsh, physicists would calculate air pollution emissions and geologists would assist in understanding the marsh soils and bay muds.
Environmental chemistry 
Environmental chemistry is the study of chemical alterations in the environment. Principal areas of study include soil contamination and water pollution. The topics of analysis include chemical degradation in the environment, multi-phase transport of chemicals (for example, evaporation of a solvent containing lake to yield solvent as an air pollutant), and chemical effects upon biota.
As an example study, consider the case of a leaking solvent tank which has entered the habitat soil of an endangered species of amphibian. As a method to resolve or understand the extent of soil contamination and subsurface transport of solvent, a computer model would be implemented. Chemists would then characterize the molecular bonding of the solvent to the specific soil type, and biologists would study the impacts upon soil arthropods, plants, and ultimately pond-dwelling organisms that are the food of the endangered amphibian.
Geosciences 
Geosciences include environmental geology, environmental soil science, volcanic phenomena and evolution of the Earth's crust. In some classification systems this can also include hydrology, including oceanography.
As an example study, of soils erosion, calculations would be made of surface runoff by soil scientists. Fluvial geomorphologists would assist in examining sediment transport in overland flow. Physicists would contribute by assessing the changes in light transmission in the receiving waters. Biologists would analyze subsequent impacts to aquatic flora and fauna from increases in water turbidity.
Regulations driving the studies 
Environmental science examines the effects of humans on nature, such as the Glen Canyon Dam in the United States
In the United States the National Environmental Policy Act (NEPA) of 1969 set forth requirements for analysis of federal government actions (such as highway construction projects and land management decisions) in terms of specific environmental criteria. Numerous state laws have echoed these mandates, applying the principles to local-scale actions. The upshot has been an explosion of documentation and study of environmental consequences before the fact of development actions.
One can examine the specifics of environmental science by reading examples of Environmental Impact Statements prepared under NEPA such as: Wastewater treatment expansion options discharging into the San Diego/Tijuana Estuary, Expansion of the San Francisco International Airport, Development of the Houston, Metro Transportation system, Expansion of the metropolitan Boston MBTA transit system, and Construction of Interstate 66 through Arlington, Virginia.
In England and Wales the Environment Agency (EA), formed in 1996, is a public body for protecting and improving the environment and enforces the regulations listed on the communities and local government site. (formerly the office of the deputy prime minister). The agency was set up under the Environment Act 1995 as an independent body and works closely with UK Government to enforce the regulations."	8524
geography	[]		"Design of outdoor public areas, landmarks, and structures to achieve environmental, social-behavioral, or aesthetic outcomes

Landscape architecture is the design of outdoor areas, landmarks, and structures to achieve environmental, social-behavioural, or aesthetic outcomes.[2] It involves the systematic design and general engineering of various structures for construction and human use, investigation of existing social, ecological, and soil conditions and processes in the landscape, and the design of other interventions that will produce desired outcomes. The scope of the profession is broad and can be subdivided into several sub-categories including professional or licensed landscape architects who are regulated by governmental agencies and possess the expertise to design a wide range of structures and landforms for human use; landscape design which is not a licensed profession; site planning; stormwater management; erosion control; environmental restoration; parks, recreation and urban planning; visual resource management; green infrastructure planning and provision; and private estate and residence landscape master planning and design; all at varying scales of design, planning and management. A practitioner in the profession of landscape architecture may be called a landscape architect, however in jurisdictions where professional licenses are required it is often only those who possess a landscape architect license who can be called a landscape architect.

Definition of landscape architecture [ edit ]

Landscape architecture is a multi-disciplinary field, incorporating aspects of urban design, architecture, geography, ecology, civil engineering, structural engineering, horticulture, environmental psychology, industrial design, soil sciences, botany, and fine arts. The activities of a landscape architect can range from the creation of public parks and parkways to site planning for campuses and corporate office parks; from the design of residential estates to the design of civil infrastructure; and from the management of large wilderness areas to reclamation of degraded landscapes such as mines or landfills. Landscape architects work on structures and external spaces in the landscape aspect of the design – large or small, urban, suburban and rural, and with ""hard"" (built) and ""soft"" (planted) materials, while integrating ecological sustainability. The most valuable contribution can be made at the first stage of a project to generate ideas with technical understanding and creative flair for the design, organization, and use of spaces. The landscape architect can conceive the overall concept and prepare the master plan, from which detailed design drawings and technical specifications are prepared. They can also review proposals to authorize and supervise contracts for the construction work. Other skills include preparing design impact assessments, conducting environmental assessments and audits, and serving as an expert witness at inquiries on land use issues. The majority of their time will most likely be spent inside an office building designing and preparing models for clients.[citation needed]

Education [ edit ]

Landscape Architects are required to take four to five years of college to get a degree in the field (MLA). They learn how to create projects from scratch, such as residential or commercial planting and designing outdoor living spacesl[3] they are willing to work with others to get a better outcome for the customers when doing a project; they will have to learn the basics of how to create a project on a manner of time and will require to get your license in a certain state to be allowed to work; students of Landscape Architects will learn how to interact with clients and will learn how to explain a design from scratch when giving the final project.[4]

Landscape architecture has been taught in the University of Manchester since the 1950s. The course in the Manchester School of Architecture enables students to gain various bachelor's and master's degrees, including MLPM(Hons) which is accredited by the Landscape Institute and by the Royal Town Planning Institute.[5]

History [ edit ]

Orangery at the Palace of Versailles, outside Paris

For the period before 1800, the history of landscape gardening (later called landscape architecture) is largely that of master planning and garden design for manor houses, palaces and royal properties, religious complexes, and centers of government. An example is the extensive work by André Le Nôtre for King Louis XIV of France at the Palace of Versailles. The first person to write of making a landscape was Joseph Addison in 1712. The term landscape architecture was invented by Gilbert Laing Meason in 1828, and John Claudius Loudon (1783–1843) was instrumental in the adoption of the term landscape architecture by the modern profession. He took up the term from Meason and gave it publicity in his Encyclopedias and in his 1840 book on the Landscape Gardening and Landscape Architecture of the Late Humphry Repton.[6]

The practice of landscape architecture spread from the Old to the New World. The term ""landscape architect"" was used as a professional title by Frederick Law Olmsted in the United States in 1863[citation needed] and Andrew Jackson Downing, another early American landscape designer, was editor of The Horticulturist magazine (1846–52). In 1841 his first book, A Treatise on the Theory and Practice of Landscape Gardening, Adapted to North America, was published to a great success; it was the first book of its kind published in the United States.[7] During the latter 19th century, the term landscape architect began to be used by professional landscapes designers, and was firmly established after Frederick Law Olmsted, Jr. and Beatrix Jones (later Farrand) with others founded the American Society of Landscape Architects (ASLA) in 1899. IFLA was founded at Cambridge, England, in 1948 with Sir Geoffrey Jellicoe as its first president, representing 15 countries from Europe and North America. Later, in 1978, IFLA's Headquarters were established in Versailles.[8][9][10]

Fields of activity [ edit ]

Urban design in city squares. Water feature in London, by Tadao Ando who also works with landscapes and gardens

The variety of the professional tasks that landscape architects collaborate on is very broad, but some examples of project types include:[11]

Landscape managers use their knowledge of landscape processes to advise on the long-term care and development of the landscape. They often work in forestry, nature conservation and agriculture.[citation needed]

Landscape scientists have specialist skills such as soil science, hydrology, geomorphology or botany that they relate to the practical problems of landscape work. Their projects can range from site surveys to the ecological assessment of broad areas for planning or management purposes. They may also report on the impact of development or the importance of particular species in a given area.[citation needed]

Landscape planners are concerned with landscape planning for the location, scenic, ecological and recreational aspects of urban, rural, and coastal land use. Their work is embodied in written statements of policy and strategy, and their remit includes master planning for new developments, landscape evaluations and assessments, and preparing countryside management or policy plans. Some may also apply an additional specialism such as landscape archaeology or law to the process of landscape planning.[citation needed]

Green roof (or more specifically, vegetative roof) designers design extensive and intensive roof gardens for storm water management, evapo-transpirative cooling, sustainable architecture, aesthetics, and habitat creation.[12] Therapeutic/ Healing gardens are also a part of rooftop gardens. These types of gardens have proven to help reduce stress, anxiety, and even help hospital patients recover faster. Plants have always been the center of a landscape design and now with these types of gardens, it is proof that nature is a vital part of the well-being of humans. In recent years the need and interest of therapeutic gardens have been increasingly rising. The topic was vastly known and a main reason for this was because there was nothing written on the topic. Recently there has been a lot of research done and published on the topic and as a result, the increase of interest for therapeutic gardens has spiked.[citation needed]

Relation to urban planning [ edit ]

The combination of the traditional landscape gardening and the emerging city planning combined together gave landscape architecture its unique focus. Frederick Law Olmsted used the term 'landscape architecture' using the word as a profession for the first time when designing the Central Park

Through the 19th century, urban planning became a focal point and central issue in cities. The combination of the tradition of landscape gardening and the emerging field of urban planning offered landscape architecture an opportunity to serve these needs.[13] In the second half of the century, Frederick Law Olmsted completed a series of parks that continue to have a significant influence on the practices of landscape architecture today. Among these were Central Park in New York City, Prospect Park in Brooklyn, New York and Boston's Emerald Necklace park system. Jens Jensen designed sophisticated and naturalistic urban and regional parks for Chicago, Illinois, and private estates for the Ford family including Fair Lane and Gaukler Point. One of the original eleven founding members of the American Society of Landscape Architects (ASLA), and the only woman, was Beatrix Farrand. She was design consultant for over a dozen universities including: Princeton in Princeton, New Jersey; Yale in New Haven, Connecticut; and the Arnold Arboretum for Harvard in Boston, Massachusetts. Her numerous private estate projects include the landmark Dumbarton Oaks in the Georgetown neighborhood of Washington, D.C..[14] Since that time, other architects – most notably Ruth Havey and Alden Hopkins – changed certain elements of the Farrand design.[citation needed]

Since this period urban planning has developed into a separate independent profession that has incorporated important contributions from other fields such as civil engineering, architecture and public administration. Urban Planners are qualified to perform tasks independent of landscape architects, and in general, the curriculum of landscape architecture programs do not prepare students to become urban planners.[15]

Landscape architecture continues to develop as a design discipline and to respond to the various movements in architecture and design throughout the 20th and 21st centuries. Thomas Church was a mid-century landscape architect significant in the profession. Roberto Burle Marx in Brazil combined the International style and native Brazilian plants and culture for a new aesthetic. Innovation continues today solving challenging problems with contemporary design solutions for master planning, landscapes, and gardens.[citation needed]

Ian McHarg was known for introducing environmental concerns in landscape architecture.[16][17] He popularized a system of analyzing the layers of a site in order to compile a complete understanding of the qualitative attributes of a place. This system became the foundation of today's Geographic Information Systems (GIS). McHarg would give every qualitative aspect of the site a layer, such as the history, hydrology, topography, vegetation, etc. GIS software is ubiquitously used in the landscape architecture profession today to analyze materials in and on the Earth's surface and is similarly used by urban planners, geographers, forestry and natural resources professionals, etc.[citation needed]

Profession [ edit ]

In many countries, a professional institute, comprising members of the professional community, exists in order to protect the standing of the profession and promote its interests, and sometimes also regulate the practice of landscape architecture. The standard and strength of legal regulations governing landscape architecture practice varies from nation to nation, with some requiring licensure in order to practice; and some having little or no regulation. In Europe, North America, parts of South America, Australia, India, and New Zealand, landscape architecture is a regulated profession.[18]

Argentina [ edit ]

Since 1889, with the arrival of the French architect and urbanist landscaper Carlos Thays, recommended to recreate the National Capital's parks and public gardens, it was consolidated an apprentice and training program in landscaping that eventually became a regulated profession, currently the leading academic institution is the UBA University of Buenos Aires""UBA Facultad de Arquitectura, Diseño y Urbanismo"" (Faculty of Architecture, Design and Urbanism) offering a Bacherlor's degree in Urban Landscaping Design and Planning, the profession itself is regulated by the National Ministry of Urban Planning of Argentina and the Institute of the Buenos Aires Botanical Garden.[citation needed]

Australia [ edit ]

The Australian Institute of Landscape Architects (AILA) provides accreditation of university degrees and non-statutory professional registration for landscape architects. Once recognized by AILA, landscape architects use the title 'Registered Landscape Architect' across the six states and territories within Australia.[citation needed]

AILA's system of professional recognition is a national system overseen by the AILA National Office in Canberra. To apply for AILA Registration, an applicant usually needs to satisfy a number of pre-requisites, including university qualification, a minimum number years of practice and a record of professional experience.[19]

Landscape Architecture within Australia covers a broad spectrum of planning, design, management, and research. From specialist design services for government and private sector developments through to specialist professional advice as an expert witness.[citation needed]

Canada [ edit ]

In Canada, landscape architecture, like law and medicine, is a self-regulating profession pursuant to provincial statute. For example, Ontario's profession is governed by the Ontario Association of Landscape Architects pursuant to the Ontario Association of Landscape Architects Act. Landscape architects in Ontario, British Columbia, and Alberta must complete the specified components of L.A.R.E (Landscape Architecture Registration Examination) as a prerequisite to full professional standing.

Provincial regulatory bodies are members of a national organization, the Canadian Society of Landscape Architects / L'Association des Architectes Paysagistes du Canada (CSLA-AAPC), and individual membership in the CSLA-AAPC is obtained through joining one of the provincial or territorial components.[20]

Indonesia [ edit ]

ISLA (Indonesia Society of Landscape Architects) is the Indonesian society for professional landscape architects formed on 4 February 1978 and is a member of IFLA APR and IFLA World. The main aim is to increase the dignity of the professional members of landscape architects by increasing their activity role in community service, national and international development. The management of IALI consists of National Administrators who are supported by 20 Regional Administrators (Provincial level) and 3 Branch Managers at city level throughout Indonesia.[citation needed]

Landscape architecture education in Indonesia was held in 18 universities, which graduated D3, Bachelor and Magister graduates. The landscape architecture education incorporate in Association of Indonesian Landscape Architecture Education.[citation needed]

Italy [ edit ]

AIAPP (Associazione Italiana Architettura del Paesaggio) is the Italian association of professional landscape architects formed in 1950 and is a member of IFLA and IFLA Europe (formerly known as EFLA). AIAPP is in the process of contesting this new law which has given the Architects' Association the new title of Architects, Landscape Architects, Planners and Conservationists whether or not they have had any training or experience in any of these fields other than Architecture. In Italy, there are several different professions involved in landscape architecture:

Architects

Landscape designers

Doctor landscape agronomists and Doctor landscape foresters, often called Landscape agronomists.

Agrarian Experts and Graduated Agrarian experts.

New Zealand [ edit ]

The New Zealand Institute of Landscape Architects (NZILA) is the professional body for Landscape Architects in NZ.[21]

In April 2013, NZILA jointly with AILA, hosted the 50th International Federation of Landscape Architects (IFLA) World Congress in Auckland, New Zealand. The World Congress is an international conference where Landscape Architects from all around the globe meet to share ideas around a particular topic.[citation needed]

Within NZ, Members of NZILA when they achieve their professional standing, can use the title Registered Landscape Architect NZILA.[citation needed]

NZILA provides an education policy and an accreditation process to review education programme providers; currently there are three accredited undergraduate Landscape Architecture programmes in New Zealand. Lincoln University also has an accredited masters programme in landscape architecture.[citation needed]

Norway [ edit ]

Landscape architecture in Norway was established in 1919 at the Norwegian University of Life Sciences (NMBU) at Ås. The Norwegian School of Landscape Architecture at the Faculty of Landscape and Society is responsible for Europe's oldest landscape architecture education on an academic level. The departments areas include design and design of cities and places, garden art history, landscape engineering, greenery, zone planning, site development, place making and place keeping.[citation needed]

South Africa [ edit ]

In May 1962, Joane Pim, Ann Sutton, Peter Leutscher and Roelf Botha (considered the forefathers of the profession in South Africa) established the Institute for Landscape Architects, now known as the Institute for Landscape Architecture in South Africa (ILASA).[22] ILASA is a voluntary organisation registered with the South African Council for the Landscape Architectural Profession (SACLAP).[23] It consists of three regional bodies, namely, Gauteng, KwaZula-Natal and the Western Cape. ILASA's mission is to advance the profession of landscape architecture and uphold high standards of professional service to its members, and to represent the profession of landscape architecture in any matter which may affect the interests of the members of the institute. ILASA holds the country's membership with The International Federation of Landscape Architects (IFLA).[24]

In South Africa, the profession is regulated by SACLAP, established as a statutory council in terms of Section 2 of the South African Council for the Landscape Architectural Profession Act – Act 45 of 2000. The Council evolved out of the Board of Control for Landscape Architects (BOCLASA), which functioned under the Council of Architects in terms of The Architectural Act, Act 73 of 1970. SACLAP's mission is to establish, direct, sustain and ensure a high level of professional responsibilities and ethical conduct within the art and science of landscape architecture with honesty, dignity and integrity in the broad interest of public health, safety and welfare of the community.[citation needed]

After completion of an accredited under-graduate and/or post-graduate qualification in landscape architecture at either the University of Cape Town or the University of Pretoria, or landscape technology at the Cape Peninsula University of Technology, professional registration is attained via a mandatory mentored candidacy period (minimum of two years) and sitting of the professional registration exam. After successfully completing the exam, the individual is entitled to the status of Professional Landscape Architect or Professional Landscape Technologist.[citation needed]

United Kingdom [ edit ]

The UK's professional body is the Landscape Institute (LI). It is a chartered body that accredits landscape professionals and university courses. At present there are fifteen accredited programmes in the UK. Membership of the LI is available to students, academics and professionals, and there are over 3,000 professionally qualified members.[citation needed]

The Institute provides services to assist members including support and promotion of the work of landscape architects; information and guidance to the public and industry about the specific expertise offered by those in the profession; and training and educational advice to students and professionals looking to build upon their experience.[citation needed]

In 2008, the LI launched a major recruitment drive entitled ""I want to be a Landscape Architect"" to encourage the study of Landscape Architecture. The campaign aimed to raise the profile of landscape architecture and highlight its valuable role in building sustainable communities and fighting climate change.[25]

As of July 2018, the ""I want to be a Landscape Architect"" initiative was replaced by a brand new careers campaign entitled #ChooseLandscape, which aims to raise awareness of landscape as a profession; improve and increase access to landscape education; and inspire young people to choose landscape as a career.[26] This new campaign includes other landscape-related professions such as landscape management, landscape planning, landscape science and urban design.[27]

United States [ edit ]

In the United States, Landscape Architecture is regulated by individual state governments. For a landscape architect, obtaining licensure requires advanced education and work experience, plus passage of the national examination called The Landscape Architect Registration Examination (L.A.R.E.). Several states require passage of a state exam as well. In the United States licensing is overseen both at the state level, and nationally by the Council of Landscape Architectural Registration Boards (CLARB). Landscape architecture has been identified as an above-average growth profession by the US Bureau of Labor Statistics and was listed in U.S. News & World Report's list of Best Jobs to Have in 2006, 2007, 2008, 2009 and 2010.[28] The national trade association for United States landscape architects is the American Society of Landscape Architects. Frederic Law Olmsted, who designed Central Park in New York City, is known as the ""father of American Landscape Architecture"".[29]

Examples [ edit ]

See also [ edit ]

References [ edit ]"	https://en.wikipedia.org/wiki/Landscape_architecture	"Design of outdoor public areas, landmarks, and structures to achieve environmental, social-behavioral, or aesthetic outcomes
Landscape architecture is the design of outdoor areas, landmarks, and structures to achieve environmental, social-behavioural, or aesthetic outcomes. It involves the systematic design and general engineering of various structures for construction and human use, investigation of existing social, ecological, and soil conditions and processes in the landscape, and the design of other interventions that will produce desired outcomes. The scope of the profession is broad and can be subdivided into several sub-categories including professional or licensed landscape architects who are regulated by governmental agencies and possess the expertise to design a wide range of structures and landforms for human use; landscape design which is not a licensed profession; site planning; stormwater management; erosion control; environmental restoration; parks, recreation and urban planning; visual resource management; green infrastructure planning and provision; and private estate and residence landscape master planning and design; all at varying scales of design, planning and management. A practitioner in the profession of landscape architecture may be called a landscape architect, however in jurisdictions where professional licenses are required it is often only those who possess a landscape architect license who can be called a landscape architect.
Definition of landscape architecture 
Landscape architecture is a multi-disciplinary field, incorporating aspects of urban design, architecture, geography, ecology, civil engineering, structural engineering, horticulture, environmental psychology, industrial design, soil sciences, botany, and fine arts. The activities of a landscape architect can range from the creation of public parks and parkways to site planning for campuses and corporate office parks; from the design of residential estates to the design of civil infrastructure; and from the management of large wilderness areas to reclamation of degraded landscapes such as mines or landfills. Landscape architects work on structures and external spaces in the landscape aspect of the design – large or small, urban, suburban and rural, and with ""hard"" (built) and ""soft"" (planted) materials, while integrating ecological sustainability. The most valuable contribution can be made at the first stage of a project to generate ideas with technical understanding and creative flair for the design, organization, and use of spaces. The landscape architect can conceive the overall concept and prepare the master plan, from which detailed design drawings and technical specifications are prepared. They can also review proposals to authorize and supervise contracts for the construction work. Other skills include preparing design impact assessments, conducting environmental assessments and audits, and serving as an expert witness at inquiries on land use issues. The majority of their time will most likely be spent inside an office building designing and preparing models for clients.
Education 
Landscape Architects are required to take four to five years of college to get a degree in the field (MLA). They learn how to create projects from scratch, such as residential or commercial planting and designing outdoor living spacesl they are willing to work with others to get a better outcome for the customers when doing a project; they will have to learn the basics of how to create a project on a manner of time and will require to get your license in a certain state to be allowed to work; students of Landscape Architects will learn how to interact with clients and will learn how to explain a design from scratch when giving the final project.
Landscape architecture has been taught in the University of Manchester since the 1950s. The course in the Manchester School of Architecture enables students to gain various bachelor's and master's degrees, including MLPM(Hons) which is accredited by the Landscape Institute and by the Royal Town Planning Institute.
History 
Orangery at the Palace of Versailles, outside Paris
For the period before 1800, the history of landscape gardening (later called landscape architecture) is largely that of master planning and garden design for manor houses, palaces and royal properties, religious complexes, and centers of government. An example is the extensive work by André Le Nôtre for King Louis XIV of France at the Palace of Versailles. The first person to write of making a landscape was Joseph Addison in 1712. The term landscape architecture was invented by Gilbert Laing Meason in 1828, and John Claudius Loudon (1783–1843) was instrumental in the adoption of the term landscape architecture by the modern profession. He took up the term from Meason and gave it publicity in his Encyclopedias and in his 1840 book on the Landscape Gardening and Landscape Architecture of the Late Humphry Repton.
The practice of landscape architecture spread from the Old to the New World. The term ""landscape architect"" was used as a professional title by Frederick Law Olmsted in the United States in 1863 and Andrew Jackson Downing, another early American landscape designer, was editor of The Horticulturist magazine (1846–52). In 1841 his first book, A Treatise on the Theory and Practice of Landscape Gardening, Adapted to North America, was published to a great success; it was the first book of its kind published in the United States. During the latter 19th century, the term landscape architect began to be used by professional landscapes designers, and was firmly established after Frederick Law Olmsted, Jr. and Beatrix Jones (later Farrand) with others founded the American Society of Landscape Architects (ASLA) in 1899. IFLA was founded at Cambridge, England, in 1948 with Sir Geoffrey Jellicoe as its first president, representing 15 countries from Europe and North America. Later, in 1978, IFLA's Headquarters were established in Versailles.
Fields of activity 
Urban design in city squares. Water feature in London, by Tadao Ando who also works with landscapes and gardens
The variety of the professional tasks that landscape architects collaborate on is very broad, but some examples of project types include:
Landscape managers use their knowledge of landscape processes to advise on the long-term care and development of the landscape. They often work in forestry, nature conservation and agriculture.
Landscape scientists have specialist skills such as soil science, hydrology, geomorphology or botany that they relate to the practical problems of landscape work. Their projects can range from site surveys to the ecological assessment of broad areas for planning or management purposes. They may also report on the impact of development or the importance of particular species in a given area.
Landscape planners are concerned with landscape planning for the location, scenic, ecological and recreational aspects of urban, rural, and coastal land use. Their work is embodied in written statements of policy and strategy, and their remit includes master planning for new developments, landscape evaluations and assessments, and preparing countryside management or policy plans. Some may also apply an additional specialism such as landscape archaeology or law to the process of landscape planning.
Green roof (or more specifically, vegetative roof) designers design extensive and intensive roof gardens for storm water management, evapo-transpirative cooling, sustainable architecture, aesthetics, and habitat creation. Therapeutic/ Healing gardens are also a part of rooftop gardens. These types of gardens have proven to help reduce stress, anxiety, and even help hospital patients recover faster. Plants have always been the center of a landscape design and now with these types of gardens, it is proof that nature is a vital part of the well-being of humans. In recent years the need and interest of therapeutic gardens have been increasingly rising. The topic was vastly known and a main reason for this was because there was nothing written on the topic. Recently there has been a lot of research done and published on the topic and as a result, the increase of interest for therapeutic gardens has spiked.
Relation to urban planning 
The combination of the traditional landscape gardening and the emerging city planning combined together gave landscape architecture its unique focus. Frederick Law Olmsted used the term 'landscape architecture' using the word as a profession for the first time when designing the Central Park
Through the 19th century, urban planning became a focal point and central issue in cities. The combination of the tradition of landscape gardening and the emerging field of urban planning offered landscape architecture an opportunity to serve these needs. In the second half of the century, Frederick Law Olmsted completed a series of parks that continue to have a significant influence on the practices of landscape architecture today. Among these were Central Park in New York City, Prospect Park in Brooklyn, New York and Boston's Emerald Necklace park system. Jens Jensen designed sophisticated and naturalistic urban and regional parks for Chicago, Illinois, and private estates for the Ford family including Fair Lane and Gaukler Point. One of the original eleven founding members of the American Society of Landscape Architects (ASLA), and the only woman, was Beatrix Farrand. She was design consultant for over a dozen universities including: Princeton in Princeton, New Jersey; Yale in New Haven, Connecticut; and the Arnold Arboretum for Harvard in Boston, Massachusetts. Her numerous private estate projects include the landmark Dumbarton Oaks in the Georgetown neighborhood of Washington, D.C.. Since that time, other architects – most notably Ruth Havey and Alden Hopkins – changed certain elements of the Farrand design.
Since this period urban planning has developed into a separate independent profession that has incorporated important contributions from other fields such as civil engineering, architecture and public administration. Urban Planners are qualified to perform tasks independent of landscape architects, and in general, the curriculum of landscape architecture programs do not prepare students to become urban planners.
Landscape architecture continues to develop as a design discipline and to respond to the various movements in architecture and design throughout the 20th and 21st centuries. Thomas Church was a mid-century landscape architect significant in the profession. Roberto Burle Marx in Brazil combined the International style and native Brazilian plants and culture for a new aesthetic. Innovation continues today solving challenging problems with contemporary design solutions for master planning, landscapes, and gardens.
Ian McHarg was known for introducing environmental concerns in landscape architecture. He popularized a system of analyzing the layers of a site in order to compile a complete understanding of the qualitative attributes of a place. This system became the foundation of today's Geographic Information Systems (GIS). McHarg would give every qualitative aspect of the site a layer, such as the history, hydrology, topography, vegetation, etc. GIS software is ubiquitously used in the landscape architecture profession today to analyze materials in and on the Earth's surface and is similarly used by urban planners, geographers, forestry and natural resources professionals, etc.
Profession 
In many countries, a professional institute, comprising members of the professional community, exists in order to protect the standing of the profession and promote its interests, and sometimes also regulate the practice of landscape architecture. The standard and strength of legal regulations governing landscape architecture practice varies from nation to nation, with some requiring licensure in order to practice; and some having little or no regulation. In Europe, North America, parts of South America, Australia, India, and New Zealand, landscape architecture is a regulated profession.
Argentina 
Since 1889, with the arrival of the French architect and urbanist landscaper Carlos Thays, recommended to recreate the National Capital's parks and public gardens, it was consolidated an apprentice and training program in landscaping that eventually became a regulated profession, currently the leading academic institution is the UBA University of Buenos Aires""UBA Facultad de Arquitectura, Diseño y Urbanismo"" (Faculty of Architecture, Design and Urbanism) offering a Bacherlor's degree in Urban Landscaping Design and Planning, the profession itself is regulated by the National Ministry of Urban Planning of Argentina and the Institute of the Buenos Aires Botanical Garden.
Australia 
The Australian Institute of Landscape Architects (AILA) provides accreditation of university degrees and non-statutory professional registration for landscape architects. Once recognized by AILA, landscape architects use the title 'Registered Landscape Architect' across the six states and territories within Australia.
AILA's system of professional recognition is a national system overseen by the AILA National Office in Canberra. To apply for AILA Registration, an applicant usually needs to satisfy a number of pre-requisites, including university qualification, a minimum number years of practice and a record of professional experience.
Landscape Architecture within Australia covers a broad spectrum of planning, design, management, and research. From specialist design services for government and private sector developments through to specialist professional advice as an expert witness.
Canada 
In Canada, landscape architecture, like law and medicine, is a self-regulating profession pursuant to provincial statute. For example, Ontario's profession is governed by the Ontario Association of Landscape Architects pursuant to the Ontario Association of Landscape Architects Act. Landscape architects in Ontario, British Columbia, and Alberta must complete the specified components of L.A.R.E (Landscape Architecture Registration Examination) as a prerequisite to full professional standing.
Provincial regulatory bodies are members of a national organization, the Canadian Society of Landscape Architects / L'Association des Architectes Paysagistes du Canada (CSLA-AAPC), and individual membership in the CSLA-AAPC is obtained through joining one of the provincial or territorial components.
Indonesia 
ISLA (Indonesia Society of Landscape Architects) is the Indonesian society for professional landscape architects formed on 4 February 1978 and is a member of IFLA APR and IFLA World. The main aim is to increase the dignity of the professional members of landscape architects by increasing their activity role in community service, national and international development. The management of IALI consists of National Administrators who are supported by 20 Regional Administrators (Provincial level) and 3 Branch Managers at city level throughout Indonesia.
Landscape architecture education in Indonesia was held in 18 universities, which graduated D3, Bachelor and Magister graduates. The landscape architecture education incorporate in Association of Indonesian Landscape Architecture Education.
Italy 
AIAPP (Associazione Italiana Architettura del Paesaggio) is the Italian association of professional landscape architects formed in 1950 and is a member of IFLA and IFLA Europe (formerly known as EFLA). AIAPP is in the process of contesting this new law which has given the Architects' Association the new title of Architects, Landscape Architects, Planners and Conservationists whether or not they have had any training or experience in any of these fields other than Architecture. In Italy, there are several different professions involved in landscape architecture:
Architects
Landscape designers
Doctor landscape agronomists and Doctor landscape foresters, often called Landscape agronomists.
Agrarian Experts and Graduated Agrarian experts.
New Zealand 
The New Zealand Institute of Landscape Architects (NZILA) is the professional body for Landscape Architects in NZ.
In April 2013, NZILA jointly with AILA, hosted the 50th International Federation of Landscape Architects (IFLA) World Congress in Auckland, New Zealand. The World Congress is an international conference where Landscape Architects from all around the globe meet to share ideas around a particular topic.
Within NZ, Members of NZILA when they achieve their professional standing, can use the title Registered Landscape Architect NZILA.
NZILA provides an education policy and an accreditation process to review education programme providers; currently there are three accredited undergraduate Landscape Architecture programmes in New Zealand. Lincoln University also has an accredited masters programme in landscape architecture.
Norway 
Landscape architecture in Norway was established in 1919 at the Norwegian University of Life Sciences (NMBU) at Ås. The Norwegian School of Landscape Architecture at the Faculty of Landscape and Society is responsible for Europe's oldest landscape architecture education on an academic level. The departments areas include design and design of cities and places, garden art history, landscape engineering, greenery, zone planning, site development, place making and place keeping.
South Africa 
In May 1962, Joane Pim, Ann Sutton, Peter Leutscher and Roelf Botha (considered the forefathers of the profession in South Africa) established the Institute for Landscape Architects, now known as the Institute for Landscape Architecture in South Africa (ILASA). ILASA is a voluntary organisation registered with the South African Council for the Landscape Architectural Profession (SACLAP). It consists of three regional bodies, namely, Gauteng, KwaZula-Natal and the Western Cape. ILASA's mission is to advance the profession of landscape architecture and uphold high standards of professional service to its members, and to represent the profession of landscape architecture in any matter which may affect the interests of the members of the institute. ILASA holds the country's membership with The International Federation of Landscape Architects (IFLA).
In South Africa, the profession is regulated by SACLAP, established as a statutory council in terms of Section 2 of the South African Council for the Landscape Architectural Profession Act – Act 45 of 2000. The Council evolved out of the Board of Control for Landscape Architects (BOCLASA), which functioned under the Council of Architects in terms of The Architectural Act, Act 73 of 1970. SACLAP's mission is to establish, direct, sustain and ensure a high level of professional responsibilities and ethical conduct within the art and science of landscape architecture with honesty, dignity and integrity in the broad interest of public health, safety and welfare of the community.
After completion of an accredited under-graduate and/or post-graduate qualification in landscape architecture at either the University of Cape Town or the University of Pretoria, or landscape technology at the Cape Peninsula University of Technology, professional registration is attained via a mandatory mentored candidacy period (minimum of two years) and sitting of the professional registration exam. After successfully completing the exam, the individual is entitled to the status of Professional Landscape Architect or Professional Landscape Technologist.
United Kingdom 
The UK's professional body is the Landscape Institute (LI). It is a chartered body that accredits landscape professionals and university courses. At present there are fifteen accredited programmes in the UK. Membership of the LI is available to students, academics and professionals, and there are over 3,000 professionally qualified members.
The Institute provides services to assist members including support and promotion of the work of landscape architects; information and guidance to the public and industry about the specific expertise offered by those in the profession; and training and educational advice to students and professionals looking to build upon their experience.
In 2008, the LI launched a major recruitment drive entitled ""I want to be a Landscape Architect"" to encourage the study of Landscape Architecture. The campaign aimed to raise the profile of landscape architecture and highlight its valuable role in building sustainable communities and fighting climate change.
As of July 2018, the ""I want to be a Landscape Architect"" initiative was replaced by a brand new careers campaign entitled #ChooseLandscape, which aims to raise awareness of landscape as a profession; improve and increase access to landscape education; and inspire young people to choose landscape as a career. This new campaign includes other landscape-related professions such as landscape management, landscape planning, landscape science and urban design.
United States 
In the United States, Landscape Architecture is regulated by individual state governments. For a landscape architect, obtaining licensure requires advanced education and work experience, plus passage of the national examination called The Landscape Architect Registration Examination (L.A.R.E.). Several states require passage of a state exam as well. In the United States licensing is overseen both at the state level, and nationally by the Council of Landscape Architectural Registration Boards (CLARB). Landscape architecture has been identified as an above-average growth profession by the US Bureau of Labor Statistics and was listed in U.S. News & World Report's list of Best Jobs to Have in 2006, 2007, 2008, 2009 and 2010. The national trade association for United States landscape architects is the American Society of Landscape Architects. Frederic Law Olmsted, who designed Central Park in New York City, is known as the ""father of American Landscape Architecture"".
Examples"	22076
environment	[]		"All living and non-living things occurring naturally, generally on Earth

For the biology term, see Biophysical environment . For other uses, see Environment

""Natural force"" redirects here. For the album by Bonnie Tyler, see Natural Force

An image of the Sahara desert from satellite. It is the world's largest hot desert and third-largest desert after the polar deserts

The natural environment or natural world encompasses all living and non-living things occurring naturally, meaning in this case not artificial. The term is most often applied to the Earth or some parts of Earth. This environment encompasses the interaction of all living species, climate, weather and natural resources that affect human survival and economic activity.[1] The concept of the natural environment can be distinguished as components:

In contrast to the natural environment is the built environment. In such areas where humans have fundamentally transformed landscapes such as urban settings and agricultural land conversion, the natural environment is greatly changed into a simplified human environment. Even acts which seem less extreme, such as building a mud hut or a photovoltaic system in the desert, the modified environment becomes an artificial one. Though many animals build things to provide a better environment for themselves, they are not human, hence beaver dams, and the works of mound-building termites, are thought of as natural.

People seldom find absolutely natural environments on Earth, and naturalness usually varies in a continuum, from 100% natural in one extreme to 0% natural in the other. More precisely, we can consider the different aspects or components of an environment, and see that their degree of naturalness is not uniform.[2] If, for instance, in an agricultural field, the mineralogic composition and the structure of its soil are similar to those of an undisturbed forest soil, but the structure is quite different.

Natural environment is often used as a synonym for habitat, for instance, when we say that the natural environment of giraffes is the savanna.

Composition [ edit ]

Earth science generally recognizes four spheres, the lithosphere, the hydrosphere, the atmosphere, and the biosphere[3] as correspondent to rocks, water, air, and life respectively. Some scientists include as part of the spheres of the Earth, the cryosphere (corresponding to ice) as a distinct portion of the hydrosphere, as well as the pedosphere (corresponding to soil) as an active and intermixed sphere. Earth science (also known as geoscience, the geographical sciences or the Earth Sciences), is an all-embracing term for the sciences related to the planet Earth.[4] There are four major disciplines in earth sciences, namely geography, geology, geophysics and geodesy. These major disciplines use physics, chemistry, biology, chronology and mathematics to build a qualitative and quantitative understanding of the principal areas or spheres of Earth.

Geological activity [ edit ]

The Earth's crust, or lithosphere, is the outermost solid surface of the planet and is chemically and mechanically different from underlying mantle. It has been generated greatly by igneous processes in which magma cools and solidifies to form solid rock. Beneath the lithosphere lies the mantle which is heated by the decay of radioactive elements. The mantle though solid is in a state of rheic convection. This convection process causes the lithospheric plates to move, albeit slowly. The resulting process is known as plate tectonics. Volcanoes result primarily from the melting of subducted crust material or of rising mantle at mid-ocean ridges and mantle plumes.

Water on Earth [ edit ]

Most water is found in various kinds of natural body of water.

Oceans [ edit ]

An ocean is a major body of saline water, and a component of the hydrosphere. Approximately 71% of the surface of the Earth (an area of some 362 million square kilometers) is covered by ocean, a continuous body of water that is customarily divided into several principal oceans and smaller seas. More than half of this area is over 3,000 meters (9,800 ft) deep. Average oceanic salinity is around 35 parts per thousand (ppt) (3.5%), and nearly all seawater has a salinity in the range of 30 to 38 ppt. Though generally recognized as several separate oceans, these waters comprise one global, interconnected body of salt water often referred to as the World Ocean or global ocean.[5][6] The deep seabeds are more than half the Earth's surface, and are among the least-modified natural environments. The major oceanic divisions are defined in part by the continents, various archipelagos, and other criteria: these divisions are (in descending order of size) the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Southern Ocean and the Arctic Ocean.

Rivers [ edit ]

A river is a natural watercourse,[7] usually freshwater, flowing toward an ocean, a lake, a sea or another river. A few rivers simply flow into the ground and dry up completely without reaching another body of water.

Rocky stream in the U.S. state of Hawaii

The water in a river is usually in a channel, made up of a stream bed between banks. In larger rivers there is often also a wider floodplain shaped by waters over-topping the channel. Flood plains may be very wide in relation to the size of the river channel. Rivers are a part of the hydrological cycle. Water within a river is generally collected from precipitation through surface runoff, groundwater recharge, springs, and the release of water stored in glaciers and snowpacks.

Small rivers may also be called by several other names, including stream, creek and brook. Their current is confined within a bed and stream banks. Streams play an important corridor role in connecting fragmented habitats and thus in conserving biodiversity. The study of streams and waterways in general is known as surface hydrology.[8]

Lakes [ edit ]

A lake (from Latin lacus) is a terrain feature, a body of water that is localized to the bottom of basin. A body of water is considered a lake when it is inland, is not part of an ocean, and is larger and deeper than a pond.[9][10]

Natural lakes on Earth are generally found in mountainous areas, rift zones, and areas with ongoing or recent glaciation. Other lakes are found in endorheic basins or along the courses of mature rivers. In some parts of the world, there are many lakes because of chaotic drainage patterns left over from the last Ice Age. All lakes are temporary over geologic time scales, as they will slowly fill in with sediments or spill out of the basin containing them.

Ponds [ edit ]

A pond is a body of standing water, either natural or man-made, that is usually smaller than a lake. A wide variety of man-made bodies of water are classified as ponds, including water gardens designed for aesthetic ornamentation, fish ponds designed for commercial fish breeding, and solar ponds designed to store thermal energy. Ponds and lakes are distinguished from streams by their current speed. While currents in streams are easily observed, ponds and lakes possess thermally driven micro-currents and moderate wind driven currents. These features distinguish a pond from many other aquatic terrain features, such as stream pools and tide pools.

Human impact on water [ edit ]

Humans impact the water in different ways such as modifying rivers (through dams and stream channelization), urbanization, and deforestation. These impact lake levels, groundwater conditions, water pollution, thermal pollution, and marine pollution. Humans modify rivers by using direct channel manipulation.[11] We build dams and reservoirs and manipulate the direction of the rivers and water path. Dams can usefully create reservoirs and hydroelectric power. However, reservoirs and dams may negatively impact the environment and wildlife. Dams stop fish migration and the movement of organisms downstream. Urbanization affects the environment because of deforestation and changing lake levels, groundwater conditions, etc. Deforestation and urbanization go hand in hand. Deforestation may cause flooding, declining stream flow, and changes in riverside vegetation. The changing vegetation occurs because when trees cannot get adequate water they start to deteriorate, leading to a decreased food supply for the wildlife in an area.[11]

Atmosphere, climate and weather [ edit ]

A view of Earth's troposphere from an airplane

The atmosphere of the Earth serves as a key factor in sustaining the planetary ecosystem. The thin layer of gases that envelops the Earth is held in place by the planet's gravity. Dry air consists of 78% nitrogen, 21% oxygen, 1% argon and other inert gases, and carbon dioxide. The remaining gases are often referred to as trace gases.[13] The atmosphere includes greenhouse gases such as carbon dioxide, methane, nitrous oxide, and ozone. Filtered air includes trace amounts of many other chemical compounds. Air also contains a variable amount of water vapor and suspensions of water droplets and ice crystals seen as clouds. Many natural substances may be present in tiny amounts in an unfiltered air sample, including dust, pollen and spores, sea spray, volcanic ash, and meteoroids. Various industrial pollutants also may be present, such as chlorine (elementary or in compounds), fluorine compounds, elemental mercury, and sulphur compounds such as sulphur dioxide (SO 2 ).

The ozone layer of the Earth's atmosphere plays an important role in reducing the amount of ultraviolet (UV) radiation that reaches the surface. As DNA is readily damaged by UV light, this serves to protect life at the surface. The atmosphere also retains heat during the night, thereby reducing the daily temperature extremes.

Layers of the atmosphere [ edit ]

Principal layers [ edit ]

Earth's atmosphere can be divided into five main layers. These layers are mainly determined by whether temperature increases or decreases with altitude. From highest to lowest, these layers are:

Exosphere: The outermost layer of Earth's atmosphere extends from the exobase upward, mainly composed of hydrogen and helium.

Thermosphere: The top of the thermosphere is the bottom of the exosphere, called the exobase. Its height varies with solar activity and ranges from about 350–800 km (220–500 mi; 1,150,000–2,620,000 ft). The International Space Station orbits in this layer, between 320 and 380 km (200 and 240 mi).

Mesosphere: The mesosphere extends from the stratopause to 80–85 km (50–53 mi; 262,000–279,000 ft). It is the layer where most meteors burn up upon entering the atmosphere.

Stratosphere: The stratosphere extends from the tropopause to about 51 km (32 mi; 167,000 ft). The stratopause, which is the boundary between the stratosphere and mesosphere, typically is at 50 to 55 km (31 to 34 mi; 164,000 to 180,000 ft).

Troposphere: The troposphere begins at the surface and extends to between 7 km (23,000 ft) at the poles and 17 km (56,000 ft) at the equator, with some variation due to weather. The troposphere is mostly heated by transfer of energy from the surface, so on average the lowest part of the troposphere is warmest and temperature decreases with altitude. The tropopause is the boundary between the troposphere and stratosphere.

Other layers

Within the five principal layers determined by temperature there are several layers determined by other properties.

The ozone layer is contained within the stratosphere. It is mainly located in the lower portion of the stratosphere from about 15–35 km (9.3–21.7 mi; 49,000–115,000 ft), though the thickness varies seasonally and geographically. About 90% of the ozone in our atmosphere is contained in the stratosphere.

The ionosphere, the part of the atmosphere that is ionized by solar radiation, stretches from 50 to 1,000 km (31 to 621 mi; 160,000 to 3,280,000 ft) and typically overlaps both the exosphere and the thermosphere. It forms the inner edge of the magnetosphere.

The homosphere and heterosphere: The homosphere includes the troposphere, stratosphere, and mesosphere. The upper part of the heterosphere is composed almost completely of hydrogen, the lightest element.

The planetary boundary layer is the part of the troposphere that is nearest the Earth's surface and is directly affected by it, mainly through turbulent diffusion.

Effects of global warming [ edit ]

The dangers of global warming are being increasingly studied by a wide global consortium of scientists.[14] These scientists are increasingly concerned about the potential long-term effects of global warming on our natural environment and on the planet. Of particular concern is how climate change and global warming caused by anthropogenic, or human-made releases of greenhouse gases, most notably carbon dioxide, can act interactively, and have adverse effects upon the planet, its natural environment and humans' existence. It is clear the planet is warming, and warming rapidly. This is due to the greenhouse effect, which is caused by greenhouse gases, which trap heat inside the Earth's atmosphere because of their more complex molecular structure which allows them to vibrate and in turn trap heat and release it back towards the Earth.[15] This warming is also responsible for the extinction of natural habitats, which in turn leads to a reduction in wildlife population.The most recent report from the Intergovernmental Panel on Climate Change (the group of the leading climate scientists in the world) concluded that the earth will warm anywhere from 2.7 to almost 11 degrees Fahrenheit (1.5 to 6 degrees Celsius) between 1990 and 2100.[16] Efforts have been increasingly focused on the mitigation of greenhouse gases that are causing climatic changes, on developing adaptative strategies to global warming, to assist humans, other animal, and plant species, ecosystems, regions and nations in adjusting to the effects of global warming. Some examples of recent collaboration to address climate change and global warming include:

A significantly profound challenge is to identify the natural environmental dynamics in contrast to environmental changes not within natural variances. A common solution is to adapt a static view neglecting natural variances to exist. Methodologically, this view could be defended when looking at processes which change slowly and short time series, while the problem arrives when fast processes turns essential in the object of the study.

Climate [ edit ]

Worldwide climate classifications map

Climate looks at the statistics of temperature, humidity, atmospheric pressure, wind, rainfall, atmospheric particle count and other meteorological elements in a given region over long periods of time.[20] Weather, on the other hand, is the present condition of these same elements over periods up to two weeks.[21]

Climates can be classified according to the average and typical ranges of different variables, most commonly temperature and precipitation. The most commonly used classification scheme is the one originally developed by Wladimir Köppen. The Thornthwaite system,[22] in use since 1948, uses evapotranspiration as well as temperature and precipitation information to study animal species diversity and the potential impacts of climate changes.[23]

Weather [ edit ]

Weather is a set of all the phenomena occurring in a given atmospheric area at a given time.[24] Most weather phenomena occur in the troposphere,[25][26] just below the stratosphere. Weather refers, generally, to day-to-day temperature and precipitation activity, whereas climate is the term for the average atmospheric conditions over longer periods of time.[27] When used without qualification, ""weather"" is understood to be the weather of Earth.

Weather occurs due to density (temperature and moisture) differences between one place and another. These differences can occur due to the sun angle at any particular spot, which varies by latitude from the tropics. The strong temperature contrast between polar and tropical air gives rise to the jet stream. Weather systems in the mid-latitudes, such as extratropical cyclones, are caused by instabilities of the jet stream flow. Because the Earth's axis is tilted relative to its orbital plane, sunlight is incident at different angles at different times of the year. On the Earth's surface, temperatures usually range ±40 °C (100 °F to −40 °F) annually. Over thousands of years, changes in the Earth's orbit have affected the amount and distribution of solar energy received by the Earth and influence long-term climate

Surface temperature differences in turn cause pressure differences. Higher altitudes are cooler than lower altitudes due to differences in compressional heating. Weather forecasting is the application of science and technology to predict the state of the atmosphere for a future time and a given location. The atmosphere is a chaotic system, and small changes to one part of the system can grow to have large effects on the system as a whole. Human attempts to control the weather have occurred throughout human history, and there is evidence that civilized human activity such as agriculture and industry has inadvertently modified weather patterns.

Life [ edit ]

There are many plant species on the planet.

An example of the many animal species on the Earth

Evidence suggests that life on Earth has existed for about 3.7 billion years.[28] All known life forms share fundamental molecular mechanisms, and based on these observations, theories on the origin of life attempt to find a mechanism explaining the formation of a primordial single cell organism from which all life originates. There are many different hypotheses regarding the path that might have been taken from simple organic molecules via pre-cellular life to protocells and metabolism.

Although there is no universal agreement on the definition of life, scientists generally accept that the biological manifestation of life is characterized by organization, metabolism, growth, adaptation, response to stimuli and reproduction.[29] Life may also be said to be simply the characteristic state of organisms. In biology, the science of living organisms, ""life"" is the condition which distinguishes active organisms from inorganic matter, including the capacity for growth, functional activity and the continual change preceding death.[30][31]

A diverse variety of living organisms (life forms) can be found in the biosphere on Earth, and properties common to these organisms—plants, animals, fungi, protists, archaea, and bacteria—are a carbon- and water-based cellular form with complex organization and heritable genetic information. Living organisms undergo metabolism, maintain homeostasis, possess a capacity to grow, respond to stimuli, reproduce and, through natural selection, adapt to their environment in successive generations. More complex living organisms can communicate through various means.

Ecosystems [ edit ]

An ecosystem (also called as environment) is a natural unit consisting of all plants, animals and micro-organisms (biotic factors) in an area functioning together with all of the non-living physical (abiotic) factors of the environment.[32]

Central to the ecosystem concept is the idea that living organisms are continually engaged in a highly interrelated set of relationships with every other element constituting the environment in which they exist. Eugene Odum, one of the founders of the science of ecology, stated: ""Any unit that includes all of the organisms (i.e.: the ""community"") in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e.: exchange of materials between living and nonliving parts) within the system is an ecosystem.""[33]

The human ecosystem concept is then grounded in the deconstruction of the human/nature dichotomy, and the emergent premise that all species are ecologically integrated with each other, as well as with the abiotic constituents of their biotope.

A greater number or variety of species or biological diversity of an ecosystem may contribute to greater resilience of an ecosystem, because there are more species present at a location to respond to change and thus ""absorb"" or reduce its effects. This reduces the effect before the ecosystem's structure is fundamentally changed to a different state. This is not universally the case and there is no proven relationship between the species diversity of an ecosystem and its ability to provide goods and services on a sustainable level.

The term ecosystem can also pertain to human-made environments, such as human ecosystems and human-influenced ecosystems, and can describe any situation where there is relationship between living organisms and their environment. Fewer areas on the surface of the earth today exist free from human contact, although some genuine wilderness areas continue to exist without any forms of human intervention.

Biomes [ edit ]

Map of terrestrial biomes classified by vegetation

Biomes are terminologically similar to the concept of ecosystems, and are climatically and geographically defined areas of ecologically similar climatic conditions on the Earth, such as communities of plants, animals, and soil organisms, often referred to as ecosystems. Biomes are defined on the basis of factors such as plant structures (such as trees, shrubs, and grasses), leaf types (such as broadleaf and needleleaf), plant spacing (forest, woodland, savanna), and climate. Unlike ecozones, biomes are not defined by genetic, taxonomic, or historical similarities. Biomes are often identified with particular patterns of ecological succession and climax vegetation.

Biogeochemical cycles [ edit ]

Global biogeochemical cycles are critical to life, most notably those of water, oxygen, carbon, nitrogen and phosphorus.[34]

The nitrogen cycle is the transformation of nitrogen and nitrogen-containing compounds in nature. It is a cycle which includes gaseous components.

The water cycle, is the continuous movement of water on, above, and below the surface of the Earth. Water can change states among liquid, vapour, and ice at various places in the water cycle. Although the balance of water on Earth remains fairly constant over time, individual water molecules can come and go.

The carbon cycle is the biogeochemical cycle by which carbon is exchanged among the biosphere, pedosphere, geosphere, hydrosphere, and atmosphere of the Earth.

The oxygen cycle is the movement of oxygen within and between its three main reservoirs: the atmosphere, the biosphere, and the lithosphere. The main driving factor of the oxygen cycle is photosynthesis, which is responsible for the modern Earth's atmospheric composition and life.

The phosphorus cycle is the movement of phosphorus through the lithosphere, hydrosphere, and biosphere. The atmosphere does not play a significant role in the movements of phosphorus, because phosphorus and phosphorus compounds are usually solids at the typical ranges of temperature and pressure found on Earth.

Wilderness [ edit ]

Wilderness is generally defined as a natural environment on Earth that has not been significantly modified by human activity. The WILD Foundation goes into more detail, defining wilderness as: ""The most intact, undisturbed wild natural areas left on our planet – those last truly wild places that humans do not control and have not developed with roads, pipelines or other industrial infrastructure.""[35] Wilderness areas and protected parks are considered important for the survival of certain species, ecological studies, conservation, solitude, and recreation. Wilderness is deeply valued for cultural, spiritual, moral, and aesthetic reasons. Some nature writers believe wilderness areas are vital for the human spirit and creativity.[36]

The word, ""wilderness"", derives from the notion of wildness; in other words that which is not controllable by humans. The word's etymology is from the Old English wildeornes, which in turn derives from wildeor meaning wild beast (wild + deor = beast, deer).[37] From this point of view, it is the wildness of a place that makes it a wilderness. The mere presence or activity of people does not disqualify an area from being ""wilderness."" Many ecosystems that are, or have been, inhabited or influenced by activities of people may still be considered ""wild."" This way of looking at wilderness includes areas within which natural processes operate without very noticeable human interference.

Wildlife includes all non-domesticated plants, animals and other organisms. Domesticating wild plant and animal species for human benefit has occurred many times all over the planet, and has a major impact on the environment, both positive and negative. Wildlife can be found in all ecosystems. Deserts, rain forests, plains, and other areas—including the most developed urban sites—all have distinct forms of wildlife. While the term in popular culture usually refers to animals that are untouched by civilized human factors, most scientists agree that wildlife around the world is (now) impacted by human activities.

A view of wilderness in Estonia

Challenges [ edit ]

[38][39] Amazon rainforest in Brazil . The tropical rainforests of South America contain the largest diversity of species on Earth, including some that have evolved within the past few hundred thousand years.

It is the common understanding of natural environment that underlies environmentalism — a broad political, social, and philosophical movement that advocates various actions and policies in the interest of protecting what nature remains in the natural environment, or restoring or expanding the role of nature in this environment. While true wilderness is increasingly rare, wild nature (e.g., unmanaged forests, uncultivated grasslands, wildlife, wildflowers) can be found in many locations previously inhabited by humans.

Goals for the benefit of people and natural systems, commonly expressed by environmental scientists and environmentalists include:

Criticism [ edit ]

In some cultures the term environment is meaningless because there is no separation between people and what they view as the natural world, or their surroundings.[44] Specifically in the United States and Arabian countries many native cultures do not recognize the ""environment"", or see themselves as environmentalists.[45]

See also [ edit ]

References [ edit ]"	https://en.wikipedia.org/wiki/Natural_environment	"All living and non-living things occurring naturally, generally on Earth
For the biology term, see Biophysical environment . For other uses, see Environment
""Natural force"" redirects here. For the album by Bonnie Tyler, see Natural Force
An image of the Sahara desert from satellite. It is the world's largest hot desert and third-largest desert after the polar deserts
The natural environment or natural world encompasses all living and non-living things occurring naturally, meaning in this case not artificial. The term is most often applied to the Earth or some parts of Earth. This environment encompasses the interaction of all living species, climate, weather and natural resources that affect human survival and economic activity. The concept of the natural environment can be distinguished as components:
In contrast to the natural environment is the built environment. In such areas where humans have fundamentally transformed landscapes such as urban settings and agricultural land conversion, the natural environment is greatly changed into a simplified human environment. Even acts which seem less extreme, such as building a mud hut or a photovoltaic system in the desert, the modified environment becomes an artificial one. Though many animals build things to provide a better environment for themselves, they are not human, hence beaver dams, and the works of mound-building termites, are thought of as natural.
People seldom find absolutely natural environments on Earth, and naturalness usually varies in a continuum, from 100% natural in one extreme to 0% natural in the other. More precisely, we can consider the different aspects or components of an environment, and see that their degree of naturalness is not uniform. If, for instance, in an agricultural field, the mineralogic composition and the structure of its soil are similar to those of an undisturbed forest soil, but the structure is quite different.
Natural environment is often used as a synonym for habitat, for instance, when we say that the natural environment of giraffes is the savanna.
Composition 
Earth science generally recognizes four spheres, the lithosphere, the hydrosphere, the atmosphere, and the biosphere as correspondent to rocks, water, air, and life respectively. Some scientists include as part of the spheres of the Earth, the cryosphere (corresponding to ice) as a distinct portion of the hydrosphere, as well as the pedosphere (corresponding to soil) as an active and intermixed sphere. Earth science (also known as geoscience, the geographical sciences or the Earth Sciences), is an all-embracing term for the sciences related to the planet Earth. There are four major disciplines in earth sciences, namely geography, geology, geophysics and geodesy. These major disciplines use physics, chemistry, biology, chronology and mathematics to build a qualitative and quantitative understanding of the principal areas or spheres of Earth.
Geological activity 
The Earth's crust, or lithosphere, is the outermost solid surface of the planet and is chemically and mechanically different from underlying mantle. It has been generated greatly by igneous processes in which magma cools and solidifies to form solid rock. Beneath the lithosphere lies the mantle which is heated by the decay of radioactive elements. The mantle though solid is in a state of rheic convection. This convection process causes the lithospheric plates to move, albeit slowly. The resulting process is known as plate tectonics. Volcanoes result primarily from the melting of subducted crust material or of rising mantle at mid-ocean ridges and mantle plumes.
Water on Earth 
Most water is found in various kinds of natural body of water.
Oceans 
An ocean is a major body of saline water, and a component of the hydrosphere. Approximately 71% of the surface of the Earth (an area of some 362 million square kilometers) is covered by ocean, a continuous body of water that is customarily divided into several principal oceans and smaller seas. More than half of this area is over 3,000 meters (9,800 ft) deep. Average oceanic salinity is around 35 parts per thousand (ppt) (3.5%), and nearly all seawater has a salinity in the range of 30 to 38 ppt. Though generally recognized as several separate oceans, these waters comprise one global, interconnected body of salt water often referred to as the World Ocean or global ocean. The deep seabeds are more than half the Earth's surface, and are among the least-modified natural environments. The major oceanic divisions are defined in part by the continents, various archipelagos, and other criteria: these divisions are (in descending order of size) the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Southern Ocean and the Arctic Ocean.
Rivers 
A river is a natural watercourse, usually freshwater, flowing toward an ocean, a lake, a sea or another river. A few rivers simply flow into the ground and dry up completely without reaching another body of water.
Rocky stream in the U.S. state of Hawaii
The water in a river is usually in a channel, made up of a stream bed between banks. In larger rivers there is often also a wider floodplain shaped by waters over-topping the channel. Flood plains may be very wide in relation to the size of the river channel. Rivers are a part of the hydrological cycle. Water within a river is generally collected from precipitation through surface runoff, groundwater recharge, springs, and the release of water stored in glaciers and snowpacks.
Small rivers may also be called by several other names, including stream, creek and brook. Their current is confined within a bed and stream banks. Streams play an important corridor role in connecting fragmented habitats and thus in conserving biodiversity. The study of streams and waterways in general is known as surface hydrology.
Lakes 
A lake (from Latin lacus) is a terrain feature, a body of water that is localized to the bottom of basin. A body of water is considered a lake when it is inland, is not part of an ocean, and is larger and deeper than a pond.
Natural lakes on Earth are generally found in mountainous areas, rift zones, and areas with ongoing or recent glaciation. Other lakes are found in endorheic basins or along the courses of mature rivers. In some parts of the world, there are many lakes because of chaotic drainage patterns left over from the last Ice Age. All lakes are temporary over geologic time scales, as they will slowly fill in with sediments or spill out of the basin containing them.
Ponds 
A pond is a body of standing water, either natural or man-made, that is usually smaller than a lake. A wide variety of man-made bodies of water are classified as ponds, including water gardens designed for aesthetic ornamentation, fish ponds designed for commercial fish breeding, and solar ponds designed to store thermal energy. Ponds and lakes are distinguished from streams by their current speed. While currents in streams are easily observed, ponds and lakes possess thermally driven micro-currents and moderate wind driven currents. These features distinguish a pond from many other aquatic terrain features, such as stream pools and tide pools.
Human impact on water 
Humans impact the water in different ways such as modifying rivers (through dams and stream channelization), urbanization, and deforestation. These impact lake levels, groundwater conditions, water pollution, thermal pollution, and marine pollution. Humans modify rivers by using direct channel manipulation. We build dams and reservoirs and manipulate the direction of the rivers and water path. Dams can usefully create reservoirs and hydroelectric power. However, reservoirs and dams may negatively impact the environment and wildlife. Dams stop fish migration and the movement of organisms downstream. Urbanization affects the environment because of deforestation and changing lake levels, groundwater conditions, etc. Deforestation and urbanization go hand in hand. Deforestation may cause flooding, declining stream flow, and changes in riverside vegetation. The changing vegetation occurs because when trees cannot get adequate water they start to deteriorate, leading to a decreased food supply for the wildlife in an area.
Atmosphere, climate and weather 
A view of Earth's troposphere from an airplane
The atmosphere of the Earth serves as a key factor in sustaining the planetary ecosystem. The thin layer of gases that envelops the Earth is held in place by the planet's gravity. Dry air consists of 78% nitrogen, 21% oxygen, 1% argon and other inert gases, and carbon dioxide. The remaining gases are often referred to as trace gases. The atmosphere includes greenhouse gases such as carbon dioxide, methane, nitrous oxide, and ozone. Filtered air includes trace amounts of many other chemical compounds. Air also contains a variable amount of water vapor and suspensions of water droplets and ice crystals seen as clouds. Many natural substances may be present in tiny amounts in an unfiltered air sample, including dust, pollen and spores, sea spray, volcanic ash, and meteoroids. Various industrial pollutants also may be present, such as chlorine (elementary or in compounds), fluorine compounds, elemental mercury, and sulphur compounds such as sulphur dioxide (SO 2 ).
The ozone layer of the Earth's atmosphere plays an important role in reducing the amount of ultraviolet (UV) radiation that reaches the surface. As DNA is readily damaged by UV light, this serves to protect life at the surface. The atmosphere also retains heat during the night, thereby reducing the daily temperature extremes.
Layers of the atmosphere 
Principal layers 
Earth's atmosphere can be divided into five main layers. These layers are mainly determined by whether temperature increases or decreases with altitude. From highest to lowest, these layers are:
Exosphere: The outermost layer of Earth's atmosphere extends from the exobase upward, mainly composed of hydrogen and helium.
Thermosphere: The top of the thermosphere is the bottom of the exosphere, called the exobase. Its height varies with solar activity and ranges from about 350–800 km (220–500 mi; 1,150,000–2,620,000 ft). The International Space Station orbits in this layer, between 320 and 380 km (200 and 240 mi).
Mesosphere: The mesosphere extends from the stratopause to 80–85 km (50–53 mi; 262,000–279,000 ft). It is the layer where most meteors burn up upon entering the atmosphere.
Stratosphere: The stratosphere extends from the tropopause to about 51 km (32 mi; 167,000 ft). The stratopause, which is the boundary between the stratosphere and mesosphere, typically is at 50 to 55 km (31 to 34 mi; 164,000 to 180,000 ft).
Troposphere: The troposphere begins at the surface and extends to between 7 km (23,000 ft) at the poles and 17 km (56,000 ft) at the equator, with some variation due to weather. The troposphere is mostly heated by transfer of energy from the surface, so on average the lowest part of the troposphere is warmest and temperature decreases with altitude. The tropopause is the boundary between the troposphere and stratosphere.
Other layers
Within the five principal layers determined by temperature there are several layers determined by other properties.
The ozone layer is contained within the stratosphere. It is mainly located in the lower portion of the stratosphere from about 15–35 km (9.3–21.7 mi; 49,000–115,000 ft), though the thickness varies seasonally and geographically. About 90% of the ozone in our atmosphere is contained in the stratosphere.
The ionosphere, the part of the atmosphere that is ionized by solar radiation, stretches from 50 to 1,000 km (31 to 621 mi; 160,000 to 3,280,000 ft) and typically overlaps both the exosphere and the thermosphere. It forms the inner edge of the magnetosphere.
The homosphere and heterosphere: The homosphere includes the troposphere, stratosphere, and mesosphere. The upper part of the heterosphere is composed almost completely of hydrogen, the lightest element.
The planetary boundary layer is the part of the troposphere that is nearest the Earth's surface and is directly affected by it, mainly through turbulent diffusion.
Effects of global warming 
The dangers of global warming are being increasingly studied by a wide global consortium of scientists. These scientists are increasingly concerned about the potential long-term effects of global warming on our natural environment and on the planet. Of particular concern is how climate change and global warming caused by anthropogenic, or human-made releases of greenhouse gases, most notably carbon dioxide, can act interactively, and have adverse effects upon the planet, its natural environment and humans' existence. It is clear the planet is warming, and warming rapidly. This is due to the greenhouse effect, which is caused by greenhouse gases, which trap heat inside the Earth's atmosphere because of their more complex molecular structure which allows them to vibrate and in turn trap heat and release it back towards the Earth. This warming is also responsible for the extinction of natural habitats, which in turn leads to a reduction in wildlife population.The most recent report from the Intergovernmental Panel on Climate Change (the group of the leading climate scientists in the world) concluded that the earth will warm anywhere from 2.7 to almost 11 degrees Fahrenheit (1.5 to 6 degrees Celsius) between 1990 and 2100. Efforts have been increasingly focused on the mitigation of greenhouse gases that are causing climatic changes, on developing adaptative strategies to global warming, to assist humans, other animal, and plant species, ecosystems, regions and nations in adjusting to the effects of global warming. Some examples of recent collaboration to address climate change and global warming include:
A significantly profound challenge is to identify the natural environmental dynamics in contrast to environmental changes not within natural variances. A common solution is to adapt a static view neglecting natural variances to exist. Methodologically, this view could be defended when looking at processes which change slowly and short time series, while the problem arrives when fast processes turns essential in the object of the study.
Climate 
Worldwide climate classifications map
Climate looks at the statistics of temperature, humidity, atmospheric pressure, wind, rainfall, atmospheric particle count and other meteorological elements in a given region over long periods of time. Weather, on the other hand, is the present condition of these same elements over periods up to two weeks.
Climates can be classified according to the average and typical ranges of different variables, most commonly temperature and precipitation. The most commonly used classification scheme is the one originally developed by Wladimir Köppen. The Thornthwaite system, in use since 1948, uses evapotranspiration as well as temperature and precipitation information to study animal species diversity and the potential impacts of climate changes.
Weather 
Weather is a set of all the phenomena occurring in a given atmospheric area at a given time. Most weather phenomena occur in the troposphere, just below the stratosphere. Weather refers, generally, to day-to-day temperature and precipitation activity, whereas climate is the term for the average atmospheric conditions over longer periods of time. When used without qualification, ""weather"" is understood to be the weather of Earth.
Weather occurs due to density (temperature and moisture) differences between one place and another. These differences can occur due to the sun angle at any particular spot, which varies by latitude from the tropics. The strong temperature contrast between polar and tropical air gives rise to the jet stream. Weather systems in the mid-latitudes, such as extratropical cyclones, are caused by instabilities of the jet stream flow. Because the Earth's axis is tilted relative to its orbital plane, sunlight is incident at different angles at different times of the year. On the Earth's surface, temperatures usually range ±40 °C (100 °F to −40 °F) annually. Over thousands of years, changes in the Earth's orbit have affected the amount and distribution of solar energy received by the Earth and influence long-term climate
Surface temperature differences in turn cause pressure differences. Higher altitudes are cooler than lower altitudes due to differences in compressional heating. Weather forecasting is the application of science and technology to predict the state of the atmosphere for a future time and a given location. The atmosphere is a chaotic system, and small changes to one part of the system can grow to have large effects on the system as a whole. Human attempts to control the weather have occurred throughout human history, and there is evidence that civilized human activity such as agriculture and industry has inadvertently modified weather patterns.
Life 
There are many plant species on the planet.
An example of the many animal species on the Earth
Evidence suggests that life on Earth has existed for about 3.7 billion years. All known life forms share fundamental molecular mechanisms, and based on these observations, theories on the origin of life attempt to find a mechanism explaining the formation of a primordial single cell organism from which all life originates. There are many different hypotheses regarding the path that might have been taken from simple organic molecules via pre-cellular life to protocells and metabolism.
Although there is no universal agreement on the definition of life, scientists generally accept that the biological manifestation of life is characterized by organization, metabolism, growth, adaptation, response to stimuli and reproduction. Life may also be said to be simply the characteristic state of organisms. In biology, the science of living organisms, ""life"" is the condition which distinguishes active organisms from inorganic matter, including the capacity for growth, functional activity and the continual change preceding death.
A diverse variety of living organisms (life forms) can be found in the biosphere on Earth, and properties common to these organisms—plants, animals, fungi, protists, archaea, and bacteria—are a carbon- and water-based cellular form with complex organization and heritable genetic information. Living organisms undergo metabolism, maintain homeostasis, possess a capacity to grow, respond to stimuli, reproduce and, through natural selection, adapt to their environment in successive generations. More complex living organisms can communicate through various means.
Ecosystems 
An ecosystem (also called as environment) is a natural unit consisting of all plants, animals and micro-organisms (biotic factors) in an area functioning together with all of the non-living physical (abiotic) factors of the environment.
Central to the ecosystem concept is the idea that living organisms are continually engaged in a highly interrelated set of relationships with every other element constituting the environment in which they exist. Eugene Odum, one of the founders of the science of ecology, stated: ""Any unit that includes all of the organisms (i.e.: the ""community"") in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e.: exchange of materials between living and nonliving parts) within the system is an ecosystem.""
The human ecosystem concept is then grounded in the deconstruction of the human/nature dichotomy, and the emergent premise that all species are ecologically integrated with each other, as well as with the abiotic constituents of their biotope.
A greater number or variety of species or biological diversity of an ecosystem may contribute to greater resilience of an ecosystem, because there are more species present at a location to respond to change and thus ""absorb"" or reduce its effects. This reduces the effect before the ecosystem's structure is fundamentally changed to a different state. This is not universally the case and there is no proven relationship between the species diversity of an ecosystem and its ability to provide goods and services on a sustainable level.
The term ecosystem can also pertain to human-made environments, such as human ecosystems and human-influenced ecosystems, and can describe any situation where there is relationship between living organisms and their environment. Fewer areas on the surface of the earth today exist free from human contact, although some genuine wilderness areas continue to exist without any forms of human intervention.
Biomes 
Map of terrestrial biomes classified by vegetation
Biomes are terminologically similar to the concept of ecosystems, and are climatically and geographically defined areas of ecologically similar climatic conditions on the Earth, such as communities of plants, animals, and soil organisms, often referred to as ecosystems. Biomes are defined on the basis of factors such as plant structures (such as trees, shrubs, and grasses), leaf types (such as broadleaf and needleleaf), plant spacing (forest, woodland, savanna), and climate. Unlike ecozones, biomes are not defined by genetic, taxonomic, or historical similarities. Biomes are often identified with particular patterns of ecological succession and climax vegetation.
Biogeochemical cycles 
Global biogeochemical cycles are critical to life, most notably those of water, oxygen, carbon, nitrogen and phosphorus.
The nitrogen cycle is the transformation of nitrogen and nitrogen-containing compounds in nature. It is a cycle which includes gaseous components.
The water cycle, is the continuous movement of water on, above, and below the surface of the Earth. Water can change states among liquid, vapour, and ice at various places in the water cycle. Although the balance of water on Earth remains fairly constant over time, individual water molecules can come and go.
The carbon cycle is the biogeochemical cycle by which carbon is exchanged among the biosphere, pedosphere, geosphere, hydrosphere, and atmosphere of the Earth.
The oxygen cycle is the movement of oxygen within and between its three main reservoirs: the atmosphere, the biosphere, and the lithosphere. The main driving factor of the oxygen cycle is photosynthesis, which is responsible for the modern Earth's atmospheric composition and life.
The phosphorus cycle is the movement of phosphorus through the lithosphere, hydrosphere, and biosphere. The atmosphere does not play a significant role in the movements of phosphorus, because phosphorus and phosphorus compounds are usually solids at the typical ranges of temperature and pressure found on Earth.
Wilderness 
Wilderness is generally defined as a natural environment on Earth that has not been significantly modified by human activity. The WILD Foundation goes into more detail, defining wilderness as: ""The most intact, undisturbed wild natural areas left on our planet – those last truly wild places that humans do not control and have not developed with roads, pipelines or other industrial infrastructure."" Wilderness areas and protected parks are considered important for the survival of certain species, ecological studies, conservation, solitude, and recreation. Wilderness is deeply valued for cultural, spiritual, moral, and aesthetic reasons. Some nature writers believe wilderness areas are vital for the human spirit and creativity.
The word, ""wilderness"", derives from the notion of wildness; in other words that which is not controllable by humans. The word's etymology is from the Old English wildeornes, which in turn derives from wildeor meaning wild beast (wild + deor = beast, deer). From this point of view, it is the wildness of a place that makes it a wilderness. The mere presence or activity of people does not disqualify an area from being ""wilderness."" Many ecosystems that are, or have been, inhabited or influenced by activities of people may still be considered ""wild."" This way of looking at wilderness includes areas within which natural processes operate without very noticeable human interference.
Wildlife includes all non-domesticated plants, animals and other organisms. Domesticating wild plant and animal species for human benefit has occurred many times all over the planet, and has a major impact on the environment, both positive and negative. Wildlife can be found in all ecosystems. Deserts, rain forests, plains, and other areas—including the most developed urban sites—all have distinct forms of wildlife. While the term in popular culture usually refers to animals that are untouched by civilized human factors, most scientists agree that wildlife around the world is (now) impacted by human activities.
A view of wilderness in Estonia
Challenges 
 Amazon rainforest in Brazil . The tropical rainforests of South America contain the largest diversity of species on Earth, including some that have evolved within the past few hundred thousand years.
It is the common understanding of natural environment that underlies environmentalism — a broad political, social, and philosophical movement that advocates various actions and policies in the interest of protecting what nature remains in the natural environment, or restoring or expanding the role of nature in this environment. While true wilderness is increasingly rare, wild nature (e.g., unmanaged forests, uncultivated grasslands, wildlife, wildflowers) can be found in many locations previously inhabited by humans.
Goals for the benefit of people and natural systems, commonly expressed by environmental scientists and environmentalists include:
Criticism 
In some cultures the term environment is meaningless because there is no separation between people and what they view as the natural world, or their surroundings. Specifically in the United States and Arabian countries many native cultures do not recognize the ""environment"", or see themselves as environmentalists."	26116
environment	[]		"Energy that is collected from renewable resources

[1] Renewable energy capacity additions in 2020 expanded by more than 45% from 2019, including a 90% rise in global wind capacity (green) and a 23% expansion of new solar photovoltaic installations (yellow).

Renewable electricity generation breakdown (+nuclear) as of 2018.[2] Hydro (45%) Nuclear (28%) Wind (13%) Solar (6%) Biofuels (5%) Other (3%)

Renewable energy is useful energy that is collected from renewable resources, which are naturally replenished on a human timescale, including carbon neutral sources like sunlight, wind, rain, tides, waves, and geothermal heat.[3] The term often also encompasses biomass as well, whose carbon neutral status is under debate. [4][5] This type of energy source stands in contrast to fossil fuels, which are being used far more quickly than they are being replenished.

Renewable energy often provides energy in four important areas: electricity generation, air and water heating/cooling, transportation, and rural (off-grid) energy services.[6]

Based on REN21's 2017 report, renewables contributed 19.3% to humans' global energy consumption and 24.5% to their generation of electricity in 2015 and 2016, respectively. This energy consumption is divided as 8.9% coming from traditional biomass, 4.2% as heat energy (modern biomass, geothermal and solar heat), 3.9% from hydroelectricity and the remaining 2.2% is electricity from wind, solar, geothermal, and other forms of biomass.[7] In 2017, worldwide investments in renewable energy amounted to US$279.8 billion with China accounting for 45% of the global investments, and the United States and Europe both around 15%.[8] Globally there were an estimated 10.5 million jobs associated with the renewable energy industries, with solar photovoltaics being the largest renewable employer.[9] Renewable energy systems are rapidly becoming more efficient and cheaper and their share of total energy consumption is increasing.[10] As of 2019, more than two-thirds of worldwide newly installed electricity capacity was renewable.[11] Growth in consumption of coal and oil could end by 2020 due to increased uptake of renewables and natural gas.[12][13][14] As of 2020, in most countries, photovoltaic solar and onshore wind are the cheapest forms of building new electricity-generating plants.[15]

At the national level, at least 30 nations around the world already have renewable energy contributing more than 20 percent of their energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond.[16] At least two countries, Iceland and Norway, generate all their electricity using renewable energy already, and many other countries have the set a goal to reach 100% renewable energy in the future.[17] At least 47 nations around the world already have over 50 percent of electricity from renewable resources.[18][19][20] Renewable energy resources exist over wide geographical areas, in contrast to fossil fuels, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency technologies is resulting in significant energy security, climate change mitigation, and economic benefits.[21] In international public opinion surveys there is strong support for promoting renewable sources such as solar power and wind power.[22][23]

While many renewable energy projects are large-scale, renewable technologies are also suited to rural and remote areas and developing countries, where energy is often crucial in human development.[24][25] As most of renewable energy technologies provide electricity, renewable energy deployment is often applied in conjunction with further electrification, which has several benefits: electricity can be converted to heat, can be converted into mechanical energy with high efficiency, and is clean at the point of consumption.[26][27] In addition, electrification with renewable energy is more efficient and therefore leads to significant reductions in primary energy requirements.[28]

Overview

[29] Coal, oil, and natural gas remain the primary global energy sources even as renewables have begun rapidly increasing.

PlanetSolar , the world's largest solar-powered boat and the first ever solar electric vehicle to circumnavigate the globe (in 2012)

Renewable energy flows involve natural phenomena such as sunlight, wind, tides, plant growth, and geothermal heat, as the International Energy Agency explains:[30]

Renewable energy is derived from natural processes that are replenished constantly. In its various forms, it derives directly from the sun, or from heat generated deep within the earth. Included in the definition is electricity and heat generated from solar, wind, ocean, hydropower, biomass, geothermal resources, and biofuels and hydrogen derived from renewable resources.

Renewable energy resources and significant opportunities for energy efficiency exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency, and technological diversification of energy sources, would result in significant energy security and economic benefits.[21] It would also reduce environmental pollution such as air pollution caused by burning of fossil fuels and improve public health, reduce premature mortalities due to pollution and save associated health costs that amount to several hundred billion dollars annually only in the United States.[31] Multiple analyses of U.S. decarbonization strategies have found that quantified health benefits can significantly offset the costs of implementing these strategies.[32] Renewable energy sources, that derive their energy from the sun, either directly or indirectly, such as hydro and wind, are expected to be capable of supplying humanity energy for almost another 1 billion years, at which point the predicted increase in heat from the Sun is expected to make the surface of the Earth too hot for liquid water to exist.[33][34][35]

Climate change and global warming concerns, coupled with the continuing fall in the costs of some renewable energy equipment, such as wind turbines and solar panels, are driving increased use of renewables.[22] New government spending, regulation and policies helped the industry weather the global financial crisis better than many other sectors.[36] As of 2019 , however, according to the International Renewable Energy Agency, renewables overall share in the energy mix (including power, heat and transport) needs to grow six times faster, in order to keep the rise in average global temperatures ""well below"" 2.0 °C (3.6 °F) during the present century, compared to pre-industrial levels.[37]

As of 2011, small solar PV systems provide electricity to a few million households, and micro-hydro configured into mini-grids serves many more. Over 44 million households use biogas made in household-scale digesters for lighting and/or cooking, and more than 166 million households rely on a new generation of more-efficient biomass cookstoves.[38] [needs update] United Nations' eighth Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity.[39] At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond, and some 120 countries have various policy targets for longer-term shares of renewable energy, including a 20% target of all electricity generated for the European Union by 2020. Some countries have much higher long-term policy targets of up to 100% renewables. Outside Europe, a diverse group of 20 or more other countries targets renewable energy shares in the 2020–2030 time frame that range from 10% to 50%.[16]

Renewable energy often displaces conventional fuels in four areas: electricity generation, hot water/space heating, transportation, and rural (off-grid) energy services:[6]

Power generation

By 2040, renewable energy is projected to equal coal and natural gas electricity generation. Several jurisdictions, including Denmark, Germany, the state of South Australia and some US states have achieved high integration of variable renewables. For example, in 2015 wind power met 42% of electricity demand in Denmark, 23.2% in Portugal and 15.5% in Uruguay. Interconnectors enable countries to balance electricity systems by allowing the import and export of renewable energy. Innovative hybrid systems have emerged between countries and regions.[40]

Heating

Solar water heating makes an important contribution to renewable heat in many countries, most notably in China, which now has 70% of the global total (180 GWth). Most of these systems are installed on multi-family apartment buildings and meet a portion of the hot water needs of an estimated 50–60 million households in China. Worldwide, total installed solar water heating systems meet a portion of the water heating needs of over 70 million households. The use of biomass for heating continues to grow as well. In Sweden, national use of biomass energy has surpassed that of oil. Direct geothermal for heating is also growing rapidly.[41] The newest addition to Heating is from Geothermal Heat Pumps which provide both heating and cooling, and also flatten the electric demand curve and are thus an increasing national priority[42][43] (see also Renewable thermal energy).

Transportation

A bus fueled by biodiesel

Bioethanol is an alcohol made by fermentation, mostly from carbohydrates produced in sugar or starch crops such as corn, sugarcane, or sweet sorghum. Cellulosic biomass, derived from non-food sources such as trees and grasses is also being developed as a feedstock for ethanol production. Ethanol can be used as a fuel for vehicles in its pure form, but it is usually used as a gasoline additive to increase octane and improve vehicle emissions. Bioethanol is widely used in the USA and in Brazil. Biodiesel can be used as a fuel for vehicles in its pure form, but it is usually used as a diesel additive to reduce levels of particulates, carbon monoxide, and hydrocarbons from diesel-powered vehicles. Biodiesel is produced from oils or fats using transesterification and is the most common biofuel in Europe.

History

Prior to the development of coal in the mid 19th century, nearly all energy used was renewable. Almost without a doubt the oldest known use of renewable energy, in the form of traditional biomass to fuel fires, dates from more than a million years ago. The use of biomass for fire did not become commonplace until many hundreds of thousands of years later.[44] Probably the second oldest usage of renewable energy is harnessing the wind in order to drive ships over water. This practice can be traced back some 7000 years, to ships in the Persian Gulf and on the Nile.[45] From hot springs, geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times.[46] Moving into the time of recorded history, the primary sources of traditional renewable energy were human labor, animal power, water power, wind, in grain crushing windmills, and firewood, a traditional biomass.

In the 1860s and 1870s, there were already fears that civilization would run out of fossil fuels and the need was felt for a better source. In 1873 Professor Augustin Mouchot wrote:

The time will arrive when the industry of Europe will cease to find those natural resources, so necessary for it. Petroleum springs and coal mines are not inexhaustible but are rapidly diminishing in many places. Will man, then, return to the power of water and wind? Or will he emigrate where the most powerful source of heat sends its rays to all? History will show what will come.[47]

In 1885, Werner von Siemens, commenting on the discovery of the photovoltaic effect in the solid state, wrote:

In conclusion, I would say that however great the scientific importance of this discovery may be, its practical value will be no less obvious when we reflect that the supply of solar energy is both without limit and without cost, and that it will continue to pour down upon us for countless ages after all the coal deposits of the earth have been exhausted and forgotten.[48]

Max Weber mentioned the end of fossil fuel in the concluding paragraphs of his Die protestantische Ethik und der Geist des Kapitalismus (The Protestant Ethic and the Spirit of Capitalism), published in 1905.[49] Development of solar engines continued until the outbreak of World War I. The importance of solar energy was recognized in a 1911 Scientific American article: ""in the far distant future, natural fuels having been exhausted [solar power] will remain as the only means of existence of the human race"".[50]

The theory of peak oil was published in 1956.[51] In the 1970s environmentalists promoted the development of renewable energy both as a replacement for the eventual depletion of oil, as well as for an escape from dependence on oil, and the first electricity-generating wind turbines appeared. Solar had long been used for heating and cooling, but solar panels were too costly to build solar farms until 1980.[52]

Mainstream technologies

Wind power

[53] Wind energy generation by region over time.

[54] Global map of wind power density potential.

At the end of 2020, worldwide installed wind power capacity was 733 GW.[55]

Air flow can be used to run wind turbines. Modern utility-scale wind turbines range from around 600 kW to 9 MW of rated power. The power available from the wind is a function of the cube of the wind speed, so as wind speed increases, power output increases up to the maximum output for the particular turbine.[56] Areas where winds are stronger and more constant, such as offshore and high-altitude sites, are preferred locations for wind farms. Typically, full load hours of wind turbines vary between 16 and 57 percent annually but might be higher in particularly favorable offshore sites.[57]

Wind-generated electricity met nearly 4% of global electricity demand in 2015, with nearly 63 GW of new wind power capacity installed. Wind energy was the leading source of new capacity in Europe, the US and Canada, and the second largest in China. In Denmark, wind energy met more than 40% of its electricity demand while Ireland, Portugal and Spain each met nearly 20%.[citation needed]

Globally, the long-term technical potential of wind energy is believed to be five times total current global energy production, or 40 times current electricity demand, assuming all practical barriers needed were overcome. This would require wind turbines to be installed over large areas, particularly in areas of higher wind resources, such as offshore. As offshore wind speeds average ~90% greater than that of land, so offshore resources can contribute substantially more energy than land-stationed turbines.[58]

Hydropower

At the end of 2020, worldwide renewable hydropower capacity was 1,211 GW.[55]

Since water is about 800 times denser than air, even a slow flowing stream of water, or moderate sea swell, can yield considerable amounts of energy. There are many forms of water energy:

Historically, hydroelectric power came from constructing large hydroelectric dams and reservoirs, which are still popular in developing countries. [59] The largest of them are the Three Gorges Dam (2003) in China and the Itaipu Dam (1984) built by Brazil and Paraguay.

The largest of them are the Three Gorges Dam (2003) in China and the Itaipu Dam (1984) built by Brazil and Paraguay. Small hydro systems are hydroelectric power installations that typically produce up to 50 MW of power. They are often used on small rivers or as a low-impact development on larger rivers. China is the largest producer of hydroelectricity in the world and has more than 45,000 small hydro installations. [60]

of power. They are often used on small rivers or as a low-impact development on larger rivers. China is the largest producer of hydroelectricity in the world and has more than 45,000 small hydro installations. Run-of-the-river hydroelectricity plants derive energy from rivers without the creation of a large reservoir. The water is typically conveyed along the side of the river valley (using channels, pipes and/or tunnels) until it is high above the valley floor, whereupon it can be allowed to fall through a penstock to drive a turbine. This style of generation may still produce a large amount of electricity, such as the Chief Joseph Dam on the Columbia River in the United States.[61] Many run-of-the-river hydro power plants are micro hydro or pico hydro plants.

Hydropower is produced in 150 countries, with the Asia-Pacific region generating 32 percent of global hydropower in 2010. For countries having the largest percentage of electricity from renewables, the top 50 are primarily hydroelectric. China is the largest hydroelectricity producer, with 721 terawatt-hours of production in 2010, representing around 17 percent of domestic electricity use. There are now three hydroelectricity stations larger than 10 GW: the Three Gorges Dam in China, Itaipu Dam across the Brazil/Paraguay border, and Guri Dam in Venezuela.[62]

Wave power, which captures the energy of ocean surface waves, and tidal power, converting the energy of tides, are two forms of hydropower with future potential; however, they are not yet widely employed commercially. A demonstration project operated by the Ocean Renewable Power Company on the coast of Maine, and connected to the grid, harnesses tidal power from the Bay of Fundy, location of the world's highest tidal flow. Ocean thermal energy conversion, which uses the temperature difference between cooler deep and warmer surface waters, currently has no economic feasibility.[63][64]

Solar energy

At the end of 2020, global installed solar capacity was 714 GW.[55]

Solar energy, radiant light and heat from the sun, is harnessed using a range of ever-evolving technologies such as solar heating, photovoltaics, concentrated solar power (CSP), concentrator photovoltaics (CPV), solar architecture and artificial photosynthesis.[66][67] Solar technologies are broadly characterized as either passive solar or active solar depending on the way they capture, convert, and distribute solar energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light dispersing properties, and designing spaces that naturally circulate air. Active solar technologies encompass solar thermal energy, using solar collectors for heating, and solar power, converting sunlight into electricity either directly using photovoltaics (PV), or indirectly using concentrated solar power (CSP).

A photovoltaic system converts light into electrical direct current (DC) by taking advantage of the photoelectric effect.[68] Solar PV has turned into a multi-billion, fast-growing industry, continues to improve its cost-effectiveness, and has the most potential of any renewable technologies together with CSP.[69][70] Concentrated solar power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. Commercial concentrated solar power plants were first developed in the 1980s. CSP-Stirling has by far the highest efficiency among all solar energy technologies.

In 2011, the International Energy Agency said that ""the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries' energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared"".[66] Australia has the largest proportion of solar electricity in the world; in 2020, solar supplied 9.9% of electricity demand. [71]

Geothermal energy

At the end of 2020, global geothermal capacity was 14 GW.[55]

High temperature geothermal energy is from thermal energy generated and stored in the Earth. Thermal energy is the energy that determines the temperature of matter. Earth's geothermal energy originates from the original formation of the planet and from radioactive decay of minerals (in currently uncertain[72] but possibly roughly equal[73] proportions). The geothermal gradient, which is the difference in temperature between the core of the planet and its surface, drives a continuous conduction of thermal energy in the form of heat from the core to the surface. The adjective geothermal originates from the Greek roots geo, meaning earth, and thermos, meaning heat.

The heat that is used for geothermal energy can be from deep within the Earth, all the way down to Earth's core – 4,000 miles (6,400 km) down. At the core, temperatures may reach over 9,000 °F (5,000 °C). Heat conducts from the core to the surrounding rock. Extremely high temperature and pressure cause some rock to melt, which is commonly known as magma. Magma convects upward since it is lighter than the solid rock. This magma then heats rock and water in the crust, sometimes up to 700 °F (371 °C).[74]

Low temperature geothermal[42] refers to the use of the outer crust of the Earth as a thermal battery to facilitate renewable thermal energy for heating and cooling buildings, and other refrigeration and industrial uses. In this form of geothermal, a geothermal heat pump and ground-coupled heat exchanger are used together to move heat energy into the Earth (for cooling) and out of the Earth (for heating) on a varying seasonal basis. Low-temperature geothermal (generally referred to as ""GHP"") is an increasingly important renewable technology because it both reduces total annual energy loads associated with heating and cooling, and it also flattens the electric demand curve eliminating the extreme summer and winter peak electric supply requirements. Thus low temperature geothermal/GHP is becoming an increasing national priority with multiple tax credit support[75] and focus as part of the ongoing movement toward net zero energy.[43]

Bioenergy

At the end of 2020, bioenergy global capacity was 127 GW.[55]

Biomass is biological material derived from living, or recently living organisms. It most often refers to plants or plant-derived materials which are specifically called lignocellulosic biomass.[76] As an energy source, biomass can either be used directly via combustion to produce heat, or indirectly after converting it to various forms of biofuel. Conversion of biomass to biofuel can be achieved by different methods which are broadly classified into: thermal, chemical, and biochemical methods. Wood remains the largest biomass energy source today;[77] examples include forest residues – such as dead trees, branches and tree stumps –, yard clippings, wood chips and even municipal solid waste. In the second sense, biomass includes plant or animal matter that can be converted into fibers or other industrial chemicals, including biofuels. Industrial biomass can be grown from numerous types of plants, including miscanthus, switchgrass, hemp, corn, poplar, willow, sorghum, sugarcane, bamboo,[78] and a variety of tree species, ranging from eucalyptus to oil palm (palm oil).

Plant energy is produced by crops specifically grown for use as fuel that offer high biomass output per hectare with low input energy.[79] The grain can be used for liquid transportation fuels while the straw can be burned to produce heat or electricity. Plant biomass can also be degraded from cellulose to glucose through a series of chemical treatments, and the resulting sugar can then be used as a first-generation biofuel.

Biomass can be converted to other usable forms of energy such as methane gas[80] or transportation fuels such as ethanol and biodiesel. Rotting garbage, and agricultural and human waste, all release methane gas – also called landfill gas or biogas. Crops, such as corn and sugarcane, can be fermented to produce the transportation fuel, ethanol. Biodiesel, another transportation fuel, can be produced from left-over food products such as vegetable oils and animal fats.[81] Also, biomass to liquids (BTLs) and cellulosic ethanol are still under research.[82][83] There is a great deal of research involving algal fuel or algae-derived biomass due to the fact that it is a non-food resource and can be produced at rates 5 to 10 times those of other types of land-based agriculture, such as corn and soy. Once harvested, it can be fermented to produce biofuels such as ethanol, butanol, and methane, as well as biodiesel and hydrogen. The biomass used for electricity generation varies by region. Forest by-products, such as wood residues, are common in the United States. Agricultural waste is common in Mauritius (sugar cane residue) and Southeast Asia (rice husks). Animal husbandry residues, such as poultry litter, are common in the United Kingdom.[84]

Biofuels include a wide range of fuels which are derived from biomass. The term covers solid, liquid, and gaseous fuels.[85] Liquid biofuels include bioalcohols, such as bioethanol, and oils, such as biodiesel. Gaseous biofuels include biogas, landfill gas and synthetic gas. Bioethanol is an alcohol made by fermenting the sugar components of plant materials and it is made mostly from sugar and starch crops. These include maize, sugarcane and, more recently, sweet sorghum. The latter crop is particularly suitable for growing in dryland conditions, and is being investigated by International Crops Research Institute for the Semi-Arid Tropics for its potential to provide fuel, along with food and animal feed, in arid parts of Asia and Africa.[86]

With advanced technology being developed, cellulosic biomass, such as trees and grasses, are also used as feedstocks for ethanol production. Ethanol can be used as a fuel for vehicles in its pure form, but it is usually used as a gasoline additive to increase octane and improve vehicle emissions. Bioethanol is widely used in the United States and in Brazil. The energy costs for producing bio-ethanol are almost equal to, the energy yields from bio-ethanol. However, according to the European Environment Agency, biofuels do not address global warming concerns.[87] Biodiesel is made from vegetable oils, animal fats or recycled greases. It can be used as a fuel for vehicles in its pure form, or more commonly as a diesel additive to reduce levels of particulates, carbon monoxide, and hydrocarbons from diesel-powered vehicles. Biodiesel is produced from oils or fats using transesterification and is the most common biofuel in Europe. Biofuels provided 2.7% of the world's transport fuel in 2010.[88]

Biomass, biogas and biofuels are burned to produce heat/power and in doing so harm the environment. Pollutants such as sulphurous oxides (SO x ), nitrous oxides (NO x ), and particulate matter (PM) are produced from the combustion of biomass; the World Health Organisation estimates that 7 million premature deaths are caused each year by air pollution.[89] Biomass combustion is a major contributor.[89][90][91][failed verification]

Emerging technologies

Other renewable energy technologies are still under development, and include cellulosic ethanol, hot-dry-rock geothermal power, and marine energy.[92] These technologies are not yet widely demonstrated or have limited commercialization. Many are on the horizon and may have potential comparable to other renewable energy technologies, but still depend on attracting sufficient attention and research, development and demonstration (RD&D) funding.[92]

There are numerous organizations within the academic, federal, and commercial sectors conducting large-scale advanced research in the field of renewable energy. This research spans several areas of focus across the renewable energy spectrum. Most of the research is targeted at improving efficiency and increasing overall energy yields.[93] Multiple federally supported research organizations have focused on renewable energy in recent years. Two of the most prominent of these labs are Sandia National Laboratories and the National Renewable Energy Laboratory (NREL), both of which are funded by the United States Department of Energy and supported by various corporate partners.[94] Sandia has a total budget of $2.4 billion[95] while NREL has a budget of $375 million.[96]

Enhanced geothermal system

Enhanced geothermal systems (EGS) are a new type of geothermal power technology that does not require natural convective hydrothermal resources. The vast majority of geothermal energy within drilling reach is in dry and non-porous rock.[97] EGS technologies ""enhance"" and/or create geothermal resources in this ""hot dry rock (HDR)"" through hydraulic fracturing. EGS and HDR technologies, such as hydrothermal geothermal, are expected to be baseload resources that produce power 24 hours a day like a fossil plant. Distinct from hydrothermal, HDR and EGS may be feasible anywhere in the world, depending on the economic limits of drill depth. Good locations are over deep granite covered by a thick (3–5 km) layer of insulating sediments which slow heat loss.[98] There are HDR and EGS systems currently being developed and tested in France, Australia, Japan, Germany, the U.S., and Switzerland. The largest EGS project in the world is a 25 megawatt demonstration plant currently being developed in the Cooper Basin, Australia. The Cooper Basin has the potential to generate 5,000–10,000 MW.

Cellulosic ethanol

Several refineries that can process biomass and turn it into ethanol are built by companies such as Iogen, POET, and Abengoa, while other companies such as the Verenium Corporation, Novozymes, and Dyadic International[99] are producing enzymes which could enable future commercialization. The shift from food crop feedstocks to waste residues and native grasses offers significant opportunities for a range of players, from farmers to biotechnology firms, and from project developers to investors.[100]

Marine energy

Marine energy (also sometimes referred to as ocean energy) refers to the energy carried by ocean waves, tides, salinity, and ocean temperature differences. The movement of water in the world's oceans creates a vast store of kinetic energy, or energy in motion. This energy can be harnessed to generate electricity to power homes, transport and industries. The term marine energy encompasses both wave power – power from surface waves, and tidal power – obtained from the kinetic energy of large bodies of moving water. Reverse electrodialysis (RED) is a technology for generating electricity by mixing fresh river water and salty sea water in large power cells designed for this purpose; as of 2016, it is being tested at a small scale (50 kW). Offshore wind power is not a form of marine energy, as wind power is derived from the wind, even if the wind turbines are placed over water. The oceans have a tremendous amount of energy and are close to many if not most concentrated populations. Ocean energy has the potential of providing a substantial amount of new renewable energy around the world.[101]

Solar energy developments

Experimental solar power

Concentrated photovoltaics (CPV) systems employ sunlight concentrated onto photovoltaic surfaces for the purpose of electricity generation. Thermoelectric, or ""thermovoltaic"" devices convert a temperature difference between dissimilar materials into an electric current.

Floating solar arrays

Floating solar arrays are PV systems that float on the surface of drinking water reservoirs, quarry lakes, irrigation canals or remediation and tailing ponds. A small number of such systems exist in France, India, Japan, South Korea, the United Kingdom, Singapore, and the United States.[104][105][106][107][108] The systems are said to have advantages over photovoltaics on land. The cost of land is more expensive, and there are fewer rules and regulations for structures built on bodies of water not used for recreation. Unlike most land-based solar plants, floating arrays can be unobtrusive because they are hidden from public view. They achieve higher efficiencies than PV panels on land, because water cools the panels. The panels have a special coating to prevent rust or corrosion.[109] In May 2008, the Far Niente Winery in Oakville, California, pioneered the world's first floatovoltaic system by installing 994 solar PV modules with a total capacity of 477 kW onto 130 pontoons and floating them on the winery's irrigation pond.[110] Utility-scale floating PV farms are starting to be built. Kyocera will develop the world's largest, a 13.4 MW farm on the reservoir above Yamakura Dam in Chiba Prefecture[111] using 50,000 solar panels.[112][113] Salt-water resistant floating farms are also being constructed for ocean use.[114] The largest so far announced floatovoltaic project is a 350 MW power station in the Amazon region of Brazil.[115]

Solar-assisted heat pump

A heat pump is a device that provides heat energy from a source of heat to a destination called a ""heat sink"". Heat pumps are designed to move thermal energy opposite to the direction of spontaneous heat flow by absorbing heat from a cold space and releasing it to a warmer one. A solar-assisted heat pump represents the integration of a heat pump and thermal solar panels in a single integrated system. Typically these two technologies are used separately (or only placing them in parallel) to produce hot water.[116] In this system the solar thermal panel performs the function of the low temperature heat source and the heat produced is used to feed the heat pump's evaporator.[117] The goal of this system is to get high COP and then produce energy in a more efficient and less expensive way.

It is possible to use any type of solar thermal panel (sheet and tubes, roll-bond, heat pipe, thermal plates) or hybrid (mono/polycrystalline, thin film) in combination with the heat pump. The use of a hybrid panel is preferable because it allows covering a part of the electricity demand of the heat pump and reduces the power consumption and consequently the variable costs of the system.

Solar aircraft

An electric aircraft is an aircraft that runs on electric motors rather than internal combustion engines, with electricity coming from fuel cells, solar cells, ultracapacitors, power beaming,[118] or batteries.

Currently, flying manned electric aircraft are mostly experimental demonstrators, though many small unmanned aerial vehicles are powered by batteries. Electrically powered model aircraft have been flown since the 1970s, with one report in 1957.[119][120] The first man-carrying electrically powered flights were made in 1973.[121] Between 2015–2016, a manned, solar-powered plane, Solar Impulse 2, completed a circumnavigation of the Earth.[122]

Solar updraft tower

A solar updraft tower is a renewable-energy power plant for generating electricity from low-temperature solar heat. Sunshine heats the air beneath a very wide greenhouse-like roofed collector structure surrounding the central base of a very tall chimney tower. The resulting convection causes a hot air updraft in the tower by the chimney effect. This airflow drives wind turbines placed in the chimney updraft or around the chimney base to produce electricity. Plans for scaled-up versions of demonstration models will allow significant power generation and may allow the development of other applications, such as water extraction or distillation, and agriculture or horticulture. A more advanced version of a similarly themed technology is the Vortex engine which aims to replace large physical chimneys with a vortex of air created by a shorter, less-expensive structure.

Space-based solar power

For either photovoltaic or thermal systems, one option is to loft them into space, particularly Geosynchronous orbit. To be competitive with Earth-based solar power systems, the specific mass (kg/kW) times the cost to loft mass plus the cost of the parts needs to be $2400 or less. I.e., for a parts cost plus rectenna of $1100/kW, the product of the $/kg and kg/kW must be $1300/kW or less.[123] Thus for 6.5 kg/kW, the transport cost cannot exceed $200/kg. While that will require a 100 to one reduction, SpaceX is targeting a ten to one reduction, Reaction Engines may make a 100 to one reduction possible.

Artificial photosynthesis

Artificial photosynthesis uses techniques including nanotechnology to store solar electromagnetic energy in chemical bonds by splitting water to produce hydrogen and then using carbon dioxide to make methanol.[124] Researchers in this field are striving to design molecular mimics of photosynthesis that use a wider region of the solar spectrum, employ catalytic systems made from abundant, inexpensive materials that are robust, readily repaired, non-toxic, stable in a variety of environmental conditions and perform more efficiently allowing a greater proportion of photon energy to end up in the storage compounds, i.e., carbohydrates (rather than building and sustaining living cells).[125] However, prominent research faces hurdles, Sun Catalytix a MIT spin-off stopped scaling up their prototype fuel-cell in 2012, because it offers few savings over other ways to make hydrogen from sunlight.[126]

Others

Algae fuels

Producing liquid fuels from oil-rich varieties of algae is an ongoing research topic. Various microalgae grown in open or closed systems are being tried including some systems that can be set up in brownfield and desert lands.

Water vapor

Collection of static electricity charges from water droplets on metal surfaces is an experimental technology that would be especially useful in low-income countries with relative air humidity over 60%.[127]

Crop wastes

AuREUS devices (Aurora Renewable Energy & UV Sequestration),[128] which are based on crop wastes can absorb ultraviolet light from the sun and turn it into renewable energy.[129][130]

Integration into the energy system

Renewable energy production from some sources such as wind and solar is more variable and more geographically spread than technology based on fossil fuels and nuclear. While integrating it into the wider energy system is feasible, it does lead to some additional challenges. In order for the energy system to remain stable, a set of measurements can be taken. Implementation of energy storage, using a wide variety of renewable energy technologies, and implementing a smart grid in which energy is automatically used at the moment it is produced can reduce risks and costs of renewable energy implementation.[131] In some locations, individual households can opt to purchase renewable energy through a consumer green energy program.

Electrical energy storage

Electrical energy storage is a collection of methods used to store electrical energy. Electrical energy is stored during times when production (especially from intermittent sources such as wind power, tidal power, solar power) exceeds consumption, and returned to the grid when production falls below consumption. Pumped-storage hydroelectricity accounts for more than 90% of all grid power storage. Costs of lithium-ion batteries are dropping rapidly, and are increasingly being deployed grid ancillary services and for domestic storage. Additionally, power can be stored in hydrogen fuel cells.

Market and industry trends

Renewable power has been more effective in creating jobs than coal or oil in the United States.[132] In 2016, employment in the sector increased 6 percent in the United States, causing employment in the non-renewable energy sector to decrease 18 percent. Worldwide, renewables employ about 8.1 million as of 2016.[133]

Growth of renewables

[134] Investment: Companies, governments and households committed $501.3 billion to decarbonization in 2020, including renewable energy (solar, wind), electric vehicles and associated charging infrastructure, energy storage, energy-efficient heating systems, carbon capture and storage, and hydrogen. Renewable energy investment by region [135]

(LCOE) is a measure of the average net present cost of electricity generation for a generating plant over its lifetime. Cost: With increasingly widespread implementation of renewable energy sources, costs have declined, most notably for energy generated by solar panels. Levelized cost of energy (LCOE) is a measure of the average net present cost of electricity generation for a generating plant over its lifetime.

[136] In 2020, renewables overtook fossil fuels as the European Union's main source of electricity for the first time.

[137] Comparing worldwide energy use, the growth of renewable energy is shown by the green line

From the end of 2004, worldwide renewable energy capacity grew at rates of 10–60% annually for many technologies. In 2015 global investment in renewables rose 5% to $285.9 billion, breaking the previous record of $278.5 billion in 2011. 2015 was also the first year that saw renewables, excluding large hydro, account for the majority of all new power capacity (134 GW, making up 54% of the total).[citation needed] Of the renewables total, wind accounted for 72 GW and solar photovoltaics 56 GW; both record-breaking numbers and sharply up from 2014 figures (49 GW and 45 GW respectively). In financial terms, solar made up 56% of total new investment and wind accounted for 38%.

In 2014 global wind power capacity expanded 16% to 369,553 MW.[138] Yearly wind energy production is also growing rapidly and has reached around 4% of worldwide electricity usage,[139] 11.4% in the EU,[140] and it is widely used in Asia, and the United States. In 2015, worldwide installed photovoltaics capacity increased to 227 gigawatts (GW), sufficient to supply 1 percent of global electricity demands.[141] Solar thermal energy stations operate in the United States and Spain, and as of 2016, the largest of these is the 392 MW Ivanpah Solar Electric Generating System in California.[142][143] The world's largest geothermal power installation is The Geysers in California, with a rated capacity of 750 MW. Brazil has one of the largest renewable energy programs in the world, involving production of ethanol fuel from sugar cane, and ethanol now provides 18% of the country's automotive fuel. Ethanol fuel is also widely available in the United States.

In 2017, investments in renewable energy amounted to US$279.8 billion worldwide, with China accounting for US$126.6 billion or 45% of the global investments, the United States for US$40.5 billion, and Europe for US$40.9 billion.[8] The results of a recent review of the literature concluded that as greenhouse gas (GHG) emitters begin to be held liable for damages resulting from GHG emissions resulting in climate change, a high value for liability mitigation would provide powerful incentives for deployment of renewable energy technologies.[144]

In the decade of 2010–2019, worldwide investment in renewable energy capacity excluding large hydropower amounted to US$2.7 trillion, of which the top countries China contributed US$818 billion, the United States contributed US$392.3 billion, Japan contributed US$210.9 billion, Germany contributed US$183.4 billion, and the United Kingdom contributed US$126.5 billion.[145] This was an increase of over three and possibly four times the equivalent amount invested in the decade of 2000–2009 (no data is available for 2000–2003).[145]

Selected renewable energy global indicators 2008 2009 2010 2011 2012 2013 2014 2015 2016 Investment in new renewable capacity (annual) (109 USD)[146] 182 178 237 279 256 232 270 285 241 Renewables power capacity (existing) (GWe) 1,140 1,230 1,320 1,360 1,470 1,578 1,712 1,849 2,017 Hydropower capacity (existing) (GWe) 885 915 945 970 990 1,018 1,055 1,064 1,096 Wind power capacity (existing) (GWe) 121 159 198 238 283 319 370 433 487 Solar PV capacity (grid-connected) (GWe) 16 23 40 70 100 138 177 227 303 Solar hot water capacity (existing) (GWth) 130 160 185 232 255 373 406 435 456 Ethanol production (annual) (109 litres) 67 76 86 86 83 87 94 98 98.6 Biodiesel production (annual) (109 litres) 12 17.8 18.5 21.4 22.5 26 29.7 30 30.8 Countries with policy targets

for renewable energy use 79 89 98 118 138 144 164 173 176 Source: The Renewable Energy Policy Network for the 21st Century (REN21)–Global Status Report[147][148][149][150][151][152]

Future projections

Renewable energy technologies are getting cheaper, through technological change and through the benefits of mass production and market competition. A 2018 report from the International Renewable Energy Agency (IRENA), found that the cost of renewable energy is quickly falling, and will likely be equal to or less than the cost of non-renewables such as fossil fuels by 2020. The report found that solar power costs have dropped 73% since 2010 and onshore wind costs have dropped by 23% in that same timeframe.[155]

Current projections concerning the future cost of renewables vary, however. The EIA has predicted that almost two-thirds of net additions to power capacity will come from renewables by 2020 due to the combined policy benefits of local pollution, decarbonisation and energy diversification.

According to a 2018 report by Bloomberg New Energy Finance, wind and solar power are expected to generate roughly 50% of the world's energy needs by 2050, while coal powered electricity plants are expected to drop to just 11%.[156] Hydro-electricity and geothermal electricity produced at favourable sites are now the cheapest way to generate electricity. Renewable energy costs continue to drop, and the levelised cost of electricity (LCOE) is declining for wind power, solar photovoltaic (PV), concentrated solar power (CSP) and some biomass technologies.[157] Renewable energy is also the most economic solution for new grid-connected capacity in areas with good resources. As the cost of renewable power falls, the scope of economically viable applications increases. Renewable technologies are now often the most economic solution for new generating capacity. Where ""oil-fired generation is the predominant power generation source (e.g. on islands, off-grid and in some countries) a lower-cost renewable solution almost always exists today"".[157] A series of studies by the US National Renewable Energy Laboratory modeled the ""grid in the Western US under a number of different scenarios where intermittent renewables accounted for 33 percent of the total power."" In the models, inefficiencies in cycling the fossil fuel plants to compensate for the variation in solar and wind energy resulted in an additional cost of ""between $0.47 and $1.28 to each MegaWatt hour generated""; however, the savings in the cost of the fuels saved ""adds up to $7 billion, meaning the added costs are, at most, two percent of the savings.""[158]

Trends for individual technologies

Hydroelectricity

In 2017 the world renewable hydropower capacity was 1,154 GW.[19] Only a quarter of the worlds estimated hydroelectric potential of 14,000 TWh/year has been developed, the regional potentials for the growth of hydropower around the world are, 71% Europe, 75% North America, 79% South America, 95% Africa, 95% Middle East, 82% Asia Pacific. However, the political realities of new reservoirs in western countries, economic limitations in the third world and the lack of a transmission system in undeveloped areas result in the possibility of developing 25% of the remaining potential before 2050, with the bulk of that being in the Asia Pacific area.[159] There is slow growth taking place in Western counties,[citation needed] but not in the conventional dam and reservoir style of the past. New projects take the form of run-of-the-river and small hydro, neither using large reservoirs. It is popular to repower old dams thereby increasing their efficiency and capacity as well as quicker responsiveness on the grid.[160] Where circumstances permit existing dams such as the Russell Dam built in 1985 may be updated with ""pump back"" facilities for pumped-storage which is useful for peak loads or to support intermittent wind and solar power. Countries with large hydroelectric developments such as Canada and Norway are spending billions to expand their grids to trade with neighboring countries having limited hydro.[161]

Wind power development

Worldwide growth of wind capacity (1996–2018)

Wind power is widely used in Europe, China, and the United States. From 2004 to 2017, worldwide installed capacity of wind power has been growing from 47 GW to 514 GW—a more than tenfold increase within 13 years[19] As of the end of 2014, China, the United States and Germany combined accounted for half of total global capacity.[138] Several other countries have achieved relatively high levels of wind power penetration, such as 21% of stationary electricity production in Denmark, 18% in Portugal, 16% in Spain, and 14% in Ireland in 2010 and have since continued to expand their installed capacity.[162][163] More than 80 countries around the world are using wind power on a commercial basis.[88]

Wind turbines are increasing in power with some commercially deployed models generating over 8MW per turbine.[164][165][166] More powerful models are in development, see list of most powerful wind turbines.

As of 2017, offshore wind power amounted to 18.7 GW of global installed capacity, accounting for only 3.6% of the total wind power capacity.[19]

List of offshore and onshore wind farms

As of 2013, the Alta Wind Energy Center (California, 1.5 GW) is the world's largest single wind farm.[167] The Walney Extension (London, 0.7 GW) is the largest offshore wind farm in the world. Gansu Wind Farm (China, 7.9 GW) is the largest wind energy project generating project consisting of 18 wind farms.[168]

Solar thermal

Solar thermal energy capacity has increased from 1.3 GW in 2012 to 5.0 GW in 2017.[19]

Spain is the world leader in solar thermal power deployment with 2.3 GW deployed.[19] The United States has 1.8 GW,[19] most of it in California where 1.4 GW of solar thermal power projects are operational.[169] Several power plants have been constructed in the Mojave Desert, Southwestern United States. As of 2017 only 4 other countries have deployments above 100 MW:[19] South Africa (300 MW) India (229 MW) Morocco (180 MW) and United Arab Emirates (100 MW).

The United States conducted much early research in photovoltaics and concentrated solar power. The U.S. is among the top countries in the world in electricity generated by the Sun and several of the world's largest utility-scale installations are located in the desert Southwest.

The oldest solar thermal power plant in the world is the 354 megawatt (MW) SEGS thermal power plant, in California.[170] The Ivanpah Solar Electric Generating System is a solar thermal power project in the California Mojave Desert, 40 miles (64 km) southwest of Las Vegas, with a gross capacity of 377 MW.[171] The 280 MW Solana Generating Station is a solar power plant near Gila Bend, Arizona, about 70 miles (110 km) southwest of Phoenix, completed in 2013. When commissioned it was the largest parabolic trough plant in the world and the first U.S. solar plant with molten salt thermal energy storage.[172]

In developing countries, three World Bank projects for integrated solar thermal/combined-cycle gas-turbine power plants in Egypt, Mexico, and Morocco have been approved.[173]

Photovoltaic development

50,000 100,000 150,000 200,000 2006 2010 2014 Europe Asia-Pacific Americas China Middle East and Africa Worldwide growth of PV capacity grouped by region in MW (2006–2014)

Photovoltaics (PV) is rapidly-growing with global capacity increasing from 177 GW at the end of 2014 to 385 GW in 2017.[19]

PV uses solar cells assembled into solar panels to convert sunlight into electricity. PV systems range from small, residential and commercial rooftop or building integrated installations, to large utility-scale photovoltaic power station. The predominant PV technology is crystalline silicon, while thin-film solar cell technology accounts for about 10 percent of global photovoltaic deployment. In recent years, PV technology has improved its electricity generating efficiency, reduced the installation cost per watt as well as its energy payback time, and reached grid parity in at least 30 different markets by 2014.[174] Building-integrated photovoltaics or ""onsite"" PV systems use existing land and structures and generate power close to where it is consumed.[175]

Photovoltaics grew fastest in China, followed by Japan and the United States. Solar power is forecasted to become the world's largest source of electricity by 2050, with solar photovoltaics and concentrated solar power contributing 16% and 11%, respectively. This requires an increase of installed PV capacity to 4,600 GW, of which more than half is expected to be deployed in China and India.[176]

Commercial concentrated solar power plants were first developed in the 1980s. As the cost of solar electricity has fallen, the number of grid-connected solar PV systems has grown into the millions and utility-scale solar power stations with hundreds of megawatts are being built. Many solar photovoltaic power stations have been built, mainly in Europe, China and the United States.[177] The 1.5 GW Tengger Desert Solar Park, in China is the world's largest PV power station. Many of these plants are integrated with agriculture and some use tracking systems that follow the sun's daily path across the sky to generate more electricity than fixed-mounted systems.

Biofuel development

Brazil produces bioethanol made from sugarcane available throughout the country. A typical gas station with dual fuel service is marked ""A"" for alcohol (ethanol) and ""G"" for gasoline.

Bioenergy global capacity in 2017 was 109 GW.[19] Biofuels provided 3% of the world's transport fuel in 2017.[178]

Mandates for blending biofuels exist in 31 countries at the national level and in 29 states/provinces.[88] According to the International Energy Agency, biofuels have the potential to meet more than a quarter of world demand for transportation fuels by 2050.[179]

Since the 1970s, Brazil has had an ethanol fuel program which has allowed the country to become the world's second largest producer of ethanol (after the United States) and the world's largest exporter.[180] Brazil's ethanol fuel program uses modern equipment and cheap sugarcane as feedstock, and the residual cane-waste (bagasse) is used to produce heat and power.[181] There are no longer light vehicles in Brazil running on pure gasoline. By the end of 2008 there were 35,000 filling stations throughout Brazil with at least one ethanol pump.[182] Unfortunately, Operation Car Wash has seriously eroded public trust in oil companies and has implicated several high ranking Brazilian officials.

Nearly all the gasoline sold in the United States today is mixed with 10% ethanol,[183] and motor vehicle manufacturers already produce vehicles designed to run on much higher ethanol blends. Ford, Daimler AG, and GM are among the automobile companies that sell ""flexible-fuel"" cars, trucks, and minivans that can use gasoline and ethanol blends ranging from pure gasoline up to 85% ethanol. By mid-2006, there were approximately 6 million ethanol compatible vehicles on U.S. roads.[184]

Geothermal development

Global geothermal capacity in 2017 was 12.9 GW.[19]

Geothermal power is cost effective, reliable, sustainable, and environmentally friendly,[185] but has historically been limited to areas near tectonic plate boundaries. Recent technological advances have expanded the range and size of viable resources, especially for applications such as home heating, opening a potential for widespread exploitation. Geothermal wells release greenhouse gases trapped deep within the earth, but these emissions are usually much lower per energy unit than those of fossil fuels. As a result, geothermal power has the potential to help mitigate global warming if widely deployed in place of fossil fuels.

In 2017, the United States led the world in geothermal electricity production with 12.9 GW of installed capacity.[19] The largest group of geothermal power plants in the world is located at The Geysers, a geothermal field in California.[186] The Philippines follows the US as the second highest producer of geothermal power in the world, with 1.9 GW of capacity online.[19]

Developing countries

Renewable energy technology has sometimes been seen as a costly luxury item by critics, and affordable only in the affluent developed world. This erroneous view has persisted for many years, however between 2016 and 2017, investments in renewable energy were higher in developing countries than in developed countries, with China leading global investment with a record 126.6 billion dollars. Many Latin American and African countries increased their investments significantly as well.[187] Renewable energy can be particularly suitable for developing countries. In rural and remote areas, transmission and distribution of energy generated from fossil fuels can be difficult and expensive. Producing renewable energy locally can offer a viable alternative.[188]

Technology advances are opening up a huge new market for solar power: the approximately 1.3 billion people around the world who do not have access to grid electricity. Even though they are typically very poor, these people have to pay far more for lighting than people in rich countries because they use inefficient kerosene lamps. Solar power costs half as much as lighting with kerosene.[189] As of 2010, an estimated 3 million households get power from small solar PV systems.[190] Kenya is the world leader in the number of solar power systems installed per capita. More than 30,000 very small solar panels, each producing 1[191] 2 to 30 watts, are sold in Kenya annually. Some Small Island Developing States (SIDS) are also turning to solar power to reduce their costs and increase their sustainability.

Micro-hydro configured into mini-grids also provide power. Over 44 million households use biogas made in household-scale digesters for lighting and/or cooking, and more than 166 million households rely on a new generation of more-efficient biomass cookstoves.[38] Clean liquid fuel sourced from renewable feedstocks are used for cooking and lighting in energy-poor areas of the developing world. Alcohol fuels (ethanol and methanol) can be produced sustainably from non-food sugary, starchy, and cellulosic feedstocks. Project Gaia, Inc. and CleanStar Mozambique are implementing clean cooking programs with liquid ethanol stoves in Ethiopia, Kenya, Nigeria and Mozambique.[192]

Renewable energy projects in many developing countries have demonstrated that renewable energy can directly contribute to poverty reduction by providing the energy needed for creating businesses and employment. Renewable energy technologies can also make indirect contributions to alleviating poverty by providing energy for cooking, space heating, and lighting. Renewable energy can also contribute to education, by providing electricity to schools.[193]

Policy

Policies to support renewable energy have been vital in their expansion. Where Europe dominated in establishing energy policy in early 2000s, most countries around the world now have some form of energy policy.[194]

Policy trends

The International Renewable Energy Agency (IRENA) is an intergovernmental organization for promoting the adoption of renewable energy worldwide. It aims to provide concrete policy advice and facilitate capacity building and technology transfer. IRENA was formed in 2009, by 75 countries signing the charter of IRENA.[195] As of April 2019, IRENA has 160 member states.[196] The then United Nations' Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity,[39] and in September 2011 he launched the UN Sustainable Energy for All initiative to improve energy access, efficiency and the deployment of renewable energy.[197]

The 2015 Paris Agreement on climate change motivated many countries to develop or improve renewable energy policies.[16] In 2017, a total of 121 countries have adapted some form of renewable energy policy.[194] National targets that year existed in at 176 countries.[16] In addition, there is also a wide range of policies at state/provincial and local levels.[88] Some public utilities help plan or install residential energy upgrades. Under president Barack Obama, the United States policy encouraged the uptake of renewable energy in line with commitments to the Paris agreement. Even though Trump has abandoned these goals, renewable investment is still on the rise.[198]

Many national, state, and local governments have created green banks. A green bank is a quasi-public financial institution that uses public capital to leverage private investment in clean energy technologies.[199] Green banks use a variety of financial tools to bridge market gaps that hinder the deployment of clean energy. The US military has also focused on the use of renewable fuels for military vehicles. Unlike fossil fuels, renewable fuels can be produced in any country, creating a strategic advantage. The US military has already committed itself to have 50% of its energy consumption come from alternative sources.[200]

Full renewable energy

The incentive to use 100% renewable energy, for electricity, transport, or even total primary energy supply globally, has been motivated by global warming and other ecological as well as economic concerns. The Intergovernmental Panel on Climate Change has said that there are few fundamental technological limits to integrating a portfolio of renewable energy technologies to meet most of the total global energy demand. Renewable energy use has grown much faster than even advocates anticipated.[201] At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. Also, Professors S. Pacala and Robert H. Socolow have developed a series of ""stabilization wedges"" that can allow us to maintain our quality of life while avoiding catastrophic climate change, and ""renewable energy sources,"" in aggregate, constitute the largest number of their ""wedges"".[202]

Using 100% renewable energy was first suggested in a Science paper published in 1975 by Danish physicist Bent Sørensen.[203] It was followed by several other proposals, until in 1998 the first detailed analysis of scenarios with very high shares of renewables were published. These were followed by the first detailed 100% scenarios. In 2006 a PhD thesis was published by Czisch in which it was shown that in a 100% renewable scenario energy supply could match demand in every hour of the year in Europe and North Africa. In the same year Danish Energy professor Henrik Lund published a first paper[204] in which he addresses the optimal combination of renewables, which was followed by several other papers on the transition to 100% renewable energy in Denmark. Since then Lund has been publishing several papers on 100% renewable energy. After 2009 publications began to rise steeply, covering 100% scenarios for countries in Europe, America, Australia and other parts of the world.[205]

In 2011 Mark Z. Jacobson, professor of civil and environmental engineering at Stanford University, and Mark Delucchi published a study on 100% renewable global energy supply in the journal Energy Policy. They found producing all new energy with wind power, solar power, and hydropower by 2030 is feasible and existing energy supply arrangements could be replaced by 2050. Barriers to implementing the renewable energy plan are seen to be ""primarily social and political, not technological or economic"".[206] They also found that energy costs with a wind, solar, water system should be similar to today's energy costs.[207]

Similarly, in the United States, the independent National Research Council has noted that ""sufficient domestic renewable resources exist to allow renewable electricity to play a significant role in future electricity generation and thus help confront issues related to climate change, energy security, and the escalation of energy costs … Renewable energy is an attractive option because renewable resources available in the United States, taken collectively, can supply significantly greater amounts of electricity than the total current or projected domestic demand.""[208]

The most significant barriers to the widespread implementation of large-scale renewable energy and low carbon energy strategies are primarily political and not technological.[209][210] According to the 2013 Post Carbon Pathways report, which reviewed many international studies, the key roadblocks are: climate change denial, the fossil fuels lobby, political inaction, unsustainable energy consumption, outdated energy infrastructure, and financial constraints.[211]

According to World Bank the ""below 2°C"" climate scenario requires 3 billions of tonnes of metals and minerals by 2050. Supply of mined resources such as zinc, molybdenum, silver, nickel, copper must increase by up to 500%.[212] A 2018 analysis estimated required increases in stock of metals required by various sectors from 1000% (wind power) to 87'000% (personal vehicle batteries).[213]

Debate

Renewable electricity production, from sources such as wind power and solar power, is variable which results in reduced capacity factor and require either energy storage of capacity equal to its total output, or base load power sources from non-intermittent sources like hydropower, fossil fuels or nuclear power.

Since renewable energy sources power density per land area is at best three orders of magnitude smaller than fossil or nuclear power,[214] renewable power plants tends to occupy thousands of hectares causing environmental concerns and opposition from local residents, especially in densely populated countries. Solar power plants are competing with arable land and nature reserves,[215] while on-shore wind farms face opposition due to aesthetic concerns and noise, which is impacting both humans and wildlife.[216][217][218][219] In the United States, the Massachusetts Cape Wind project was delayed for years partly because of aesthetic concerns. However, residents in other areas have been more positive. According to a town councilor, the overwhelming majority of locals believe that the Ardrossan Wind Farm in Scotland has enhanced the area.[220] These concerns, when directed against renewable energy, are sometimes described as ""not in my back yard"" attitude (NIMBY).

A recent[when?] UK Government document states that ""projects are generally more likely to succeed if they have broad public support and the consent of local communities. This means giving communities both a say and a stake"".[221] In countries such as Germany and Denmark many renewable projects are owned by communities, particularly through cooperative structures, and contribute significantly to overall levels of renewable energy deployment.[222][223]

The market for renewable energy technologies has continued to grow. Climate change concerns and increasing in green jobs, coupled with high oil prices, peak oil, oil wars, oil spills, promotion of electric vehicles and renewable electricity, nuclear disasters and increasing government support, are driving increasing renewable energy legislation, incentives and commercialization.[22] New government spending, regulation and policies helped the industry weather the 2009 economic crisis better than many other sectors.[36]

While renewables have been very successful in their ever-growing contribution to electrical power there are no countries dominated by fossil fuels who have a plan to stop and get that power from renewables. Only Scotland and Ontario have stopped burning coal, largely due to good natural gas supplies. In the area of transportation, fossil fuels are even more entrenched and solutions harder to find.[224] It's unclear if there are failures with policy or renewable energy, but twenty years after the Kyoto Protocol fossil fuels are still our primary energy source and consumption continues to grow.[225]

The International Energy Agency has stated that deployment of renewable technologies usually increases the diversity of electricity sources and, through local generation, contributes to the flexibility of the system and its resistance to central shocks.[226]

Geopolitics of renewable energy

From around 2010 onwards, there was increasing discussion about the geopolitical impact of the growing use of renewable energy.[227] It was argued that former fossil fuels exporters would experience a weakening of their position in international affairs, while countries with abundant renewable energy resources would be strengthened.[228] Also countries rich in critical materials for renewable energy technologies were expected to rise in importance in international affairs.[229]

The GeGaLo index of geopolitical gains and losses assesses how the geopolitical position of 156 countries may change if the world fully transitions to renewable energy resources. Former fossil fuels exporters are expected to lose power, while the positions of former fossil fuel importers and countries rich in renewable energy resources is expected to strengthen.[230]

Environmental impact

The ability of biomass and biofuels to contribute to a reduction in CO

2 emissions is limited because both biomass and biofuels emit large amounts of air pollution when burned and in some cases compete with food supply. Furthermore, biomass and biofuels consume large amounts of water.[231] Other renewable sources such as wind power, photovoltaics, and hydroelectricity have the advantage of being able to conserve water, lower pollution and reduce CO

2 emissions. The installations used to produce wind, solar and hydro power are an increasing threat to key conservation areas, with facilities built in areas set aside for nature conservation and other environmentally sensitive areas. They are often much larger than fossil fuel power plants, needing areas of land up to 10 times greater than coal or gas to produce equivalent energy amounts.[232] More than 2000 renewable energy facilities are built, and more are under construction, in areas of environmental importance and threaten the habitats of plant and animal species across the globe. The authors' team emphasized that their work should not be interpreted as anti-renewables because renewable energy is crucial for reducing carbon emissions. The key is ensuring that renewable energy facilities are built in places where they do not damage biodiversity.[233]

Renewable energy devices depend on non-renewable resources such as mined metals and use vast amounts of land due to their small surface power density. Manufacturing of photovoltaic panels, wind turbines and batteries requires significant amounts of rare-earth elements[234] and increases mining operations, which have significant social and environmental impact.[235][236] Due to co-occurrence of rare-earth and radioactive elements (thorium, uranium and radium), rare-earth mining results in production of low-level radioactive waste.[237]

Solar panels change the albedo of the surface what increases their contribution to global warming.[238]

Mining for materials needed for renewable energy production is expected to increase threats to biodiversity. In September 2020 scientists published a world map of areas that contain renewable energy materials as well as estimations of their overlaps with ""Key Biodiversity Areas"", ""Remaining Wilderness"" and ""Protected Areas"". The authors assessed that careful strategic planning is needed.[239][240][241]

Gallery

See also

References

Bibliography"	https://en.wikipedia.org/wiki/Renewable_energy	"Energy that is collected from renewable resources
 Renewable energy capacity additions in 2020 expanded by more than 45% from 2019, including a 90% rise in global wind capacity (green) and a 23% expansion of new solar photovoltaic installations (yellow).
Renewable electricity generation breakdown (+nuclear) as of 2018. Hydro (45%) Nuclear (28%) Wind (13%) Solar (6%) Biofuels (5%) Other (3%)
Renewable energy is useful energy that is collected from renewable resources, which are naturally replenished on a human timescale, including carbon neutral sources like sunlight, wind, rain, tides, waves, and geothermal heat. The term often also encompasses biomass as well, whose carbon neutral status is under debate.  This type of energy source stands in contrast to fossil fuels, which are being used far more quickly than they are being replenished.
Renewable energy often provides energy in four important areas: electricity generation, air and water heating/cooling, transportation, and rural (off-grid) energy services.
Based on REN21's 2017 report, renewables contributed 19.3% to humans' global energy consumption and 24.5% to their generation of electricity in 2015 and 2016, respectively. This energy consumption is divided as 8.9% coming from traditional biomass, 4.2% as heat energy (modern biomass, geothermal and solar heat), 3.9% from hydroelectricity and the remaining 2.2% is electricity from wind, solar, geothermal, and other forms of biomass. In 2017, worldwide investments in renewable energy amounted to US$279.8 billion with China accounting for 45% of the global investments, and the United States and Europe both around 15%. Globally there were an estimated 10.5 million jobs associated with the renewable energy industries, with solar photovoltaics being the largest renewable employer. Renewable energy systems are rapidly becoming more efficient and cheaper and their share of total energy consumption is increasing. As of 2019, more than two-thirds of worldwide newly installed electricity capacity was renewable. Growth in consumption of coal and oil could end by 2020 due to increased uptake of renewables and natural gas. As of 2020, in most countries, photovoltaic solar and onshore wind are the cheapest forms of building new electricity-generating plants.
At the national level, at least 30 nations around the world already have renewable energy contributing more than 20 percent of their energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond. At least two countries, Iceland and Norway, generate all their electricity using renewable energy already, and many other countries have the set a goal to reach 100% renewable energy in the future. At least 47 nations around the world already have over 50 percent of electricity from renewable resources. Renewable energy resources exist over wide geographical areas, in contrast to fossil fuels, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency technologies is resulting in significant energy security, climate change mitigation, and economic benefits. In international public opinion surveys there is strong support for promoting renewable sources such as solar power and wind power.
While many renewable energy projects are large-scale, renewable technologies are also suited to rural and remote areas and developing countries, where energy is often crucial in human development. As most of renewable energy technologies provide electricity, renewable energy deployment is often applied in conjunction with further electrification, which has several benefits: electricity can be converted to heat, can be converted into mechanical energy with high efficiency, and is clean at the point of consumption. In addition, electrification with renewable energy is more efficient and therefore leads to significant reductions in primary energy requirements.
Overview
 Coal, oil, and natural gas remain the primary global energy sources even as renewables have begun rapidly increasing.
PlanetSolar , the world's largest solar-powered boat and the first ever solar electric vehicle to circumnavigate the globe (in 2012)
Renewable energy flows involve natural phenomena such as sunlight, wind, tides, plant growth, and geothermal heat, as the International Energy Agency explains:
Renewable energy is derived from natural processes that are replenished constantly. In its various forms, it derives directly from the sun, or from heat generated deep within the earth. Included in the definition is electricity and heat generated from solar, wind, ocean, hydropower, biomass, geothermal resources, and biofuels and hydrogen derived from renewable resources.
Renewable energy resources and significant opportunities for energy efficiency exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency, and technological diversification of energy sources, would result in significant energy security and economic benefits. It would also reduce environmental pollution such as air pollution caused by burning of fossil fuels and improve public health, reduce premature mortalities due to pollution and save associated health costs that amount to several hundred billion dollars annually only in the United States. Multiple analyses of U.S. decarbonization strategies have found that quantified health benefits can significantly offset the costs of implementing these strategies. Renewable energy sources, that derive their energy from the sun, either directly or indirectly, such as hydro and wind, are expected to be capable of supplying humanity energy for almost another 1 billion years, at which point the predicted increase in heat from the Sun is expected to make the surface of the Earth too hot for liquid water to exist.
Climate change and global warming concerns, coupled with the continuing fall in the costs of some renewable energy equipment, such as wind turbines and solar panels, are driving increased use of renewables. New government spending, regulation and policies helped the industry weather the global financial crisis better than many other sectors. As of 2019 , however, according to the International Renewable Energy Agency, renewables overall share in the energy mix (including power, heat and transport) needs to grow six times faster, in order to keep the rise in average global temperatures ""well below"" 2.0 °C (3.6 °F) during the present century, compared to pre-industrial levels.
As of 2011, small solar PV systems provide electricity to a few million households, and micro-hydro configured into mini-grids serves many more. Over 44 million households use biogas made in household-scale digesters for lighting and/or cooking, and more than 166 million households rely on a new generation of more-efficient biomass cookstoves.  United Nations' eighth Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond, and some 120 countries have various policy targets for longer-term shares of renewable energy, including a 20% target of all electricity generated for the European Union by 2020. Some countries have much higher long-term policy targets of up to 100% renewables. Outside Europe, a diverse group of 20 or more other countries targets renewable energy shares in the 2020–2030 time frame that range from 10% to 50%.
Renewable energy often displaces conventional fuels in four areas: electricity generation, hot water/space heating, transportation, and rural (off-grid) energy services:
Power generation
By 2040, renewable energy is projected to equal coal and natural gas electricity generation. Several jurisdictions, including Denmark, Germany, the state of South Australia and some US states have achieved high integration of variable renewables. For example, in 2015 wind power met 42% of electricity demand in Denmark, 23.2% in Portugal and 15.5% in Uruguay. Interconnectors enable countries to balance electricity systems by allowing the import and export of renewable energy. Innovative hybrid systems have emerged between countries and regions.
Heating
Solar water heating makes an important contribution to renewable heat in many countries, most notably in China, which now has 70% of the global total (180 GWth). Most of these systems are installed on multi-family apartment buildings and meet a portion of the hot water needs of an estimated 50–60 million households in China. Worldwide, total installed solar water heating systems meet a portion of the water heating needs of over 70 million households. The use of biomass for heating continues to grow as well. In Sweden, national use of biomass energy has surpassed that of oil. Direct geothermal for heating is also growing rapidly. The newest addition to Heating is from Geothermal Heat Pumps which provide both heating and cooling, and also flatten the electric demand curve and are thus an increasing national priority (see also Renewable thermal energy).
Transportation
A bus fueled by biodiesel
Bioethanol is an alcohol made by fermentation, mostly from carbohydrates produced in sugar or starch crops such as corn, sugarcane, or sweet sorghum. Cellulosic biomass, derived from non-food sources such as trees and grasses is also being developed as a feedstock for ethanol production. Ethanol can be used as a fuel for vehicles in its pure form, but it is usually used as a gasoline additive to increase octane and improve vehicle emissions. Bioethanol is widely used in the USA and in Brazil. Biodiesel can be used as a fuel for vehicles in its pure form, but it is usually used as a diesel additive to reduce levels of particulates, carbon monoxide, and hydrocarbons from diesel-powered vehicles. Biodiesel is produced from oils or fats using transesterification and is the most common biofuel in Europe.
History
Prior to the development of coal in the mid 19th century, nearly all energy used was renewable. Almost without a doubt the oldest known use of renewable energy, in the form of traditional biomass to fuel fires, dates from more than a million years ago. The use of biomass for fire did not become commonplace until many hundreds of thousands of years later. Probably the second oldest usage of renewable energy is harnessing the wind in order to drive ships over water. This practice can be traced back some 7000 years, to ships in the Persian Gulf and on the Nile. From hot springs, geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times. Moving into the time of recorded history, the primary sources of traditional renewable energy were human labor, animal power, water power, wind, in grain crushing windmills, and firewood, a traditional biomass.
In the 1860s and 1870s, there were already fears that civilization would run out of fossil fuels and the need was felt for a better source. In 1873 Professor Augustin Mouchot wrote:
The time will arrive when the industry of Europe will cease to find those natural resources, so necessary for it. Petroleum springs and coal mines are not inexhaustible but are rapidly diminishing in many places. Will man, then, return to the power of water and wind? Or will he emigrate where the most powerful source of heat sends its rays to all? History will show what will come.
In 1885, Werner von Siemens, commenting on the discovery of the photovoltaic effect in the solid state, wrote:
In conclusion, I would say that however great the scientific importance of this discovery may be, its practical value will be no less obvious when we reflect that the supply of solar energy is both without limit and without cost, and that it will continue to pour down upon us for countless ages after all the coal deposits of the earth have been exhausted and forgotten.
Max Weber mentioned the end of fossil fuel in the concluding paragraphs of his Die protestantische Ethik und der Geist des Kapitalismus (The Protestant Ethic and the Spirit of Capitalism), published in 1905. Development of solar engines continued until the outbreak of World War I. The importance of solar energy was recognized in a 1911 Scientific American article: ""in the far distant future, natural fuels having been exhausted  will remain as the only means of existence of the human race"".
The theory of peak oil was published in 1956. In the 1970s environmentalists promoted the development of renewable energy both as a replacement for the eventual depletion of oil, as well as for an escape from dependence on oil, and the first electricity-generating wind turbines appeared. Solar had long been used for heating and cooling, but solar panels were too costly to build solar farms until 1980.
Mainstream technologies
Wind power
 Wind energy generation by region over time.
 Global map of wind power density potential.
At the end of 2020, worldwide installed wind power capacity was 733 GW.
Air flow can be used to run wind turbines. Modern utility-scale wind turbines range from around 600 kW to 9 MW of rated power. The power available from the wind is a function of the cube of the wind speed, so as wind speed increases, power output increases up to the maximum output for the particular turbine. Areas where winds are stronger and more constant, such as offshore and high-altitude sites, are preferred locations for wind farms. Typically, full load hours of wind turbines vary between 16 and 57 percent annually but might be higher in particularly favorable offshore sites.
Wind-generated electricity met nearly 4% of global electricity demand in 2015, with nearly 63 GW of new wind power capacity installed. Wind energy was the leading source of new capacity in Europe, the US and Canada, and the second largest in China. In Denmark, wind energy met more than 40% of its electricity demand while Ireland, Portugal and Spain each met nearly 20%.
Globally, the long-term technical potential of wind energy is believed to be five times total current global energy production, or 40 times current electricity demand, assuming all practical barriers needed were overcome. This would require wind turbines to be installed over large areas, particularly in areas of higher wind resources, such as offshore. As offshore wind speeds average ~90% greater than that of land, so offshore resources can contribute substantially more energy than land-stationed turbines.
Hydropower
At the end of 2020, worldwide renewable hydropower capacity was 1,211 GW.
Since water is about 800 times denser than air, even a slow flowing stream of water, or moderate sea swell, can yield considerable amounts of energy. There are many forms of water energy:
Historically, hydroelectric power came from constructing large hydroelectric dams and reservoirs, which are still popular in developing countries.  The largest of them are the Three Gorges Dam (2003) in China and the Itaipu Dam (1984) built by Brazil and Paraguay.
The largest of them are the Three Gorges Dam (2003) in China and the Itaipu Dam (1984) built by Brazil and Paraguay. Small hydro systems are hydroelectric power installations that typically produce up to 50 MW of power. They are often used on small rivers or as a low-impact development on larger rivers. China is the largest producer of hydroelectricity in the world and has more than 45,000 small hydro installations. 
of power. They are often used on small rivers or as a low-impact development on larger rivers. China is the largest producer of hydroelectricity in the world and has more than 45,000 small hydro installations. Run-of-the-river hydroelectricity plants derive energy from rivers without the creation of a large reservoir. The water is typically conveyed along the side of the river valley (using channels, pipes and/or tunnels) until it is high above the valley floor, whereupon it can be allowed to fall through a penstock to drive a turbine. This style of generation may still produce a large amount of electricity, such as the Chief Joseph Dam on the Columbia River in the United States. Many run-of-the-river hydro power plants are micro hydro or pico hydro plants.
Hydropower is produced in 150 countries, with the Asia-Pacific region generating 32 percent of global hydropower in 2010. For countries having the largest percentage of electricity from renewables, the top 50 are primarily hydroelectric. China is the largest hydroelectricity producer, with 721 terawatt-hours of production in 2010, representing around 17 percent of domestic electricity use. There are now three hydroelectricity stations larger than 10 GW: the Three Gorges Dam in China, Itaipu Dam across the Brazil/Paraguay border, and Guri Dam in Venezuela.
Wave power, which captures the energy of ocean surface waves, and tidal power, converting the energy of tides, are two forms of hydropower with future potential; however, they are not yet widely employed commercially. A demonstration project operated by the Ocean Renewable Power Company on the coast of Maine, and connected to the grid, harnesses tidal power from the Bay of Fundy, location of the world's highest tidal flow. Ocean thermal energy conversion, which uses the temperature difference between cooler deep and warmer surface waters, currently has no economic feasibility.
Solar energy
At the end of 2020, global installed solar capacity was 714 GW.
Solar energy, radiant light and heat from the sun, is harnessed using a range of ever-evolving technologies such as solar heating, photovoltaics, concentrated solar power (CSP), concentrator photovoltaics (CPV), solar architecture and artificial photosynthesis. Solar technologies are broadly characterized as either passive solar or active solar depending on the way they capture, convert, and distribute solar energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light dispersing properties, and designing spaces that naturally circulate air. Active solar technologies encompass solar thermal energy, using solar collectors for heating, and solar power, converting sunlight into electricity either directly using photovoltaics (PV), or indirectly using concentrated solar power (CSP).
A photovoltaic system converts light into electrical direct current (DC) by taking advantage of the photoelectric effect. Solar PV has turned into a multi-billion, fast-growing industry, continues to improve its cost-effectiveness, and has the most potential of any renewable technologies together with CSP. Concentrated solar power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. Commercial concentrated solar power plants were first developed in the 1980s. CSP-Stirling has by far the highest efficiency among all solar energy technologies.
In 2011, the International Energy Agency said that ""the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries' energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared"". Australia has the largest proportion of solar electricity in the world; in 2020, solar supplied 9.9% of electricity demand. 
Geothermal energy
At the end of 2020, global geothermal capacity was 14 GW.
High temperature geothermal energy is from thermal energy generated and stored in the Earth. Thermal energy is the energy that determines the temperature of matter. Earth's geothermal energy originates from the original formation of the planet and from radioactive decay of minerals (in currently uncertain but possibly roughly equal proportions). The geothermal gradient, which is the difference in temperature between the core of the planet and its surface, drives a continuous conduction of thermal energy in the form of heat from the core to the surface. The adjective geothermal originates from the Greek roots geo, meaning earth, and thermos, meaning heat.
The heat that is used for geothermal energy can be from deep within the Earth, all the way down to Earth's core – 4,000 miles (6,400 km) down. At the core, temperatures may reach over 9,000 °F (5,000 °C). Heat conducts from the core to the surrounding rock. Extremely high temperature and pressure cause some rock to melt, which is commonly known as magma. Magma convects upward since it is lighter than the solid rock. This magma then heats rock and water in the crust, sometimes up to 700 °F (371 °C).
Low temperature geothermal refers to the use of the outer crust of the Earth as a thermal battery to facilitate renewable thermal energy for heating and cooling buildings, and other refrigeration and industrial uses. In this form of geothermal, a geothermal heat pump and ground-coupled heat exchanger are used together to move heat energy into the Earth (for cooling) and out of the Earth (for heating) on a varying seasonal basis. Low-temperature geothermal (generally referred to as ""GHP"") is an increasingly important renewable technology because it both reduces total annual energy loads associated with heating and cooling, and it also flattens the electric demand curve eliminating the extreme summer and winter peak electric supply requirements. Thus low temperature geothermal/GHP is becoming an increasing national priority with multiple tax credit support and focus as part of the ongoing movement toward net zero energy.
Bioenergy
At the end of 2020, bioenergy global capacity was 127 GW.
Biomass is biological material derived from living, or recently living organisms. It most often refers to plants or plant-derived materials which are specifically called lignocellulosic biomass. As an energy source, biomass can either be used directly via combustion to produce heat, or indirectly after converting it to various forms of biofuel. Conversion of biomass to biofuel can be achieved by different methods which are broadly classified into: thermal, chemical, and biochemical methods. Wood remains the largest biomass energy source today; examples include forest residues – such as dead trees, branches and tree stumps –, yard clippings, wood chips and even municipal solid waste. In the second sense, biomass includes plant or animal matter that can be converted into fibers or other industrial chemicals, including biofuels. Industrial biomass can be grown from numerous types of plants, including miscanthus, switchgrass, hemp, corn, poplar, willow, sorghum, sugarcane, bamboo, and a variety of tree species, ranging from eucalyptus to oil palm (palm oil).
Plant energy is produced by crops specifically grown for use as fuel that offer high biomass output per hectare with low input energy. The grain can be used for liquid transportation fuels while the straw can be burned to produce heat or electricity. Plant biomass can also be degraded from cellulose to glucose through a series of chemical treatments, and the resulting sugar can then be used as a first-generation biofuel.
Biomass can be converted to other usable forms of energy such as methane gas or transportation fuels such as ethanol and biodiesel. Rotting garbage, and agricultural and human waste, all release methane gas – also called landfill gas or biogas. Crops, such as corn and sugarcane, can be fermented to produce the transportation fuel, ethanol. Biodiesel, another transportation fuel, can be produced from left-over food products such as vegetable oils and animal fats. Also, biomass to liquids (BTLs) and cellulosic ethanol are still under research. There is a great deal of research involving algal fuel or algae-derived biomass due to the fact that it is a non-food resource and can be produced at rates 5 to 10 times those of other types of land-based agriculture, such as corn and soy. Once harvested, it can be fermented to produce biofuels such as ethanol, butanol, and methane, as well as biodiesel and hydrogen. The biomass used for electricity generation varies by region. Forest by-products, such as wood residues, are common in the United States. Agricultural waste is common in Mauritius (sugar cane residue) and Southeast Asia (rice husks). Animal husbandry residues, such as poultry litter, are common in the United Kingdom.
Biofuels include a wide range of fuels which are derived from biomass. The term covers solid, liquid, and gaseous fuels. Liquid biofuels include bioalcohols, such as bioethanol, and oils, such as biodiesel. Gaseous biofuels include biogas, landfill gas and synthetic gas. Bioethanol is an alcohol made by fermenting the sugar components of plant materials and it is made mostly from sugar and starch crops. These include maize, sugarcane and, more recently, sweet sorghum. The latter crop is particularly suitable for growing in dryland conditions, and is being investigated by International Crops Research Institute for the Semi-Arid Tropics for its potential to provide fuel, along with food and animal feed, in arid parts of Asia and Africa.
With advanced technology being developed, cellulosic biomass, such as trees and grasses, are also used as feedstocks for ethanol production. Ethanol can be used as a fuel for vehicles in its pure form, but it is usually used as a gasoline additive to increase octane and improve vehicle emissions. Bioethanol is widely used in the United States and in Brazil. The energy costs for producing bio-ethanol are almost equal to, the energy yields from bio-ethanol. However, according to the European Environment Agency, biofuels do not address global warming concerns. Biodiesel is made from vegetable oils, animal fats or recycled greases. It can be used as a fuel for vehicles in its pure form, or more commonly as a diesel additive to reduce levels of particulates, carbon monoxide, and hydrocarbons from diesel-powered vehicles. Biodiesel is produced from oils or fats using transesterification and is the most common biofuel in Europe. Biofuels provided 2.7% of the world's transport fuel in 2010.
Biomass, biogas and biofuels are burned to produce heat/power and in doing so harm the environment. Pollutants such as sulphurous oxides (SO x ), nitrous oxides (NO x ), and particulate matter (PM) are produced from the combustion of biomass; the World Health Organisation estimates that 7 million premature deaths are caused each year by air pollution. Biomass combustion is a major contributor.
Emerging technologies
Other renewable energy technologies are still under development, and include cellulosic ethanol, hot-dry-rock geothermal power, and marine energy. These technologies are not yet widely demonstrated or have limited commercialization. Many are on the horizon and may have potential comparable to other renewable energy technologies, but still depend on attracting sufficient attention and research, development and demonstration (RD&D) funding.
There are numerous organizations within the academic, federal, and commercial sectors conducting large-scale advanced research in the field of renewable energy. This research spans several areas of focus across the renewable energy spectrum. Most of the research is targeted at improving efficiency and increasing overall energy yields. Multiple federally supported research organizations have focused on renewable energy in recent years. Two of the most prominent of these labs are Sandia National Laboratories and the National Renewable Energy Laboratory (NREL), both of which are funded by the United States Department of Energy and supported by various corporate partners. Sandia has a total budget of $2.4 billion while NREL has a budget of $375 million.
Enhanced geothermal system
Enhanced geothermal systems (EGS) are a new type of geothermal power technology that does not require natural convective hydrothermal resources. The vast majority of geothermal energy within drilling reach is in dry and non-porous rock. EGS technologies ""enhance"" and/or create geothermal resources in this ""hot dry rock (HDR)"" through hydraulic fracturing. EGS and HDR technologies, such as hydrothermal geothermal, are expected to be baseload resources that produce power 24 hours a day like a fossil plant. Distinct from hydrothermal, HDR and EGS may be feasible anywhere in the world, depending on the economic limits of drill depth. Good locations are over deep granite covered by a thick (3–5 km) layer of insulating sediments which slow heat loss. There are HDR and EGS systems currently being developed and tested in France, Australia, Japan, Germany, the U.S., and Switzerland. The largest EGS project in the world is a 25 megawatt demonstration plant currently being developed in the Cooper Basin, Australia. The Cooper Basin has the potential to generate 5,000–10,000 MW.
Cellulosic ethanol
Several refineries that can process biomass and turn it into ethanol are built by companies such as Iogen, POET, and Abengoa, while other companies such as the Verenium Corporation, Novozymes, and Dyadic International are producing enzymes which could enable future commercialization. The shift from food crop feedstocks to waste residues and native grasses offers significant opportunities for a range of players, from farmers to biotechnology firms, and from project developers to investors.
Marine energy
Marine energy (also sometimes referred to as ocean energy) refers to the energy carried by ocean waves, tides, salinity, and ocean temperature differences. The movement of water in the world's oceans creates a vast store of kinetic energy, or energy in motion. This energy can be harnessed to generate electricity to power homes, transport and industries. The term marine energy encompasses both wave power – power from surface waves, and tidal power – obtained from the kinetic energy of large bodies of moving water. Reverse electrodialysis (RED) is a technology for generating electricity by mixing fresh river water and salty sea water in large power cells designed for this purpose; as of 2016, it is being tested at a small scale (50 kW). Offshore wind power is not a form of marine energy, as wind power is derived from the wind, even if the wind turbines are placed over water. The oceans have a tremendous amount of energy and are close to many if not most concentrated populations. Ocean energy has the potential of providing a substantial amount of new renewable energy around the world.
Solar energy developments
Experimental solar power
Concentrated photovoltaics (CPV) systems employ sunlight concentrated onto photovoltaic surfaces for the purpose of electricity generation. Thermoelectric, or ""thermovoltaic"" devices convert a temperature difference between dissimilar materials into an electric current.
Floating solar arrays
Floating solar arrays are PV systems that float on the surface of drinking water reservoirs, quarry lakes, irrigation canals or remediation and tailing ponds. A small number of such systems exist in France, India, Japan, South Korea, the United Kingdom, Singapore, and the United States. The systems are said to have advantages over photovoltaics on land. The cost of land is more expensive, and there are fewer rules and regulations for structures built on bodies of water not used for recreation. Unlike most land-based solar plants, floating arrays can be unobtrusive because they are hidden from public view. They achieve higher efficiencies than PV panels on land, because water cools the panels. The panels have a special coating to prevent rust or corrosion. In May 2008, the Far Niente Winery in Oakville, California, pioneered the world's first floatovoltaic system by installing 994 solar PV modules with a total capacity of 477 kW onto 130 pontoons and floating them on the winery's irrigation pond. Utility-scale floating PV farms are starting to be built. Kyocera will develop the world's largest, a 13.4 MW farm on the reservoir above Yamakura Dam in Chiba Prefecture using 50,000 solar panels. Salt-water resistant floating farms are also being constructed for ocean use. The largest so far announced floatovoltaic project is a 350 MW power station in the Amazon region of Brazil.
Solar-assisted heat pump
A heat pump is a device that provides heat energy from a source of heat to a destination called a ""heat sink"". Heat pumps are designed to move thermal energy opposite to the direction of spontaneous heat flow by absorbing heat from a cold space and releasing it to a warmer one. A solar-assisted heat pump represents the integration of a heat pump and thermal solar panels in a single integrated system. Typically these two technologies are used separately (or only placing them in parallel) to produce hot water. In this system the solar thermal panel performs the function of the low temperature heat source and the heat produced is used to feed the heat pump's evaporator. The goal of this system is to get high COP and then produce energy in a more efficient and less expensive way.
It is possible to use any type of solar thermal panel (sheet and tubes, roll-bond, heat pipe, thermal plates) or hybrid (mono/polycrystalline, thin film) in combination with the heat pump. The use of a hybrid panel is preferable because it allows covering a part of the electricity demand of the heat pump and reduces the power consumption and consequently the variable costs of the system.
Solar aircraft
An electric aircraft is an aircraft that runs on electric motors rather than internal combustion engines, with electricity coming from fuel cells, solar cells, ultracapacitors, power beaming, or batteries.
Currently, flying manned electric aircraft are mostly experimental demonstrators, though many small unmanned aerial vehicles are powered by batteries. Electrically powered model aircraft have been flown since the 1970s, with one report in 1957. The first man-carrying electrically powered flights were made in 1973. Between 2015–2016, a manned, solar-powered plane, Solar Impulse 2, completed a circumnavigation of the Earth.
Solar updraft tower
A solar updraft tower is a renewable-energy power plant for generating electricity from low-temperature solar heat. Sunshine heats the air beneath a very wide greenhouse-like roofed collector structure surrounding the central base of a very tall chimney tower. The resulting convection causes a hot air updraft in the tower by the chimney effect. This airflow drives wind turbines placed in the chimney updraft or around the chimney base to produce electricity. Plans for scaled-up versions of demonstration models will allow significant power generation and may allow the development of other applications, such as water extraction or distillation, and agriculture or horticulture. A more advanced version of a similarly themed technology is the Vortex engine which aims to replace large physical chimneys with a vortex of air created by a shorter, less-expensive structure.
Space-based solar power
For either photovoltaic or thermal systems, one option is to loft them into space, particularly Geosynchronous orbit. To be competitive with Earth-based solar power systems, the specific mass (kg/kW) times the cost to loft mass plus the cost of the parts needs to be $2400 or less. I.e., for a parts cost plus rectenna of $1100/kW, the product of the $/kg and kg/kW must be $1300/kW or less. Thus for 6.5 kg/kW, the transport cost cannot exceed $200/kg. While that will require a 100 to one reduction, SpaceX is targeting a ten to one reduction, Reaction Engines may make a 100 to one reduction possible.
Artificial photosynthesis
Artificial photosynthesis uses techniques including nanotechnology to store solar electromagnetic energy in chemical bonds by splitting water to produce hydrogen and then using carbon dioxide to make methanol. Researchers in this field are striving to design molecular mimics of photosynthesis that use a wider region of the solar spectrum, employ catalytic systems made from abundant, inexpensive materials that are robust, readily repaired, non-toxic, stable in a variety of environmental conditions and perform more efficiently allowing a greater proportion of photon energy to end up in the storage compounds, i.e., carbohydrates (rather than building and sustaining living cells). However, prominent research faces hurdles, Sun Catalytix a MIT spin-off stopped scaling up their prototype fuel-cell in 2012, because it offers few savings over other ways to make hydrogen from sunlight.
Others
Algae fuels
Producing liquid fuels from oil-rich varieties of algae is an ongoing research topic. Various microalgae grown in open or closed systems are being tried including some systems that can be set up in brownfield and desert lands.
Water vapor
Collection of static electricity charges from water droplets on metal surfaces is an experimental technology that would be especially useful in low-income countries with relative air humidity over 60%.
Crop wastes
AuREUS devices (Aurora Renewable Energy & UV Sequestration), which are based on crop wastes can absorb ultraviolet light from the sun and turn it into renewable energy.
Integration into the energy system
Renewable energy production from some sources such as wind and solar is more variable and more geographically spread than technology based on fossil fuels and nuclear. While integrating it into the wider energy system is feasible, it does lead to some additional challenges. In order for the energy system to remain stable, a set of measurements can be taken. Implementation of energy storage, using a wide variety of renewable energy technologies, and implementing a smart grid in which energy is automatically used at the moment it is produced can reduce risks and costs of renewable energy implementation. In some locations, individual households can opt to purchase renewable energy through a consumer green energy program.
Electrical energy storage
Electrical energy storage is a collection of methods used to store electrical energy. Electrical energy is stored during times when production (especially from intermittent sources such as wind power, tidal power, solar power) exceeds consumption, and returned to the grid when production falls below consumption. Pumped-storage hydroelectricity accounts for more than 90% of all grid power storage. Costs of lithium-ion batteries are dropping rapidly, and are increasingly being deployed grid ancillary services and for domestic storage. Additionally, power can be stored in hydrogen fuel cells.
Market and industry trends
Renewable power has been more effective in creating jobs than coal or oil in the United States. In 2016, employment in the sector increased 6 percent in the United States, causing employment in the non-renewable energy sector to decrease 18 percent. Worldwide, renewables employ about 8.1 million as of 2016.
Growth of renewables
 Investment: Companies, governments and households committed $501.3 billion to decarbonization in 2020, including renewable energy (solar, wind), electric vehicles and associated charging infrastructure, energy storage, energy-efficient heating systems, carbon capture and storage, and hydrogen. Renewable energy investment by region 
(LCOE) is a measure of the average net present cost of electricity generation for a generating plant over its lifetime. Cost: With increasingly widespread implementation of renewable energy sources, costs have declined, most notably for energy generated by solar panels. Levelized cost of energy (LCOE) is a measure of the average net present cost of electricity generation for a generating plant over its lifetime.
 In 2020, renewables overtook fossil fuels as the European Union's main source of electricity for the first time.
 Comparing worldwide energy use, the growth of renewable energy is shown by the green line
From the end of 2004, worldwide renewable energy capacity grew at rates of 10–60% annually for many technologies. In 2015 global investment in renewables rose 5% to $285.9 billion, breaking the previous record of $278.5 billion in 2011. 2015 was also the first year that saw renewables, excluding large hydro, account for the majority of all new power capacity (134 GW, making up 54% of the total). Of the renewables total, wind accounted for 72 GW and solar photovoltaics 56 GW; both record-breaking numbers and sharply up from 2014 figures (49 GW and 45 GW respectively). In financial terms, solar made up 56% of total new investment and wind accounted for 38%.
In 2014 global wind power capacity expanded 16% to 369,553 MW. Yearly wind energy production is also growing rapidly and has reached around 4% of worldwide electricity usage, 11.4% in the EU, and it is widely used in Asia, and the United States. In 2015, worldwide installed photovoltaics capacity increased to 227 gigawatts (GW), sufficient to supply 1 percent of global electricity demands. Solar thermal energy stations operate in the United States and Spain, and as of 2016, the largest of these is the 392 MW Ivanpah Solar Electric Generating System in California. The world's largest geothermal power installation is The Geysers in California, with a rated capacity of 750 MW. Brazil has one of the largest renewable energy programs in the world, involving production of ethanol fuel from sugar cane, and ethanol now provides 18% of the country's automotive fuel. Ethanol fuel is also widely available in the United States.
In 2017, investments in renewable energy amounted to US$279.8 billion worldwide, with China accounting for US$126.6 billion or 45% of the global investments, the United States for US$40.5 billion, and Europe for US$40.9 billion. The results of a recent review of the literature concluded that as greenhouse gas (GHG) emitters begin to be held liable for damages resulting from GHG emissions resulting in climate change, a high value for liability mitigation would provide powerful incentives for deployment of renewable energy technologies.
In the decade of 2010–2019, worldwide investment in renewable energy capacity excluding large hydropower amounted to US$2.7 trillion, of which the top countries China contributed US$818 billion, the United States contributed US$392.3 billion, Japan contributed US$210.9 billion, Germany contributed US$183.4 billion, and the United Kingdom contributed US$126.5 billion. This was an increase of over three and possibly four times the equivalent amount invested in the decade of 2000–2009 (no data is available for 2000–2003).
Selected renewable energy global indicators 2008 2009 2010 2011 2012 2013 2014 2015 2016 Investment in new renewable capacity (annual) (109 USD) 182 178 237 279 256 232 270 285 241 Renewables power capacity (existing) (GWe) 1,140 1,230 1,320 1,360 1,470 1,578 1,712 1,849 2,017 Hydropower capacity (existing) (GWe) 885 915 945 970 990 1,018 1,055 1,064 1,096 Wind power capacity (existing) (GWe) 121 159 198 238 283 319 370 433 487 Solar PV capacity (grid-connected) (GWe) 16 23 40 70 100 138 177 227 303 Solar hot water capacity (existing) (GWth) 130 160 185 232 255 373 406 435 456 Ethanol production (annual) (109 litres) 67 76 86 86 83 87 94 98 98.6 Biodiesel production (annual) (109 litres) 12 17.8 18.5 21.4 22.5 26 29.7 30 30.8 Countries with policy targets
for renewable energy use 79 89 98 118 138 144 164 173 176 Source: The Renewable Energy Policy Network for the 21st Century (REN21)–Global Status Report
Future projections
Renewable energy technologies are getting cheaper, through technological change and through the benefits of mass production and market competition. A 2018 report from the International Renewable Energy Agency (IRENA), found that the cost of renewable energy is quickly falling, and will likely be equal to or less than the cost of non-renewables such as fossil fuels by 2020. The report found that solar power costs have dropped 73% since 2010 and onshore wind costs have dropped by 23% in that same timeframe.
Current projections concerning the future cost of renewables vary, however. The EIA has predicted that almost two-thirds of net additions to power capacity will come from renewables by 2020 due to the combined policy benefits of local pollution, decarbonisation and energy diversification.
According to a 2018 report by Bloomberg New Energy Finance, wind and solar power are expected to generate roughly 50% of the world's energy needs by 2050, while coal powered electricity plants are expected to drop to just 11%. Hydro-electricity and geothermal electricity produced at favourable sites are now the cheapest way to generate electricity. Renewable energy costs continue to drop, and the levelised cost of electricity (LCOE) is declining for wind power, solar photovoltaic (PV), concentrated solar power (CSP) and some biomass technologies. Renewable energy is also the most economic solution for new grid-connected capacity in areas with good resources. As the cost of renewable power falls, the scope of economically viable applications increases. Renewable technologies are now often the most economic solution for new generating capacity. Where ""oil-fired generation is the predominant power generation source (e.g. on islands, off-grid and in some countries) a lower-cost renewable solution almost always exists today"". A series of studies by the US National Renewable Energy Laboratory modeled the ""grid in the Western US under a number of different scenarios where intermittent renewables accounted for 33 percent of the total power."" In the models, inefficiencies in cycling the fossil fuel plants to compensate for the variation in solar and wind energy resulted in an additional cost of ""between $0.47 and $1.28 to each MegaWatt hour generated""; however, the savings in the cost of the fuels saved ""adds up to $7 billion, meaning the added costs are, at most, two percent of the savings.""
Trends for individual technologies
Hydroelectricity
In 2017 the world renewable hydropower capacity was 1,154 GW. Only a quarter of the worlds estimated hydroelectric potential of 14,000 TWh/year has been developed, the regional potentials for the growth of hydropower around the world are, 71% Europe, 75% North America, 79% South America, 95% Africa, 95% Middle East, 82% Asia Pacific. However, the political realities of new reservoirs in western countries, economic limitations in the third world and the lack of a transmission system in undeveloped areas result in the possibility of developing 25% of the remaining potential before 2050, with the bulk of that being in the Asia Pacific area. There is slow growth taking place in Western counties, but not in the conventional dam and reservoir style of the past. New projects take the form of run-of-the-river and small hydro, neither using large reservoirs. It is popular to repower old dams thereby increasing their efficiency and capacity as well as quicker responsiveness on the grid. Where circumstances permit existing dams such as the Russell Dam built in 1985 may be updated with ""pump back"" facilities for pumped-storage which is useful for peak loads or to support intermittent wind and solar power. Countries with large hydroelectric developments such as Canada and Norway are spending billions to expand their grids to trade with neighboring countries having limited hydro.
Wind power development
Worldwide growth of wind capacity (1996–2018)
Wind power is widely used in Europe, China, and the United States. From 2004 to 2017, worldwide installed capacity of wind power has been growing from 47 GW to 514 GW—a more than tenfold increase within 13 years As of the end of 2014, China, the United States and Germany combined accounted for half of total global capacity. Several other countries have achieved relatively high levels of wind power penetration, such as 21% of stationary electricity production in Denmark, 18% in Portugal, 16% in Spain, and 14% in Ireland in 2010 and have since continued to expand their installed capacity. More than 80 countries around the world are using wind power on a commercial basis.
Wind turbines are increasing in power with some commercially deployed models generating over 8MW per turbine. More powerful models are in development, see list of most powerful wind turbines.
As of 2017, offshore wind power amounted to 18.7 GW of global installed capacity, accounting for only 3.6% of the total wind power capacity.
List of offshore and onshore wind farms
As of 2013, the Alta Wind Energy Center (California, 1.5 GW) is the world's largest single wind farm. The Walney Extension (London, 0.7 GW) is the largest offshore wind farm in the world. Gansu Wind Farm (China, 7.9 GW) is the largest wind energy project generating project consisting of 18 wind farms.
Solar thermal
Solar thermal energy capacity has increased from 1.3 GW in 2012 to 5.0 GW in 2017.
Spain is the world leader in solar thermal power deployment with 2.3 GW deployed. The United States has 1.8 GW, most of it in California where 1.4 GW of solar thermal power projects are operational. Several power plants have been constructed in the Mojave Desert, Southwestern United States. As of 2017 only 4 other countries have deployments above 100 MW: South Africa (300 MW) India (229 MW) Morocco (180 MW) and United Arab Emirates (100 MW).
The United States conducted much early research in photovoltaics and concentrated solar power. The U.S. is among the top countries in the world in electricity generated by the Sun and several of the world's largest utility-scale installations are located in the desert Southwest.
The oldest solar thermal power plant in the world is the 354 megawatt (MW) SEGS thermal power plant, in California. The Ivanpah Solar Electric Generating System is a solar thermal power project in the California Mojave Desert, 40 miles (64 km) southwest of Las Vegas, with a gross capacity of 377 MW. The 280 MW Solana Generating Station is a solar power plant near Gila Bend, Arizona, about 70 miles (110 km) southwest of Phoenix, completed in 2013. When commissioned it was the largest parabolic trough plant in the world and the first U.S. solar plant with molten salt thermal energy storage.
In developing countries, three World Bank projects for integrated solar thermal/combined-cycle gas-turbine power plants in Egypt, Mexico, and Morocco have been approved.
Photovoltaic development
50,000 100,000 150,000 200,000 2006 2010 2014 Europe Asia-Pacific Americas China Middle East and Africa Worldwide growth of PV capacity grouped by region in MW (2006–2014)
Photovoltaics (PV) is rapidly-growing with global capacity increasing from 177 GW at the end of 2014 to 385 GW in 2017.
PV uses solar cells assembled into solar panels to convert sunlight into electricity. PV systems range from small, residential and commercial rooftop or building integrated installations, to large utility-scale photovoltaic power station. The predominant PV technology is crystalline silicon, while thin-film solar cell technology accounts for about 10 percent of global photovoltaic deployment. In recent years, PV technology has improved its electricity generating efficiency, reduced the installation cost per watt as well as its energy payback time, and reached grid parity in at least 30 different markets by 2014. Building-integrated photovoltaics or ""onsite"" PV systems use existing land and structures and generate power close to where it is consumed.
Photovoltaics grew fastest in China, followed by Japan and the United States. Solar power is forecasted to become the world's largest source of electricity by 2050, with solar photovoltaics and concentrated solar power contributing 16% and 11%, respectively. This requires an increase of installed PV capacity to 4,600 GW, of which more than half is expected to be deployed in China and India.
Commercial concentrated solar power plants were first developed in the 1980s. As the cost of solar electricity has fallen, the number of grid-connected solar PV systems has grown into the millions and utility-scale solar power stations with hundreds of megawatts are being built. Many solar photovoltaic power stations have been built, mainly in Europe, China and the United States. The 1.5 GW Tengger Desert Solar Park, in China is the world's largest PV power station. Many of these plants are integrated with agriculture and some use tracking systems that follow the sun's daily path across the sky to generate more electricity than fixed-mounted systems.
Biofuel development
Brazil produces bioethanol made from sugarcane available throughout the country. A typical gas station with dual fuel service is marked ""A"" for alcohol (ethanol) and ""G"" for gasoline.
Bioenergy global capacity in 2017 was 109 GW. Biofuels provided 3% of the world's transport fuel in 2017.
Mandates for blending biofuels exist in 31 countries at the national level and in 29 states/provinces. According to the International Energy Agency, biofuels have the potential to meet more than a quarter of world demand for transportation fuels by 2050.
Since the 1970s, Brazil has had an ethanol fuel program which has allowed the country to become the world's second largest producer of ethanol (after the United States) and the world's largest exporter. Brazil's ethanol fuel program uses modern equipment and cheap sugarcane as feedstock, and the residual cane-waste (bagasse) is used to produce heat and power. There are no longer light vehicles in Brazil running on pure gasoline. By the end of 2008 there were 35,000 filling stations throughout Brazil with at least one ethanol pump. Unfortunately, Operation Car Wash has seriously eroded public trust in oil companies and has implicated several high ranking Brazilian officials.
Nearly all the gasoline sold in the United States today is mixed with 10% ethanol, and motor vehicle manufacturers already produce vehicles designed to run on much higher ethanol blends. Ford, Daimler AG, and GM are among the automobile companies that sell ""flexible-fuel"" cars, trucks, and minivans that can use gasoline and ethanol blends ranging from pure gasoline up to 85% ethanol. By mid-2006, there were approximately 6 million ethanol compatible vehicles on U.S. roads.
Geothermal development
Global geothermal capacity in 2017 was 12.9 GW.
Geothermal power is cost effective, reliable, sustainable, and environmentally friendly, but has historically been limited to areas near tectonic plate boundaries. Recent technological advances have expanded the range and size of viable resources, especially for applications such as home heating, opening a potential for widespread exploitation. Geothermal wells release greenhouse gases trapped deep within the earth, but these emissions are usually much lower per energy unit than those of fossil fuels. As a result, geothermal power has the potential to help mitigate global warming if widely deployed in place of fossil fuels.
In 2017, the United States led the world in geothermal electricity production with 12.9 GW of installed capacity. The largest group of geothermal power plants in the world is located at The Geysers, a geothermal field in California. The Philippines follows the US as the second highest producer of geothermal power in the world, with 1.9 GW of capacity online.
Developing countries
Renewable energy technology has sometimes been seen as a costly luxury item by critics, and affordable only in the affluent developed world. This erroneous view has persisted for many years, however between 2016 and 2017, investments in renewable energy were higher in developing countries than in developed countries, with China leading global investment with a record 126.6 billion dollars. Many Latin American and African countries increased their investments significantly as well. Renewable energy can be particularly suitable for developing countries. In rural and remote areas, transmission and distribution of energy generated from fossil fuels can be difficult and expensive. Producing renewable energy locally can offer a viable alternative.
Technology advances are opening up a huge new market for solar power: the approximately 1.3 billion people around the world who do not have access to grid electricity. Even though they are typically very poor, these people have to pay far more for lighting than people in rich countries because they use inefficient kerosene lamps. Solar power costs half as much as lighting with kerosene. As of 2010, an estimated 3 million households get power from small solar PV systems. Kenya is the world leader in the number of solar power systems installed per capita. More than 30,000 very small solar panels, each producing 1 2 to 30 watts, are sold in Kenya annually. Some Small Island Developing States (SIDS) are also turning to solar power to reduce their costs and increase their sustainability.
Micro-hydro configured into mini-grids also provide power. Over 44 million households use biogas made in household-scale digesters for lighting and/or cooking, and more than 166 million households rely on a new generation of more-efficient biomass cookstoves. Clean liquid fuel sourced from renewable feedstocks are used for cooking and lighting in energy-poor areas of the developing world. Alcohol fuels (ethanol and methanol) can be produced sustainably from non-food sugary, starchy, and cellulosic feedstocks. Project Gaia, Inc. and CleanStar Mozambique are implementing clean cooking programs with liquid ethanol stoves in Ethiopia, Kenya, Nigeria and Mozambique.
Renewable energy projects in many developing countries have demonstrated that renewable energy can directly contribute to poverty reduction by providing the energy needed for creating businesses and employment. Renewable energy technologies can also make indirect contributions to alleviating poverty by providing energy for cooking, space heating, and lighting. Renewable energy can also contribute to education, by providing electricity to schools.
Policy
Policies to support renewable energy have been vital in their expansion. Where Europe dominated in establishing energy policy in early 2000s, most countries around the world now have some form of energy policy.
Policy trends
The International Renewable Energy Agency (IRENA) is an intergovernmental organization for promoting the adoption of renewable energy worldwide. It aims to provide concrete policy advice and facilitate capacity building and technology transfer. IRENA was formed in 2009, by 75 countries signing the charter of IRENA. As of April 2019, IRENA has 160 member states. The then United Nations' Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity, and in September 2011 he launched the UN Sustainable Energy for All initiative to improve energy access, efficiency and the deployment of renewable energy.
The 2015 Paris Agreement on climate change motivated many countries to develop or improve renewable energy policies. In 2017, a total of 121 countries have adapted some form of renewable energy policy. National targets that year existed in at 176 countries. In addition, there is also a wide range of policies at state/provincial and local levels. Some public utilities help plan or install residential energy upgrades. Under president Barack Obama, the United States policy encouraged the uptake of renewable energy in line with commitments to the Paris agreement. Even though Trump has abandoned these goals, renewable investment is still on the rise.
Many national, state, and local governments have created green banks. A green bank is a quasi-public financial institution that uses public capital to leverage private investment in clean energy technologies. Green banks use a variety of financial tools to bridge market gaps that hinder the deployment of clean energy. The US military has also focused on the use of renewable fuels for military vehicles. Unlike fossil fuels, renewable fuels can be produced in any country, creating a strategic advantage. The US military has already committed itself to have 50% of its energy consumption come from alternative sources.
Full renewable energy
The incentive to use 100% renewable energy, for electricity, transport, or even total primary energy supply globally, has been motivated by global warming and other ecological as well as economic concerns. The Intergovernmental Panel on Climate Change has said that there are few fundamental technological limits to integrating a portfolio of renewable energy technologies to meet most of the total global energy demand. Renewable energy use has grown much faster than even advocates anticipated. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. Also, Professors S. Pacala and Robert H. Socolow have developed a series of ""stabilization wedges"" that can allow us to maintain our quality of life while avoiding catastrophic climate change, and ""renewable energy sources,"" in aggregate, constitute the largest number of their ""wedges"".
Using 100% renewable energy was first suggested in a Science paper published in 1975 by Danish physicist Bent Sørensen. It was followed by several other proposals, until in 1998 the first detailed analysis of scenarios with very high shares of renewables were published. These were followed by the first detailed 100% scenarios. In 2006 a PhD thesis was published by Czisch in which it was shown that in a 100% renewable scenario energy supply could match demand in every hour of the year in Europe and North Africa. In the same year Danish Energy professor Henrik Lund published a first paper in which he addresses the optimal combination of renewables, which was followed by several other papers on the transition to 100% renewable energy in Denmark. Since then Lund has been publishing several papers on 100% renewable energy. After 2009 publications began to rise steeply, covering 100% scenarios for countries in Europe, America, Australia and other parts of the world.
In 2011 Mark Z. Jacobson, professor of civil and environmental engineering at Stanford University, and Mark Delucchi published a study on 100% renewable global energy supply in the journal Energy Policy. They found producing all new energy with wind power, solar power, and hydropower by 2030 is feasible and existing energy supply arrangements could be replaced by 2050. Barriers to implementing the renewable energy plan are seen to be ""primarily social and political, not technological or economic"". They also found that energy costs with a wind, solar, water system should be similar to today's energy costs.
Similarly, in the United States, the independent National Research Council has noted that ""sufficient domestic renewable resources exist to allow renewable electricity to play a significant role in future electricity generation and thus help confront issues related to climate change, energy security, and the escalation of energy costs … Renewable energy is an attractive option because renewable resources available in the United States, taken collectively, can supply significantly greater amounts of electricity than the total current or projected domestic demand.""
The most significant barriers to the widespread implementation of large-scale renewable energy and low carbon energy strategies are primarily political and not technological. According to the 2013 Post Carbon Pathways report, which reviewed many international studies, the key roadblocks are: climate change denial, the fossil fuels lobby, political inaction, unsustainable energy consumption, outdated energy infrastructure, and financial constraints.
According to World Bank the ""below 2°C"" climate scenario requires 3 billions of tonnes of metals and minerals by 2050. Supply of mined resources such as zinc, molybdenum, silver, nickel, copper must increase by up to 500%. A 2018 analysis estimated required increases in stock of metals required by various sectors from 1000% (wind power) to 87'000% (personal vehicle batteries).
Debate
Renewable electricity production, from sources such as wind power and solar power, is variable which results in reduced capacity factor and require either energy storage of capacity equal to its total output, or base load power sources from non-intermittent sources like hydropower, fossil fuels or nuclear power.
Since renewable energy sources power density per land area is at best three orders of magnitude smaller than fossil or nuclear power, renewable power plants tends to occupy thousands of hectares causing environmental concerns and opposition from local residents, especially in densely populated countries. Solar power plants are competing with arable land and nature reserves, while on-shore wind farms face opposition due to aesthetic concerns and noise, which is impacting both humans and wildlife. In the United States, the Massachusetts Cape Wind project was delayed for years partly because of aesthetic concerns. However, residents in other areas have been more positive. According to a town councilor, the overwhelming majority of locals believe that the Ardrossan Wind Farm in Scotland has enhanced the area. These concerns, when directed against renewable energy, are sometimes described as ""not in my back yard"" attitude (NIMBY).
A recent UK Government document states that ""projects are generally more likely to succeed if they have broad public support and the consent of local communities. This means giving communities both a say and a stake"". In countries such as Germany and Denmark many renewable projects are owned by communities, particularly through cooperative structures, and contribute significantly to overall levels of renewable energy deployment.
The market for renewable energy technologies has continued to grow. Climate change concerns and increasing in green jobs, coupled with high oil prices, peak oil, oil wars, oil spills, promotion of electric vehicles and renewable electricity, nuclear disasters and increasing government support, are driving increasing renewable energy legislation, incentives and commercialization. New government spending, regulation and policies helped the industry weather the 2009 economic crisis better than many other sectors.
While renewables have been very successful in their ever-growing contribution to electrical power there are no countries dominated by fossil fuels who have a plan to stop and get that power from renewables. Only Scotland and Ontario have stopped burning coal, largely due to good natural gas supplies. In the area of transportation, fossil fuels are even more entrenched and solutions harder to find. It's unclear if there are failures with policy or renewable energy, but twenty years after the Kyoto Protocol fossil fuels are still our primary energy source and consumption continues to grow.
The International Energy Agency has stated that deployment of renewable technologies usually increases the diversity of electricity sources and, through local generation, contributes to the flexibility of the system and its resistance to central shocks.
Geopolitics of renewable energy
From around 2010 onwards, there was increasing discussion about the geopolitical impact of the growing use of renewable energy. It was argued that former fossil fuels exporters would experience a weakening of their position in international affairs, while countries with abundant renewable energy resources would be strengthened. Also countries rich in critical materials for renewable energy technologies were expected to rise in importance in international affairs.
The GeGaLo index of geopolitical gains and losses assesses how the geopolitical position of 156 countries may change if the world fully transitions to renewable energy resources. Former fossil fuels exporters are expected to lose power, while the positions of former fossil fuel importers and countries rich in renewable energy resources is expected to strengthen.
Environmental impact
The ability of biomass and biofuels to contribute to a reduction in CO
2 emissions is limited because both biomass and biofuels emit large amounts of air pollution when burned and in some cases compete with food supply. Furthermore, biomass and biofuels consume large amounts of water. Other renewable sources such as wind power, photovoltaics, and hydroelectricity have the advantage of being able to conserve water, lower pollution and reduce CO
2 emissions. The installations used to produce wind, solar and hydro power are an increasing threat to key conservation areas, with facilities built in areas set aside for nature conservation and other environmentally sensitive areas. They are often much larger than fossil fuel power plants, needing areas of land up to 10 times greater than coal or gas to produce equivalent energy amounts. More than 2000 renewable energy facilities are built, and more are under construction, in areas of environmental importance and threaten the habitats of plant and animal species across the globe. The authors' team emphasized that their work should not be interpreted as anti-renewables because renewable energy is crucial for reducing carbon emissions. The key is ensuring that renewable energy facilities are built in places where they do not damage biodiversity.
Renewable energy devices depend on non-renewable resources such as mined metals and use vast amounts of land due to their small surface power density. Manufacturing of photovoltaic panels, wind turbines and batteries requires significant amounts of rare-earth elements and increases mining operations, which have significant social and environmental impact. Due to co-occurrence of rare-earth and radioactive elements (thorium, uranium and radium), rare-earth mining results in production of low-level radioactive waste.
Solar panels change the albedo of the surface what increases their contribution to global warming.
Mining for materials needed for renewable energy production is expected to increase threats to biodiversity. In September 2020 scientists published a world map of areas that contain renewable energy materials as well as estimations of their overlaps with ""Key Biodiversity Areas"", ""Remaining Wilderness"" and ""Protected Areas"". The authors assessed that careful strategic planning is needed.
Gallery"	70880
environment	[]		"Current rise in Earth's average temperature and its effects

Average surface air temperatures from 2011 to 2020 compared to a baseline average from 1951 to 1980 (Source: NASA

[1] Observed temperature from NASA versus the 1850–1900 average as a pre-industrial baseline. The main driver for increased global temperatures in the industrial era is human activity, with natural forces adding variability.

Climate change includes both global warming driven by human-induced emissions of greenhouse gases and the resulting large-scale shifts in weather patterns. Though there have been previous periods of climatic change, since the mid-20th century humans have had an unprecedented impact on Earth's climate system and caused change on a global scale.[2]

The largest driver of warming is the emission of gases that create a greenhouse effect, of which more than 90% are carbon dioxide (CO

2) and methane. Fossil fuel burning (coal, oil, and natural gas) for energy consumption is the main source of these emissions, with additional contributions from agriculture, deforestation, and manufacturing.[4] The human cause of climate change is not disputed by any scientific body of national or international standing.[5] Temperature rise is accelerated or tempered by climate feedbacks, such as loss of sunlight-reflecting snow and ice cover, increased water vapour (a greenhouse gas itself), and changes to land and ocean carbon sinks.

Temperature rise on land is about twice the global average increase, leading to desert expansion and more common heat waves and wildfires.[6] Temperature rise is also amplified in the Arctic, where it has contributed to melting permafrost, glacial retreat and sea ice loss.[7] Warmer temperatures are increasing rates of evaporation, causing more intense storms and weather extremes.[8] Impacts on ecosystems include the relocation or extinction of many species as their environment changes, most immediately in coral reefs, mountains, and the Arctic.[9] Climate change threatens people with food insecurity, water scarcity, flooding, infectious diseases, extreme heat, economic losses, and displacement. These impacts have led the World Health Organization to call climate change the greatest threat to global health in the 21st century.[10] Even if efforts to minimise future warming are successful, some effects will continue for centuries, including rising sea levels, rising ocean temperatures, and ocean acidification.[11]

2.[12] Energy flows between space, the atmosphere, and Earth's surface. Current greenhouse gas levels are causing a radiative imbalance of about 0.9 W/m

Many of these impacts are already felt at the current level of warming, which is about 1.2 °C (2.2 °F).[13] The Intergovernmental Panel on Climate Change (IPCC) has issued a series of reports that project significant increases in these impacts as warming continues to 1.5 °C (2.7 °F) and beyond.[14] Additional warming also increases the risk of triggering critical thresholds called tipping points.[15] Responding to climate change involves mitigation and adaptation.[16] Mitigation – limiting climate change – consists of reducing greenhouse gas emissions and removing them from the atmosphere;[16] methods include the development and deployment of low-carbon energy sources such as wind and solar, a phase-out of coal, enhanced energy efficiency, reforestation, and forest preservation. Adaptation consists of adjusting to actual or expected climate,[16] such as through improved coastline protection, better disaster management, assisted colonisation, and the development of more resistant crops. Adaptation alone cannot avert the risk of ""severe, widespread and irreversible"" impacts.[17]

Under the 2015 Paris Agreement, nations collectively agreed to keep warming ""well under 2.0 °C (3.6 °F)"" through mitigation efforts. However, with pledges made under the Agreement, global warming would still reach about 2.8 °C (5.0 °F) by the end of the century.[18] Limiting warming to 1.5 °C (2.7 °F) would require halving emissions by 2030 and achieving near-zero emissions by 2050.[19]

Terminology

Before the 1980s, when it was unclear whether warming by greenhouse gases would dominate aerosol-induced cooling, scientists often used the term inadvertent climate modification to refer to humankind's impact on the climate. In the 1980s, the terms global warming and climate change were popularised, the former referring only to increased surface warming, while the latter describes the full effect of greenhouse gases on the climate.[20] Global warming became the most popular term after NASA climate scientist James Hansen used it in his 1988 testimony in the U.S. Senate.[21] In the 2000s, the term climate change increased in popularity.[22] Global warming usually refers to human-induced warming of the Earth system, whereas climate change can refer to natural as well as anthropogenic change.[23] The two terms are often used interchangeably.[24]

Various scientists, politicians and media figures have adopted the terms climate crisis or climate emergency to talk about climate change, while using global heating instead of global warming.[25] The policy editor-in-chief of The Guardian explained that they included this language in their editorial guidelines ""to ensure that we are being scientifically precise, while also communicating clearly with readers on this very important issue"".[26] Oxford Dictionary chose climate emergency as its word of the year in 2019 and defines the term as ""a situation in which urgent action is required to reduce or halt climate change and avoid potentially irreversible environmental damage resulting from it"".[27]

Observed temperature rise

[28] Directly observational data is in red.[29] Global surface temperature reconstruction over the last 2000 years using proxy data from tree rings, corals, and ice cores in blue.Directly observational data is in red.

[29] shows that land surface temperatures have increased faster than ocean temperatures. NASA datashows that land surface temperatures have increased faster than ocean temperatures.

Multiple independently produced instrumental datasets show that the climate system is warming,[30] with the 2009–2018 decade being 0.93 ± 0.07 °C (1.67 ± 0.13 °F) warmer than the pre-industrial baseline (1850–1900).[31] Currently, surface temperatures are rising by about 0.2 °C (0.36 °F) per decade,[32] with 2020 reaching a temperature of 1.2 °C (2.2 °F) above pre-industrial.[13] Since 1950, the number of cold days and nights has decreased, and the number of warm days and nights has increased.[33]

There was little net warming between the 18th century and the mid-19th century. Climate proxies, sources of climate information from natural archives such as trees and ice cores, show that natural variations offset the early effects of the Industrial Revolution.[34] Thermometer records began to provide global coverage around 1850.[35] Historical patterns of warming and cooling, like the Medieval Climate Anomaly and the Little Ice Age, did not occur at the same time across different regions, but temperatures may have reached as high as those of the late-20th century in a limited set of regions.[36] There have been prehistorical episodes of global warming, such as the Paleocene–Eocene Thermal Maximum.[37] However, the modern observed rise in temperature and CO

2 concentrations has been so rapid that even abrupt geophysical events that took place in Earth's history do not approach current rates.[38]

Evidence of warming from air temperature measurements are reinforced with a wide range of other observations.[39] There has been an increase in the frequency and intensity of heavy precipitation, melting of snow and land ice, and increased atmospheric humidity.[40] Flora and fauna are also behaving in a manner consistent with warming; for instance, plants are flowering earlier in spring.[41] Another key indicator is the cooling of the upper atmosphere, which demonstrates that greenhouse gases are trapping heat near the Earth's surface and preventing it from radiating into space.[42]

While locations of warming vary, the patterns are independent of where greenhouse gases are emitted, because the gases persist long enough to diffuse across the planet. Since the pre-industrial period, global average land temperatures have increased almost twice as fast as global average surface temperatures.[43] This is because of the larger heat capacity of oceans, and because oceans lose more heat by evaporation.[44] Over 90% of the additional energy in the climate system over the last 50 years has been stored in the ocean, with the remainder warming the atmosphere, melting ice, and warming the continents.[45][46]

The Northern Hemisphere and the North Pole have warmed much faster than the South Pole and Southern Hemisphere. The Northern Hemisphere not only has much more land, but also more seasonal snow cover and sea ice, because of how the land masses are arranged around the Arctic Ocean. As these surfaces flip from reflecting a lot of light to being dark after the ice has melted, they start absorbing more heat.[47] Localised black carbon deposits on snow and ice also contribute to Arctic warming.[48] Arctic temperatures have increased and are predicted to continue to increase during this century at over twice the rate of the rest of the world.[49] Melting of glaciers and ice sheets in the Arctic disrupts ocean circulation, including a weakened Gulf Stream, further changing the climate.[50]

Drivers of recent temperature rise

The climate system experiences various cycles on its own which can last for years (such as the El Niño–Southern Oscillation), decades or even centuries.[51] Other changes are caused by an imbalance of energy that is ""external"" to the climate system, but not always external to the Earth.[52] Examples of external forcings include changes in the composition of the atmosphere (e.g. increased concentrations of greenhouse gases), solar luminosity, volcanic eruptions, and variations in the Earth's orbit around the Sun.[53]

To determine the human contribution to climate change, known internal climate variability and natural external forcings need to be ruled out. A key approach is to determine unique ""fingerprints"" for all potential causes, then compare these fingerprints with observed patterns of climate change.[54] For example, solar forcing can be ruled out as a major cause because its fingerprint is warming in the entire atmosphere, and only the lower atmosphere has warmed, as expected from greenhouse gases (which trap heat energy radiating from the surface).[55] Attribution of recent climate change shows that the primary driver is elevated greenhouse gases, but that aerosols also have a strong effect.[56]

Greenhouse gases

CO

2 concentrations over the last 800,000 years as measured from ice cores (blue/green) and directly (black) concentrations over the last 800,000 years as measured from ice cores (blue/green) and directly (black)

The Earth absorbs sunlight, then radiates it as heat. Greenhouse gases in the atmosphere absorb and reemit infrared radiation, slowing the rate at which it can pass through the atmosphere and escape into space.[57] Before the Industrial Revolution, naturally-occurring amounts of greenhouse gases caused the air near the surface to be about 33 °C (59 °F) warmer than it would have been in their absence.[58][59] While water vapour (~50%) and clouds (~25%) are the biggest contributors to the greenhouse effect, they increase as a function of temperature and are therefore considered feedbacks. On the other hand, concentrations of gases such as CO

2 (~20%), tropospheric ozone,[60] CFCs and nitrous oxide are not temperature-dependent, and are therefor considered external forcings.[61]

Human activity since the Industrial Revolution, mainly extracting and burning fossil fuels (coal, oil, and natural gas),[62] has increased the amount of greenhouse gases in the atmosphere, resulting in a radiative imbalance. In 2018, the concentrations of CO

2 and methane had increased by about 45% and 160%, respectively, since 1750.[63] These CO

2 levels are much higher than they have been at any time during the last 800,000 years, the period for which reliable data have been collected from air trapped in ice cores.[64] Less direct geological evidence indicates that CO

2 values have not been this high for millions of years.[65]

CO

2 since 1880 have been caused by different sources ramping up one after another. The Global Carbon Project shows how additions tosince 1880 have been caused by different sources ramping up one after another.

Global anthropogenic greenhouse gas emissions in 2018, excluding those from land use change, were equivalent to 52 billion tonnes of CO

2. Of these emissions, 72% was actual CO

2, 19% was methane, 6% was nitrous oxide, and 3% was fluorinated gases. CO

2 emissions primarily come from burning fossil fuels to provide energy for transport, manufacturing, heating, and electricity.[66] Additional CO

2 emissions come from deforestation and industrial processes, which include the CO

2 released by the chemical reactions for making cement, steel, aluminum, and fertiliser.[67] Methane emissions come from livestock, manure, rice cultivation, landfills, wastewater, coal mining, as well as oil and gas extraction.[68] Nitrous oxide emissions largely come from the microbial decomposition of inorganic and organic fertiliser.[69] From a production standpoint, the primary sources of global greenhouse gas emissions are estimated as: electricity and heat (25%), agriculture and forestry (24%), industry and manufacturing (21%), transport (14%), and buildings (6%).[70]

Despite the contribution of deforestation to greenhouse gas emissions, the Earth's land surface, particularly its forests, remain a significant carbon sink for CO

2. Natural processes, such as carbon fixation in the soil and photosynthesis, more than offset the greenhouse gas contributions from deforestation. The land-surface sink is estimated to remove about 29% of annual global CO

2 emissions.[71] The ocean also serves as a significant carbon sink via a two-step process. First, CO

2 dissolves in the surface water. Afterwards, the ocean's overturning circulation distributes it deep into the ocean's interior, where it accumulates over time as part of the carbon cycle. Over the last two decades, the world's oceans have absorbed 20 to 30% of emitted CO

2.[72]

Aerosols and clouds

Air pollution, in the form of aerosols, not only puts a large burden on human health, but also affects the climate on a large scale.[73] From 1961 to 1990, a gradual reduction in the amount of sunlight reaching the Earth's surface was observed, a phenomenon popularly known as global dimming,[74] typically attributed to aerosols from biofuel and fossil fuel burning.[75] Aerosol removal by precipitation gives tropospheric aerosols an atmospheric lifetime of only about a week, while stratospheric aerosols can remain in the atmosphere for a few years.[76] Globally, aerosols have been declining since 1990, meaning that they no longer mask greenhouse gas warming as much.[77]

In addition to their direct effects (scattering and absorbing solar radiation), aerosols have indirect effects on the Earth's radiation budget. Sulfate aerosols act as cloud condensation nuclei and thus lead to clouds that have more and smaller cloud droplets. These clouds reflect solar radiation more efficiently than clouds with fewer and larger droplets.[78] This effect also causes droplets to be more uniform in size, which reduces the growth of raindrops and makes clouds more reflective to incoming sunlight.[79] Indirect effects of aerosols are the largest uncertainty in radiative forcing.[80]

While aerosols typically limit global warming by reflecting sunlight, black carbon in soot that falls on snow or ice can contribute to global warming. Not only does this increase the absorption of sunlight, it also increases melting and sea-level rise.[81] Limiting new black carbon deposits in the Arctic could reduce global warming by 0.2 °C (0.36 °F) by 2050.[82]

Changes of the land surface

[83] The rate of global tree cover loss has approximately doubled since 2001, to an annual loss approaching an area the size of Italy.

Humans change the Earth's surface mainly to create more agricultural land. Today, agriculture takes up 34% of Earth's land area, while 26% is forests, and 30% is uninhabitable (glaciers, deserts, etc.).[84] The amount of forested land continues to decrease, largely due to conversion to cropland in the tropics.[85] This deforestation is the most significant aspect of land surface change affecting global warming. The main causes of deforestation are: permanent land-use change from forest to agricultural land producing products such as beef and palm oil (27%), logging to produce forestry/forest products (26%), short term shifting cultivation (24%), and wildfires (23%).[86]

In addition to affecting greenhouse gas concentrations, land-use changes affect global warming through a variety of other chemical and physical mechanisms. Changing the type of vegetation in a region affects the local temperature, by changing how much of the sunlight gets reflected back into space (albedo), and how much heat is lost by evaporation. For instance, the change from a dark forest to grassland makes the surface lighter, causing it to reflect more sunlight. Deforestation can also contribute to changing temperatures by affecting the release of aerosols and other chemical compounds that influence clouds, and by changing wind patterns.[87] In tropic and temperate areas the net effect is to produce significant warming, while at latitudes closer to the poles a gain of albedo (as forest is replaced by snow cover) leads to an overall cooling effect.[87] Globally, these effects are estimated to have led to a slight cooling, dominated by an increase in surface albedo.[88]

Solar and volcanic activity

Physical climate models are unable to reproduce the rapid warming observed in recent decades when taking into account only variations in solar output and volcanic activity.[89] As the Sun is the Earth's primary energy source, changes in incoming sunlight directly affect the climate system.[90] Solar irradiance has been measured directly by satellites,[91] and indirect measurements are available from the early 1600s.[90] There has been no upward trend in the amount of the Sun's energy reaching the Earth.[92] Further evidence for greenhouse gases being the cause of recent climate change come from measurements showing the warming of the lower atmosphere (the troposphere), coupled with the cooling of the upper atmosphere (the stratosphere).[93] If solar variations were responsible for the observed warming, warming of both the troposphere and the stratosphere would be expected, but that has not been the case.[55]

Explosive volcanic eruptions represent the largest natural forcing over the industrial era. When the eruption is sufficiently strong (with sulfur dioxide reaching the stratosphere) sunlight can be partially blocked for a couple of years, with a temperature signal lasting about twice as long. In the industrial era, volcanic activity has had negligible impacts on global temperature trends.[94] Present-day volcanic CO 2 emissions are equivalent to less than 1% of current anthropogenic CO 2 emissions.

Climate change feedback

[96] Sea ice reflects 50% to 70% of incoming solar radiation while the dark ocean surface only reflects 6%, so melting sea ice is a self-reinforcing feedback.

The response of the climate system to an initial forcing is modified by feedbacks: increased by self-reinforcing feedbacks and reduced by balancing feedbacks.[97] The main reinforcing feedbacks are the water-vapour feedback, the ice–albedo feedback, and probably the net effect of clouds. The primary balancing feedback to global temperature change is radiative cooling to space as infrared radiation in response to rising surface temperature. In addition to temperature feedbacks, there are feedbacks in the carbon cycle, such as the fertilizing effect of CO

2 on plant growth.[100] Uncertainty over feedbacks is the major reason why different climate models project different magnitudes of warming for a given amount of emissions.[101]

As air gets warmer, it can hold more moisture. After initial warming due to emissions of greenhouse gases, the atmosphere will hold more water. As water vapour is a potent greenhouse gas, this further heats the atmosphere. If cloud cover increases, more sunlight will be reflected back into space, cooling the planet. If clouds become more high and thin, they act as an insulator, reflecting heat from below back downwards and warming the planet. Overall, the net cloud feedback over the industrial era has probably exacerbated temperature rise.[103] The reduction of snow cover and sea ice in the Arctic reduces the albedo of the Earth's surface.[104] More of the Sun's energy is now absorbed in these regions, contributing to amplification of Arctic temperature changes.[105] Arctic amplification is also melting permafrost, which releases methane and CO

2 into the atmosphere.[106]

Around half of human-caused CO

2 emissions have been absorbed by land plants and by the oceans.[107] On land, elevated CO

2 and an extended growing season have stimulated plant growth. Climate change increases droughts and heat waves that inhibit plant growth, which makes it uncertain whether this carbon sink will continue to grow in the future.[108] Soils contain large quantities of carbon and may release some when they heat up.[109] As more CO

2 and heat are absorbed by the ocean, it acidifies, its circulation changes and phytoplankton takes up less carbon, decreasing the rate at which the ocean absorbs atmospheric carbon. Climate change can increase methane emissions from wetlands, marine and freshwater systems, and permafrost.

Future warming and the carbon budget

Average climate model projections for 2081–2100 relative to 1986–2005, under low and high emission scenarios

Future warming depends on the strengths of climate feedbacks and on emissions of greenhouse gases.[112] The former are often estimated using various climate models, developed by multiple scientific institutions.[113] A climate model is a representation of the physical, chemical, and biological processes that affect the climate system.[114] Models include changes in the Earth's orbit, historical changes in the Sun's activity, and volcanic forcing.[115] Computer models attempt to reproduce and predict the circulation of the oceans, the annual cycle of the seasons, and the flows of carbon between the land surface and the atmosphere.[116] Models project different future temperature rises for given emissions of greenhouse gases; they also do not fully agree on the strength of different feedbacks on climate sensitivity and magnitude of inertia of the climate system.[117]

The physical realism of models is tested by examining their ability to simulate contemporary or past climates.[118] Past models have underestimated the rate of Arctic shrinkage[119] and underestimated the rate of precipitation increase.[120] Sea level rise since 1990 was underestimated in older models, but more recent models agree well with observations.[121] The 2017 United States-published National Climate Assessment notes that ""climate models may still be underestimating or missing relevant feedback processes"".[122]

Various Representative Concentration Pathways (RCPs) can be used as input for climate models: ""a stringent mitigation scenario (RCP2.6), two intermediate scenarios (RCP4.5 and RCP6.0) and one scenario with very high [greenhouse gas] emissions (RCP8.5)"".[123] RCPs only look at concentrations of greenhouse gases, and so do not include the response of the carbon cycle.[124] Climate model projections summarised in the IPCC Fifth Assessment Report indicate that, during the 21st century, the global surface temperature is likely to rise a further 0.3 to 1.7 °C (0.5 to 3.1 °F) in a moderate scenario, or as much as 2.6 to 4.8 °C (4.7 to 8.6 °F) in an extreme scenario, depending on the rate of future greenhouse gas emissions and on climate feedback effects.[125]

CO

2 and other gases' CO

2 -equivalents Four possible future concentration pathways, includingand other gases'-equivalents

A subset of climate models add societal factors to a simple physical climate model. These models simulate how population, economic growth, and energy use affect – and interact with – the physical climate. With this information, these models can produce scenarios of how greenhouse gas emissions may vary in the future. This output is then used as input for physical climate models to generate climate change projections.[126] In some scenarios emissions continue to rise over the century, while others have reduced emissions.[127] Fossil fuel resources are too abundant for shortages to be relied on to limit carbon emissions in the 21st century.[128] Emissions scenarios can be combined with modelling of the carbon cycle to predict how atmospheric concentrations of greenhouse gases might change in the future.[129] According to these combined models, by 2100 the atmospheric concentration of CO 2 could be as low as 380 or as high as 1400 ppm, depending on the socioeconomic scenario and the mitigation scenario.[130]

The remaining carbon emissions budget is determined by modelling the carbon cycle and the climate sensitivity to greenhouse gases.[131] According to the IPCC, global warming can be kept below 1.5 °C (2.7 °F) with a two-thirds chance if emissions after 2018 do not exceed 420 or 570 gigatonnes of CO

2, depending on exactly how the global temperature is defined. This amount corresponds to 10 to 13 years of current emissions. There are high uncertainties about the budget; for instance, it may be 100 gigatonnes of CO

2 smaller due to methane release from permafrost and wetlands.[132]

Impacts

Physical environment

[133] Historical sea level reconstruction and projections up to 2100 published in 2017 by the U.S. Global Change Research Program

The environmental effects of climate change are broad and far-reaching, affecting oceans, ice, and weather. Changes may occur gradually or rapidly. Evidence for these effects comes from studying climate change in the past, from modelling, and from modern observations.[134] Since the 1950s, droughts and heat waves have appeared simultaneously with increasing frequency.[135] Extremely wet or dry events within the monsoon period have increased in India and East Asia.[136] The maximum rainfall and wind speed from hurricanes and typhoons is likely increasing.[8]

Global sea level is rising as a consequence of glacial melt, melt of the ice sheets in Greenland and Antarctica, and thermal expansion. Between 1993 and 2017, the rise increased over time, averaging 3.1 ± 0.3 mm per year.[137] Over the 21st century, the IPCC projects that in a very high emissions scenario the sea level could rise by 61–110 cm.[138] Increased ocean warmth is undermining and threatening to unplug Antarctic glacier outlets, risking a large melt of the ice sheet[139] and the possibility of a 2-meter sea level rise by 2100 under high emissions.

Climate change has led to decades of shrinking and thinning of the Arctic sea ice, making it vulnerable to atmospheric anomalies.[141] While ice-free summers are expected to be rare at 1.5 °C (2.7 °F) degrees of warming, they are set to occur once every three to ten years at a warming level of 2.0 °C (3.6 °F).[142] Higher atmospheric CO

2 concentrations have led to changes in ocean chemistry. An increase in dissolved CO

2 is causing oceans to acidify.[143] In addition, oxygen levels are decreasing as oxygen is less soluble in warmer water,[144] with hypoxic dead zones expanding as a result of algal blooms stimulated by higher temperatures, higher CO

2 levels, ocean deoxygenation, and eutrophication.[145]

Tipping points and long-term impacts

The greater the amount of global warming, the greater the risk of passing through ‘tipping points’, thresholds beyond which certain impacts can no longer be avoided even if temperatures are reduced.[146] An example is the collapse of West Antarctic and Greenland ice sheets, where a temperature rise of 1.5 to 2.0 °C (2.7 to 3.6 °F) may commit the ice sheets to melt, although the time scale of melt is uncertain and depends on future warming.[147][14] Some large-scale changes could occur over a short time period, such as a collapse of the Atlantic Meridional Overturning Circulation,[148] which would trigger major climate changes in the North Atlantic, Europe, and North America.[149]

The long-term effects of climate change include further ice melt, ocean warming, sea level rise, and ocean acidification. On the timescale of centuries to millennia, the magnitude of climate change will be determined primarily by anthropogenic CO

2 emissions.[150] This is due to CO

2's long atmospheric lifetime.[150] Oceanic CO

2 uptake is slow enough that ocean acidification will continue for hundreds to thousands of years. These emissions are estimated to have prolonged the current interglacial period by at least 100,000 years.[152] Sea level rise will continue over many centuries, with an estimated rise of 2.3 metres per degree Celsius (4.2 ft/°F) after 2000 years.[153]

Nature and wildlife

Recent warming has driven many terrestrial and freshwater species poleward and towards higher altitudes.[154] Higher atmospheric CO

2 levels and an extended growing season have resulted in global greening, whereas heatwaves and drought have reduced ecosystem productivity in some regions. The future balance of these opposing effects is unclear. Climate change has contributed to the expansion of drier climate zones, such as the expansion of deserts in the subtropics.[156] The size and speed of global warming is making abrupt changes in ecosystems more likely. Overall, it is expected that climate change will result in the extinction of many species.

The oceans have heated more slowly than the land, but plants and animals in the ocean have migrated towards the colder poles faster than species on land.[159] Just as on land, heat waves in the ocean occur more frequently due to climate change, with harmful effects found on a wide range of organisms such as corals, kelp, and seabirds.[160] Ocean acidification is impacting organisms who produce shells and skeletons, such as mussels and barnacles, and coral reefs; coral reefs have seen extensive bleaching after heat waves. Harmful algae bloom enhanced by climate change and eutrophication cause anoxia, disruption of food webs and massive large-scale mortality of marine life.[162] Coastal ecosystems are under particular stress, with almost half of wetlands having disappeared as a consequence of climate change and other human impacts.

Humans

The effects of climate change on humans, mostly due to warming and shifts in precipitation, have been detected worldwide. Regional impacts of climate change are now observable on all continents and across ocean regions,[168] with low-latitude, less developed areas facing the greatest risk.[169] Continued emission of greenhouse gases will lead to further warming and long-lasting changes in the climate system, with potentially “severe, pervasive and irreversible impacts” for both people and ecosystems.[170] Climate change risks are unevenly distributed, but are generally greater for disadvantaged people in developing and developed countries.[171]

Food and health

Health impacts include both the direct effects of extreme weather, leading to injury and loss of life,[172] as well as indirect effects, such as undernutrition brought on by crop failures.[173] Various infectious diseases are more easily transmitted in a warmer climate, such as dengue fever, which affects children most severely, and malaria. Young children are the most vulnerable to food shortages, and together with older people, to extreme heat. The World Health Organization (WHO) has estimated that between 2030 and 2050, climate change is expected to cause approximately 250,000 additional deaths per year from heat exposure in elderly people, increases in diarrheal disease, malaria, dengue, coastal flooding, and childhood undernutrition.[176] Over 500,000 additional adult deaths are projected yearly by 2050 due to reductions in food availability and quality.[177] Other major health risks associated with climate change include air and water quality.[178] The WHO has classified human impacts from climate change as the greatest threat to global health in the 21st century.[179]

Climate change is affecting food security and has caused reduction in global mean yields of maize, wheat, and soybeans between 1981 and 2010.[180] Future warming could further reduce global yields of major crops.[181] Crop production will probably be negatively affected in low-latitude countries, while effects at northern latitudes may be positive or negative.[182] Up to an additional 183 million people worldwide, particularly those with lower incomes, are at risk of hunger as a consequence of these impacts.[183] The effects of warming on the oceans impact fish stocks, with a global decline in the maximum catch potential. Only polar stocks are showing an increased potential. Regions dependent on glacier water, regions that are already dry, and small islands are at increased risk of water stress due to climate change.[185]

Livelihoods

Economic damages due to climate change have been underestimated, and may be severe, with the probability of disastrous tail-risk events being nontrivial.[186] Climate change has likely already increased global economic inequality, and is projected to continue doing so.[187] Most of the severe impacts are expected in sub-Saharan Africa and South-East Asia, where existing poverty is already exacerbated.[188] The World Bank estimates that climate change could drive over 120 million people into poverty by 2030. Current inequalities between men and women, between rich and poor, and between different ethnicities have been observed to worsen as a consequence of climate variability and climate change.[190] An expert elicitation concluded that the role of climate change in armed conflict has been small compared to factors such as socio-economic inequality and state capabilities, but that future warming will bring increasing risks.

Low-lying islands and coastal communities are threatened through hazards posed by sea level rise, such as flooding and permanent submergence. This could lead to statelessness for populations in island nations, such as the Maldives and Tuvalu.[193] In some regions, rise in temperature and humidity may be too severe for humans to adapt to. With worst-case climate change, models project that almost one-third of humanity might live in extremely hot and uninhabitable climates, similar to the current climate found mainly in the Sahara.[195] These factors, plus weather extremes, can drive environmental migration, both within and between countries.[196] Displacement of people is expected to increase as a consequence of more frequent extreme weather, sea level rise, and conflict arising from increased competition over natural resources. Climate change may also increase vulnerabilities, leading to ""trapped populations"" in some areas who are not able to move due to a lack of resources.[197]

Responses: mitigation and adaptation

Mitigation

Scenarios of global greenhouse gas emissions. If all countries achieve their current Paris Agreement pledges, average warming by 2100 would still significantly exceed the maximum 2°C target set by the Agreement.

Climate change impacts can be mitigated by reducing greenhouse gas emissions and by enhancing sinks that absorb greenhouse gases from the atmosphere.[203] In order to limit global warming to less than 1.5 °C with a high likelihood of success, global greenhouse gas emissions needs to be net-zero by 2050, or by 2070 with a 2 °C target.[204] This requires far-reaching, systemic changes on an unprecedented scale in energy, land, cities, transport, buildings, and industry.[205] Scenarios that limit global warming to 1.5 °C often describe reaching net negative emissions at some point.[206] To make progress towards a goal of limiting warming to 2 °C, the United Nations Environment Programme estimates that, within the next decade, countries need to triple the amount of reductions they have committed to in their current Paris Agreements; an even greater level of reduction is required to meet the 1.5 °C goal.[207]

Although there is no single pathway to limit global warming to 1.5 or 2.0 °C (2.7 or 3.6 °F),[208] most scenarios and strategies see a major increase in the use of renewable energy in combination with increased energy efficiency measures to generate the needed greenhouse gas reductions.[209] To reduce pressures on ecosystems and enhance their carbon sequestration capabilities, changes would also be necessary in sectors such as forestry and agriculture.[210]

Other approaches to mitigating climate change entail a higher level of risk. Scenarios that limit global warming to 1.5 °C typically project the large-scale use of carbon dioxide removal methods over the 21st century.[211] There are concerns, though, about over-reliance on these technologies, as well as possible environmental impacts.[212] Solar radiation management (SRM) methods have also been explored as a possible supplement to deep reductions in emissions. However, SRM would raise significant ethical and legal issues, and the risks are poorly understood.[213]

Clean energy

[214] Coal, oil, and natural gas remain the primary global energy sources even as renewables have begun rapidly increasing.

Economic sectors with more greenhouse gas contributions have a greater stake in climate change policies.

Long-term decarbonisation scenarios point to rapid and significant investment in renewable energy,[215] which includes solar and wind power, bioenergy, geothermal energy, and hydropower.[216] Fossil fuels accounted for 80% of the world's energy in 2018, while the remaining share was split between nuclear power and renewables;[217] that mix is projected to change significantly over the next 30 years.[209] Solar and wind have seen substantial growth and progress over the last few years; photovoltaic solar and onshore wind are the cheapest forms of adding new power generation capacity in most countries.[218] Renewables represented 75% of all new electricity generation installed in 2019, with solar and wind constituting nearly all of that amount.[219] Meanwhile, nuclear power costs are increasing amidst stagnant power share, so that nuclear power generation is now several times more expensive per megawatt-hour than wind and solar.[220]

To achieve carbon neutrality by 2050, renewable energy would become the dominant form of electricity generation, rising to 85% or more by 2050 in some scenarios. The use of electricity for other needs, such as heating, would rise to the point where electricity becomes the largest form of overall energy supply.[221] Investment in coal would be eliminated and coal use nearly phased out by 2050.[222]

There are obstacles to the continued rapid development of renewables. For solar and wind power, a key challenge is their intermittency and seasonal variability. Traditionally, hydro dams with reservoirs and conventional power plants have been used when variable energy production is low. Intermittency can further be countered by demand flexibility, and by expanding battery storage and long-distance transmission to smooth variability of renewable output across wider geographic areas.[215] Some environmental and land use concerns have been associated with large solar and wind projects,[223] while bioenergy is often not carbon neutral and may have negative consequences for food security.[224] Hydropower growth has been slowing and is set to decline further due to concerns about social and environmental impacts.[225]

Clean energy improves human health by minimizing climate change and has the near-term benefit of reducing air pollution deaths,[226] which were estimated at 7 million annually in 2016.[227] Meeting the Paris Agreement goals that limit warming to a 2 °C increase could save about a million of those lives per year by 2050, whereas limiting global warming to 1.5 °C could save millions and simultaneously increase energy security and reduce poverty.[228]

Energy efficiency

Reducing energy demand is another major feature of decarbonisation scenarios and plans.[229] In addition to directly reducing emissions, energy demand reduction measures provide more flexibility for low carbon energy development, aid in the management of the electricity grid, and minimise carbon-intensive infrastructure development.[230] Over the next few decades, major increases in energy efficiency investment will be required to achieve these reductions, comparable to the expected level of investment in renewable energy.[231] However, several COVID-19 related changes in energy use patterns, energy efficiency investments, and funding have made forecasts for this decade more difficult and uncertain.[232]

Efficiency strategies to reduce energy demand vary by sector. In transport, gains can be made by switching passengers and freight to more efficient travel modes, such as buses and trains, and increasing the use of electric vehicles.[233] Industrial strategies to reduce energy demand include increasing the energy efficiency of heating systems and motors, designing less energy-intensive products, and increasing product lifetimes.[234] In the building sector the focus is on better design of new buildings, and incorporating higher levels of energy efficiency in retrofitting techniques for existing structures.[235] Buildings would see additional electrification with the use of technologies like heat pumps, which have higher efficiency than fossil fuels.[236]

Agriculture, industry and transport

Agriculture and forestry face a triple challenge of limiting greenhouse gas emissions, preventing the further conversion of forests to agricultural land, and meeting increases in world food demand.[237] A suite of actions could reduce agriculture/forestry-based greenhouse gas emissions by 66% from 2010 levels by reducing growth in demand for food and other agricultural products, increasing land productivity, protecting and restoring forests, and reducing greenhouse gas emissions from agricultural production.[238]

In addition to the industrial demand reduction measures mentioned earlier, steel and cement production, which together are responsible for about 13% of industrial CO

2 emissions, present particular challenges. In these industries, carbon-intensive materials such as coke and lime play an integral role in the production process. Reducing CO

2 emissions here requires research driven efforts aimed at decarbonizing the chemistry of these processes.[239] In transport, scenarios envision sharp increases in the market share of electric vehicles, and low carbon fuel substitution for other transportation modes like shipping.[240]

Carbon sequestration

CO

2 emissions have been absorbed by carbon sinks, including plant growth, soil uptake, and ocean uptake ( Mostemissions have been absorbed by carbon sinks, including plant growth, soil uptake, and ocean uptake ( 2020 Global Carbon Budget ).

Natural carbon sinks can be enhanced to sequester significantly larger amounts of CO

2 beyond naturally occurring levels.[241] Reforestation and tree planting on non-forest lands are among the most mature sequestration techniques, although they raise food security concerns. Soil carbon sequestration and coastal carbon sequestration are less understood options.[242] The feasibility of land-based negative emissions methods for mitigation are uncertain in models; the IPCC has described mitigation strategies based on them as risky.

Where energy production or CO

2-intensive heavy industries continue to produce waste CO

2, the gas can be captured and stored instead of being released to the atmosphere. Although its current use is limited in scale and expensive,[244] carbon capture and storage (CCS) may be able to play a significant role in limiting CO

2 emissions by mid-century. This technique, in combination with bio-energy production (BECCS) can result in net-negative emissions, where the amount of greenhouse gasses that are released into the atmosphere are less than the sequestered, or stored, amount in the bio-energy fuel being grown.[246] It remains highly uncertain whether carbon dioxide removal techniques, such as BECCS, will be able to play a large role in limiting warming to 1.5 °C, and policy decisions based on reliance on carbon dioxide removal increases the risk of global warming increasing beyond international goals.[247]

Adaptation

Adaptation is ""the process of adjustment to current or expected changes in climate and its effects"". Without additional mitigation, adaptation cannot avert the risk of ""severe, widespread and irreversible"" impacts. More severe climate change requires more transformative adaptation, which can be prohibitively expensive. The capacity and potential for humans to adapt, called adaptive capacity, is unevenly distributed across different regions and populations, and developing countries generally have less.[250] The first two decades of the 21st century saw an increase in adaptive capacity in most low- and middle-income countries with improved access to basic sanitation and electricity, but progress is slow. Many countries have implemented adaptation policies. However, there is a considerable gap between necessary and available finance.

Adaptation to sea level rise consists of avoiding at-risk areas, learning to live with increased flooding, protection and, if needed, the more transformative option of managed retreat.[252] There are economic barriers for moderation of dangerous heat impact: avoiding strenuous work or employing private air conditioning is not possible for everybody. In agriculture, adaptation options include a switch to more sustainable diets, diversification, erosion control and genetic improvements for increased tolerance to a changing climate. Insurance allows for risk-sharing, but is often difficult to obtain for people on lower incomes.[255] Education, migration and early warning systems can reduce climate vulnerability.

Ecosystems adapt to climate change, a process that can be supported by human intervention. Possible responses include increasing connectivity between ecosystems, allowing species to migrate to more favourable climate conditions and species relocation. Protection and restoration of natural and semi-natural areas helps build resilience, making it easier for ecosystems to adapt. Many of the actions that promote adaptation in ecosystems, also help humans adapt via ecosystem-based adaptation. For instance, restoration of natural fire regimes makes catastrophic fires less likely, and reduces human exposure. Giving rivers more space allows for more water storage in the natural system, reducing flood risk. Restored forest act as a carbon sink, but planting trees in unsuitable regions can exacerbate climate impacts.[257]

There are some synergies and trade-offs between adaptation and mitigation. Adaptation measures often offer short-term benefits, whereas mitigation has longer-term benefits.[258] Increased use of air conditioning allows people to better cope with heat, but increases energy demand. Compact urban development may lead to reduced emissions from transport and construction. Simultaneously, it may increase the urban heat island effect, leading to higher temperatures and increased exposure.[259] Increased food productivity has large benefits for both adaptation and mitigation.[260]

Policies and politics

High Medium Low Very Low The Climate Change Performance Index ranks countries by greenhouse gas emissions (40% of score), renewable energy (20%), energy use (20%), and climate policy (20%).

Countries that are most vulnerable to climate change have typically been responsible for a small share of global emissions, which raises questions about justice and fairness.[261] Climate change is strongly linked to sustainable development. Limiting global warming makes it easier to achieve sustainable development goals, such as eradicating poverty and reducing inequalities. The connection between the two is recognised in the Sustainable Development Goal 13 which is to ""Take urgent action to combat climate change and its impacts"".[262] The goals on food, clean water and ecosystem protections have synergies with climate mitigation.

The geopolitics of climate change is complex and has often been framed as a free-rider problem, in which all countries benefit from mitigation done by other countries, but individual countries would lose from investing in a transition to a low-carbon economy themselves. This framing has been challenged. For instance, the benefits in terms of public health and local environmental improvements of coal phase-out exceed the costs in almost all regions.[264] Another argument against this framing is that net importers of fossil fuels win economically from transitioning, causing net exporters to face stranded assets: fossil fuels they cannot sell.[265]

Policy options

A wide range of policies, regulations and laws are being used to reduce greenhouse gases. Carbon pricing mechanisms include carbon taxes and emissions trading systems.[266] As of 2019, carbon pricing covers about 20% of global greenhouse gas emissions.[267] Direct global fossil fuel subsidies reached $319 billion in 2017, and $5.2 trillion when indirect costs such as air pollution are priced in.[268] Ending these can cause a 28% reduction in global carbon emissions and a 46% reduction in air pollution deaths.[269] Subsidies could also be redirected to support the transition to clean energy.[270] More prescriptive methods that can reduce greenhouse gases include vehicle efficiency standards, renewable fuel standards, and air pollution regulations on heavy industry.[271] Renewable portfolio standards have been enacted in several countries requiring utilities to increase the percentage of electricity they generate from renewable sources.[272]

As the use of fossil fuels is reduced, there are Just Transition considerations involving the social and economic challenges that arise. An example is the employment of workers in the affected industries, along with the well-being of the broader communities involved.[273] Climate justice considerations, such as those facing indigenous populations in the Arctic,[274] are another important aspect of mitigation policies.[275]

International climate agreements

CO

2 emissions in China and the rest of world have surpassed the output of the United States and Europe.[276] Since 2000, risingemissions in China and the rest of world have surpassed the output of the United States and Europe.

CO

2 at a far faster rate than other primary regions.[276] Per person, the United States generatesat a far faster rate than other primary regions.

Nearly all countries in the world are parties to the 1994 United Nations Framework Convention on Climate Change (UNFCCC).[277] The objective of the UNFCCC is to prevent dangerous human interference with the climate system.[278] As stated in the convention, this requires that greenhouse gas concentrations are stabilised in the atmosphere at a level where ecosystems can adapt naturally to climate change, food production is not threatened, and economic development can be sustained.[279] Global emissions have risen since signing of the UNFCCC, which does not actually restrict emissions but rather provides a framework for protocols that do.[70] Its yearly conferences are the stage of global negotiations.[280]

The 1997 Kyoto Protocol extended the UNFCCC and included legally binding commitments for most developed countries to limit their emissions,[281] During Kyoto Protocol negotiations, the G77 (representing developing countries) pushed for a mandate requiring developed countries to ""[take] the lead"" in reducing their emissions,[282] since developed countries contributed most to the accumulation of greenhouse gases in the atmosphere, and since per-capita emissions were still relatively low in developing countries and emissions of developing countries would grow to meet their development needs.[283]

The 2009 Copenhagen Accord has been widely portrayed as disappointing because of its low goals, and was rejected by poorer nations including the G77.[284] Associated parties aimed to limit the increase in global mean temperature to below 2.0 °C (3.6 °F).[285] The Accord set the goal of sending $100 billion per year to developing countries in assistance for mitigation and adaptation by 2020, and proposed the founding of the Green Climate Fund.[286] As of 2020 , the fund has failed to reach its expected target, and risks a shrinkage in its funding.[287]

In 2015 all UN countries negotiated the Paris Agreement, which aims to keep global warming well below 1.5 °C (2.7 °F) and contains an aspirational goal of keeping warming under 1.5 °C. The agreement replaced the Kyoto Protocol. Unlike Kyoto, no binding emission targets were set in the Paris Agreement. Instead, the procedure of regularly setting ever more ambitious goals and reevaluating these goals every five years has been made binding.[289] The Paris Agreement reiterated that developing countries must be financially supported.[290] As of February 2021 , 194 states and the European Union have signed the treaty and 188 states and the EU have ratified or acceded to the agreement.[291]

The 1987 Montreal Protocol, an international agreement to stop emitting ozone-depleting gases, may have been more effective at curbing greenhouse gas emissions than the Kyoto Protocol specifically designed to do so.[292] The 2016 Kigali Amendment to the Montreal Protocol aims to reduce the emissions of hydrofluorocarbons, a group of powerful greenhouse gases which served as a replacement for banned ozone-depleting gases. This strengthened the makes the Montreal Protocol a stronger agreement against climate change.[293]

National responses

In 2019, the United Kingdom parliament became the first national government in the world to officially declare a climate emergency.[294] Other countries and jurisdictions followed suit.[295] In November 2019 the European Parliament declared a ""climate and environmental emergency"",[296] and the European Commission presented its European Green Deal with the goal of making the EU carbon-neutral by 2050.[297] Major countries in Asia have made similar pledges: South Korea and Japan have committed to become carbon neutral by 2050, and China by 2060.[298]

As of 2021, based on information from 48 NDCs which represent 40% of the parties to the Paris Agreement, estimated total greenhouse gas emissions will be 0.5% lower compared to 2010 levels, below the 45% or 25% reduction goals to limit global warming to 1.5 °C or 2 °C, respectively.[299]

Scientific consensus and society

Scientific consensus

There is an overwhelming scientific consensus that global surface temperatures have increased in recent decades and that the trend is caused mainly by human-induced emissions of greenhouse gases, with 90–100% (depending on the exact question, timing and sampling methodology) of publishing climate scientists agreeing.[301] The consensus has grown to 100% among research scientists on anthropogenic global warming as of 2019.[302] No scientific body of national or international standing disagrees with this view.[303] Consensus has further developed that some form of action should be taken to protect people against the impacts of climate change, and national science academies have called on world leaders to cut global emissions.[304]

Scientific discussion takes place in journal articles that are peer-reviewed, which scientists subject to assessment every couple of years in the Intergovernmental Panel on Climate Change reports.[305] In 2013, the IPCC Fifth Assessment Report stated that ""it is extremely likely that human influence has been the dominant cause of the observed warming since the mid-20th century"".[306] Their 2018 report expressed the scientific consensus as: ""human influence on climate has been the dominant cause of observed warming since the mid-20th century"". Scientists have issued two warnings to humanity, in 2017 and 2019, expressing concern about the current trajectory of potentially catastrophic climate change, and about untold human suffering as a consequence.[308]

The public

Climate change came to international public attention in the late 1980s.[309] Due to confusing media coverage in the early 1990s, understanding was often confounded by conflation with other environmental issues like ozone depletion.[310] In popular culture, the first movie to reach a mass public on the topic was The Day After Tomorrow in 2004, followed a few years later by the Al Gore documentary An Inconvenient Truth. Books, stories and films about climate change fall under the genre of climate fiction.[309]

Significant regional differences exist in both public concern for and public understanding of climate change. In 2015, a median of 54% of respondents considered it ""a very serious problem"", but Americans and Chinese (whose economies are responsible for the greatest annual CO 2 emissions) were among the least concerned. A 2018 survey found increased concern globally on the issue compared to 2013 in most countries. More highly educated people, and in some countries, women and younger people were more likely to see climate change as a serious threat. In the United States, there was a large partisan gap in opinion.

Denial and misinformation

One deceptive approach is cherry picking data from short time periods to falsely assert that global average temperatures are not rising. Blue trendlines show short-term countertrends that mask longer-term warming trends (red trendlines). Blue dots show the so-called global warming hiatus

Public debate about climate change has been strongly affected by climate change denial and misinformation, which originated in the United States and has since spread to other countries, particularly Canada and Australia. The actors behind climate change denial form a well-funded and relatively coordinated coalition of fossil fuel companies, industry groups, conservative think tanks, and contrarian scientists.[314] Like the tobacco industry before, the main strategy of these groups has been to manufacture doubt about scientific data and results.[315] Many who deny, dismiss, or hold unwarranted doubt about the scientific consensus on anthropogenic climate change are labelled as ""climate change skeptics"", which several scientists have noted is a misnomer.[316]

There are different variants of climate denial: some deny that warming takes place at all, some acknowledge warming but attribute it to natural influences, and some minimise the negative impacts of climate change.[317] Manufacturing uncertainty about the science later developed into a manufacturing controversy: creating the belief that there is significant uncertainty about climate change within the scientific community in order to delay policy changes.[318] Strategies to promote these ideas include criticism of scientific institutions,[319] and questioning the motives of individual scientists.[317] An echo chamber of climate-denying blogs and media has further fomented misunderstanding of climate change.[320]

Protest and litigation

Climate protests have risen in popularity in the 2010s in such forms as public demonstrations,[321] fossil fuel divestment, and lawsuits.[322] Prominent recent demonstrations include the school strike for climate, and civil disobedience. In the school strike, youth across the globe have protested by skipping school, inspired by Swedish teenager Greta Thunberg.[323] Mass civil disobedience actions by groups like Extinction Rebellion have protested by causing disruption.[324] Litigation is increasingly used as a tool to strengthen climate action, with many lawsuits targeting governments to demand that they take ambitious action or enforce existing laws regarding climate change.[325] Lawsuits against fossil-fuel companies, from activists, shareholders and investors, generally seek compensation for loss and damage.[326]

Discovery

For broader coverage of this topic, see History of climate change science

Tyndall's ratio spectrophotometer (drawing from 1861) measured how much infrared radiation was absorbed and emitted by various gases filling its central tube.

To explain why Earth's temperature was higher than expected considering only incoming solar radiation, Joseph Fourier proposed the existence of a greenhouse effect. Solar energy reaches the surface as the atmosphere is transparent to solar radiation. The warmed surface emits infrared radiation, but the atmosphere is relatively opaque to infrared and slows the emission of energy, warming the planet.[327] Starting in 1859,[328] John Tyndall established that nitrogen and oxygen (99% of dry air) are transparent to infrared, but water vapour and traces of some gases (significantly methane and carbon dioxide) both absorb infrared and, when warmed, emit infrared radiation. Changing concentrations of these gases could have caused ""all the mutations of climate which the researches of geologists reveal"" including ice ages.[329]

Svante Arrhenius noted that water vapour in air continuously varied, but carbon dioxide (CO

2) was determined by long term geological processes. At the end of an ice age, warming from increased CO

2 would increase the amount of water vapour, amplifying its effect in a feedback process. In 1896, he published the first climate model of its kind, showing that halving of CO

2 could have produced the drop in temperature initiating the ice age. Arrhenius calculated the temperature increase expected from doubling CO

2 to be around 5–6 °C (9.0–10.8 °F). Other scientists were initially sceptical and believed the greenhouse effect to be saturated so that adding more CO

2 would make no difference. They thought climate would be self-regulating.[331] From 1938 Guy Stewart Callendar published evidence that climate was warming and CO

2 levels increasing,[332] but his calculations met the same objections.[331]

In the 1950s, Gilbert Plass created a detailed computer model that included different atmospheric layers and the infrared spectrum and found that increasing CO

2 levels would cause warming. In the same decade Hans Suess found evidence CO

2 levels had been rising, Roger Revelle showed the oceans would not absorb the increase, and together they helped Charles Keeling to begin a record of continued increase, the Keeling Curve.[331] Scientists alerted the public,[333] and the dangers were highlighted at James Hansen's 1988 Congressional testimony.[21] The Intergovernmental Panel on Climate Change, set up in 1988 to provide formal advice to the world's governments, spurred interdisciplinary research.[334]

See also

2020s in environmental history

Anthropocene – proposed new geological time interval in which humans are having significant geological impact

Global cooling – minority view held by scientists in the 1970s that imminent cooling of the Earth would take place

References

Notes

Sources

IPCC reports

Other peer-reviewed sources

Books, reports and legal documents

Non-technical sources

Scholia has a profile for global warming (Q7942) ."	https://en.wikipedia.org/wiki/Climate_change	"Current rise in Earth's average temperature and its effects
Average surface air temperatures from 2011 to 2020 compared to a baseline average from 1951 to 1980 (Source: NASA
 Observed temperature from NASA versus the 1850–1900 average as a pre-industrial baseline. The main driver for increased global temperatures in the industrial era is human activity, with natural forces adding variability.
Climate change includes both global warming driven by human-induced emissions of greenhouse gases and the resulting large-scale shifts in weather patterns. Though there have been previous periods of climatic change, since the mid-20th century humans have had an unprecedented impact on Earth's climate system and caused change on a global scale.
The largest driver of warming is the emission of gases that create a greenhouse effect, of which more than 90% are carbon dioxide (CO
2) and methane. Fossil fuel burning (coal, oil, and natural gas) for energy consumption is the main source of these emissions, with additional contributions from agriculture, deforestation, and manufacturing. The human cause of climate change is not disputed by any scientific body of national or international standing. Temperature rise is accelerated or tempered by climate feedbacks, such as loss of sunlight-reflecting snow and ice cover, increased water vapour (a greenhouse gas itself), and changes to land and ocean carbon sinks.
Temperature rise on land is about twice the global average increase, leading to desert expansion and more common heat waves and wildfires. Temperature rise is also amplified in the Arctic, where it has contributed to melting permafrost, glacial retreat and sea ice loss. Warmer temperatures are increasing rates of evaporation, causing more intense storms and weather extremes. Impacts on ecosystems include the relocation or extinction of many species as their environment changes, most immediately in coral reefs, mountains, and the Arctic. Climate change threatens people with food insecurity, water scarcity, flooding, infectious diseases, extreme heat, economic losses, and displacement. These impacts have led the World Health Organization to call climate change the greatest threat to global health in the 21st century. Even if efforts to minimise future warming are successful, some effects will continue for centuries, including rising sea levels, rising ocean temperatures, and ocean acidification.
2. Energy flows between space, the atmosphere, and Earth's surface. Current greenhouse gas levels are causing a radiative imbalance of about 0.9 W/m
Many of these impacts are already felt at the current level of warming, which is about 1.2 °C (2.2 °F). The Intergovernmental Panel on Climate Change (IPCC) has issued a series of reports that project significant increases in these impacts as warming continues to 1.5 °C (2.7 °F) and beyond. Additional warming also increases the risk of triggering critical thresholds called tipping points. Responding to climate change involves mitigation and adaptation. Mitigation – limiting climate change – consists of reducing greenhouse gas emissions and removing them from the atmosphere; methods include the development and deployment of low-carbon energy sources such as wind and solar, a phase-out of coal, enhanced energy efficiency, reforestation, and forest preservation. Adaptation consists of adjusting to actual or expected climate, such as through improved coastline protection, better disaster management, assisted colonisation, and the development of more resistant crops. Adaptation alone cannot avert the risk of ""severe, widespread and irreversible"" impacts.
Under the 2015 Paris Agreement, nations collectively agreed to keep warming ""well under 2.0 °C (3.6 °F)"" through mitigation efforts. However, with pledges made under the Agreement, global warming would still reach about 2.8 °C (5.0 °F) by the end of the century. Limiting warming to 1.5 °C (2.7 °F) would require halving emissions by 2030 and achieving near-zero emissions by 2050.
Terminology
Before the 1980s, when it was unclear whether warming by greenhouse gases would dominate aerosol-induced cooling, scientists often used the term inadvertent climate modification to refer to humankind's impact on the climate. In the 1980s, the terms global warming and climate change were popularised, the former referring only to increased surface warming, while the latter describes the full effect of greenhouse gases on the climate. Global warming became the most popular term after NASA climate scientist James Hansen used it in his 1988 testimony in the U.S. Senate. In the 2000s, the term climate change increased in popularity. Global warming usually refers to human-induced warming of the Earth system, whereas climate change can refer to natural as well as anthropogenic change. The two terms are often used interchangeably.
Various scientists, politicians and media figures have adopted the terms climate crisis or climate emergency to talk about climate change, while using global heating instead of global warming. The policy editor-in-chief of The Guardian explained that they included this language in their editorial guidelines ""to ensure that we are being scientifically precise, while also communicating clearly with readers on this very important issue"". Oxford Dictionary chose climate emergency as its word of the year in 2019 and defines the term as ""a situation in which urgent action is required to reduce or halt climate change and avoid potentially irreversible environmental damage resulting from it"".
Observed temperature rise
 Directly observational data is in red. Global surface temperature reconstruction over the last 2000 years using proxy data from tree rings, corals, and ice cores in blue.Directly observational data is in red.
 shows that land surface temperatures have increased faster than ocean temperatures. NASA datashows that land surface temperatures have increased faster than ocean temperatures.
Multiple independently produced instrumental datasets show that the climate system is warming, with the 2009–2018 decade being 0.93 ± 0.07 °C (1.67 ± 0.13 °F) warmer than the pre-industrial baseline (1850–1900). Currently, surface temperatures are rising by about 0.2 °C (0.36 °F) per decade, with 2020 reaching a temperature of 1.2 °C (2.2 °F) above pre-industrial. Since 1950, the number of cold days and nights has decreased, and the number of warm days and nights has increased.
There was little net warming between the 18th century and the mid-19th century. Climate proxies, sources of climate information from natural archives such as trees and ice cores, show that natural variations offset the early effects of the Industrial Revolution. Thermometer records began to provide global coverage around 1850. Historical patterns of warming and cooling, like the Medieval Climate Anomaly and the Little Ice Age, did not occur at the same time across different regions, but temperatures may have reached as high as those of the late-20th century in a limited set of regions. There have been prehistorical episodes of global warming, such as the Paleocene–Eocene Thermal Maximum. However, the modern observed rise in temperature and CO
2 concentrations has been so rapid that even abrupt geophysical events that took place in Earth's history do not approach current rates.
Evidence of warming from air temperature measurements are reinforced with a wide range of other observations. There has been an increase in the frequency and intensity of heavy precipitation, melting of snow and land ice, and increased atmospheric humidity. Flora and fauna are also behaving in a manner consistent with warming; for instance, plants are flowering earlier in spring. Another key indicator is the cooling of the upper atmosphere, which demonstrates that greenhouse gases are trapping heat near the Earth's surface and preventing it from radiating into space.
While locations of warming vary, the patterns are independent of where greenhouse gases are emitted, because the gases persist long enough to diffuse across the planet. Since the pre-industrial period, global average land temperatures have increased almost twice as fast as global average surface temperatures. This is because of the larger heat capacity of oceans, and because oceans lose more heat by evaporation. Over 90% of the additional energy in the climate system over the last 50 years has been stored in the ocean, with the remainder warming the atmosphere, melting ice, and warming the continents.
The Northern Hemisphere and the North Pole have warmed much faster than the South Pole and Southern Hemisphere. The Northern Hemisphere not only has much more land, but also more seasonal snow cover and sea ice, because of how the land masses are arranged around the Arctic Ocean. As these surfaces flip from reflecting a lot of light to being dark after the ice has melted, they start absorbing more heat. Localised black carbon deposits on snow and ice also contribute to Arctic warming. Arctic temperatures have increased and are predicted to continue to increase during this century at over twice the rate of the rest of the world. Melting of glaciers and ice sheets in the Arctic disrupts ocean circulation, including a weakened Gulf Stream, further changing the climate.
Drivers of recent temperature rise
The climate system experiences various cycles on its own which can last for years (such as the El Niño–Southern Oscillation), decades or even centuries. Other changes are caused by an imbalance of energy that is ""external"" to the climate system, but not always external to the Earth. Examples of external forcings include changes in the composition of the atmosphere (e.g. increased concentrations of greenhouse gases), solar luminosity, volcanic eruptions, and variations in the Earth's orbit around the Sun.
To determine the human contribution to climate change, known internal climate variability and natural external forcings need to be ruled out. A key approach is to determine unique ""fingerprints"" for all potential causes, then compare these fingerprints with observed patterns of climate change. For example, solar forcing can be ruled out as a major cause because its fingerprint is warming in the entire atmosphere, and only the lower atmosphere has warmed, as expected from greenhouse gases (which trap heat energy radiating from the surface). Attribution of recent climate change shows that the primary driver is elevated greenhouse gases, but that aerosols also have a strong effect.
Greenhouse gases
CO
2 concentrations over the last 800,000 years as measured from ice cores (blue/green) and directly (black) concentrations over the last 800,000 years as measured from ice cores (blue/green) and directly (black)
The Earth absorbs sunlight, then radiates it as heat. Greenhouse gases in the atmosphere absorb and reemit infrared radiation, slowing the rate at which it can pass through the atmosphere and escape into space. Before the Industrial Revolution, naturally-occurring amounts of greenhouse gases caused the air near the surface to be about 33 °C (59 °F) warmer than it would have been in their absence. While water vapour (~50%) and clouds (~25%) are the biggest contributors to the greenhouse effect, they increase as a function of temperature and are therefore considered feedbacks. On the other hand, concentrations of gases such as CO
2 (~20%), tropospheric ozone, CFCs and nitrous oxide are not temperature-dependent, and are therefor considered external forcings.
Human activity since the Industrial Revolution, mainly extracting and burning fossil fuels (coal, oil, and natural gas), has increased the amount of greenhouse gases in the atmosphere, resulting in a radiative imbalance. In 2018, the concentrations of CO
2 and methane had increased by about 45% and 160%, respectively, since 1750. These CO
2 levels are much higher than they have been at any time during the last 800,000 years, the period for which reliable data have been collected from air trapped in ice cores. Less direct geological evidence indicates that CO
2 values have not been this high for millions of years.
CO
2 since 1880 have been caused by different sources ramping up one after another. The Global Carbon Project shows how additions tosince 1880 have been caused by different sources ramping up one after another.
Global anthropogenic greenhouse gas emissions in 2018, excluding those from land use change, were equivalent to 52 billion tonnes of CO
2. Of these emissions, 72% was actual CO
2, 19% was methane, 6% was nitrous oxide, and 3% was fluorinated gases. CO
2 emissions primarily come from burning fossil fuels to provide energy for transport, manufacturing, heating, and electricity. Additional CO
2 emissions come from deforestation and industrial processes, which include the CO
2 released by the chemical reactions for making cement, steel, aluminum, and fertiliser. Methane emissions come from livestock, manure, rice cultivation, landfills, wastewater, coal mining, as well as oil and gas extraction. Nitrous oxide emissions largely come from the microbial decomposition of inorganic and organic fertiliser. From a production standpoint, the primary sources of global greenhouse gas emissions are estimated as: electricity and heat (25%), agriculture and forestry (24%), industry and manufacturing (21%), transport (14%), and buildings (6%).
Despite the contribution of deforestation to greenhouse gas emissions, the Earth's land surface, particularly its forests, remain a significant carbon sink for CO
2. Natural processes, such as carbon fixation in the soil and photosynthesis, more than offset the greenhouse gas contributions from deforestation. The land-surface sink is estimated to remove about 29% of annual global CO
2 emissions. The ocean also serves as a significant carbon sink via a two-step process. First, CO
2 dissolves in the surface water. Afterwards, the ocean's overturning circulation distributes it deep into the ocean's interior, where it accumulates over time as part of the carbon cycle. Over the last two decades, the world's oceans have absorbed 20 to 30% of emitted CO
2.
Aerosols and clouds
Air pollution, in the form of aerosols, not only puts a large burden on human health, but also affects the climate on a large scale. From 1961 to 1990, a gradual reduction in the amount of sunlight reaching the Earth's surface was observed, a phenomenon popularly known as global dimming, typically attributed to aerosols from biofuel and fossil fuel burning. Aerosol removal by precipitation gives tropospheric aerosols an atmospheric lifetime of only about a week, while stratospheric aerosols can remain in the atmosphere for a few years. Globally, aerosols have been declining since 1990, meaning that they no longer mask greenhouse gas warming as much.
In addition to their direct effects (scattering and absorbing solar radiation), aerosols have indirect effects on the Earth's radiation budget. Sulfate aerosols act as cloud condensation nuclei and thus lead to clouds that have more and smaller cloud droplets. These clouds reflect solar radiation more efficiently than clouds with fewer and larger droplets. This effect also causes droplets to be more uniform in size, which reduces the growth of raindrops and makes clouds more reflective to incoming sunlight. Indirect effects of aerosols are the largest uncertainty in radiative forcing.
While aerosols typically limit global warming by reflecting sunlight, black carbon in soot that falls on snow or ice can contribute to global warming. Not only does this increase the absorption of sunlight, it also increases melting and sea-level rise. Limiting new black carbon deposits in the Arctic could reduce global warming by 0.2 °C (0.36 °F) by 2050.
Changes of the land surface
 The rate of global tree cover loss has approximately doubled since 2001, to an annual loss approaching an area the size of Italy.
Humans change the Earth's surface mainly to create more agricultural land. Today, agriculture takes up 34% of Earth's land area, while 26% is forests, and 30% is uninhabitable (glaciers, deserts, etc.). The amount of forested land continues to decrease, largely due to conversion to cropland in the tropics. This deforestation is the most significant aspect of land surface change affecting global warming. The main causes of deforestation are: permanent land-use change from forest to agricultural land producing products such as beef and palm oil (27%), logging to produce forestry/forest products (26%), short term shifting cultivation (24%), and wildfires (23%).
In addition to affecting greenhouse gas concentrations, land-use changes affect global warming through a variety of other chemical and physical mechanisms. Changing the type of vegetation in a region affects the local temperature, by changing how much of the sunlight gets reflected back into space (albedo), and how much heat is lost by evaporation. For instance, the change from a dark forest to grassland makes the surface lighter, causing it to reflect more sunlight. Deforestation can also contribute to changing temperatures by affecting the release of aerosols and other chemical compounds that influence clouds, and by changing wind patterns. In tropic and temperate areas the net effect is to produce significant warming, while at latitudes closer to the poles a gain of albedo (as forest is replaced by snow cover) leads to an overall cooling effect. Globally, these effects are estimated to have led to a slight cooling, dominated by an increase in surface albedo.
Solar and volcanic activity
Physical climate models are unable to reproduce the rapid warming observed in recent decades when taking into account only variations in solar output and volcanic activity. As the Sun is the Earth's primary energy source, changes in incoming sunlight directly affect the climate system. Solar irradiance has been measured directly by satellites, and indirect measurements are available from the early 1600s. There has been no upward trend in the amount of the Sun's energy reaching the Earth. Further evidence for greenhouse gases being the cause of recent climate change come from measurements showing the warming of the lower atmosphere (the troposphere), coupled with the cooling of the upper atmosphere (the stratosphere). If solar variations were responsible for the observed warming, warming of both the troposphere and the stratosphere would be expected, but that has not been the case.
Explosive volcanic eruptions represent the largest natural forcing over the industrial era. When the eruption is sufficiently strong (with sulfur dioxide reaching the stratosphere) sunlight can be partially blocked for a couple of years, with a temperature signal lasting about twice as long. In the industrial era, volcanic activity has had negligible impacts on global temperature trends. Present-day volcanic CO 2 emissions are equivalent to less than 1% of current anthropogenic CO 2 emissions.
Climate change feedback
 Sea ice reflects 50% to 70% of incoming solar radiation while the dark ocean surface only reflects 6%, so melting sea ice is a self-reinforcing feedback.
The response of the climate system to an initial forcing is modified by feedbacks: increased by self-reinforcing feedbacks and reduced by balancing feedbacks. The main reinforcing feedbacks are the water-vapour feedback, the ice–albedo feedback, and probably the net effect of clouds. The primary balancing feedback to global temperature change is radiative cooling to space as infrared radiation in response to rising surface temperature. In addition to temperature feedbacks, there are feedbacks in the carbon cycle, such as the fertilizing effect of CO
2 on plant growth. Uncertainty over feedbacks is the major reason why different climate models project different magnitudes of warming for a given amount of emissions.
As air gets warmer, it can hold more moisture. After initial warming due to emissions of greenhouse gases, the atmosphere will hold more water. As water vapour is a potent greenhouse gas, this further heats the atmosphere. If cloud cover increases, more sunlight will be reflected back into space, cooling the planet. If clouds become more high and thin, they act as an insulator, reflecting heat from below back downwards and warming the planet. Overall, the net cloud feedback over the industrial era has probably exacerbated temperature rise. The reduction of snow cover and sea ice in the Arctic reduces the albedo of the Earth's surface. More of the Sun's energy is now absorbed in these regions, contributing to amplification of Arctic temperature changes. Arctic amplification is also melting permafrost, which releases methane and CO
2 into the atmosphere.
Around half of human-caused CO
2 emissions have been absorbed by land plants and by the oceans. On land, elevated CO
2 and an extended growing season have stimulated plant growth. Climate change increases droughts and heat waves that inhibit plant growth, which makes it uncertain whether this carbon sink will continue to grow in the future. Soils contain large quantities of carbon and may release some when they heat up. As more CO
2 and heat are absorbed by the ocean, it acidifies, its circulation changes and phytoplankton takes up less carbon, decreasing the rate at which the ocean absorbs atmospheric carbon. Climate change can increase methane emissions from wetlands, marine and freshwater systems, and permafrost.
Future warming and the carbon budget
Average climate model projections for 2081–2100 relative to 1986–2005, under low and high emission scenarios
Future warming depends on the strengths of climate feedbacks and on emissions of greenhouse gases. The former are often estimated using various climate models, developed by multiple scientific institutions. A climate model is a representation of the physical, chemical, and biological processes that affect the climate system. Models include changes in the Earth's orbit, historical changes in the Sun's activity, and volcanic forcing. Computer models attempt to reproduce and predict the circulation of the oceans, the annual cycle of the seasons, and the flows of carbon between the land surface and the atmosphere. Models project different future temperature rises for given emissions of greenhouse gases; they also do not fully agree on the strength of different feedbacks on climate sensitivity and magnitude of inertia of the climate system.
The physical realism of models is tested by examining their ability to simulate contemporary or past climates. Past models have underestimated the rate of Arctic shrinkage and underestimated the rate of precipitation increase. Sea level rise since 1990 was underestimated in older models, but more recent models agree well with observations. The 2017 United States-published National Climate Assessment notes that ""climate models may still be underestimating or missing relevant feedback processes"".
Various Representative Concentration Pathways (RCPs) can be used as input for climate models: ""a stringent mitigation scenario (RCP2.6), two intermediate scenarios (RCP4.5 and RCP6.0) and one scenario with very high  emissions (RCP8.5)"". RCPs only look at concentrations of greenhouse gases, and so do not include the response of the carbon cycle. Climate model projections summarised in the IPCC Fifth Assessment Report indicate that, during the 21st century, the global surface temperature is likely to rise a further 0.3 to 1.7 °C (0.5 to 3.1 °F) in a moderate scenario, or as much as 2.6 to 4.8 °C (4.7 to 8.6 °F) in an extreme scenario, depending on the rate of future greenhouse gas emissions and on climate feedback effects.
CO
2 and other gases' CO
2 -equivalents Four possible future concentration pathways, includingand other gases'-equivalents
A subset of climate models add societal factors to a simple physical climate model. These models simulate how population, economic growth, and energy use affect – and interact with – the physical climate. With this information, these models can produce scenarios of how greenhouse gas emissions may vary in the future. This output is then used as input for physical climate models to generate climate change projections. In some scenarios emissions continue to rise over the century, while others have reduced emissions. Fossil fuel resources are too abundant for shortages to be relied on to limit carbon emissions in the 21st century. Emissions scenarios can be combined with modelling of the carbon cycle to predict how atmospheric concentrations of greenhouse gases might change in the future. According to these combined models, by 2100 the atmospheric concentration of CO 2 could be as low as 380 or as high as 1400 ppm, depending on the socioeconomic scenario and the mitigation scenario.
The remaining carbon emissions budget is determined by modelling the carbon cycle and the climate sensitivity to greenhouse gases. According to the IPCC, global warming can be kept below 1.5 °C (2.7 °F) with a two-thirds chance if emissions after 2018 do not exceed 420 or 570 gigatonnes of CO
2, depending on exactly how the global temperature is defined. This amount corresponds to 10 to 13 years of current emissions. There are high uncertainties about the budget; for instance, it may be 100 gigatonnes of CO
2 smaller due to methane release from permafrost and wetlands.
Impacts
Physical environment
 Historical sea level reconstruction and projections up to 2100 published in 2017 by the U.S. Global Change Research Program
The environmental effects of climate change are broad and far-reaching, affecting oceans, ice, and weather. Changes may occur gradually or rapidly. Evidence for these effects comes from studying climate change in the past, from modelling, and from modern observations. Since the 1950s, droughts and heat waves have appeared simultaneously with increasing frequency. Extremely wet or dry events within the monsoon period have increased in India and East Asia. The maximum rainfall and wind speed from hurricanes and typhoons is likely increasing.
Global sea level is rising as a consequence of glacial melt, melt of the ice sheets in Greenland and Antarctica, and thermal expansion. Between 1993 and 2017, the rise increased over time, averaging 3.1 ± 0.3 mm per year. Over the 21st century, the IPCC projects that in a very high emissions scenario the sea level could rise by 61–110 cm. Increased ocean warmth is undermining and threatening to unplug Antarctic glacier outlets, risking a large melt of the ice sheet and the possibility of a 2-meter sea level rise by 2100 under high emissions.
Climate change has led to decades of shrinking and thinning of the Arctic sea ice, making it vulnerable to atmospheric anomalies. While ice-free summers are expected to be rare at 1.5 °C (2.7 °F) degrees of warming, they are set to occur once every three to ten years at a warming level of 2.0 °C (3.6 °F). Higher atmospheric CO
2 concentrations have led to changes in ocean chemistry. An increase in dissolved CO
2 is causing oceans to acidify. In addition, oxygen levels are decreasing as oxygen is less soluble in warmer water, with hypoxic dead zones expanding as a result of algal blooms stimulated by higher temperatures, higher CO
2 levels, ocean deoxygenation, and eutrophication.
Tipping points and long-term impacts
The greater the amount of global warming, the greater the risk of passing through ‘tipping points’, thresholds beyond which certain impacts can no longer be avoided even if temperatures are reduced. An example is the collapse of West Antarctic and Greenland ice sheets, where a temperature rise of 1.5 to 2.0 °C (2.7 to 3.6 °F) may commit the ice sheets to melt, although the time scale of melt is uncertain and depends on future warming. Some large-scale changes could occur over a short time period, such as a collapse of the Atlantic Meridional Overturning Circulation, which would trigger major climate changes in the North Atlantic, Europe, and North America.
The long-term effects of climate change include further ice melt, ocean warming, sea level rise, and ocean acidification. On the timescale of centuries to millennia, the magnitude of climate change will be determined primarily by anthropogenic CO
2 emissions. This is due to CO
2's long atmospheric lifetime. Oceanic CO
2 uptake is slow enough that ocean acidification will continue for hundreds to thousands of years. These emissions are estimated to have prolonged the current interglacial period by at least 100,000 years. Sea level rise will continue over many centuries, with an estimated rise of 2.3 metres per degree Celsius (4.2 ft/°F) after 2000 years.
Nature and wildlife
Recent warming has driven many terrestrial and freshwater species poleward and towards higher altitudes. Higher atmospheric CO
2 levels and an extended growing season have resulted in global greening, whereas heatwaves and drought have reduced ecosystem productivity in some regions. The future balance of these opposing effects is unclear. Climate change has contributed to the expansion of drier climate zones, such as the expansion of deserts in the subtropics. The size and speed of global warming is making abrupt changes in ecosystems more likely. Overall, it is expected that climate change will result in the extinction of many species.
The oceans have heated more slowly than the land, but plants and animals in the ocean have migrated towards the colder poles faster than species on land. Just as on land, heat waves in the ocean occur more frequently due to climate change, with harmful effects found on a wide range of organisms such as corals, kelp, and seabirds. Ocean acidification is impacting organisms who produce shells and skeletons, such as mussels and barnacles, and coral reefs; coral reefs have seen extensive bleaching after heat waves. Harmful algae bloom enhanced by climate change and eutrophication cause anoxia, disruption of food webs and massive large-scale mortality of marine life. Coastal ecosystems are under particular stress, with almost half of wetlands having disappeared as a consequence of climate change and other human impacts.
Humans
The effects of climate change on humans, mostly due to warming and shifts in precipitation, have been detected worldwide. Regional impacts of climate change are now observable on all continents and across ocean regions, with low-latitude, less developed areas facing the greatest risk. Continued emission of greenhouse gases will lead to further warming and long-lasting changes in the climate system, with potentially “severe, pervasive and irreversible impacts” for both people and ecosystems. Climate change risks are unevenly distributed, but are generally greater for disadvantaged people in developing and developed countries.
Food and health
Health impacts include both the direct effects of extreme weather, leading to injury and loss of life, as well as indirect effects, such as undernutrition brought on by crop failures. Various infectious diseases are more easily transmitted in a warmer climate, such as dengue fever, which affects children most severely, and malaria. Young children are the most vulnerable to food shortages, and together with older people, to extreme heat. The World Health Organization (WHO) has estimated that between 2030 and 2050, climate change is expected to cause approximately 250,000 additional deaths per year from heat exposure in elderly people, increases in diarrheal disease, malaria, dengue, coastal flooding, and childhood undernutrition. Over 500,000 additional adult deaths are projected yearly by 2050 due to reductions in food availability and quality. Other major health risks associated with climate change include air and water quality. The WHO has classified human impacts from climate change as the greatest threat to global health in the 21st century.
Climate change is affecting food security and has caused reduction in global mean yields of maize, wheat, and soybeans between 1981 and 2010. Future warming could further reduce global yields of major crops. Crop production will probably be negatively affected in low-latitude countries, while effects at northern latitudes may be positive or negative. Up to an additional 183 million people worldwide, particularly those with lower incomes, are at risk of hunger as a consequence of these impacts. The effects of warming on the oceans impact fish stocks, with a global decline in the maximum catch potential. Only polar stocks are showing an increased potential. Regions dependent on glacier water, regions that are already dry, and small islands are at increased risk of water stress due to climate change.
Livelihoods
Economic damages due to climate change have been underestimated, and may be severe, with the probability of disastrous tail-risk events being nontrivial. Climate change has likely already increased global economic inequality, and is projected to continue doing so. Most of the severe impacts are expected in sub-Saharan Africa and South-East Asia, where existing poverty is already exacerbated. The World Bank estimates that climate change could drive over 120 million people into poverty by 2030. Current inequalities between men and women, between rich and poor, and between different ethnicities have been observed to worsen as a consequence of climate variability and climate change. An expert elicitation concluded that the role of climate change in armed conflict has been small compared to factors such as socio-economic inequality and state capabilities, but that future warming will bring increasing risks.
Low-lying islands and coastal communities are threatened through hazards posed by sea level rise, such as flooding and permanent submergence. This could lead to statelessness for populations in island nations, such as the Maldives and Tuvalu. In some regions, rise in temperature and humidity may be too severe for humans to adapt to. With worst-case climate change, models project that almost one-third of humanity might live in extremely hot and uninhabitable climates, similar to the current climate found mainly in the Sahara. These factors, plus weather extremes, can drive environmental migration, both within and between countries. Displacement of people is expected to increase as a consequence of more frequent extreme weather, sea level rise, and conflict arising from increased competition over natural resources. Climate change may also increase vulnerabilities, leading to ""trapped populations"" in some areas who are not able to move due to a lack of resources.
Responses: mitigation and adaptation
Mitigation
Scenarios of global greenhouse gas emissions. If all countries achieve their current Paris Agreement pledges, average warming by 2100 would still significantly exceed the maximum 2°C target set by the Agreement.
Climate change impacts can be mitigated by reducing greenhouse gas emissions and by enhancing sinks that absorb greenhouse gases from the atmosphere. In order to limit global warming to less than 1.5 °C with a high likelihood of success, global greenhouse gas emissions needs to be net-zero by 2050, or by 2070 with a 2 °C target. This requires far-reaching, systemic changes on an unprecedented scale in energy, land, cities, transport, buildings, and industry. Scenarios that limit global warming to 1.5 °C often describe reaching net negative emissions at some point. To make progress towards a goal of limiting warming to 2 °C, the United Nations Environment Programme estimates that, within the next decade, countries need to triple the amount of reductions they have committed to in their current Paris Agreements; an even greater level of reduction is required to meet the 1.5 °C goal.
Although there is no single pathway to limit global warming to 1.5 or 2.0 °C (2.7 or 3.6 °F), most scenarios and strategies see a major increase in the use of renewable energy in combination with increased energy efficiency measures to generate the needed greenhouse gas reductions. To reduce pressures on ecosystems and enhance their carbon sequestration capabilities, changes would also be necessary in sectors such as forestry and agriculture.
Other approaches to mitigating climate change entail a higher level of risk. Scenarios that limit global warming to 1.5 °C typically project the large-scale use of carbon dioxide removal methods over the 21st century. There are concerns, though, about over-reliance on these technologies, as well as possible environmental impacts. Solar radiation management (SRM) methods have also been explored as a possible supplement to deep reductions in emissions. However, SRM would raise significant ethical and legal issues, and the risks are poorly understood.
Clean energy
 Coal, oil, and natural gas remain the primary global energy sources even as renewables have begun rapidly increasing.
Economic sectors with more greenhouse gas contributions have a greater stake in climate change policies.
Long-term decarbonisation scenarios point to rapid and significant investment in renewable energy, which includes solar and wind power, bioenergy, geothermal energy, and hydropower. Fossil fuels accounted for 80% of the world's energy in 2018, while the remaining share was split between nuclear power and renewables; that mix is projected to change significantly over the next 30 years. Solar and wind have seen substantial growth and progress over the last few years; photovoltaic solar and onshore wind are the cheapest forms of adding new power generation capacity in most countries. Renewables represented 75% of all new electricity generation installed in 2019, with solar and wind constituting nearly all of that amount. Meanwhile, nuclear power costs are increasing amidst stagnant power share, so that nuclear power generation is now several times more expensive per megawatt-hour than wind and solar.
To achieve carbon neutrality by 2050, renewable energy would become the dominant form of electricity generation, rising to 85% or more by 2050 in some scenarios. The use of electricity for other needs, such as heating, would rise to the point where electricity becomes the largest form of overall energy supply. Investment in coal would be eliminated and coal use nearly phased out by 2050.
There are obstacles to the continued rapid development of renewables. For solar and wind power, a key challenge is their intermittency and seasonal variability. Traditionally, hydro dams with reservoirs and conventional power plants have been used when variable energy production is low. Intermittency can further be countered by demand flexibility, and by expanding battery storage and long-distance transmission to smooth variability of renewable output across wider geographic areas. Some environmental and land use concerns have been associated with large solar and wind projects, while bioenergy is often not carbon neutral and may have negative consequences for food security. Hydropower growth has been slowing and is set to decline further due to concerns about social and environmental impacts.
Clean energy improves human health by minimizing climate change and has the near-term benefit of reducing air pollution deaths, which were estimated at 7 million annually in 2016. Meeting the Paris Agreement goals that limit warming to a 2 °C increase could save about a million of those lives per year by 2050, whereas limiting global warming to 1.5 °C could save millions and simultaneously increase energy security and reduce poverty.
Energy efficiency
Reducing energy demand is another major feature of decarbonisation scenarios and plans. In addition to directly reducing emissions, energy demand reduction measures provide more flexibility for low carbon energy development, aid in the management of the electricity grid, and minimise carbon-intensive infrastructure development. Over the next few decades, major increases in energy efficiency investment will be required to achieve these reductions, comparable to the expected level of investment in renewable energy. However, several COVID-19 related changes in energy use patterns, energy efficiency investments, and funding have made forecasts for this decade more difficult and uncertain.
Efficiency strategies to reduce energy demand vary by sector. In transport, gains can be made by switching passengers and freight to more efficient travel modes, such as buses and trains, and increasing the use of electric vehicles. Industrial strategies to reduce energy demand include increasing the energy efficiency of heating systems and motors, designing less energy-intensive products, and increasing product lifetimes. In the building sector the focus is on better design of new buildings, and incorporating higher levels of energy efficiency in retrofitting techniques for existing structures. Buildings would see additional electrification with the use of technologies like heat pumps, which have higher efficiency than fossil fuels.
Agriculture, industry and transport
Agriculture and forestry face a triple challenge of limiting greenhouse gas emissions, preventing the further conversion of forests to agricultural land, and meeting increases in world food demand. A suite of actions could reduce agriculture/forestry-based greenhouse gas emissions by 66% from 2010 levels by reducing growth in demand for food and other agricultural products, increasing land productivity, protecting and restoring forests, and reducing greenhouse gas emissions from agricultural production.
In addition to the industrial demand reduction measures mentioned earlier, steel and cement production, which together are responsible for about 13% of industrial CO
2 emissions, present particular challenges. In these industries, carbon-intensive materials such as coke and lime play an integral role in the production process. Reducing CO
2 emissions here requires research driven efforts aimed at decarbonizing the chemistry of these processes. In transport, scenarios envision sharp increases in the market share of electric vehicles, and low carbon fuel substitution for other transportation modes like shipping.
Carbon sequestration
CO
2 emissions have been absorbed by carbon sinks, including plant growth, soil uptake, and ocean uptake ( Mostemissions have been absorbed by carbon sinks, including plant growth, soil uptake, and ocean uptake ( 2020 Global Carbon Budget ).
Natural carbon sinks can be enhanced to sequester significantly larger amounts of CO
2 beyond naturally occurring levels. Reforestation and tree planting on non-forest lands are among the most mature sequestration techniques, although they raise food security concerns. Soil carbon sequestration and coastal carbon sequestration are less understood options. The feasibility of land-based negative emissions methods for mitigation are uncertain in models; the IPCC has described mitigation strategies based on them as risky.
Where energy production or CO
2-intensive heavy industries continue to produce waste CO
2, the gas can be captured and stored instead of being released to the atmosphere. Although its current use is limited in scale and expensive, carbon capture and storage (CCS) may be able to play a significant role in limiting CO
2 emissions by mid-century. This technique, in combination with bio-energy production (BECCS) can result in net-negative emissions, where the amount of greenhouse gasses that are released into the atmosphere are less than the sequestered, or stored, amount in the bio-energy fuel being grown. It remains highly uncertain whether carbon dioxide removal techniques, such as BECCS, will be able to play a large role in limiting warming to 1.5 °C, and policy decisions based on reliance on carbon dioxide removal increases the risk of global warming increasing beyond international goals.
Adaptation
Adaptation is ""the process of adjustment to current or expected changes in climate and its effects"". Without additional mitigation, adaptation cannot avert the risk of ""severe, widespread and irreversible"" impacts. More severe climate change requires more transformative adaptation, which can be prohibitively expensive. The capacity and potential for humans to adapt, called adaptive capacity, is unevenly distributed across different regions and populations, and developing countries generally have less. The first two decades of the 21st century saw an increase in adaptive capacity in most low- and middle-income countries with improved access to basic sanitation and electricity, but progress is slow. Many countries have implemented adaptation policies. However, there is a considerable gap between necessary and available finance.
Adaptation to sea level rise consists of avoiding at-risk areas, learning to live with increased flooding, protection and, if needed, the more transformative option of managed retreat. There are economic barriers for moderation of dangerous heat impact: avoiding strenuous work or employing private air conditioning is not possible for everybody. In agriculture, adaptation options include a switch to more sustainable diets, diversification, erosion control and genetic improvements for increased tolerance to a changing climate. Insurance allows for risk-sharing, but is often difficult to obtain for people on lower incomes. Education, migration and early warning systems can reduce climate vulnerability.
Ecosystems adapt to climate change, a process that can be supported by human intervention. Possible responses include increasing connectivity between ecosystems, allowing species to migrate to more favourable climate conditions and species relocation. Protection and restoration of natural and semi-natural areas helps build resilience, making it easier for ecosystems to adapt. Many of the actions that promote adaptation in ecosystems, also help humans adapt via ecosystem-based adaptation. For instance, restoration of natural fire regimes makes catastrophic fires less likely, and reduces human exposure. Giving rivers more space allows for more water storage in the natural system, reducing flood risk. Restored forest act as a carbon sink, but planting trees in unsuitable regions can exacerbate climate impacts.
There are some synergies and trade-offs between adaptation and mitigation. Adaptation measures often offer short-term benefits, whereas mitigation has longer-term benefits. Increased use of air conditioning allows people to better cope with heat, but increases energy demand. Compact urban development may lead to reduced emissions from transport and construction. Simultaneously, it may increase the urban heat island effect, leading to higher temperatures and increased exposure. Increased food productivity has large benefits for both adaptation and mitigation.
Policies and politics
High Medium Low Very Low The Climate Change Performance Index ranks countries by greenhouse gas emissions (40% of score), renewable energy (20%), energy use (20%), and climate policy (20%).
Countries that are most vulnerable to climate change have typically been responsible for a small share of global emissions, which raises questions about justice and fairness. Climate change is strongly linked to sustainable development. Limiting global warming makes it easier to achieve sustainable development goals, such as eradicating poverty and reducing inequalities. The connection between the two is recognised in the Sustainable Development Goal 13 which is to ""Take urgent action to combat climate change and its impacts"". The goals on food, clean water and ecosystem protections have synergies with climate mitigation.
The geopolitics of climate change is complex and has often been framed as a free-rider problem, in which all countries benefit from mitigation done by other countries, but individual countries would lose from investing in a transition to a low-carbon economy themselves. This framing has been challenged. For instance, the benefits in terms of public health and local environmental improvements of coal phase-out exceed the costs in almost all regions. Another argument against this framing is that net importers of fossil fuels win economically from transitioning, causing net exporters to face stranded assets: fossil fuels they cannot sell.
Policy options
A wide range of policies, regulations and laws are being used to reduce greenhouse gases. Carbon pricing mechanisms include carbon taxes and emissions trading systems. As of 2019, carbon pricing covers about 20% of global greenhouse gas emissions. Direct global fossil fuel subsidies reached $319 billion in 2017, and $5.2 trillion when indirect costs such as air pollution are priced in. Ending these can cause a 28% reduction in global carbon emissions and a 46% reduction in air pollution deaths. Subsidies could also be redirected to support the transition to clean energy. More prescriptive methods that can reduce greenhouse gases include vehicle efficiency standards, renewable fuel standards, and air pollution regulations on heavy industry. Renewable portfolio standards have been enacted in several countries requiring utilities to increase the percentage of electricity they generate from renewable sources.
As the use of fossil fuels is reduced, there are Just Transition considerations involving the social and economic challenges that arise. An example is the employment of workers in the affected industries, along with the well-being of the broader communities involved. Climate justice considerations, such as those facing indigenous populations in the Arctic, are another important aspect of mitigation policies.
International climate agreements
CO
2 emissions in China and the rest of world have surpassed the output of the United States and Europe. Since 2000, risingemissions in China and the rest of world have surpassed the output of the United States and Europe.
CO
2 at a far faster rate than other primary regions. Per person, the United States generatesat a far faster rate than other primary regions.
Nearly all countries in the world are parties to the 1994 United Nations Framework Convention on Climate Change (UNFCCC). The objective of the UNFCCC is to prevent dangerous human interference with the climate system. As stated in the convention, this requires that greenhouse gas concentrations are stabilised in the atmosphere at a level where ecosystems can adapt naturally to climate change, food production is not threatened, and economic development can be sustained. Global emissions have risen since signing of the UNFCCC, which does not actually restrict emissions but rather provides a framework for protocols that do. Its yearly conferences are the stage of global negotiations.
The 1997 Kyoto Protocol extended the UNFCCC and included legally binding commitments for most developed countries to limit their emissions, During Kyoto Protocol negotiations, the G77 (representing developing countries) pushed for a mandate requiring developed countries to "" the lead"" in reducing their emissions, since developed countries contributed most to the accumulation of greenhouse gases in the atmosphere, and since per-capita emissions were still relatively low in developing countries and emissions of developing countries would grow to meet their development needs.
The 2009 Copenhagen Accord has been widely portrayed as disappointing because of its low goals, and was rejected by poorer nations including the G77. Associated parties aimed to limit the increase in global mean temperature to below 2.0 °C (3.6 °F). The Accord set the goal of sending $100 billion per year to developing countries in assistance for mitigation and adaptation by 2020, and proposed the founding of the Green Climate Fund. As of 2020 , the fund has failed to reach its expected target, and risks a shrinkage in its funding.
In 2015 all UN countries negotiated the Paris Agreement, which aims to keep global warming well below 1.5 °C (2.7 °F) and contains an aspirational goal of keeping warming under 1.5 °C. The agreement replaced the Kyoto Protocol. Unlike Kyoto, no binding emission targets were set in the Paris Agreement. Instead, the procedure of regularly setting ever more ambitious goals and reevaluating these goals every five years has been made binding. The Paris Agreement reiterated that developing countries must be financially supported. As of February 2021 , 194 states and the European Union have signed the treaty and 188 states and the EU have ratified or acceded to the agreement.
The 1987 Montreal Protocol, an international agreement to stop emitting ozone-depleting gases, may have been more effective at curbing greenhouse gas emissions than the Kyoto Protocol specifically designed to do so. The 2016 Kigali Amendment to the Montreal Protocol aims to reduce the emissions of hydrofluorocarbons, a group of powerful greenhouse gases which served as a replacement for banned ozone-depleting gases. This strengthened the makes the Montreal Protocol a stronger agreement against climate change.
National responses
In 2019, the United Kingdom parliament became the first national government in the world to officially declare a climate emergency. Other countries and jurisdictions followed suit. In November 2019 the European Parliament declared a ""climate and environmental emergency"", and the European Commission presented its European Green Deal with the goal of making the EU carbon-neutral by 2050. Major countries in Asia have made similar pledges: South Korea and Japan have committed to become carbon neutral by 2050, and China by 2060.
As of 2021, based on information from 48 NDCs which represent 40% of the parties to the Paris Agreement, estimated total greenhouse gas emissions will be 0.5% lower compared to 2010 levels, below the 45% or 25% reduction goals to limit global warming to 1.5 °C or 2 °C, respectively.
Scientific consensus and society
Scientific consensus
There is an overwhelming scientific consensus that global surface temperatures have increased in recent decades and that the trend is caused mainly by human-induced emissions of greenhouse gases, with 90–100% (depending on the exact question, timing and sampling methodology) of publishing climate scientists agreeing. The consensus has grown to 100% among research scientists on anthropogenic global warming as of 2019. No scientific body of national or international standing disagrees with this view. Consensus has further developed that some form of action should be taken to protect people against the impacts of climate change, and national science academies have called on world leaders to cut global emissions.
Scientific discussion takes place in journal articles that are peer-reviewed, which scientists subject to assessment every couple of years in the Intergovernmental Panel on Climate Change reports. In 2013, the IPCC Fifth Assessment Report stated that ""it is extremely likely that human influence has been the dominant cause of the observed warming since the mid-20th century"". Their 2018 report expressed the scientific consensus as: ""human influence on climate has been the dominant cause of observed warming since the mid-20th century"". Scientists have issued two warnings to humanity, in 2017 and 2019, expressing concern about the current trajectory of potentially catastrophic climate change, and about untold human suffering as a consequence.
The public
Climate change came to international public attention in the late 1980s. Due to confusing media coverage in the early 1990s, understanding was often confounded by conflation with other environmental issues like ozone depletion. In popular culture, the first movie to reach a mass public on the topic was The Day After Tomorrow in 2004, followed a few years later by the Al Gore documentary An Inconvenient Truth. Books, stories and films about climate change fall under the genre of climate fiction.
Significant regional differences exist in both public concern for and public understanding of climate change. In 2015, a median of 54% of respondents considered it ""a very serious problem"", but Americans and Chinese (whose economies are responsible for the greatest annual CO 2 emissions) were among the least concerned. A 2018 survey found increased concern globally on the issue compared to 2013 in most countries. More highly educated people, and in some countries, women and younger people were more likely to see climate change as a serious threat. In the United States, there was a large partisan gap in opinion.
Denial and misinformation
One deceptive approach is cherry picking data from short time periods to falsely assert that global average temperatures are not rising. Blue trendlines show short-term countertrends that mask longer-term warming trends (red trendlines). Blue dots show the so-called global warming hiatus
Public debate about climate change has been strongly affected by climate change denial and misinformation, which originated in the United States and has since spread to other countries, particularly Canada and Australia. The actors behind climate change denial form a well-funded and relatively coordinated coalition of fossil fuel companies, industry groups, conservative think tanks, and contrarian scientists. Like the tobacco industry before, the main strategy of these groups has been to manufacture doubt about scientific data and results. Many who deny, dismiss, or hold unwarranted doubt about the scientific consensus on anthropogenic climate change are labelled as ""climate change skeptics"", which several scientists have noted is a misnomer.
There are different variants of climate denial: some deny that warming takes place at all, some acknowledge warming but attribute it to natural influences, and some minimise the negative impacts of climate change. Manufacturing uncertainty about the science later developed into a manufacturing controversy: creating the belief that there is significant uncertainty about climate change within the scientific community in order to delay policy changes. Strategies to promote these ideas include criticism of scientific institutions, and questioning the motives of individual scientists. An echo chamber of climate-denying blogs and media has further fomented misunderstanding of climate change.
Protest and litigation
Climate protests have risen in popularity in the 2010s in such forms as public demonstrations, fossil fuel divestment, and lawsuits. Prominent recent demonstrations include the school strike for climate, and civil disobedience. In the school strike, youth across the globe have protested by skipping school, inspired by Swedish teenager Greta Thunberg. Mass civil disobedience actions by groups like Extinction Rebellion have protested by causing disruption. Litigation is increasingly used as a tool to strengthen climate action, with many lawsuits targeting governments to demand that they take ambitious action or enforce existing laws regarding climate change. Lawsuits against fossil-fuel companies, from activists, shareholders and investors, generally seek compensation for loss and damage.
Discovery
For broader coverage of this topic, see History of climate change science
Tyndall's ratio spectrophotometer (drawing from 1861) measured how much infrared radiation was absorbed and emitted by various gases filling its central tube.
To explain why Earth's temperature was higher than expected considering only incoming solar radiation, Joseph Fourier proposed the existence of a greenhouse effect. Solar energy reaches the surface as the atmosphere is transparent to solar radiation. The warmed surface emits infrared radiation, but the atmosphere is relatively opaque to infrared and slows the emission of energy, warming the planet. Starting in 1859, John Tyndall established that nitrogen and oxygen (99% of dry air) are transparent to infrared, but water vapour and traces of some gases (significantly methane and carbon dioxide) both absorb infrared and, when warmed, emit infrared radiation. Changing concentrations of these gases could have caused ""all the mutations of climate which the researches of geologists reveal"" including ice ages.
Svante Arrhenius noted that water vapour in air continuously varied, but carbon dioxide (CO
2) was determined by long term geological processes. At the end of an ice age, warming from increased CO
2 would increase the amount of water vapour, amplifying its effect in a feedback process. In 1896, he published the first climate model of its kind, showing that halving of CO
2 could have produced the drop in temperature initiating the ice age. Arrhenius calculated the temperature increase expected from doubling CO
2 to be around 5–6 °C (9.0–10.8 °F). Other scientists were initially sceptical and believed the greenhouse effect to be saturated so that adding more CO
2 would make no difference. They thought climate would be self-regulating. From 1938 Guy Stewart Callendar published evidence that climate was warming and CO
2 levels increasing, but his calculations met the same objections.
In the 1950s, Gilbert Plass created a detailed computer model that included different atmospheric layers and the infrared spectrum and found that increasing CO
2 levels would cause warming. In the same decade Hans Suess found evidence CO
2 levels had been rising, Roger Revelle showed the oceans would not absorb the increase, and together they helped Charles Keeling to begin a record of continued increase, the Keeling Curve. Scientists alerted the public, and the dangers were highlighted at James Hansen's 1988 Congressional testimony. The Intergovernmental Panel on Climate Change, set up in 1988 to provide formal advice to the world's governments, spurred interdisciplinary research."	61788
environment	[]		"Total set of greenhouse gas emissions caused by an individual, event, organisation, or product, expressed as carbon dioxide equivalent

The carbon footprint explained

A carbon footprint is the total greenhouse gas (GHG) emissions caused by an individual, event, organization, service, place or product, expressed as carbon dioxide equivalent.[1] Greenhouse gases, including the carbon-containing gases carbon dioxide and methane, can be emitted through the burning of fossil fuels, land clearance and the production and consumption of food, manufactured goods, materials, wood, roads, buildings, transportation and other services.[2] The term was popularized by a $250 million advertising campaign by the oil and gas company BP in an attempt to move public attention away from restricting the activities of fossil fuel companies and onto individual responsibility for solving climate change.[3]

In most cases, the total carbon footprint cannot be calculated exactly because of inadequate knowledge of and data about the complex interactions between contributing processes, including the influence of natural processes that store or release carbon dioxide. For this reason, Wright, Kemp, and Williams proposed the following definition of a carbon footprint:

A measure of the total amount of carbon dioxide (CO 2 ) and methane (CH 4 ) emissions of a defined population, system or activity, considering all relevant sources, sinks and storage within the spatial and temporal boundary of the population, system or activity of interest. Calculated as carbon dioxide equivalent using the relevant 100-year global warming potential (GWP100).[4]

The global average annual carbon footprint per person in 2014 was about 5 tonnes CO 2 eq.[5]

Background [ edit ]

Human activities are one of the main causes of greenhouse gas emissions. These increase the earth's temperature and are emitted from fossil fuel usage in electricity and other byproducts of manufacturing. The major effects of such practices mainly consist of climate changes, such as extreme precipitation and acidification and warming of oceans. Climate change has been occurring since the start of the Industrial Revolution in the 1820s. Due to humans' heavy reliance on fossil fuels, energy usage, and constant deforestation, the amount of greenhouse gas in the atmosphere is increasing, which makes reducing a greenhouse gas footprint harder to achieve. However, there are several ways to reduce one's greenhouse gas footprint, choosing more energy-efficient eating habits, using more energy efficient household appliances, increase usage of fuel efficient cars, and saving electricity.[6]

Greenhouse gases (GHGs) are gases that increase the temperature of the Earth due to their absorption of infrared radiation.[7] Although some emissions are natural, the rate of which they are being produced has increased because of humans. These gases are emitted from fossil fuel usage in electricity, in heat and transportation, as well as being emitted as byproducts of manufacturing. The most common GHGs are carbon dioxide (CO 2 ), methane (CH 4 ), nitrous oxide (N 2 O), and many fluorinated gases.[8] A greenhouse gas footprint is the numerical quantity of these gases that a single entity emits. The calculations can be computed ranging from a single person to the entire world.[9]

Origin of the concept [ edit ]

The concept and name of the carbon footprint derive from the ecological footprint concept,[10] which was developed by William E. Rees and Mathis Wackernagel in the 1990s. While carbon footprints are usually reported in tons of emissions (CO 2 -equivalent) per year, ecological footprints are usually reported in comparison to what the planet can renew. This assesses the number of ""earths"" that would be required if everyone on the planet consumed resources at the same level as the person calculating their ecological footprint. The carbon footprint is one part of the ecological footprint. Carbon footprints are more focused than ecological footprints since they measure merely emissions of gases that cause climate change into the atmosphere.

Carbon footprint is one of a family of footprint indicators,[11] which also include ecological footprints, water footprints and land footprints.

The carbon part was popularized by a large campaign of BP in 2005, designed by Ogilvy .[10] The deceptive PR campaign instructed individuals to calculate their personal footprints and provided ways for people to lower their own impact, while BP itself continued to extract just as much fossil fuels.[12] The use of household carbon footprint calculators was called ""effective propaganda"" as strategic communication to shift responsibility of climate change-causing pollution away from the corporations and institutions that created a society where carbon emissions are unavoidable and onto personal lifestyle choices.[12]

Common Greenhouse Gases Carbon Dioxide (84%) Methane (9%) Nitrous Oxide (5%) Fluorinated Gases (2%)

An individual's, nation's, or organization's carbon footprint can be measured by undertaking a GHG emissions assessment, a life cycle assessment, or other calculative activities denoted as carbon accounting. Once the size of a carbon footprint is known, a strategy can be devised to reduce it, for example, by technological developments, energy efficiency improvements, better process and product management, changed Green Public or Private Procurement (GPP), carbon capture, consumption strategies, carbon offsetting and others.[13]

For calculating personal carbon footprints, several free online carbon footprint calculators exist[14][15] including a few supported by publicly available peer-reviewed data and calculations including the University of California, Berkeley's CoolClimate Network research consortium and CarbonStory.[16][17][18] These websites ask you to answer more or less detailed questions about your diet, transportation choices, home size, shopping and recreational activities, usage of electricity, heating, and heavy appliances such as dryers and refrigerators, and so on. The website then estimates your carbon footprint based on your answers to these questions. A systematic literature review was conducted to objectively determine the best way to calculate individual/household carbon footprints. This review identified 13 calculation principles and subsequently used the same principles to evaluate the 15 most popular online carbon footprint calculators. A recent study's results by Carnegie Mellon's Christopher Weber found that the calculation of carbon footprints for products is often filled with large uncertainties. The variables of owning electronic goods such as the production, shipment, and previous technology used to make that product, can make it difficult to create an accurate carbon footprint. It is important to question, and address the accuracy of Carbon Footprint techniques, especially due to its overwhelming popularity.[19]

Calculating the carbon footprint of industry, product, or service is a complex task. One tool industry uses Life-cycle assessment (LCA), where carbon footprint may be one of many factors taken into consideration when assessing a product or service. The International Organization for Standardization has a standard called ISO 14040:2006 that has the framework for conducting an LCA study.[20] ISO 14060 family of standards provides further sophisticated tools for quantifying, monitoring, reporting and validating or verifying of GHG emissions and removals.[21] Another method is through the Greenhouse Gas Protocol,[22] a set of standards for tracking greenhouse gas emissions (GHG) across scope 1, 2 and 3 emissions within the value chain.[23]

Predicting the carbon footprint of a process is also possible through estimations using the above standards. By using Emission intensities/Carbon intensities and the estimated annual use of fuel, chemical, or other inputs, the carbon footprint can be determined while a process is being planned/designed.

Direct carbon emissions [ edit ]

Direct or 'scope 1' carbon emissions come from sources that are directly from the site that is producing a product or delivering a service.[24][25] An example for industry would be the emissions related to burning a fuel on site. On the individual level, emissions from personal vehicles or gas burning stoves would fall under scope 1.

Indirect carbon emissions [ edit ]

Consumption-based CO₂ emissions per capita, 2017

Indirect carbon emissions are emissions from sources upstream or downstream from the process being studied, also known as scope 2 or scope 3 emissions.[24]

Examples of upstream, indirect carbon emissions may include:[26]

Transportation of materials/fuels

Any energy used outside of the production facility

Wastes produced outside of the production facility

Examples of downstream, indirect carbon emissions may include:[8]

Any end-of-life process or treatments

Product and waste transportation

Emissions associated with selling the product

Scope 2 emissions are the other indirect related to purchased electricity, heat, and/or steam used on site.[25] Scope 3 emissions are all other indirect emissions derived from the activities of an organisation but from sources which they do not own or control.[27]

Reporting [ edit ]

In the US, the EPA has broken down electricity emission factors by state.[28]

In the UK, DEFRA provides emission factors going back to 2002 covering scope 1, 2 and 3.[29] DEFRA no longer provide international emission factors and refer visitors to the IEA who provide free highlights and paid for details covering Scope 1 and 2.[30]

According to The World Bank, the global average carbon footprint in 2014 was 4.97 metric tons CO 2 /cap.[5] The EU average for 2007 was about 13.8 tons CO 2 e/cap, whereas for the U.S., Luxembourg and Australia it was over 25 tons CO 2 e/cap. In 2017, the average for the USA was about 20 metric tons CO 2 e.[a]

Mobility (driving, flying & small amount from public transit), shelter (electricity, heating, construction) and food are the most important consumption categories determining the carbon footprint of a person. In the EU, the carbon footprint of mobility is evenly split between direct emissions (e.g. from driving private cars) and emissions embodied in purchased products related to mobility (air transport service, emissions occurring during the production of cars and during the extraction of fuel).[33]

The carbon footprint of U.S. households is about 5 times greater than the global average. For most U.S. households the single most important action to reduce their carbon footprint is driving less or switching to a more efficient vehicle.[34]

As well as calculating carbon footprints for whole countries, it is also possible to calculate the footprint of regions, cities, and neighbourhoods.[35]

Three studies concluded that hydroelectric, wind, and nuclear power produced the least CO 2 per kilowatt-hour of any other electricity sources. These figures do not allow for emissions due to accidents or terrorism. Wind power and solar power, emit no carbon from the operation, but do leave a footprint during construction phase and maintenance during operation. Hydropower from reservoirs also has large footprints from initial removal of vegetation and ongoing methane (stream detritus decays anaerobically to methane in bottom of reservoir, rather than aerobically to CO 2 if it had stayed in an unrestricted stream).[36]

Electricity generated, which is about half the world's man-made CO 2 output. The CO 2 footprint for heat is equally significant and research shows that using waste heat from power generation in combined heat and power district heating, chp/dh has the lowest carbon footprint,[37] much lower than micro-power or heat pumps.

Coal production has been refined to greatly reduce carbon emissions; since the 1980s, the amount of energy used to produce a ton of steel has decreased by 50%.[38]

This section gives representative figures for the carbon footprint of the fuel burned by different transport types (not including the carbon footprints of the vehicles or related infrastructure themselves). The precise figures vary according to a wide range of factors.

Flight [ edit ]

Some representative figures for CO 2 emissions are provided by LIPASTO's survey of average direct emissions (not accounting for high-altitude radiative effects) of airliners expressed as CO 2 and CO 2 equivalent per passenger kilometre:[39]

Domestic, short distance, less than 463 km (288 mi): 257 g/km CO 2 or 259 g/km (14.7 oz/mile) CO 2 e

or 259 g/km (14.7 oz/mile) CO e Long-distance flights: 113 g/km CO 2 or 114 g/km (6.5 oz/mile) CO 2 e

However, emissions per unit distance travelled is not necessarily the best indicator for the carbon footprint of air travel, because the distances covered are commonly longer than by other modes of travel. It is the total emissions for a trip that matters for a carbon footprint, not merely the rate of emissions. For example, because air travel makes rapid long-distance travel feasible, a holiday destination may be chosen that is much more distant than if another mode of travel were used.[40]

Road [ edit ]

CO 2 emissions per passenger-kilometre (pkm) for all road travel for 2011 in Europe as provided by the European Environment Agency:[41]

109 g/km CO 2 (Figure 2)

For vehicles, average figures for CO 2 emissions per kilometer for road travel for 2013 in Europe, normalized to the NEDC test cycle, are provided by the International Council on Clean Transportation:[42]

Newly registered passenger cars: 127 g CO 2 /km

g CO /km Hybrid-electric vehicles: 92 g CO 2 /km

g CO /km Light commercial vehicles (LCV): 175 g CO 2 /km

Average figures for the United States are provided by the US Environmental Protection Agency,[43] based on the EPA Federal Test Procedure, for the following categories:

Passenger cars: 200 g CO 2 /km (322 g/mi)

g CO /km (322 g/mi) Trucks: 280 g CO 2 /km (450 g/mi)

g CO /km (450 g/mi) Combined: 229 g CO 2 /km (369 g/mi)

Rail [ edit ]

Shipping [ edit ]

Several organizations offer footprint calculators for public and corporate use, and several organizations have calculated carbon footprints of products.[44] The US Environmental Protection Agency has addressed paper, plastic (candy wrappers), glass, cans, computers, carpet and tires. Australia has addressed lumber and other building materials. Academics in Australia, Korea and the US have addressed paved roads. Companies, nonprofits and academics have addressed mailing letters and packages. Carnegie Mellon University has estimated the CO 2 footprints of 46 large sectors of the economy in each of eight countries. Carnegie Mellon, Sweden and the Carbon Trust have addressed foods at home and in restaurants.

The Carbon Trust has worked with UK manufacturers on foods, shirts and detergents, introducing a CO 2 label in March 2007. The label is intended to comply with a new British Publicly Available Specification (i.e. not a standard), PAS 2050,[45] and is being actively piloted by The Carbon Trust and various industrial partners.[46] As of August 2012 The Carbon Trust state they have measured 27,000 certifiable product carbon footprints.[47]

Evaluating the package of some products is key to figuring out the carbon footprint.[48] The key way to determine a carbon footprint is to look at the materials used to make the item. For example, a juice carton is made of an aseptic carton, a beer can is made of aluminum, and some water bottles either made of glass or plastic. The larger the size, the larger the footprint will be.

Food [ edit ]

In a 2014 study by Scarborough et al., the real-life diets of British people were surveyed and their dietary greenhouse gas footprints estimated.[49] Average dietary greenhouse-gas emissions per day (in kilograms of carbon dioxide equivalent) were:

7.19 for high meat-eaters

5.63 for medium meat-eaters

4.67 for low meat-eaters

3.91 for fish-eaters

3.81 for vegetarians

2.89 for vegans

Textiles [ edit ]

The precise carbon footprint of different textiles varies considerably according to a wide range of factors. However, studies of textile production in Europe suggest the following carbon dioxide equivalent emissions footprints per kilo of textile at the point of purchase by a consumer:[50]

Cotton: 8

Nylon: 5.43

PET (e.g. synthetic fleece): 5.55

Wool: 5.48

Accounting for durability and energy required to wash and dry textile products, synthetic fabrics generally have a substantially lower carbon footprint than natural ones.[51]

Materials [ edit ]

The carbon footprint of materials (also known as embodied carbon) varies widely. The carbon footprint of many common materials can be found in the Inventory of Carbon & Energy database,[52] the GREET databases and models,[53] and LCA databases via openLCA Nexus.[54] The carbon footprint of any manufactured product should be verified by a third-party.[55]

Cement [ edit ]

Cement production gives a major contribution to CO 2 emissions.

Causes [ edit ]

Power plant releasing smoke that contains greenhouse gas

Although some production of greenhouse gases is natural, human activity has increased the production substantially. Major industrial sources of greenhouse gasses are power plants, residential buildings, and road transportation, as well as energy industry processes and losses, iron and steel manufacturing, coal mining, and chemical and petrochemical industries.[56] Changes in the environment also contribute the increase in greenhouse gas emission such as, deforestation, forest degradation and land use, livestock, agricultural soils and water, and wastewater. China is the largest contributor of greenhouse gas, causing up 30% of the total emissions. The United States contributes 15%, followed by the EU with 9%, then India with 7%, Russia with 5%, Japan with 4%, and other miscellaneous countries making up the remaining 30%.[57]

Although carbon dioxide (CO 2 ) is the most prevalent gas, it is not the most damaging. Carbon dioxide is essential to life because animals release it during cellular respiration when they breathe and plants use it for photosynthesis. Carbon dioxide is released naturally by decomposition, ocean release and respiration. Humans contribute an increase of carbon dioxide emissions by burning fossil fuels, deforestation, and cement production.[citation needed]

Methane (CH 4 ) is largely released by coal, oil, and natural gas industries. Although methane is not mass-produced like carbon dioxide, it is still very prevalent. Methane is more harmful than carbon dioxide because it traps heat better than CO 2 . Methane is a main component in natural gas. Recently industries as well as consumers have been using natural gas because they believe that it is better for the environment since it contains less CO 2 . However, this is not the case because methane is actually more harmful to the environment.[58]

Nitrous oxide (N 2 O) is released by fuel combustion, most of which comes from coal fired power plants, agricultural and industrial activities.

Fluorinated gases include hydroflucarbons (HFCs), perfluorocarbons (PFCs), sulfur hexafluoride (SF 6 ), and nitrogen trifluoride (NF 3 ). These gases have no natural source and are solely products of human activity. The biggest cause of these sources is the usage of ozone depleting substances; such as refrigerants, aerosol, propellants, foam blowing agents, solvents, and fire retardants.[8]

The production of all of these gases contributes to one's GHG footprint. The more that these gases are produced, the higher the GHG footprint.

Rise in greenhouse gas over time [ edit ]

2) from fossil energy sources, over time for the six top emitting countries and confederations Global annual greenhouse gas emissions (CO) from fossil energy sources, over time for the six top emitting countries and confederations

Since the Industrial Revolution, greenhouse gas emissions have increased immensely. As of 2017, the carbon dioxide (CO 2 ) levels are 142%, of what they were pre-industrial revolution. Methane is up 253% and nitrous oxide is 121% of pre-industrial levels. The energy driven consumption of fossil fuels has made GHG emissions rapidly increase, causing the Earth's temperature to rise. In the past 250 years, human activity such as, burning fossil fuels and cutting down carbon-absorbing forests, have contributed greatly to this increase. In the last 25 years alone, emissions have increased by more than 33%, most of which comes from carbon dioxide, accounting for three-fourths of this increase.[59][60][61]





A July 2017 study published in Environmental Research Letters found that the most significant way individuals could mitigate their own carbon footprint is to have one less child (""an average for developed countries of 58.6 tonnes CO 2 -equivalent (tCO 2 e) emission reductions per year""), followed by living car-free (2.4 tonnes CO 2 -equivalent per year), forgoing air travel (1.6 tonnes CO 2 -equivalent per trans-Atlantic trip) and adopting a plant-based diet (0.8 tonnes CO 2 -equivalent per year).[62][63] The study also found that most government resources on climate change focus on actions that have a relatively modest effect on greenhouse gas emissions, and concludes that ""a US family who chooses to have one fewer child would provide the same level of emissions reductions as 684 teenagers who choose to adopt comprehensive recycling for the rest of their lives"".[63]

An option is to drive less. Walking, biking, carpooling, mass transportation and combining trips result in burning less fuel and releasing fewer emissions into the atmosphere.

The choice of diet is a major influence on a person's carbon footprint. Animal sources of protein (especially red meat), rice (typically produced in high methane-emitting paddies), foods transported long-distance or via fuel-inefficient transport (e.g., highly perishable produce flown long-distance) and heavily processed and packaged foods are among the major contributors to a high carbon diet. Scientists at the University of Chicago have estimated[64] ""that the average American diet – which derives 28% of its calories from animal foods – is responsible for approximately one and a half more tonnes of greenhouse gasses – as CO

2 equivalents – per person, per year than a fully plant-based, or vegan, diet.""[65] Their calculations suggest that even replacing one third of the animal protein in the average American's diet with plant protein (e.g., beans, grains) can reduce the diet's carbon footprint by half a tonne. Exchanging two-thirds of the animal protein with plant protein is roughly equivalent to switching from a Toyota Camry to a Prius. Finally, throwing food out not only adds its associated carbon emissions to a person or household's footprint, but it also adds the emissions of transporting the wasted food to the garbage dump and the emissions of food decomposition, mostly in the form of the highly potent greenhouse gas, methane.

Options to reduce the carbon footprint of humans include Reduce, Reuse, Recycle, Refuse. This can be done by using reusable items such as thermoses for daily coffee or plastic containers for water and other cold beverages rather than disposable ones. If that option isn't available, it is best to properly recycle the disposable items after use.[66][unreliable source?] When one household recycles at least half of their household waste, they can save 1.2 tons of carbon dioxide annually.[citation needed]

Another option for reducing the carbon footprint of humans is to use less air conditioning and heating in the home. By adding insulation to the walls and attic of one's home, and installing weather stripping, or caulking around doors and windows one can lower their heating costs more than 25 percent.[citation needed] Similarly, one can very inexpensively upgrade the ""insulation"" (clothing) worn by residents of the home.[67] For example, it's estimated that wearing a base layer of long underwear with top and bottom, made from a lightweight, super-insulating fabric like microfleece, can conserve as much body heat as a full set of clothing, allowing a person to remain warm with the thermostat lowered by over 5 °C.[67][68] These measures all help because they reduce the amount of energy needed to heat and cool the house. One can also turn down the heat while sleeping at night or away during the day, and keep temperatures moderate at all times. Setting the thermostat just 2 degrees lower in winter and higher in summer could save about 1 ton of carbon dioxide each year.[66][unreliable source?]

The carbon handprint movement emphasizes individual forms of carbon offsetting, like using more public transportation or planting trees in deforested regions, to reduce one's carbon footprint and increase their ""handprint.""[69][70] The Handprint is being used around the world to strengthen action towards the fulfillment of the UN Sustainable Development Goals.[citation needed]

The most powerful industrial climate actions are:[71] refrigerant management (90 billion tonnes of CO 2 e 2017–2050,[72] since refrigerants have thousands of times the warming potential of CO 2 ); land-based wind turbines for electricity (85 billion); reduced food waste (71 billion); and restoring tropical forests by ending use of the land for other purposes (61 billion). They calculate benefits cumulatively to 2050, rather than annually, because industrial actions have long lead times.[73]

A product, service, or company's carbon footprint can be affected by several factors including, but not limited to:

Energy sources

Offsite electricity generation

Materials

These factors can also change with location or industry. However, there are some general steps that can be taken to reduce carbon footprint on a larger scale.

In 2016, the EIA reported that in the US electricity is responsible for roughly 37% of Carbon Dioxide emissions, making it a potential target for reductions.[74] Possibly the cheapest way to do this is through energy efficiency improvements. The ACEEE reported that energy efficiency has the potential to save the US over 800 billion kWh per year, based on 2015 data.[75] Some potential options to increase energy efficiency include, but are not limited to:[76]

Waste heat recovery systems

Insulation for large buildings and combustion chambers

Technology upgrades, ie different light sources, lower consumption machines

Carbon Footprints from energy consumption can be reduced through the development of alternative energy projects, such as solar and wind energy, which are renewable resources.

Reforestation, the restocking of existing forests or woodlands that have previously been depleted, is an example of Carbon Offsetting, the counteracting of carbon dioxide emissions with an equivalent reduction of carbon dioxide in the atmosphere.[77] Carbon offsetting can reduce a companies overall carbon footprint by offering a carbon credit.

A life cycle or supply chain carbon footprint study can provide useful data which will help the business to identify specific and critical areas for improvement. By calculating or predicting a process’ carbon footprint high emissions areas can be identified and steps can be taken to reduce in those areas.

Co2 projection

Schemes to reduce carbon emissions: Kyoto Protocol, carbon offsetting, and certificates [ edit ]

Carbon dioxide emissions into the atmosphere, and the emissions of other GHGs, are often associated with the burning of fossil fuels, like natural gas, crude oil and coal. While this is harmful to the environment, carbon offsets can be purchased in an attempt to make up for these harmful effects.

The Kyoto Protocol defines legally binding targets and timetables for cutting the GHG emissions of industrialized countries that ratified the Kyoto Protocol. Accordingly, from an economic or market perspective, one has to distinguish between a mandatory market and a voluntary market. Typical for both markets is the trade with emission certificates:

Mandatory market mechanisms [ edit ]

To reach the goals defined in the Kyoto Protocol, with the least economical costs, the following flexible mechanisms were introduced for the mandatory market:

The CDM and JI mechanisms requirements for projects which create a supply of emission reduction instruments, while Emissions Trading allows those instruments to be sold on international markets.

Projects which are compliant with the requirements of the CDM mechanism generate Certified Emissions Reductions (CERs).

Projects which are compliant with the requirements of the JI mechanism generate Emission Reduction Units (ERUs).

The CERs and ERUs can then be sold through Emissions Trading. The demand for the CERs and ERUs being traded is driven by:

Shortfalls in national emission reduction obligations under the Kyoto Protocol.

Shortfalls amongst entities obligated under local emissions reduction schemes.

Nations which have failed to deliver their Kyoto emissions reductions obligations can enter Emissions Trading to purchase CERs and ERUs to cover their treaty shortfalls. Nations and groups of nations can also create local emission reduction schemes which place mandatory carbon dioxide emission targets on entities within their national boundaries. If the rules of a scheme allow, the obligated entities may be able to cover all or some of any reduction shortfalls by purchasing CERs and ERUs through Emissions Trading. While local emissions reduction schemes have no status under the Kyoto Protocol itself, they play a prominent role in creating the demand for CERs and ERUs, stimulating Emissions Trading and setting a market price for emissions.

A well-known mandatory local emissions trading scheme is the EU Emissions Trading Scheme (EU ETS).

New changes are being made to the trading schemes. The EU Emissions Trading Scheme is set to make some new changes within the next year. The new changes will target the emissions produced by flight travel in and out of the European Union.[78]

Other nations are scheduled to start participating in Emissions Trading Schemes within the next few years. These nations include China, India and the United States.[78]

Voluntary market mechanisms [ edit ]

In contrast to the strict rules set out for the mandatory market, the voluntary market provides companies with different options to acquire emissions reductions. A solution, comparable with those developed for the mandatory market, has been developed for the voluntary market, the Verified Emission Reductions (VER). This measure has the great advantage that the projects/activities are managed according to the quality standards set out for CDM/JI projects but the certificates provided are not registered by the governments of the host countries or the Executive Board of the UNO. As such, high quality VERs can be acquired at lower costs for the same project quality. However, at present VERs can not be used in mandatory market.

The voluntary market in North America is divided between members of the Chicago Climate Exchange and the Over The Counter (OTC) market. The Chicago Climate Exchange is a voluntary yet legally binding cap-and-trade emission scheme whereby members commit to the capped emission reductions and must purchase allowances from other members or offset excess emissions. The OTC market does not involve a legally binding scheme and a wide array of buyers from the public and private spheres, as well as special events that want to go carbon neutral. Being carbon neutral refers to achieving net zero carbon emissions by balancing a measured amount of carbon released with an equivalent amount sequestered or offset, or buying enough carbon credits to make up the difference.

There are project developers, wholesalers, brokers, and retailers, as well as carbon funds, in the voluntary market. Some businesses and nonprofits in the voluntary market encompass more than just one of the activities listed above. A report by Ecosystem Marketplace shows that carbon offset prices increase as it moves along the supply chain—from project developer to retailer.[79]

While some mandatory emission reduction schemes exclude forest projects, these projects flourish in the voluntary markets. A major criticism concerns the imprecise nature of GHG sequestration quantification methodologies for forestry projects. However, others note the community co-benefits that forestry projects foster. Project types in the voluntary market range from avoided deforestation, afforestation/reforestation, industrial gas sequestration, increased energy efficiency, fuel switching, methane capture from coal plants and livestock, and even renewable energy. Renewable Energy Certificates (RECs) sold on the voluntary market are quite controversial due to additionality concerns.[80] Industrial Gas projects receive criticism because such projects only apply to large industrial plants that already have high fixed costs. Siphoning off industrial gas for sequestration is considered picking the low hanging fruit; which is why credits generated from industrial gas projects are the cheapest in the voluntary market.

The size and activity of the voluntary carbon market are difficult to measure. The most comprehensive report on the voluntary carbon markets to date was released by Ecosystem Marketplace and New Carbon Finance in July 2007.[79]

ÆON of Japan is firstly approved by the Japanese authority to indicate a carbon footprint on three private brand goods in October 2009.

Solutions [ edit ]

How to reduce GHGs [ edit ]

Reduction of carbon dioxide [ edit ]

In order to decrease CO 2 emissions, the reliance of fossil fuels must be lowered. These fuels produce much CO 2 across all forms of their usage. Alternatively, renewable sources are cleaner for the environment. Capturing CO 2 from power plants will also reduce emissions.[8]

Household energy conservation measures include increasing insulation in construction, using fuel-efficient vehicles and ENERGY STAR appliances, and unplugging electrical items when not in use.

Reduction of methane [ edit ]

Reducing methane gas emissions can be accomplished in several ways. Capturing CH 4 emissions from coal mines and landfills, are two ways of reducing these emissions. Manure management and livestock operations is another possible solution. Motor vehicles use fossil fuels, which produces CO2, but fossil fuels also produce CH4 as a byproduct. Thus, better technology for these vehicles to avoid leakage would be very beneficial.[8]

Reduction of nitrous oxide [ edit ]

Nitrous oxide (N 2 O) is often given off as a byproduct in various ways. Nylon production and fossil fuel usage are two ways that N 2 O is given off as a byproduct. Thus, improving technology for nylon production and the gathering of fossil fuels would greatly reduce nitrous oxide emissions. Also, many fertilizers have a nitrogenous base. A decrease in usage of these fertilizers, or changing their components, are more ways to reduce N 2 O emissions.[8]

Reduction of fluorinated gases [ edit ]

Although fluorinated gases are not produced on a massive scale, they have the worst effect on the environment. A reduction of fluorinated gas emissions can be done in many ways. Many industries that emit these gases can capture or recycle them. These same industries can also invest in more advanced technology that will not produce these gases. A reduction of leakage within power grids and motor vehicles will also decrease the emissions of fluorinated gases. There are also many air conditioning systems that emit fluorinated gases, thus an update in technology would decrease these emissions.[8]

Everyday life changes [ edit ]

There are many simple changes that can be made to the everyday lifestyle of a person that would reduce their GHG footprint. Reducing energy consumption within a household can include lowering one's dependence on air conditioning and heating, using CFL light bulbs, choosing ENERGY STAR appliances, recycling, using cold water to wash clothes, and avoiding a dryer. Another adjustment would be to use a motor vehicle that is fuel-efficient as well as reducing reliance on motor vehicles. Motor vehicles produce many GHGs, thus an adjustment to one's usage will greatly affect a GHG footprint.[24]

See also [ edit ]

Notes [ edit ]

References [ edit ]

Association, Press (2014-09-09). ""Greenhouse gas emissions rise at fastest rate for 30 years"". The Guardian. ISSN 0261-3077. Retrieved 2017-11-03.

Climate change 2014. (2015). Retrieved from INTERGOVERNMENTAL PANEL website: http://www.ipcc.ch/pdf/assessment-report/ar5/syr/SYR_AR5_FINAL_full_wcover.pdf

""CO₂ and other Greenhouse Gas Emissions"". Our World in Data. Retrieved 2017-11-03.

Division, US EPA, Office of Air and Radiation, Office of Atmospheric Programs, Climate Change. ""Household Carbon Footprint Calculator"". www3.epa.gov. Retrieved 2017-11-01

EPA, OA, US. ""Climate Change Indicators: Greenhouse Gases | US EPA"". US EPA. Retrieved 2017-11-08

EPA, OA, US. ""Global Greenhouse Gas Emissions Data | US EPA"". US EPA. Retrieved 2017-11-03.

EPA, OA, US. ""Overview of Greenhouse Gases | US EPA"". US EPA. Retrieved 2017-11-01

Holli, Riebeek, (2010-06-03). ""Global Warming : Feature Articles"". earthobservatory.nasa.gov. Retrieved 2017-11-03.

Howarth, Robert W. (2014-06-01). ""A bridge to nowhere: methane emissions and the greenhouse gas footprint of natural gas"". Energy Science & Engineering. 2 (2): 47–60. doi:10.1002/ese3.35. ISSN 2050-0505

Snyder, C. S.; Bruulsema, T. W.; Jensen, T. L.; Fixen, P. E. (2009-10-01). ""Review of greenhouse gas emissions from crop production systems and fertilizer management effects"". Agriculture, Ecosystems & Environment. Reactive nitrogen in agroecosystems: Integration with greenhouse gas interactions. 133 (3): 247–266. doi:10.1016/j.agee.2009.04.021..

""The Carbon Dioxide Greenhouse Effect"". history.aip.org. Retrieved 2017-11-01.

15 sources of greenhouse gases - About us | Allianz"". www.allianz.com. Retrieved 2017-11-03.

Carbon Management at Curlie"	https://en.wikipedia.org/wiki/Carbon_footprint	"Total set of greenhouse gas emissions caused by an individual, event, organisation, or product, expressed as carbon dioxide equivalent
The carbon footprint explained
A carbon footprint is the total greenhouse gas (GHG) emissions caused by an individual, event, organization, service, place or product, expressed as carbon dioxide equivalent. Greenhouse gases, including the carbon-containing gases carbon dioxide and methane, can be emitted through the burning of fossil fuels, land clearance and the production and consumption of food, manufactured goods, materials, wood, roads, buildings, transportation and other services. The term was popularized by a $250 million advertising campaign by the oil and gas company BP in an attempt to move public attention away from restricting the activities of fossil fuel companies and onto individual responsibility for solving climate change.
In most cases, the total carbon footprint cannot be calculated exactly because of inadequate knowledge of and data about the complex interactions between contributing processes, including the influence of natural processes that store or release carbon dioxide. For this reason, Wright, Kemp, and Williams proposed the following definition of a carbon footprint:
A measure of the total amount of carbon dioxide (CO 2 ) and methane (CH 4 ) emissions of a defined population, system or activity, considering all relevant sources, sinks and storage within the spatial and temporal boundary of the population, system or activity of interest. Calculated as carbon dioxide equivalent using the relevant 100-year global warming potential (GWP100).
The global average annual carbon footprint per person in 2014 was about 5 tonnes CO 2 eq.
Background 
Human activities are one of the main causes of greenhouse gas emissions. These increase the earth's temperature and are emitted from fossil fuel usage in electricity and other byproducts of manufacturing. The major effects of such practices mainly consist of climate changes, such as extreme precipitation and acidification and warming of oceans. Climate change has been occurring since the start of the Industrial Revolution in the 1820s. Due to humans' heavy reliance on fossil fuels, energy usage, and constant deforestation, the amount of greenhouse gas in the atmosphere is increasing, which makes reducing a greenhouse gas footprint harder to achieve. However, there are several ways to reduce one's greenhouse gas footprint, choosing more energy-efficient eating habits, using more energy efficient household appliances, increase usage of fuel efficient cars, and saving electricity.
Greenhouse gases (GHGs) are gases that increase the temperature of the Earth due to their absorption of infrared radiation. Although some emissions are natural, the rate of which they are being produced has increased because of humans. These gases are emitted from fossil fuel usage in electricity, in heat and transportation, as well as being emitted as byproducts of manufacturing. The most common GHGs are carbon dioxide (CO 2 ), methane (CH 4 ), nitrous oxide (N 2 O), and many fluorinated gases. A greenhouse gas footprint is the numerical quantity of these gases that a single entity emits. The calculations can be computed ranging from a single person to the entire world.
Origin of the concept 
The concept and name of the carbon footprint derive from the ecological footprint concept, which was developed by William E. Rees and Mathis Wackernagel in the 1990s. While carbon footprints are usually reported in tons of emissions (CO 2 -equivalent) per year, ecological footprints are usually reported in comparison to what the planet can renew. This assesses the number of ""earths"" that would be required if everyone on the planet consumed resources at the same level as the person calculating their ecological footprint. The carbon footprint is one part of the ecological footprint. Carbon footprints are more focused than ecological footprints since they measure merely emissions of gases that cause climate change into the atmosphere.
Carbon footprint is one of a family of footprint indicators, which also include ecological footprints, water footprints and land footprints.
The carbon part was popularized by a large campaign of BP in 2005, designed by Ogilvy . The deceptive PR campaign instructed individuals to calculate their personal footprints and provided ways for people to lower their own impact, while BP itself continued to extract just as much fossil fuels. The use of household carbon footprint calculators was called ""effective propaganda"" as strategic communication to shift responsibility of climate change-causing pollution away from the corporations and institutions that created a society where carbon emissions are unavoidable and onto personal lifestyle choices.
Common Greenhouse Gases Carbon Dioxide (84%) Methane (9%) Nitrous Oxide (5%) Fluorinated Gases (2%)
An individual's, nation's, or organization's carbon footprint can be measured by undertaking a GHG emissions assessment, a life cycle assessment, or other calculative activities denoted as carbon accounting. Once the size of a carbon footprint is known, a strategy can be devised to reduce it, for example, by technological developments, energy efficiency improvements, better process and product management, changed Green Public or Private Procurement (GPP), carbon capture, consumption strategies, carbon offsetting and others.
For calculating personal carbon footprints, several free online carbon footprint calculators exist including a few supported by publicly available peer-reviewed data and calculations including the University of California, Berkeley's CoolClimate Network research consortium and CarbonStory. These websites ask you to answer more or less detailed questions about your diet, transportation choices, home size, shopping and recreational activities, usage of electricity, heating, and heavy appliances such as dryers and refrigerators, and so on. The website then estimates your carbon footprint based on your answers to these questions. A systematic literature review was conducted to objectively determine the best way to calculate individual/household carbon footprints. This review identified 13 calculation principles and subsequently used the same principles to evaluate the 15 most popular online carbon footprint calculators. A recent study's results by Carnegie Mellon's Christopher Weber found that the calculation of carbon footprints for products is often filled with large uncertainties. The variables of owning electronic goods such as the production, shipment, and previous technology used to make that product, can make it difficult to create an accurate carbon footprint. It is important to question, and address the accuracy of Carbon Footprint techniques, especially due to its overwhelming popularity.
Calculating the carbon footprint of industry, product, or service is a complex task. One tool industry uses Life-cycle assessment (LCA), where carbon footprint may be one of many factors taken into consideration when assessing a product or service. The International Organization for Standardization has a standard called ISO 14040:2006 that has the framework for conducting an LCA study. ISO 14060 family of standards provides further sophisticated tools for quantifying, monitoring, reporting and validating or verifying of GHG emissions and removals. Another method is through the Greenhouse Gas Protocol, a set of standards for tracking greenhouse gas emissions (GHG) across scope 1, 2 and 3 emissions within the value chain.
Predicting the carbon footprint of a process is also possible through estimations using the above standards. By using Emission intensities/Carbon intensities and the estimated annual use of fuel, chemical, or other inputs, the carbon footprint can be determined while a process is being planned/designed.
Direct carbon emissions 
Direct or 'scope 1' carbon emissions come from sources that are directly from the site that is producing a product or delivering a service. An example for industry would be the emissions related to burning a fuel on site. On the individual level, emissions from personal vehicles or gas burning stoves would fall under scope 1.
Indirect carbon emissions 
Consumption-based CO₂ emissions per capita, 2017
Indirect carbon emissions are emissions from sources upstream or downstream from the process being studied, also known as scope 2 or scope 3 emissions.
Examples of upstream, indirect carbon emissions may include:
Transportation of materials/fuels
Any energy used outside of the production facility
Wastes produced outside of the production facility
Examples of downstream, indirect carbon emissions may include:
Any end-of-life process or treatments
Product and waste transportation
Emissions associated with selling the product
Scope 2 emissions are the other indirect related to purchased electricity, heat, and/or steam used on site. Scope 3 emissions are all other indirect emissions derived from the activities of an organisation but from sources which they do not own or control.
Reporting 
In the US, the EPA has broken down electricity emission factors by state.
In the UK, DEFRA provides emission factors going back to 2002 covering scope 1, 2 and 3. DEFRA no longer provide international emission factors and refer visitors to the IEA who provide free highlights and paid for details covering Scope 1 and 2.
According to The World Bank, the global average carbon footprint in 2014 was 4.97 metric tons CO 2 /cap. The EU average for 2007 was about 13.8 tons CO 2 e/cap, whereas for the U.S., Luxembourg and Australia it was over 25 tons CO 2 e/cap. In 2017, the average for the USA was about 20 metric tons CO 2 e.
Mobility (driving, flying & small amount from public transit), shelter (electricity, heating, construction) and food are the most important consumption categories determining the carbon footprint of a person. In the EU, the carbon footprint of mobility is evenly split between direct emissions (e.g. from driving private cars) and emissions embodied in purchased products related to mobility (air transport service, emissions occurring during the production of cars and during the extraction of fuel).
The carbon footprint of U.S. households is about 5 times greater than the global average. For most U.S. households the single most important action to reduce their carbon footprint is driving less or switching to a more efficient vehicle.
As well as calculating carbon footprints for whole countries, it is also possible to calculate the footprint of regions, cities, and neighbourhoods.
Three studies concluded that hydroelectric, wind, and nuclear power produced the least CO 2 per kilowatt-hour of any other electricity sources. These figures do not allow for emissions due to accidents or terrorism. Wind power and solar power, emit no carbon from the operation, but do leave a footprint during construction phase and maintenance during operation. Hydropower from reservoirs also has large footprints from initial removal of vegetation and ongoing methane (stream detritus decays anaerobically to methane in bottom of reservoir, rather than aerobically to CO 2 if it had stayed in an unrestricted stream).
Electricity generated, which is about half the world's man-made CO 2 output. The CO 2 footprint for heat is equally significant and research shows that using waste heat from power generation in combined heat and power district heating, chp/dh has the lowest carbon footprint, much lower than micro-power or heat pumps.
Coal production has been refined to greatly reduce carbon emissions; since the 1980s, the amount of energy used to produce a ton of steel has decreased by 50%.
This section gives representative figures for the carbon footprint of the fuel burned by different transport types (not including the carbon footprints of the vehicles or related infrastructure themselves). The precise figures vary according to a wide range of factors.
Flight 
Some representative figures for CO 2 emissions are provided by LIPASTO's survey of average direct emissions (not accounting for high-altitude radiative effects) of airliners expressed as CO 2 and CO 2 equivalent per passenger kilometre:
Domestic, short distance, less than 463 km (288 mi): 257 g/km CO 2 or 259 g/km (14.7 oz/mile) CO 2 e
or 259 g/km (14.7 oz/mile) CO e Long-distance flights: 113 g/km CO 2 or 114 g/km (6.5 oz/mile) CO 2 e
However, emissions per unit distance travelled is not necessarily the best indicator for the carbon footprint of air travel, because the distances covered are commonly longer than by other modes of travel. It is the total emissions for a trip that matters for a carbon footprint, not merely the rate of emissions. For example, because air travel makes rapid long-distance travel feasible, a holiday destination may be chosen that is much more distant than if another mode of travel were used.
Road 
CO 2 emissions per passenger-kilometre (pkm) for all road travel for 2011 in Europe as provided by the European Environment Agency:
109 g/km CO 2 (Figure 2)
For vehicles, average figures for CO 2 emissions per kilometer for road travel for 2013 in Europe, normalized to the NEDC test cycle, are provided by the International Council on Clean Transportation:
Newly registered passenger cars: 127 g CO 2 /km
g CO /km Hybrid-electric vehicles: 92 g CO 2 /km
g CO /km Light commercial vehicles (LCV): 175 g CO 2 /km
Average figures for the United States are provided by the US Environmental Protection Agency, based on the EPA Federal Test Procedure, for the following categories:
Passenger cars: 200 g CO 2 /km (322 g/mi)
g CO /km (322 g/mi) Trucks: 280 g CO 2 /km (450 g/mi)
g CO /km (450 g/mi) Combined: 229 g CO 2 /km (369 g/mi)
Rail 
Shipping 
Several organizations offer footprint calculators for public and corporate use, and several organizations have calculated carbon footprints of products. The US Environmental Protection Agency has addressed paper, plastic (candy wrappers), glass, cans, computers, carpet and tires. Australia has addressed lumber and other building materials. Academics in Australia, Korea and the US have addressed paved roads. Companies, nonprofits and academics have addressed mailing letters and packages. Carnegie Mellon University has estimated the CO 2 footprints of 46 large sectors of the economy in each of eight countries. Carnegie Mellon, Sweden and the Carbon Trust have addressed foods at home and in restaurants.
The Carbon Trust has worked with UK manufacturers on foods, shirts and detergents, introducing a CO 2 label in March 2007. The label is intended to comply with a new British Publicly Available Specification (i.e. not a standard), PAS 2050, and is being actively piloted by The Carbon Trust and various industrial partners. As of August 2012 The Carbon Trust state they have measured 27,000 certifiable product carbon footprints.
Evaluating the package of some products is key to figuring out the carbon footprint. The key way to determine a carbon footprint is to look at the materials used to make the item. For example, a juice carton is made of an aseptic carton, a beer can is made of aluminum, and some water bottles either made of glass or plastic. The larger the size, the larger the footprint will be.
Food 
In a 2014 study by Scarborough et al., the real-life diets of British people were surveyed and their dietary greenhouse gas footprints estimated. Average dietary greenhouse-gas emissions per day (in kilograms of carbon dioxide equivalent) were:
7.19 for high meat-eaters
5.63 for medium meat-eaters
4.67 for low meat-eaters
3.91 for fish-eaters
3.81 for vegetarians
2.89 for vegans
Textiles 
The precise carbon footprint of different textiles varies considerably according to a wide range of factors. However, studies of textile production in Europe suggest the following carbon dioxide equivalent emissions footprints per kilo of textile at the point of purchase by a consumer:
Cotton: 8
Nylon: 5.43
PET (e.g. synthetic fleece): 5.55
Wool: 5.48
Accounting for durability and energy required to wash and dry textile products, synthetic fabrics generally have a substantially lower carbon footprint than natural ones.
Materials 
The carbon footprint of materials (also known as embodied carbon) varies widely. The carbon footprint of many common materials can be found in the Inventory of Carbon & Energy database, the GREET databases and models, and LCA databases via openLCA Nexus. The carbon footprint of any manufactured product should be verified by a third-party.
Cement 
Cement production gives a major contribution to CO 2 emissions.
Causes 
Power plant releasing smoke that contains greenhouse gas
Although some production of greenhouse gases is natural, human activity has increased the production substantially. Major industrial sources of greenhouse gasses are power plants, residential buildings, and road transportation, as well as energy industry processes and losses, iron and steel manufacturing, coal mining, and chemical and petrochemical industries. Changes in the environment also contribute the increase in greenhouse gas emission such as, deforestation, forest degradation and land use, livestock, agricultural soils and water, and wastewater. China is the largest contributor of greenhouse gas, causing up 30% of the total emissions. The United States contributes 15%, followed by the EU with 9%, then India with 7%, Russia with 5%, Japan with 4%, and other miscellaneous countries making up the remaining 30%.
Although carbon dioxide (CO 2 ) is the most prevalent gas, it is not the most damaging. Carbon dioxide is essential to life because animals release it during cellular respiration when they breathe and plants use it for photosynthesis. Carbon dioxide is released naturally by decomposition, ocean release and respiration. Humans contribute an increase of carbon dioxide emissions by burning fossil fuels, deforestation, and cement production.
Methane (CH 4 ) is largely released by coal, oil, and natural gas industries. Although methane is not mass-produced like carbon dioxide, it is still very prevalent. Methane is more harmful than carbon dioxide because it traps heat better than CO 2 . Methane is a main component in natural gas. Recently industries as well as consumers have been using natural gas because they believe that it is better for the environment since it contains less CO 2 . However, this is not the case because methane is actually more harmful to the environment.
Nitrous oxide (N 2 O) is released by fuel combustion, most of which comes from coal fired power plants, agricultural and industrial activities.
Fluorinated gases include hydroflucarbons (HFCs), perfluorocarbons (PFCs), sulfur hexafluoride (SF 6 ), and nitrogen trifluoride (NF 3 ). These gases have no natural source and are solely products of human activity. The biggest cause of these sources is the usage of ozone depleting substances; such as refrigerants, aerosol, propellants, foam blowing agents, solvents, and fire retardants.
The production of all of these gases contributes to one's GHG footprint. The more that these gases are produced, the higher the GHG footprint.
Rise in greenhouse gas over time 
2) from fossil energy sources, over time for the six top emitting countries and confederations Global annual greenhouse gas emissions (CO) from fossil energy sources, over time for the six top emitting countries and confederations
Since the Industrial Revolution, greenhouse gas emissions have increased immensely. As of 2017, the carbon dioxide (CO 2 ) levels are 142%, of what they were pre-industrial revolution. Methane is up 253% and nitrous oxide is 121% of pre-industrial levels. The energy driven consumption of fossil fuels has made GHG emissions rapidly increase, causing the Earth's temperature to rise. In the past 250 years, human activity such as, burning fossil fuels and cutting down carbon-absorbing forests, have contributed greatly to this increase. In the last 25 years alone, emissions have increased by more than 33%, most of which comes from carbon dioxide, accounting for three-fourths of this increase.
A July 2017 study published in Environmental Research Letters found that the most significant way individuals could mitigate their own carbon footprint is to have one less child (""an average for developed countries of 58.6 tonnes CO 2 -equivalent (tCO 2 e) emission reductions per year""), followed by living car-free (2.4 tonnes CO 2 -equivalent per year), forgoing air travel (1.6 tonnes CO 2 -equivalent per trans-Atlantic trip) and adopting a plant-based diet (0.8 tonnes CO 2 -equivalent per year). The study also found that most government resources on climate change focus on actions that have a relatively modest effect on greenhouse gas emissions, and concludes that ""a US family who chooses to have one fewer child would provide the same level of emissions reductions as 684 teenagers who choose to adopt comprehensive recycling for the rest of their lives"".
An option is to drive less. Walking, biking, carpooling, mass transportation and combining trips result in burning less fuel and releasing fewer emissions into the atmosphere.
The choice of diet is a major influence on a person's carbon footprint. Animal sources of protein (especially red meat), rice (typically produced in high methane-emitting paddies), foods transported long-distance or via fuel-inefficient transport (e.g., highly perishable produce flown long-distance) and heavily processed and packaged foods are among the major contributors to a high carbon diet. Scientists at the University of Chicago have estimated ""that the average American diet – which derives 28% of its calories from animal foods – is responsible for approximately one and a half more tonnes of greenhouse gasses – as CO
2 equivalents – per person, per year than a fully plant-based, or vegan, diet."" Their calculations suggest that even replacing one third of the animal protein in the average American's diet with plant protein (e.g., beans, grains) can reduce the diet's carbon footprint by half a tonne. Exchanging two-thirds of the animal protein with plant protein is roughly equivalent to switching from a Toyota Camry to a Prius. Finally, throwing food out not only adds its associated carbon emissions to a person or household's footprint, but it also adds the emissions of transporting the wasted food to the garbage dump and the emissions of food decomposition, mostly in the form of the highly potent greenhouse gas, methane.
Options to reduce the carbon footprint of humans include Reduce, Reuse, Recycle, Refuse. This can be done by using reusable items such as thermoses for daily coffee or plastic containers for water and other cold beverages rather than disposable ones. If that option isn't available, it is best to properly recycle the disposable items after use. When one household recycles at least half of their household waste, they can save 1.2 tons of carbon dioxide annually.
Another option for reducing the carbon footprint of humans is to use less air conditioning and heating in the home. By adding insulation to the walls and attic of one's home, and installing weather stripping, or caulking around doors and windows one can lower their heating costs more than 25 percent. Similarly, one can very inexpensively upgrade the ""insulation"" (clothing) worn by residents of the home. For example, it's estimated that wearing a base layer of long underwear with top and bottom, made from a lightweight, super-insulating fabric like microfleece, can conserve as much body heat as a full set of clothing, allowing a person to remain warm with the thermostat lowered by over 5 °C. These measures all help because they reduce the amount of energy needed to heat and cool the house. One can also turn down the heat while sleeping at night or away during the day, and keep temperatures moderate at all times. Setting the thermostat just 2 degrees lower in winter and higher in summer could save about 1 ton of carbon dioxide each year.
The carbon handprint movement emphasizes individual forms of carbon offsetting, like using more public transportation or planting trees in deforested regions, to reduce one's carbon footprint and increase their ""handprint."" The Handprint is being used around the world to strengthen action towards the fulfillment of the UN Sustainable Development Goals.
The most powerful industrial climate actions are: refrigerant management (90 billion tonnes of CO 2 e 2017–2050, since refrigerants have thousands of times the warming potential of CO 2 ); land-based wind turbines for electricity (85 billion); reduced food waste (71 billion); and restoring tropical forests by ending use of the land for other purposes (61 billion). They calculate benefits cumulatively to 2050, rather than annually, because industrial actions have long lead times.
A product, service, or company's carbon footprint can be affected by several factors including, but not limited to:
Energy sources
Offsite electricity generation
Materials
These factors can also change with location or industry. However, there are some general steps that can be taken to reduce carbon footprint on a larger scale.
In 2016, the EIA reported that in the US electricity is responsible for roughly 37% of Carbon Dioxide emissions, making it a potential target for reductions. Possibly the cheapest way to do this is through energy efficiency improvements. The ACEEE reported that energy efficiency has the potential to save the US over 800 billion kWh per year, based on 2015 data. Some potential options to increase energy efficiency include, but are not limited to:
Waste heat recovery systems
Insulation for large buildings and combustion chambers
Technology upgrades, ie different light sources, lower consumption machines
Carbon Footprints from energy consumption can be reduced through the development of alternative energy projects, such as solar and wind energy, which are renewable resources.
Reforestation, the restocking of existing forests or woodlands that have previously been depleted, is an example of Carbon Offsetting, the counteracting of carbon dioxide emissions with an equivalent reduction of carbon dioxide in the atmosphere. Carbon offsetting can reduce a companies overall carbon footprint by offering a carbon credit.
A life cycle or supply chain carbon footprint study can provide useful data which will help the business to identify specific and critical areas for improvement. By calculating or predicting a process’ carbon footprint high emissions areas can be identified and steps can be taken to reduce in those areas.
Co2 projection
Schemes to reduce carbon emissions: Kyoto Protocol, carbon offsetting, and certificates 
Carbon dioxide emissions into the atmosphere, and the emissions of other GHGs, are often associated with the burning of fossil fuels, like natural gas, crude oil and coal. While this is harmful to the environment, carbon offsets can be purchased in an attempt to make up for these harmful effects.
The Kyoto Protocol defines legally binding targets and timetables for cutting the GHG emissions of industrialized countries that ratified the Kyoto Protocol. Accordingly, from an economic or market perspective, one has to distinguish between a mandatory market and a voluntary market. Typical for both markets is the trade with emission certificates:
Mandatory market mechanisms 
To reach the goals defined in the Kyoto Protocol, with the least economical costs, the following flexible mechanisms were introduced for the mandatory market:
The CDM and JI mechanisms requirements for projects which create a supply of emission reduction instruments, while Emissions Trading allows those instruments to be sold on international markets.
Projects which are compliant with the requirements of the CDM mechanism generate Certified Emissions Reductions (CERs).
Projects which are compliant with the requirements of the JI mechanism generate Emission Reduction Units (ERUs).
The CERs and ERUs can then be sold through Emissions Trading. The demand for the CERs and ERUs being traded is driven by:
Shortfalls in national emission reduction obligations under the Kyoto Protocol.
Shortfalls amongst entities obligated under local emissions reduction schemes.
Nations which have failed to deliver their Kyoto emissions reductions obligations can enter Emissions Trading to purchase CERs and ERUs to cover their treaty shortfalls. Nations and groups of nations can also create local emission reduction schemes which place mandatory carbon dioxide emission targets on entities within their national boundaries. If the rules of a scheme allow, the obligated entities may be able to cover all or some of any reduction shortfalls by purchasing CERs and ERUs through Emissions Trading. While local emissions reduction schemes have no status under the Kyoto Protocol itself, they play a prominent role in creating the demand for CERs and ERUs, stimulating Emissions Trading and setting a market price for emissions.
A well-known mandatory local emissions trading scheme is the EU Emissions Trading Scheme (EU ETS).
New changes are being made to the trading schemes. The EU Emissions Trading Scheme is set to make some new changes within the next year. The new changes will target the emissions produced by flight travel in and out of the European Union.
Other nations are scheduled to start participating in Emissions Trading Schemes within the next few years. These nations include China, India and the United States.
Voluntary market mechanisms 
In contrast to the strict rules set out for the mandatory market, the voluntary market provides companies with different options to acquire emissions reductions. A solution, comparable with those developed for the mandatory market, has been developed for the voluntary market, the Verified Emission Reductions (VER). This measure has the great advantage that the projects/activities are managed according to the quality standards set out for CDM/JI projects but the certificates provided are not registered by the governments of the host countries or the Executive Board of the UNO. As such, high quality VERs can be acquired at lower costs for the same project quality. However, at present VERs can not be used in mandatory market.
The voluntary market in North America is divided between members of the Chicago Climate Exchange and the Over The Counter (OTC) market. The Chicago Climate Exchange is a voluntary yet legally binding cap-and-trade emission scheme whereby members commit to the capped emission reductions and must purchase allowances from other members or offset excess emissions. The OTC market does not involve a legally binding scheme and a wide array of buyers from the public and private spheres, as well as special events that want to go carbon neutral. Being carbon neutral refers to achieving net zero carbon emissions by balancing a measured amount of carbon released with an equivalent amount sequestered or offset, or buying enough carbon credits to make up the difference.
There are project developers, wholesalers, brokers, and retailers, as well as carbon funds, in the voluntary market. Some businesses and nonprofits in the voluntary market encompass more than just one of the activities listed above. A report by Ecosystem Marketplace shows that carbon offset prices increase as it moves along the supply chain—from project developer to retailer.
While some mandatory emission reduction schemes exclude forest projects, these projects flourish in the voluntary markets. A major criticism concerns the imprecise nature of GHG sequestration quantification methodologies for forestry projects. However, others note the community co-benefits that forestry projects foster. Project types in the voluntary market range from avoided deforestation, afforestation/reforestation, industrial gas sequestration, increased energy efficiency, fuel switching, methane capture from coal plants and livestock, and even renewable energy. Renewable Energy Certificates (RECs) sold on the voluntary market are quite controversial due to additionality concerns. Industrial Gas projects receive criticism because such projects only apply to large industrial plants that already have high fixed costs. Siphoning off industrial gas for sequestration is considered picking the low hanging fruit; which is why credits generated from industrial gas projects are the cheapest in the voluntary market.
The size and activity of the voluntary carbon market are difficult to measure. The most comprehensive report on the voluntary carbon markets to date was released by Ecosystem Marketplace and New Carbon Finance in July 2007.
ÆON of Japan is firstly approved by the Japanese authority to indicate a carbon footprint on three private brand goods in October 2009.
Solutions 
How to reduce GHGs 
Reduction of carbon dioxide 
In order to decrease CO 2 emissions, the reliance of fossil fuels must be lowered. These fuels produce much CO 2 across all forms of their usage. Alternatively, renewable sources are cleaner for the environment. Capturing CO 2 from power plants will also reduce emissions.
Household energy conservation measures include increasing insulation in construction, using fuel-efficient vehicles and ENERGY STAR appliances, and unplugging electrical items when not in use.
Reduction of methane 
Reducing methane gas emissions can be accomplished in several ways. Capturing CH 4 emissions from coal mines and landfills, are two ways of reducing these emissions. Manure management and livestock operations is another possible solution. Motor vehicles use fossil fuels, which produces CO2, but fossil fuels also produce CH4 as a byproduct. Thus, better technology for these vehicles to avoid leakage would be very beneficial.
Reduction of nitrous oxide 
Nitrous oxide (N 2 O) is often given off as a byproduct in various ways. Nylon production and fossil fuel usage are two ways that N 2 O is given off as a byproduct. Thus, improving technology for nylon production and the gathering of fossil fuels would greatly reduce nitrous oxide emissions. Also, many fertilizers have a nitrogenous base. A decrease in usage of these fertilizers, or changing their components, are more ways to reduce N 2 O emissions.
Reduction of fluorinated gases 
Although fluorinated gases are not produced on a massive scale, they have the worst effect on the environment. A reduction of fluorinated gas emissions can be done in many ways. Many industries that emit these gases can capture or recycle them. These same industries can also invest in more advanced technology that will not produce these gases. A reduction of leakage within power grids and motor vehicles will also decrease the emissions of fluorinated gases. There are also many air conditioning systems that emit fluorinated gases, thus an update in technology would decrease these emissions.
Everyday life changes 
There are many simple changes that can be made to the everyday lifestyle of a person that would reduce their GHG footprint. Reducing energy consumption within a household can include lowering one's dependence on air conditioning and heating, using CFL light bulbs, choosing ENERGY STAR appliances, recycling, using cold water to wash clothes, and avoiding a dryer. Another adjustment would be to use a motor vehicle that is fuel-efficient as well as reducing reliance on motor vehicles. Motor vehicles produce many GHGs, thus an adjustment to one's usage will greatly affect a GHG footprint."	35596
environment	[]		"""Waste disposal"" redirects here. For the kitchen device, see Garbage disposal unit

Not to be confused with Sanitary engineering

activities and actions required to manage waste from its source to its final disposal

Containers for selective waste collection at the Gdańsk University of Technology

A recycling and waste-to-energy plant for waste that is not exported

Waste management (or waste disposal) includes the activities and actions required to manage waste from its inception to its final disposal.[1] This includes the collection, transport, treatment and disposal of waste, together with monitoring and regulation of the waste management process and waste-related laws, technologies, economic mechanisms.

Waste can be solid, liquid, or gaseous and each type has different methods of disposal and management. Waste management deals with all types of waste, including industrial, biological and household. In some cases, waste can pose a threat to human health.[2] Health issues are associated throughout the entire process of waste management. Health issues can also arise indirectly or directly. Directly, through the handling of said waste, and indirectly through the consumption of water, soil and food. Waste is produced by [3] human activity, for example, the extraction and processing of raw materials.[4] Waste management is intended to reduce adverse effects of waste on human health, the environment, planetary resources and aesthetics.

Waste management practices are not uniform among countries (developed and developing nations); regions (urban and rural areas), and residential and industrial sectors can all take different approaches.[5]

Proper management of waste is important for building sustainable and liveable cities, but it remains a challenge for many developing countries and cities. A report found that effective waste management is relatively expensive, usually comprising 20%–50% of municipal budgets. Operating this essential municipal service requires integrated systems that are efficient, sustainable, and socially supported.[6] A large portion of waste management practices deal with municipal solid waste (MSW) which is the bulk of the waste that is created by household, industrial, and commercial activity.[7] Measures of waste management include measures for integrated techno-economic mechanisms[8] of a circular economy, effective disposal facilities, export and import control[9][10] and optimal sustainable design of products that are produced.

In the first systematic review of the scientific evidence around global waste, its management and its impact on human health and life, authors concluded that about a fourth of all the municipal solid terrestrial waste is not collected and an additional fourth is mismanaged after collection, often being burned in open and uncontrolled fires – or close to one billion tons per year when combined. They also found that broad priority areas each lack a ""high-quality research base"", partly due to the absence of ""substantial research funding"", which motivated scientists often require.[11][12]

Principles of waste management [ edit ]

Diagram of the waste hierarchy

Waste hierarchy [ edit ]

The waste hierarchy refers to the ""3 Rs"" Reduce, Reuse and Recycle, which classifies waste management strategies according to their desirability in terms of waste minimisation. The waste hierarchy is the cornerstone of most waste minimization strategies. The aim of the waste hierarchy is to extract the maximum practical benefits from products and to generate the minimum amount of end waste; see: resource recovery.[13] The waste hierarchy is represented as a pyramid because the basic premise is that policies should promote measures to prevent the generation of waste. The next step or preferred action is to seek alternative uses for the waste that has been generated i.e. by re-use. The next is recycling which includes composting. Following this step is material recovery and waste-to-energy. The final action is disposal, in landfills or through incineration without energy recovery. This last step is the final resort for waste which has not been prevented, diverted or recovered.[14][page needed] The waste hierarchy represents the progression of a product or material through the sequential stages of the pyramid of waste management. The hierarchy represents the latter parts of the life-cycle for each product.

Life-cycle of a product [ edit ]

The life-cycle begins with design, then proceeds through manufacture, distribution, and primary use and then follows through the waste hierarchy's stages of reduce, reuse and recycle. Each stage in the life-cycle offers opportunities for policy intervention, to rethink the need for the product, to redesign to minimize waste potential, to extend its use.[14][page needed] Product life-cycle analysis is a way to optimize the use of the world's limited resources by avoiding the unnecessary generation of waste.

Resource efficiency [ edit ]

Resource efficiency reflects the understanding that global economic growth and development can not be sustained at current production and consumption patterns. Globally, humanity extracts more resources to produce goods than the planet can replenish.[14][page needed] Resource efficiency is the reduction of the environmental impact from the production and consumption of these goods, from final raw material extraction to the last use and disposal.

Polluter-pays principle [ edit ]

The polluter-pays principle mandates that the polluting party pays for the impact on the environment. With respect to waste management, this generally refers to the requirement for a waste generator to pay for appropriate disposal of the unrecoverable material.[15]

History [ edit ]

Throughout most of history, the amount of waste generated by humans was insignificant due to low levels of population density and exploitation of natural resources. Common waste produced during pre-modern times was mainly ashes and human biodegradable waste, and these were released back into the ground locally, with minimum environmental impact. Tools made out of wood or metal were generally reused or passed down through the generations.

However, some civilizations have been more profligate in their waste output than others. In particular, the Maya of Central America had a fixed monthly ritual, in which the people of the village would gather together and burn their rubbish in large dumps.[16]

Modern era [ edit ]

The Sanitary Condition of the Labouring Population was influential in securing the passage of the first legislation aimed at waste clearance and disposal. Sir Edwin Chadwick 's 1842 reportwas influential in securing the passage of the first legislation aimed at waste clearance and disposal.

Following the onset of industrialisation and the sustained urban growth of large population centres in England, the buildup of waste in the cities caused a rapid deterioration in levels of sanitation and the general quality of urban life. The streets became choked with filth due to the lack of waste clearance regulations.[17] Calls for the establishment of a municipal authority with waste removal powers occurred as early as 1751, when Corbyn Morris in London proposed that ""... as the preservation of the health of the people is of great importance, it is proposed that the cleaning of this city, should be put under one uniform public management, and all the filth be...conveyed by the Thames to proper distance in the country"".[18]

However, it was not until the mid-19th century, spurred by increasingly devastating cholera outbreaks and the emergence of a public health debate that the first legislation on the issue emerged. Highly influential in this new focus was the report The Sanitary Condition of the Labouring Population in 1842[19] of the social reformer, Edwin Chadwick, in which he argued for the importance of adequate waste removal and management facilities to improve the health and wellbeing of the city's population.

In the UK, the Nuisance Removal and Disease Prevention Act of 1846 began what was to be a steadily evolving process of the provision of regulated waste management in London. The Metropolitan Board of Works was the first citywide authority that centralized sanitation regulation for the rapidly expanding city and the Public Health Act 1875 made it compulsory for every household to deposit their weekly waste in ""moveable receptacles"" for disposal—the first concept for a dust-bin.[20]

The dramatic increase in waste for disposal led to the creation of the first incineration plants, or, as they were then called, ""destructors"". In 1874, the first incinerator was built in Nottingham by Manlove, Alliott & Co. Ltd. to the design of Alfred Fryer.[18] However, these were met with opposition on account of the large amounts of ash they produced and which wafted over the neighbouring areas.[21]

Similar municipal systems of waste disposal sprung up at the turn of the 20th century in other large cities of Europe and North America. In 1895, New York City became the first U.S. city with public-sector garbage management.[20]

Early garbage removal trucks were simply open bodied dump trucks pulled by a team of horses. They became motorized in the early part of the 20th century and the first closed body trucks to eliminate odours with a dumping lever mechanism were introduced in the 1920s in Britain.[22] These were soon equipped with 'hopper mechanisms' where the scooper was loaded at floor level and then hoisted mechanically to deposit the waste in the truck. The Garwood Load Packer was the first truck in 1938, to incorporate a hydraulic compactor.

Waste handling and transport [ edit ]

Moulded plastic, wheeled waste bin in Berkshire , England

Waste collection methods vary widely among different countries and regions. Domestic waste collection services are often provided by local government authorities, or by private companies for industrial and commercial waste. Some areas, especially those in less developed countries, do not have formal waste-collection systems.

Waste handling practices [ edit ]

Curbside collection is the most common method of disposal in most European countries, Canada, New Zealand, United States, and many other parts of the developed world in which waste is collected at regular intervals by specialised trucks. This is often associated with curb-side waste segregation. In rural areas, waste may need to be taken to a transfer station. Waste collected is then transported to an appropriate disposal facility. In some areas, vacuum collection is used in which waste is transported from the home or commercial premises by vacuum along small bore tubes. Systems are in use in Europe and North America.

In some jurisdictions unsegregated waste is collected at the curb-side or from waste transfer stations and then sorted into recyclables and unusable waste. Such systems are capable of sorting large volumes of solid waste, salvaging recyclables, and turning the rest into bio-gas and soil conditioner. In San Francisco, the local government established its Mandatory Recycling and Composting Ordinance in support of its goal of ""Zero waste by 2020"", requiring everyone in the city to keep recyclables and compostables out of the landfill. The three streams are collected with the curbside ""Fantastic 3"" bin system – blue for recyclables, green for compostables, and black for landfill-bound materials – provided to residents and businesses and serviced by San Francisco's sole refuse hauler, Recology. The city's ""Pay-As-You-Throw"" system charges customers by the volume of landfill-bound materials, which provides a financial incentive to separate recyclables and compostables from other discards. The city's Department of the Environment's Zero Waste Program has led the city to achieve 80% diversion, the highest diversion rate in North America.[23] Other businesses such as Waste Industries use a variety of colors to distinguish between trash and recycling cans. In addition, in some areas of the world the disposal of municipal solid waste can cause environmental strain due to official not having benchmarks that help measure the environmental sustainability of certain practices.[24]

Waste segregation [ edit ]

This is the separation of wet waste and dry waste. The purpose is to recycle dry waste easily and to use wet waste as compost. When segregating waste, the amount of waste that gets landfilled reduces considerably, resulting in lower levels of air and water pollution. It is important to remember that waste segregation should be based on the type of waste and the most appropriate treatment and disposal. This also makes it easier to apply different processes to the waste, like composting, recycling and incineration. It is important to practice waste management and segregation as a community. One way to practice waste management is to ensure there is awareness. The process of waste segregation should be explained to the community.[25]

Segregated waste is also often cheaper to dispose of because it does not require as much manual sorting as mixed waste. There are a number of important reasons why waste segregation is important such as legal obligations, cost savings and protection of human health and the environment. Institutions should make it as easy as possible for their staff to correctly segregate their waste. This can include labelling, making sure there are enough accessible bins and clearly indicating why segregation is so important.[26] Labeling is especially important when dealing with nuclear waste due to how much harm to human health the excess products of the nuclear cycle can cause.[27]

Recommended colour coding of containers [28]

• Yellow- for infectious waste

• Brown- for chemical and pharmaceutical waste

• Black- for general waste

Financial models [ edit ]

In most developed countries, domestic waste disposal is funded from a national or local tax which may be related to income, or property values. Commercial and industrial waste disposal is typically charged for as a commercial service, often as an integrated charge which includes disposal costs. This practice may encourage disposal contractors to opt for the cheapest disposal option such as landfill rather than the environmentally best solution such as re-use and recycling.

Financing of solid waste management projects can be overwhelming for the city government, especially if the government see it as an important service they should render to the citizen. Donors and grants are a funding mechanism that is dependent on the interest of the donor organization. as much as it is a good way to develop a city's waste management infrastructure, attracting and utilizing grants is solely reliant on what the donor considers as important. Therefore, it may be a challenge for a city government to dictate how the funds should be distributed among the various aspect of waste management.[29]

In some areas like Taipei, the city government charges its households and industries for the volume of rubbish they produce. Waste is collected by the city council only if it is put in government issued rubbish bags. This policy has successfully reduced the amount of waste the city produces and increased the recycling rate.[citation needed]

The World Bank finances and advises on solid waste management projects using a diverse suite of products and services, including traditional loans, results-based financing, development policy financing, and technical advisory. World Bank-financed waste management projects usually address the entire lifecycle of waste right from the point of generation to collection and transportation, and finally treatment and disposal.[6]

Disposal methods [ edit ]

Landfill [ edit ]

Spittelau incineration plant in Vienna

A landfill is a site for the disposal of waste materials by burial. Landfill is the oldest form of waste treatment, although the burial of the waste is modern; historically, refuse was simply left in piles or thrown into pits. Landfills must be open and available to users every day. While the majority of its customers are municipalities, commercial and construction companies, residents are also allowed to use the landfill in most cases.[30] Historically, landfills have been the most common method of organized waste disposal and remain so in many places around the world.

Incineration [ edit ]

Tarastejärvi Incineration Plant in Tampere , Finland

Incineration is a disposal method in which solid organic wastes are subjected to combustion so as to convert them into residue and gaseous products. This method is useful for disposal of both municipal solid waste and solid residue from waste water treatment. This process reduces the volumes of solid waste by 80 to 95 percent.[31] Incineration and other high temperature waste treatment systems are sometimes described as ""thermal treatment"". Incinerators convert waste materials into heat, gas, steam, and ash.

Incineration is carried out both on a small scale by individuals and on a large scale by industry. It is used to dispose of solid, liquid and gaseous waste. It is recognized as a practical method of disposing of certain hazardous waste materials (such as biological medical waste). Incineration is a controversial method of waste disposal, due to issues such as emission of gaseous pollutants including substantial quantities of carbon dioxide.

Incineration is common in countries such as Japan where land is more scarce, as the facilities generally do not require as much area as landfills. Waste-to-energy (WtE) or energy-from-waste (EfW) are broad terms for facilities that burn waste in a furnace or boiler to generate heat, steam or electricity. Combustion in an incinerator is not always perfect and there have been concerns about pollutants in gaseous emissions from incinerator stacks. Particular concern has focused on some very persistent organic compounds such as dioxins, furans, and PAHs, which may be created and which may have serious environmental consequences and some heavy metals such as mercury[32] and lead which can be volatilised in the combustion process..

Recycling [ edit ]

Steel crushed and baled for recycling

Recycling is a resource recovery practice that refers to the collection and reuse of waste materials such as empty beverage containers. This process involves breaking down and reusing materials that would otherwise be gotten rid of as trash. There are numerous benefits of recycling, and with so many new technologies making even more materials recyclable, it is possible to clean up the Earth.[33] Recycling not only benefits the environment but also positively effects the economy. The materials from which the items are made can be made into new products.[34] Materials for recycling may be collected separately from general waste using dedicated bins and collection vehicles, a procedure called kerbside collection. In some communities, the owner of the waste is required to separate the materials into different bins (e.g. for paper, plastics, metals) prior to its collection. In other communities, all recyclable materials are placed in a single bin for collection, and the sorting is handled later at a central facility. The latter method is known as ""single-stream recycling.""[35][36]

Finland A recycling point in Lappajärvi

The most common consumer products recycled include aluminium such as beverage cans, copper such as wire, steel from food and aerosol cans, old steel furnishings or equipment, rubber tyres, polyethylene and PET bottles, glass bottles and jars, paperboard cartons, newspapers, magazines and light paper, and corrugated fiberboard boxes.

PVC, LDPE, PP, and PS (see resin identification code) are also recyclable. These items are usually composed of a single type of material, making them relatively easy to recycle into new products. The recycling of complex products (such as computers and electronic equipment) is more difficult, due to the additional dismantling and separation required.

The type of material accepted for recycling varies by city and country. Each city and country has different recycling programs in place that can handle the various types of recyclable materials. However, certain variation in acceptance is reflected in the resale value of the material once it is reprocessed. Some of the types of recycling include waste paper and cardboard, plastic recycling, metal recycling, electronic devices, wood recycling, glass recycling, cloth and textile and so many more.[37] In July 2017, the Chinese government announced an import ban of 24 categories of recyclables and solid waste, including plastic, textiles and mixed paper, placing tremendous impact on developed countries globally, which exported directly or indirectly to China.[38]

Biological reprocessing [ edit ]

Recoverable materials that are organic in nature, such as plant material, food scraps, and paper products, can be recovered through composting and digestion processes to decompose the organic matter. The resulting organic material is then recycled as mulch or compost for agricultural or landscaping purposes. In addition, waste gas from the process (such as methane) can be captured and used for generating electricity and heat (CHP/cogeneration) maximising efficiencies. There are different types of composting and digestion methods and technologies. They vary in complexity from simple home compost heaps to large scale industrial digestion of mixed domestic waste. The different methods of biological decomposition are classified as aerobic or anaerobic methods. Some methods use the hybrids of these two methods. The anaerobic digestion of the organic fraction of solid waste is more environmentally effective than landfill, or incineration.[39] The intention of biological processing in waste management is to control and accelerate the natural process of decomposition of organic matter. (See resource recovery).

Energy recovery [ edit ]

Energy recovery from waste is the conversion of non-recyclable waste materials into usable heat, electricity, or fuel through a variety of processes, including combustion, gasification, pyrolyzation, anaerobic digestion, and landfill gas recovery.[40] This process is often called waste-to-energy. Energy recovery from waste is part of the non-hazardous waste management hierarchy. Using energy recovery to convert non-recyclable waste materials into electricity and heat, generates a renewable energy source and can reduce carbon emissions by offsetting the need for energy from fossil sources as well as reduce methane generation from landfills.[40] Globally, waste-to-energy accounts for 16% of waste management.[41]

The energy content of waste products can be harnessed directly by using them as a direct combustion fuel, or indirectly by processing them into another type of fuel. Thermal treatment ranges from using waste as a fuel source for cooking or heating and the use of the gas fuel (see above), to fuel for boilers to generate steam and electricity in a turbine. Pyrolysis and gasification are two related forms of thermal treatment where waste materials are heated to high temperatures with limited oxygen availability. The process usually occurs in a sealed vessel under high pressure. Pyrolysis of solid waste converts the material into solid, liquid and gas products. The liquid and gas can be burnt to produce energy or refined into other chemical products (chemical refinery). The solid residue (char) can be further refined into products such as activated carbon. Gasification and advanced Plasma arc gasification are used to convert organic materials directly into a synthetic gas (syngas) composed of carbon monoxide and hydrogen. The gas is then burnt to produce electricity and steam. An alternative to pyrolysis is high temperature and pressure supercritical water decomposition (hydrothermal monophasic oxidation).

Pyrolysis [ edit ]

Pyrolysis is often used to convert many types of domestic and industrial residues into a recovered fuel. Different types of waste input (such as plant waste, food waste, tyres) placed in the pyrolysis process potentially yield an alternative to fossil fuels.[42] Pyrolysis is a process of thermo-chemical decomposition of organic materials by heat in the absence of stoichiometric quantities of oxygen; the decomposition produces various hydrocarbon gases.[43] During pyrolysis, the molecules of object vibrate at high frequencies to an extent that molecules start breaking down. The rate of pyrolysis increases with temperature. In industrial applications, temperatures are above 430 °C (800 °F).[44] Slow pyrolysis produces gases and solid charcoal.[45] Pyrolysis hold promise for conversion of waste biomass into useful liquid fuel. Pyrolysis of waste wood and plastics can potentially produce fuel. The solids left from pyrolysis contain metals, glass, sand and pyrolysis coke which does not convert to gas. Compared to the process of incineration, certain types of pyrolysis processes release less harmful by-products that contain alkali metals, sulphur, and chlorine. However, pyrolysis of some waste yields gases which impact the environment such as HCl and SO 2 .[46]

Resource recovery [ edit ]

Resource recovery is the systematic diversion of waste, which was intended for disposal, for a specific next use.[47] It is the processing of recyclables to extract or recover materials and resources, or convert to energy.[48] These activities are performed at a resource recovery facility.[48] Resource recovery is not only environmentally important, but it is also cost-effective.[49] It decreases the amount of waste for disposal, saves space in landfills, and conserves natural resources.[49]

Resource recovery (as opposed to waste management) uses LCA (life cycle analysis) attempts to offer alternatives to waste management. For mixed MSW (Municipal Solid Waste) a number of broad studies have indicated that administration, source separation and collection followed by reuse and recycling of the non-organic fraction and energy and compost/fertilizer production of the organic material via anaerobic digestion to be the favoured path.

As an example of how resource recycling can be beneficial, many items thrown away contain metals that can be recycled to create a profit, such as the components in circuit boards. Wood chippings in pallets and other packaging materials can be recycled to useful products for horticulture. The recycled chips can cover paths, walkways, or arena surfaces.

Application of rational and consistent waste management practices can yield a range of benefits including:

Economic – Improving economic efficiency through the means of resource use, treatment and disposal and creating markets for recycles can lead to efficient practices in the production and consumption of products and materials resulting in valuable materials being recovered for reuse and the potential for new jobs and new business opportunities. Social – By reducing adverse impacts on health by proper waste management practises, the resulting consequences are more appealing civic communities. Better social advantages can lead to new sources of employment and potentially lifting communities out of poverty especially in some of the developing poorer countries and cities. Environmental – Reducing or eliminating adverse impacts on the environment through reducing, reusing and recycling, and minimizing resource extraction can result in improved air and water quality and help in the reduction of greenhouse gas emissions. Inter-generational Equity – Following effective waste management practises can provide subsequent generations a more robust economy, a fairer and more inclusive society and a cleaner environment.[14][ page needed ]

Sustainability [ edit ]

The management of waste is a key component in a business' ability to maintain ISO14001 accreditation. The standard encourages companies to improve their environmental efficiencies each year by eliminating waste through resource recovery practices. One way to do this is by adopting resource recovery practices like recycling materials such as glass, food scraps, paper and cardboard, plastic bottles and metal. Recycled materials can often be sold to the construction industry. Many inorganic waste streams can be used to produce materials for construction. Concrete and bricks can be recycled as artificial gravel. This topic was on the agenda of the International WASCON conference in Spain in June 2015 and on the international Conference on Green Urbanism, held in Italy 12–14 October 2016.[citation needed]

Liquid waste-management [ edit ]

Liquid waste is an important category of waste management because it is so difficult to deal with. Unlike solid wastes, liquid wastes cannot be easily picked up and removed from an environment. Liquid wastes spread out, and easily pollute other sources of liquid if brought into contact. This type of waste also soaks into objects like soil and groundwater. This in turn carries over to pollute the plants, the animals in the ecosystem, as well as the humans within the area of the pollution.[50]

Sewage sludge [ edit ]

Sewage sludge is produced by waste water treatment processes. Due to rapid urbanization, there has been an increase in municipal waste water that results 0.1–30.8 kg of sewage per population equivalent per year (kg/p.e/year).[51] Common disposal practices of sewage sludge are incineration, composting, and landfill.

Avoidance and reduction methods [ edit ]

An important method of waste management is the prevention of waste material being created, also known as waste reduction. Waste Minimization is reducing the quantity of hazardous wastes achieved through a thorough application of innovative or alternative procedures.[52] Methods of avoidance include reuse of second-hand products, repairing broken items instead of buying new ones, designing products to be refillable or reusable (such as cotton instead of plastic shopping bags), encouraging consumers to avoid using disposable products (such as disposable cutlery), removing any food/liquid remains from cans and packaging,[53] and designing products that use less material to achieve the same purpose (for example, lightweighting of beverage cans).[54]

International waste movement [ edit ]

While waste transport within a given country falls under national regulations, trans-boundary movement of waste is often subject to international treaties. A major concern to many countries in the world has been hazardous waste. The Basel Convention, ratified by 172 countries, deprecates movement of hazardous waste from developed to less developed countries. The provisions of the Basel convention have been integrated into the EU waste shipment regulation. Radioactive waste, although considered hazardous, does not fall under the jurisdiction of the Basel Convention.

A 2018 study found that ""informal networks of traders, buyers, and sellers exploit the porosity of borders to escape the constraining EU regulatory framework.[55]

Challenges in developing countries [ edit ]

Areas with developing economies often experience exhausted waste collection services and inadequately managed and uncontrolled dumpsites. The problems are worsening.[14][page needed][56] Problems with governance complicate the situation. Waste management in these countries and cities is an ongoing challenge due to weak institutions, chronic under-resourcing and rapid urbanization.[14][page needed] All of these challenges, along with the lack of understanding of different factors that contribute to the hierarchy of waste management, affect the treatment of waste.[57][full citation needed]

In developing countries, waste management activities are usually carried by poor, for their survival. It has been estimated that 2% of population in Asia, Latin America and Africa are dependent on waste for their livelihood. Family organized, or individual manual scavengers are often involved with waste management practices with very little supportive network and facilities with increased risk of health effects. Additionally, this practice prevents their children from further education. Participation level of most citizens in waste management is very low, residents in urban areas are not actively involved in the process of waste management.[58]

Technologies [ edit ]

Traditionally, the waste management industry has been a late adopter of new technologies such as RFID (Radio Frequency Identification) tags, GPS and integrated software packages which enable better quality data to be collected without the use of estimation or manual data entry.[59] This technology has been used widely by many organizations in some industrialized countries. Radio frequency identification is a tagging system for automatic identification of recyclable components of municipal solid waste stream.[60]

Waste management by region [ edit ]

Morocco [ edit ]

Morocco has seen benefits from implementing a $300 million sanitary landfill system. While it might appear to be a costly investment, the country's government predicts that it has saved them another $440 million in damages, or consequences of failing to dispose of waste properly.[61]

Turkey [ edit ]

United Kingdom [ edit ]

Waste management policy in the United Kingdom is a responsibility of the Department of the Environment, Food and Rural Affairs (DEFRA). In England, the ""Waste management plan for England"" presents a compilation of waste management policies.[62]

Zambia [ edit ]

In Zambia, ASAZA is a community-based organization whose principal purpose is to complement the efforts of Government and co-operating partners to uplift the standard of living for disadvantaged communities. The project's main objective is to minimize the problem of indiscriminate littering which leads to land degradation and pollution of the environment. ASAZA is also at the same time helping alleviate the problems of unemployment and poverty through income generation and payment of participants, women and unskilled youths.[63]

San Francisco [ edit ]

San Francisco started to make changes to their waste management policies in 2009 with the expectation to be zero waste by the year 2030.[64] Council made changes such as making recycling and composting a mandatory practice for businesses and individuals, banning Styrofoam and plastic bags, putting charges on paper bags, and increasing garbage collection rates.[65][66] Businesses are fiscally rewarded for correct disposal of recycling and composting and taxed for incorrect disposal. Besides these policies, the waste bins were manufactured in various sizes. The compost bin is the largest, the recycling bin is second, and the garbage bin is the smallest. This encourages individuals to sort their waste thoughtfully in respect to the sizes. These systems are working because they were able to divert 80% of waste from the landfill, which is the highest rate of any major U.S. city.[67] Despite all these changes, Debbie Raphael, director of the San Francisco Department of the Environment, states that zero waste is still not achievable until all products are designed differently to be able to be recycled or compostable.[68]

Scientific journals [ edit ]

Related scientific journals in this area include:

See also [ edit ]"	https://en.wikipedia.org/wiki/Waste_management	"""Waste disposal"" redirects here. For the kitchen device, see Garbage disposal unit
Not to be confused with Sanitary engineering
activities and actions required to manage waste from its source to its final disposal
Containers for selective waste collection at the Gdańsk University of Technology
A recycling and waste-to-energy plant for waste that is not exported
Waste management (or waste disposal) includes the activities and actions required to manage waste from its inception to its final disposal. This includes the collection, transport, treatment and disposal of waste, together with monitoring and regulation of the waste management process and waste-related laws, technologies, economic mechanisms.
Waste can be solid, liquid, or gaseous and each type has different methods of disposal and management. Waste management deals with all types of waste, including industrial, biological and household. In some cases, waste can pose a threat to human health. Health issues are associated throughout the entire process of waste management. Health issues can also arise indirectly or directly. Directly, through the handling of said waste, and indirectly through the consumption of water, soil and food. Waste is produced by  human activity, for example, the extraction and processing of raw materials. Waste management is intended to reduce adverse effects of waste on human health, the environment, planetary resources and aesthetics.
Waste management practices are not uniform among countries (developed and developing nations); regions (urban and rural areas), and residential and industrial sectors can all take different approaches.
Proper management of waste is important for building sustainable and liveable cities, but it remains a challenge for many developing countries and cities. A report found that effective waste management is relatively expensive, usually comprising 20%–50% of municipal budgets. Operating this essential municipal service requires integrated systems that are efficient, sustainable, and socially supported. A large portion of waste management practices deal with municipal solid waste (MSW) which is the bulk of the waste that is created by household, industrial, and commercial activity. Measures of waste management include measures for integrated techno-economic mechanisms of a circular economy, effective disposal facilities, export and import control and optimal sustainable design of products that are produced.
In the first systematic review of the scientific evidence around global waste, its management and its impact on human health and life, authors concluded that about a fourth of all the municipal solid terrestrial waste is not collected and an additional fourth is mismanaged after collection, often being burned in open and uncontrolled fires – or close to one billion tons per year when combined. They also found that broad priority areas each lack a ""high-quality research base"", partly due to the absence of ""substantial research funding"", which motivated scientists often require.
Principles of waste management 
Diagram of the waste hierarchy
Waste hierarchy 
The waste hierarchy refers to the ""3 Rs"" Reduce, Reuse and Recycle, which classifies waste management strategies according to their desirability in terms of waste minimisation. The waste hierarchy is the cornerstone of most waste minimization strategies. The aim of the waste hierarchy is to extract the maximum practical benefits from products and to generate the minimum amount of end waste; see: resource recovery. The waste hierarchy is represented as a pyramid because the basic premise is that policies should promote measures to prevent the generation of waste. The next step or preferred action is to seek alternative uses for the waste that has been generated i.e. by re-use. The next is recycling which includes composting. Following this step is material recovery and waste-to-energy. The final action is disposal, in landfills or through incineration without energy recovery. This last step is the final resort for waste which has not been prevented, diverted or recovered. The waste hierarchy represents the progression of a product or material through the sequential stages of the pyramid of waste management. The hierarchy represents the latter parts of the life-cycle for each product.
Life-cycle of a product 
The life-cycle begins with design, then proceeds through manufacture, distribution, and primary use and then follows through the waste hierarchy's stages of reduce, reuse and recycle. Each stage in the life-cycle offers opportunities for policy intervention, to rethink the need for the product, to redesign to minimize waste potential, to extend its use. Product life-cycle analysis is a way to optimize the use of the world's limited resources by avoiding the unnecessary generation of waste.
Resource efficiency 
Resource efficiency reflects the understanding that global economic growth and development can not be sustained at current production and consumption patterns. Globally, humanity extracts more resources to produce goods than the planet can replenish. Resource efficiency is the reduction of the environmental impact from the production and consumption of these goods, from final raw material extraction to the last use and disposal.
Polluter-pays principle 
The polluter-pays principle mandates that the polluting party pays for the impact on the environment. With respect to waste management, this generally refers to the requirement for a waste generator to pay for appropriate disposal of the unrecoverable material.
History 
Throughout most of history, the amount of waste generated by humans was insignificant due to low levels of population density and exploitation of natural resources. Common waste produced during pre-modern times was mainly ashes and human biodegradable waste, and these were released back into the ground locally, with minimum environmental impact. Tools made out of wood or metal were generally reused or passed down through the generations.
However, some civilizations have been more profligate in their waste output than others. In particular, the Maya of Central America had a fixed monthly ritual, in which the people of the village would gather together and burn their rubbish in large dumps.
Modern era 
The Sanitary Condition of the Labouring Population was influential in securing the passage of the first legislation aimed at waste clearance and disposal. Sir Edwin Chadwick 's 1842 reportwas influential in securing the passage of the first legislation aimed at waste clearance and disposal.
Following the onset of industrialisation and the sustained urban growth of large population centres in England, the buildup of waste in the cities caused a rapid deterioration in levels of sanitation and the general quality of urban life. The streets became choked with filth due to the lack of waste clearance regulations. Calls for the establishment of a municipal authority with waste removal powers occurred as early as 1751, when Corbyn Morris in London proposed that ""... as the preservation of the health of the people is of great importance, it is proposed that the cleaning of this city, should be put under one uniform public management, and all the filth be...conveyed by the Thames to proper distance in the country"".
However, it was not until the mid-19th century, spurred by increasingly devastating cholera outbreaks and the emergence of a public health debate that the first legislation on the issue emerged. Highly influential in this new focus was the report The Sanitary Condition of the Labouring Population in 1842 of the social reformer, Edwin Chadwick, in which he argued for the importance of adequate waste removal and management facilities to improve the health and wellbeing of the city's population.
In the UK, the Nuisance Removal and Disease Prevention Act of 1846 began what was to be a steadily evolving process of the provision of regulated waste management in London. The Metropolitan Board of Works was the first citywide authority that centralized sanitation regulation for the rapidly expanding city and the Public Health Act 1875 made it compulsory for every household to deposit their weekly waste in ""moveable receptacles"" for disposal—the first concept for a dust-bin.
The dramatic increase in waste for disposal led to the creation of the first incineration plants, or, as they were then called, ""destructors"". In 1874, the first incinerator was built in Nottingham by Manlove, Alliott & Co. Ltd. to the design of Alfred Fryer. However, these were met with opposition on account of the large amounts of ash they produced and which wafted over the neighbouring areas.
Similar municipal systems of waste disposal sprung up at the turn of the 20th century in other large cities of Europe and North America. In 1895, New York City became the first U.S. city with public-sector garbage management.
Early garbage removal trucks were simply open bodied dump trucks pulled by a team of horses. They became motorized in the early part of the 20th century and the first closed body trucks to eliminate odours with a dumping lever mechanism were introduced in the 1920s in Britain. These were soon equipped with 'hopper mechanisms' where the scooper was loaded at floor level and then hoisted mechanically to deposit the waste in the truck. The Garwood Load Packer was the first truck in 1938, to incorporate a hydraulic compactor.
Waste handling and transport 
Moulded plastic, wheeled waste bin in Berkshire , England
Waste collection methods vary widely among different countries and regions. Domestic waste collection services are often provided by local government authorities, or by private companies for industrial and commercial waste. Some areas, especially those in less developed countries, do not have formal waste-collection systems.
Waste handling practices 
Curbside collection is the most common method of disposal in most European countries, Canada, New Zealand, United States, and many other parts of the developed world in which waste is collected at regular intervals by specialised trucks. This is often associated with curb-side waste segregation. In rural areas, waste may need to be taken to a transfer station. Waste collected is then transported to an appropriate disposal facility. In some areas, vacuum collection is used in which waste is transported from the home or commercial premises by vacuum along small bore tubes. Systems are in use in Europe and North America.
In some jurisdictions unsegregated waste is collected at the curb-side or from waste transfer stations and then sorted into recyclables and unusable waste. Such systems are capable of sorting large volumes of solid waste, salvaging recyclables, and turning the rest into bio-gas and soil conditioner. In San Francisco, the local government established its Mandatory Recycling and Composting Ordinance in support of its goal of ""Zero waste by 2020"", requiring everyone in the city to keep recyclables and compostables out of the landfill. The three streams are collected with the curbside ""Fantastic 3"" bin system – blue for recyclables, green for compostables, and black for landfill-bound materials – provided to residents and businesses and serviced by San Francisco's sole refuse hauler, Recology. The city's ""Pay-As-You-Throw"" system charges customers by the volume of landfill-bound materials, which provides a financial incentive to separate recyclables and compostables from other discards. The city's Department of the Environment's Zero Waste Program has led the city to achieve 80% diversion, the highest diversion rate in North America. Other businesses such as Waste Industries use a variety of colors to distinguish between trash and recycling cans. In addition, in some areas of the world the disposal of municipal solid waste can cause environmental strain due to official not having benchmarks that help measure the environmental sustainability of certain practices.
Waste segregation 
This is the separation of wet waste and dry waste. The purpose is to recycle dry waste easily and to use wet waste as compost. When segregating waste, the amount of waste that gets landfilled reduces considerably, resulting in lower levels of air and water pollution. It is important to remember that waste segregation should be based on the type of waste and the most appropriate treatment and disposal. This also makes it easier to apply different processes to the waste, like composting, recycling and incineration. It is important to practice waste management and segregation as a community. One way to practice waste management is to ensure there is awareness. The process of waste segregation should be explained to the community.
Segregated waste is also often cheaper to dispose of because it does not require as much manual sorting as mixed waste. There are a number of important reasons why waste segregation is important such as legal obligations, cost savings and protection of human health and the environment. Institutions should make it as easy as possible for their staff to correctly segregate their waste. This can include labelling, making sure there are enough accessible bins and clearly indicating why segregation is so important. Labeling is especially important when dealing with nuclear waste due to how much harm to human health the excess products of the nuclear cycle can cause.
Recommended colour coding of containers 
• Yellow- for infectious waste
• Brown- for chemical and pharmaceutical waste
• Black- for general waste
Financial models 
In most developed countries, domestic waste disposal is funded from a national or local tax which may be related to income, or property values. Commercial and industrial waste disposal is typically charged for as a commercial service, often as an integrated charge which includes disposal costs. This practice may encourage disposal contractors to opt for the cheapest disposal option such as landfill rather than the environmentally best solution such as re-use and recycling.
Financing of solid waste management projects can be overwhelming for the city government, especially if the government see it as an important service they should render to the citizen. Donors and grants are a funding mechanism that is dependent on the interest of the donor organization. as much as it is a good way to develop a city's waste management infrastructure, attracting and utilizing grants is solely reliant on what the donor considers as important. Therefore, it may be a challenge for a city government to dictate how the funds should be distributed among the various aspect of waste management.
In some areas like Taipei, the city government charges its households and industries for the volume of rubbish they produce. Waste is collected by the city council only if it is put in government issued rubbish bags. This policy has successfully reduced the amount of waste the city produces and increased the recycling rate.
The World Bank finances and advises on solid waste management projects using a diverse suite of products and services, including traditional loans, results-based financing, development policy financing, and technical advisory. World Bank-financed waste management projects usually address the entire lifecycle of waste right from the point of generation to collection and transportation, and finally treatment and disposal.
Disposal methods 
Landfill 
Spittelau incineration plant in Vienna
A landfill is a site for the disposal of waste materials by burial. Landfill is the oldest form of waste treatment, although the burial of the waste is modern; historically, refuse was simply left in piles or thrown into pits. Landfills must be open and available to users every day. While the majority of its customers are municipalities, commercial and construction companies, residents are also allowed to use the landfill in most cases. Historically, landfills have been the most common method of organized waste disposal and remain so in many places around the world.
Incineration 
Tarastejärvi Incineration Plant in Tampere , Finland
Incineration is a disposal method in which solid organic wastes are subjected to combustion so as to convert them into residue and gaseous products. This method is useful for disposal of both municipal solid waste and solid residue from waste water treatment. This process reduces the volumes of solid waste by 80 to 95 percent. Incineration and other high temperature waste treatment systems are sometimes described as ""thermal treatment"". Incinerators convert waste materials into heat, gas, steam, and ash.
Incineration is carried out both on a small scale by individuals and on a large scale by industry. It is used to dispose of solid, liquid and gaseous waste. It is recognized as a practical method of disposing of certain hazardous waste materials (such as biological medical waste). Incineration is a controversial method of waste disposal, due to issues such as emission of gaseous pollutants including substantial quantities of carbon dioxide.
Incineration is common in countries such as Japan where land is more scarce, as the facilities generally do not require as much area as landfills. Waste-to-energy (WtE) or energy-from-waste (EfW) are broad terms for facilities that burn waste in a furnace or boiler to generate heat, steam or electricity. Combustion in an incinerator is not always perfect and there have been concerns about pollutants in gaseous emissions from incinerator stacks. Particular concern has focused on some very persistent organic compounds such as dioxins, furans, and PAHs, which may be created and which may have serious environmental consequences and some heavy metals such as mercury and lead which can be volatilised in the combustion process..
Recycling 
Steel crushed and baled for recycling
Recycling is a resource recovery practice that refers to the collection and reuse of waste materials such as empty beverage containers. This process involves breaking down and reusing materials that would otherwise be gotten rid of as trash. There are numerous benefits of recycling, and with so many new technologies making even more materials recyclable, it is possible to clean up the Earth. Recycling not only benefits the environment but also positively effects the economy. The materials from which the items are made can be made into new products. Materials for recycling may be collected separately from general waste using dedicated bins and collection vehicles, a procedure called kerbside collection. In some communities, the owner of the waste is required to separate the materials into different bins (e.g. for paper, plastics, metals) prior to its collection. In other communities, all recyclable materials are placed in a single bin for collection, and the sorting is handled later at a central facility. The latter method is known as ""single-stream recycling.""
Finland A recycling point in Lappajärvi
The most common consumer products recycled include aluminium such as beverage cans, copper such as wire, steel from food and aerosol cans, old steel furnishings or equipment, rubber tyres, polyethylene and PET bottles, glass bottles and jars, paperboard cartons, newspapers, magazines and light paper, and corrugated fiberboard boxes.
PVC, LDPE, PP, and PS (see resin identification code) are also recyclable. These items are usually composed of a single type of material, making them relatively easy to recycle into new products. The recycling of complex products (such as computers and electronic equipment) is more difficult, due to the additional dismantling and separation required.
The type of material accepted for recycling varies by city and country. Each city and country has different recycling programs in place that can handle the various types of recyclable materials. However, certain variation in acceptance is reflected in the resale value of the material once it is reprocessed. Some of the types of recycling include waste paper and cardboard, plastic recycling, metal recycling, electronic devices, wood recycling, glass recycling, cloth and textile and so many more. In July 2017, the Chinese government announced an import ban of 24 categories of recyclables and solid waste, including plastic, textiles and mixed paper, placing tremendous impact on developed countries globally, which exported directly or indirectly to China.
Biological reprocessing 
Recoverable materials that are organic in nature, such as plant material, food scraps, and paper products, can be recovered through composting and digestion processes to decompose the organic matter. The resulting organic material is then recycled as mulch or compost for agricultural or landscaping purposes. In addition, waste gas from the process (such as methane) can be captured and used for generating electricity and heat (CHP/cogeneration) maximising efficiencies. There are different types of composting and digestion methods and technologies. They vary in complexity from simple home compost heaps to large scale industrial digestion of mixed domestic waste. The different methods of biological decomposition are classified as aerobic or anaerobic methods. Some methods use the hybrids of these two methods. The anaerobic digestion of the organic fraction of solid waste is more environmentally effective than landfill, or incineration. The intention of biological processing in waste management is to control and accelerate the natural process of decomposition of organic matter. (See resource recovery).
Energy recovery 
Energy recovery from waste is the conversion of non-recyclable waste materials into usable heat, electricity, or fuel through a variety of processes, including combustion, gasification, pyrolyzation, anaerobic digestion, and landfill gas recovery. This process is often called waste-to-energy. Energy recovery from waste is part of the non-hazardous waste management hierarchy. Using energy recovery to convert non-recyclable waste materials into electricity and heat, generates a renewable energy source and can reduce carbon emissions by offsetting the need for energy from fossil sources as well as reduce methane generation from landfills. Globally, waste-to-energy accounts for 16% of waste management.
The energy content of waste products can be harnessed directly by using them as a direct combustion fuel, or indirectly by processing them into another type of fuel. Thermal treatment ranges from using waste as a fuel source for cooking or heating and the use of the gas fuel (see above), to fuel for boilers to generate steam and electricity in a turbine. Pyrolysis and gasification are two related forms of thermal treatment where waste materials are heated to high temperatures with limited oxygen availability. The process usually occurs in a sealed vessel under high pressure. Pyrolysis of solid waste converts the material into solid, liquid and gas products. The liquid and gas can be burnt to produce energy or refined into other chemical products (chemical refinery). The solid residue (char) can be further refined into products such as activated carbon. Gasification and advanced Plasma arc gasification are used to convert organic materials directly into a synthetic gas (syngas) composed of carbon monoxide and hydrogen. The gas is then burnt to produce electricity and steam. An alternative to pyrolysis is high temperature and pressure supercritical water decomposition (hydrothermal monophasic oxidation).
Pyrolysis 
Pyrolysis is often used to convert many types of domestic and industrial residues into a recovered fuel. Different types of waste input (such as plant waste, food waste, tyres) placed in the pyrolysis process potentially yield an alternative to fossil fuels. Pyrolysis is a process of thermo-chemical decomposition of organic materials by heat in the absence of stoichiometric quantities of oxygen; the decomposition produces various hydrocarbon gases. During pyrolysis, the molecules of object vibrate at high frequencies to an extent that molecules start breaking down. The rate of pyrolysis increases with temperature. In industrial applications, temperatures are above 430 °C (800 °F). Slow pyrolysis produces gases and solid charcoal. Pyrolysis hold promise for conversion of waste biomass into useful liquid fuel. Pyrolysis of waste wood and plastics can potentially produce fuel. The solids left from pyrolysis contain metals, glass, sand and pyrolysis coke which does not convert to gas. Compared to the process of incineration, certain types of pyrolysis processes release less harmful by-products that contain alkali metals, sulphur, and chlorine. However, pyrolysis of some waste yields gases which impact the environment such as HCl and SO 2 .
Resource recovery 
Resource recovery is the systematic diversion of waste, which was intended for disposal, for a specific next use. It is the processing of recyclables to extract or recover materials and resources, or convert to energy. These activities are performed at a resource recovery facility. Resource recovery is not only environmentally important, but it is also cost-effective. It decreases the amount of waste for disposal, saves space in landfills, and conserves natural resources.
Resource recovery (as opposed to waste management) uses LCA (life cycle analysis) attempts to offer alternatives to waste management. For mixed MSW (Municipal Solid Waste) a number of broad studies have indicated that administration, source separation and collection followed by reuse and recycling of the non-organic fraction and energy and compost/fertilizer production of the organic material via anaerobic digestion to be the favoured path.
As an example of how resource recycling can be beneficial, many items thrown away contain metals that can be recycled to create a profit, such as the components in circuit boards. Wood chippings in pallets and other packaging materials can be recycled to useful products for horticulture. The recycled chips can cover paths, walkways, or arena surfaces.
Application of rational and consistent waste management practices can yield a range of benefits including:
Economic – Improving economic efficiency through the means of resource use, treatment and disposal and creating markets for recycles can lead to efficient practices in the production and consumption of products and materials resulting in valuable materials being recovered for reuse and the potential for new jobs and new business opportunities. Social – By reducing adverse impacts on health by proper waste management practises, the resulting consequences are more appealing civic communities. Better social advantages can lead to new sources of employment and potentially lifting communities out of poverty especially in some of the developing poorer countries and cities. Environmental – Reducing or eliminating adverse impacts on the environment through reducing, reusing and recycling, and minimizing resource extraction can result in improved air and water quality and help in the reduction of greenhouse gas emissions. Inter-generational Equity – Following effective waste management practises can provide subsequent generations a more robust economy, a fairer and more inclusive society and a cleaner environment.
Sustainability 
The management of waste is a key component in a business' ability to maintain ISO14001 accreditation. The standard encourages companies to improve their environmental efficiencies each year by eliminating waste through resource recovery practices. One way to do this is by adopting resource recovery practices like recycling materials such as glass, food scraps, paper and cardboard, plastic bottles and metal. Recycled materials can often be sold to the construction industry. Many inorganic waste streams can be used to produce materials for construction. Concrete and bricks can be recycled as artificial gravel. This topic was on the agenda of the International WASCON conference in Spain in June 2015 and on the international Conference on Green Urbanism, held in Italy 12–14 October 2016.
Liquid waste-management 
Liquid waste is an important category of waste management because it is so difficult to deal with. Unlike solid wastes, liquid wastes cannot be easily picked up and removed from an environment. Liquid wastes spread out, and easily pollute other sources of liquid if brought into contact. This type of waste also soaks into objects like soil and groundwater. This in turn carries over to pollute the plants, the animals in the ecosystem, as well as the humans within the area of the pollution.
Sewage sludge 
Sewage sludge is produced by waste water treatment processes. Due to rapid urbanization, there has been an increase in municipal waste water that results 0.1–30.8 kg of sewage per population equivalent per year (kg/p.e/year). Common disposal practices of sewage sludge are incineration, composting, and landfill.
Avoidance and reduction methods 
An important method of waste management is the prevention of waste material being created, also known as waste reduction. Waste Minimization is reducing the quantity of hazardous wastes achieved through a thorough application of innovative or alternative procedures. Methods of avoidance include reuse of second-hand products, repairing broken items instead of buying new ones, designing products to be refillable or reusable (such as cotton instead of plastic shopping bags), encouraging consumers to avoid using disposable products (such as disposable cutlery), removing any food/liquid remains from cans and packaging, and designing products that use less material to achieve the same purpose (for example, lightweighting of beverage cans).
International waste movement 
While waste transport within a given country falls under national regulations, trans-boundary movement of waste is often subject to international treaties. A major concern to many countries in the world has been hazardous waste. The Basel Convention, ratified by 172 countries, deprecates movement of hazardous waste from developed to less developed countries. The provisions of the Basel convention have been integrated into the EU waste shipment regulation. Radioactive waste, although considered hazardous, does not fall under the jurisdiction of the Basel Convention.
A 2018 study found that ""informal networks of traders, buyers, and sellers exploit the porosity of borders to escape the constraining EU regulatory framework.
Challenges in developing countries 
Areas with developing economies often experience exhausted waste collection services and inadequately managed and uncontrolled dumpsites. The problems are worsening. Problems with governance complicate the situation. Waste management in these countries and cities is an ongoing challenge due to weak institutions, chronic under-resourcing and rapid urbanization. All of these challenges, along with the lack of understanding of different factors that contribute to the hierarchy of waste management, affect the treatment of waste.
In developing countries, waste management activities are usually carried by poor, for their survival. It has been estimated that 2% of population in Asia, Latin America and Africa are dependent on waste for their livelihood. Family organized, or individual manual scavengers are often involved with waste management practices with very little supportive network and facilities with increased risk of health effects. Additionally, this practice prevents their children from further education. Participation level of most citizens in waste management is very low, residents in urban areas are not actively involved in the process of waste management.
Technologies 
Traditionally, the waste management industry has been a late adopter of new technologies such as RFID (Radio Frequency Identification) tags, GPS and integrated software packages which enable better quality data to be collected without the use of estimation or manual data entry. This technology has been used widely by many organizations in some industrialized countries. Radio frequency identification is a tagging system for automatic identification of recyclable components of municipal solid waste stream.
Waste management by region 
Morocco 
Morocco has seen benefits from implementing a $300 million sanitary landfill system. While it might appear to be a costly investment, the country's government predicts that it has saved them another $440 million in damages, or consequences of failing to dispose of waste properly.
Turkey 
United Kingdom 
Waste management policy in the United Kingdom is a responsibility of the Department of the Environment, Food and Rural Affairs (DEFRA). In England, the ""Waste management plan for England"" presents a compilation of waste management policies.
Zambia 
In Zambia, ASAZA is a community-based organization whose principal purpose is to complement the efforts of Government and co-operating partners to uplift the standard of living for disadvantaged communities. The project's main objective is to minimize the problem of indiscriminate littering which leads to land degradation and pollution of the environment. ASAZA is also at the same time helping alleviate the problems of unemployment and poverty through income generation and payment of participants, women and unskilled youths.
San Francisco 
San Francisco started to make changes to their waste management policies in 2009 with the expectation to be zero waste by the year 2030. Council made changes such as making recycling and composting a mandatory practice for businesses and individuals, banning Styrofoam and plastic bags, putting charges on paper bags, and increasing garbage collection rates. Businesses are fiscally rewarded for correct disposal of recycling and composting and taxed for incorrect disposal. Besides these policies, the waste bins were manufactured in various sizes. The compost bin is the largest, the recycling bin is second, and the garbage bin is the smallest. This encourages individuals to sort their waste thoughtfully in respect to the sizes. These systems are working because they were able to divert 80% of waste from the landfill, which is the highest rate of any major U.S. city. Despite all these changes, Debbie Raphael, director of the San Francisco Department of the Environment, states that zero waste is still not achievable until all products are designed differently to be able to be recycled or compostable.
Scientific journals 
Related scientific journals in this area include:"	34580
environment	['Oecd Environment Focus']	2020-02-04 00:00:00	"Carbon tax, emissions reduction and employment: Some evidence from France

By Damien Dussaux, PhD, Environmental Economist, OECD Environment Directorate

Image credit: Shutterstock / EggHeadPhoto

In September 2019 the French Parliament adopted a law on energy and climate which enshrines into French law the objective of carbon neutrality by 2050, in line with the 2015 Paris Climate Agreement. Carbon neutrality means reducing carbon emissions and balancing residual emissions by capture and storage. Achieving carbon neutrality in France by 2050 will require a drastic decrease in greenhouse gas (GHG) emissions of 75%, as compared to 1990 levels.

To ensure this target is met, the French government developed a “National Low Carbon Strategy”, which acts as a roadmap for implementing a low-emission transition in each sector of the economy. For example, GHG emissions from industry account for almost one fifth of emissions in France, equivalent to total GHG emissions of Romania, and, under the proposed sectoral plan, will be reduced by a quarter within the next ten years.

France currently employs two main carbon pricing mechanisms. The European Union Emissions Trading System (EU-ETS) has been in place since 2005 and covers 75% of French industrial emissions. In 2014, France introduced a carbon tax on fossil fuel consumption. The rate started out at 7 euros per tonne of CO 2 and now amounts to 45 euros per tonne.

These increasingly stringent carbon pricing policies have taken place in a period of rising industrial energy costs, thereby generating concern about the impacts of such policies on the competitiveness of the French manufacturing sector. At first glance, such concern appears to be borne out by recent trends as real output and total employment in the sector decreased by 5% and 26% respectively, between 2001 and 2016.

However, a recent OECD report, “The joint effects of energy prices and carbon taxes on environmental and economic performance: Evidence from the French manufacturing sector”, sheds light on this issue. The study is the first to estimate the impact of energy prices and carbon taxes on environmental and economic performance using data at firm and industry levels.

The paper combines firm-level data on energy use and carbon emissions from the French annual survey on energy consumption (EACEI) with firm-level financial and economic performance data from the census of the French fiscal authority. The dataset covers 8,000 French firms observed yearly over the course of fifteen years (2001 to 2016) and represents the entire manufacturing sector.

What does the OECD study tell us?

The first major finding of the study is that, at the firm level, a 10% increase in energy costs results in a 6% decline in energy use, a 9% decrease in carbon emissions, and a 2% decrease in the number of full-time employees within one year. However, these jobs are not lost, but are reallocated to other firms. At the industry level, the study finds no statistical link between energy prices and net job creation, indicating that jobs lost at affected firms are compensated by increases in employment in other firms operating in the same sector during the same year.

Second, these effects vary both between industries and according to the size of the firm and their energy intensity. For example, when facing the same increase in the energy cost, firms in the wearing apparel industry reduces their carbon emissions twice as much as firms producing non-metallic minerals. The reallocation of workers in the food products industry is half the reallocation in the basic metals industry. On average, large and energy intensive firms experience greater reduction in carbon emissions and greater job reallocation than smaller and energy efficient firms.

With this, the paper is able to measure the causal effect of the carbon tax on the aggregate manufacturing sector since its introduction in 2014. Figure 1 plots the carbon tax on the left axis (green line) together with the impacts of the carbon tax on the French manufacturing sector’s jobs (purple line) and carbon emissions (red line) on the right axis. In five years, the carbon tax decreased carbon emissions by 5%. The net effect on employment is much smaller in magnitude and even slightly positive at +0.8%.

Figure 1. The impact of the French carbon tax on aggregate jobs and CO 2 emissions

Note: The graph shows the simulated impact of the carbon tax on the job reallocation and CO 2 emissions of the French manufacturing sector. Source: Dussaux (2020).

Finally, the paper considers a scenario where the carbon tax is doubled from its current rate of 45 € per tonne of CO 2 . Figure 2 shows the simulated effect of the tax increase on job reallocations and carbon emissions for each manufacturing industry. These job reallocations are not net job losses, but the number of people forced to change jobs (within the same industry or between industries).

Figure 2. The impact of a doubling of the carbon tax on job reallocations and CO 2 emissions

Note: The graph shows the causal impact of an increase in the carbon tax from 44.6 € to 86.2 € per ton of CO 2 on the job reallocation and CO 2 emissions of French manufacturing industries. For clarity, the food products sector is not included.

Source: Dussaux (2020) Table 11.

A simulated doubling of the carbon tax highlights significant heterogeneity across sectors. Several industries such as furniture, wood products, paper, and textiles experience large reductions in carbon emissions with little job reallocation. On the contrary, the motor vehicles and the plastic industries experience larger job reallocations and smaller declines in carbon emissions. Other industries such as metal products experience large job reallocation and emissions reduction because of their size.

Higher energy prices and carbon taxes are effective at reducing carbon emissions, but costs of job reallocation must be considered…

Although the carbon tax enables the French manufacturing sector to meet its carbon budget anddoes not affect total employment negatively, it however generates non-negligible job reallocations in several industries. Because these reallocation effects have redistributive implications and generate costs for workers who are forced to change jobs, these results call for complementary labour market policies that minimise those costs on affected workers and ease between-firms adjustments in employment. Moreover, since these transition costs are typically highly localised in regions specialised in polluting activities, they can also translate into potentially significant regional effects and thus political costs."	https://oecd-environment-focus.blog/2020/02/04/carbon-tax-emissions-reduction-and-employment-some-evidence-from-france/	"Carbon tax, emissions reduction and employment: Some evidence from France
By Damien Dussaux, PhD, Environmental Economist, OECD Environment Directorate
Image credit: Shutterstock / EggHeadPhoto
In September 2019 the French Parliament adopted a law on energy and climate which enshrines into French law the objective of carbon neutrality by 2050, in line with the 2015 Paris Climate Agreement. Carbon neutrality means reducing carbon emissions and balancing residual emissions by capture and storage. Achieving carbon neutrality in France by 2050 will require a drastic decrease in greenhouse gas (GHG) emissions of 75%, as compared to 1990 levels.
To ensure this target is met, the French government developed a “National Low Carbon Strategy”, which acts as a roadmap for implementing a low-emission transition in each sector of the economy. For example, GHG emissions from industry account for almost one fifth of emissions in France, equivalent to total GHG emissions of Romania, and, under the proposed sectoral plan, will be reduced by a quarter within the next ten years.
France currently employs two main carbon pricing mechanisms. The European Union Emissions Trading System (EU-ETS) has been in place since 2005 and covers 75% of French industrial emissions. In 2014, France introduced a carbon tax on fossil fuel consumption. The rate started out at 7 euros per tonne of CO 2 and now amounts to 45 euros per tonne.
These increasingly stringent carbon pricing policies have taken place in a period of rising industrial energy costs, thereby generating concern about the impacts of such policies on the competitiveness of the French manufacturing sector. At first glance, such concern appears to be borne out by recent trends as real output and total employment in the sector decreased by 5% and 26% respectively, between 2001 and 2016.
However, a recent OECD report, “The joint effects of energy prices and carbon taxes on environmental and economic performance: Evidence from the French manufacturing sector”, sheds light on this issue. The study is the first to estimate the impact of energy prices and carbon taxes on environmental and economic performance using data at firm and industry levels.
The paper combines firm-level data on energy use and carbon emissions from the French annual survey on energy consumption (EACEI) with firm-level financial and economic performance data from the census of the French fiscal authority. The dataset covers 8,000 French firms observed yearly over the course of fifteen years (2001 to 2016) and represents the entire manufacturing sector.
What does the OECD study tell us?
The first major finding of the study is that, at the firm level, a 10% increase in energy costs results in a 6% decline in energy use, a 9% decrease in carbon emissions, and a 2% decrease in the number of full-time employees within one year. However, these jobs are not lost, but are reallocated to other firms. At the industry level, the study finds no statistical link between energy prices and net job creation, indicating that jobs lost at affected firms are compensated by increases in employment in other firms operating in the same sector during the same year.
Second, these effects vary both between industries and according to the size of the firm and their energy intensity. For example, when facing the same increase in the energy cost, firms in the wearing apparel industry reduces their carbon emissions twice as much as firms producing non-metallic minerals. The reallocation of workers in the food products industry is half the reallocation in the basic metals industry. On average, large and energy intensive firms experience greater reduction in carbon emissions and greater job reallocation than smaller and energy efficient firms.
With this, the paper is able to measure the causal effect of the carbon tax on the aggregate manufacturing sector since its introduction in 2014. Figure 1 plots the carbon tax on the left axis (green line) together with the impacts of the carbon tax on the French manufacturing sector’s jobs (purple line) and carbon emissions (red line) on the right axis. In five years, the carbon tax decreased carbon emissions by 5%. The net effect on employment is much smaller in magnitude and even slightly positive at +0.8%.
Figure 1. The impact of the French carbon tax on aggregate jobs and CO 2 emissions
Note: The graph shows the simulated impact of the carbon tax on the job reallocation and CO 2 emissions of the French manufacturing sector. Source: Dussaux (2020).
Finally, the paper considers a scenario where the carbon tax is doubled from its current rate of 45 € per tonne of CO 2 . Figure 2 shows the simulated effect of the tax increase on job reallocations and carbon emissions for each manufacturing industry. These job reallocations are not net job losses, but the number of people forced to change jobs (within the same industry or between industries).
Figure 2. The impact of a doubling of the carbon tax on job reallocations and CO 2 emissions
Note: The graph shows the causal impact of an increase in the carbon tax from 44.6 € to 86.2 € per ton of CO 2 on the job reallocation and CO 2 emissions of French manufacturing industries. For clarity, the food products sector is not included.
Source: Dussaux (2020) Table 11.
A simulated doubling of the carbon tax highlights significant heterogeneity across sectors. Several industries such as furniture, wood products, paper, and textiles experience large reductions in carbon emissions with little job reallocation. On the contrary, the motor vehicles and the plastic industries experience larger job reallocations and smaller declines in carbon emissions. Other industries such as metal products experience large job reallocation and emissions reduction because of their size.
Higher energy prices and carbon taxes are effective at reducing carbon emissions, but costs of job reallocation must be considered…
Although the carbon tax enables the French manufacturing sector to meet its carbon budget anddoes not affect total employment negatively, it however generates non-negligible job reallocations in several industries. Because these reallocation effects have redistributive implications and generate costs for workers who are forced to change jobs, these results call for complementary labour market policies that minimise those costs on affected workers and ease between-firms adjustments in employment. Moreover, since these transition costs are typically highly localised in regions specialised in polluting activities, they can also translate into potentially significant regional effects and thus political costs."	6659
environment	['Oecd Environment Focus']	2020-12-23 00:00:00	"Securing natural capital on land

By Kumi Kitamori and Shanda Moorghen, OECD Environment Directorate

Image credit: Ivy Yin / Shutterstock

We have built our economies on the premise that land is a primary factor of production. Much of our natural capital, such as biodiversity and ecosystem services, is based on land and there is little debate over the importance of land use for economic activity and to feed growing populations. The demographic shift in the last few decades – from just over 3 billion in 1960 to more than 7 billion – has caused a dramatic rise in the demand for food production, with land used for crops and ranching increasing at the expense of natural grasslands and forests. Farmlands account for almost 38% of the global land surface. It is expected that more than 1 billion hectares of additional land, mostly in developing countries, would be converted for agricultural use by 2050 to keep up with current trends. However, the continued pressures on land, and the ecosystem services such as water retention, carbon storage, soil health and all living organisms that it supports, are bound to bring about biodiversity loss and climate change.

For example, notwithstanding varied practices across the world, land use, land-use change, and forestry cause 23% of total greenhouse gas (GHG) emissions globally. Forests also present a significant global carbon stock, and their destruction will affect a large part of the carbon stored on land. We need a transformative change in the way we use land, if we are to continue to reap the benefits of nature and avoid a catastrophic collapse of our planet’s ecosystems. The onus is now on decision-makers to find efficient ways to use land and its natural capital to meet the triple challenge of food security, decent livelihoods for farmers, and environmental sustainability.

The challenges to and solutions for sustainable land use were recently discussed at this year’s OECD Green Growth and Sustainable Development Forum under the theme Securing natural capital: Resilience, risk management and COVID-19. Angel Gurría, OECD Secretary-General, provided opening remarks and highlighted the urgency to protect our natural capital and its ecosystem services, which are worth USD140 trillion per year – more than one and a half times the size of global GDP. Mr. Gurría noted, “We know these facts, we know the evidence, yet global natural capital stocks continue to deteriorate. One-quarter of animal and plant species are facing extinction. On land, deforestation continues, with over 70% of terrestrial land degraded.”

These issues were further explored in a dedicated session on ‘Securing natural capital on land’ during the Forum, with the panellists keen to expand on the realities of sustainable land use in the age of COVID-19.

What were the main takeaways from the discussion on securing natural capital on land?

Here are four actions for governments to promote sustainable land use:

As we look to recover from the COVID-19 pandemic, land use will play an integral part in addressing many of the key socio-economic challenges ahead, including avoiding possible emergence of new zoonotic diseases. Policy makers have the opportunity to provide an innovative, data-driven response to current obstacles, taking into account local communities and indigenous populations, while also ensuring the preservation of natural capital on land and the ecosystem services it provides. It is time to find a balance between economic prosperity and environmental sustainability in order to preserve our natural capital for generations to come.

Further reading

OECD (2020), Towards Sustainable Land use, OECD Publishing, Paris

OECD (2020), Biodiversity and the economic response to COVID-19: Ensuring a green and resilient recovery, OECD Publishing, Paris

OECD (2019), Accelerating Climate Action: Refocusing Policies through a Well-being Lens, OECD Publishing, Paris"	https://oecd-environment-focus.blog/2020/12/23/securing-natural-capital-on-land/	"Securing natural capital on land
By Kumi Kitamori and Shanda Moorghen, OECD Environment Directorate
Image credit: Ivy Yin / Shutterstock
We have built our economies on the premise that land is a primary factor of production. Much of our natural capital, such as biodiversity and ecosystem services, is based on land and there is little debate over the importance of land use for economic activity and to feed growing populations. The demographic shift in the last few decades – from just over 3 billion in 1960 to more than 7 billion – has caused a dramatic rise in the demand for food production, with land used for crops and ranching increasing at the expense of natural grasslands and forests. Farmlands account for almost 38% of the global land surface. It is expected that more than 1 billion hectares of additional land, mostly in developing countries, would be converted for agricultural use by 2050 to keep up with current trends. However, the continued pressures on land, and the ecosystem services such as water retention, carbon storage, soil health and all living organisms that it supports, are bound to bring about biodiversity loss and climate change.
For example, notwithstanding varied practices across the world, land use, land-use change, and forestry cause 23% of total greenhouse gas (GHG) emissions globally. Forests also present a significant global carbon stock, and their destruction will affect a large part of the carbon stored on land. We need a transformative change in the way we use land, if we are to continue to reap the benefits of nature and avoid a catastrophic collapse of our planet’s ecosystems. The onus is now on decision-makers to find efficient ways to use land and its natural capital to meet the triple challenge of food security, decent livelihoods for farmers, and environmental sustainability.
The challenges to and solutions for sustainable land use were recently discussed at this year’s OECD Green Growth and Sustainable Development Forum under the theme Securing natural capital: Resilience, risk management and COVID-19. Angel Gurría, OECD Secretary-General, provided opening remarks and highlighted the urgency to protect our natural capital and its ecosystem services, which are worth USD140 trillion per year – more than one and a half times the size of global GDP. Mr. Gurría noted, “We know these facts, we know the evidence, yet global natural capital stocks continue to deteriorate. One-quarter of animal and plant species are facing extinction. On land, deforestation continues, with over 70% of terrestrial land degraded.”
These issues were further explored in a dedicated session on ‘Securing natural capital on land’ during the Forum, with the panellists keen to expand on the realities of sustainable land use in the age of COVID-19.
What were the main takeaways from the discussion on securing natural capital on land?
Here are four actions for governments to promote sustainable land use:
As we look to recover from the COVID-19 pandemic, land use will play an integral part in addressing many of the key socio-economic challenges ahead, including avoiding possible emergence of new zoonotic diseases. Policy makers have the opportunity to provide an innovative, data-driven response to current obstacles, taking into account local communities and indigenous populations, while also ensuring the preservation of natural capital on land and the ecosystem services it provides. It is time to find a balance between economic prosperity and environmental sustainability in order to preserve our natural capital for generations to come.
Further reading
OECD (2020), Towards Sustainable Land use, OECD Publishing, Paris
OECD (2020), Biodiversity and the economic response to COVID-19: Ensuring a green and resilient recovery, OECD Publishing, Paris
OECD (2019), Accelerating Climate Action: Refocusing Policies through a Well-being Lens, OECD Publishing, Paris"	3921
environment	['Oecd Environment Focus']	2020-12-02 00:00:00	"Is the COVID-19 crisis spurring a transition to net-zero emissions in the oil and gas sector?

By Andrew Prag and Guy Halpern, OECD Environment Directorate

Shutterstock.com / pan demin

Almost all economic sectors have suffered due to the evolving COVID-19 crisis. For the oil and gas industry, already battered on one side by low prices due to an oil price war between Russia and Saudi Arabia, and on the other by the push to decarbonise the global economy, the crisis hit at an especially challenging time.

Nevertheless, an increasing number of international oil and gas companies have set out seemingly ambitious goals to transition to “net-zero” carbon emissions by 2050. What is behind these new announcements? Are they just greenwashing, or do they represent a genuine intent to transform firms in the face of the accelerating energy transition? What do the commitments mean for achieving the Paris Agreement goals, and can they help to convince governments to be bolder and to really deliver on their plans for a “green recovery” after COVID-19?

The oil and gas sector before and during the COVID-19 crisis

Well before COVID-19, there were clear signs that change was coming to some parts of the oil and gas industry. In 2014, CEOs of 12 major firms came together form the Oil and Gas Climate Initiative, later pledging support for the Paris Agreement. In September 2019, coinciding with the UN Climate Summit in New York, the UK Offshore Oil and Gas association (OGUK) published Roadmap to 2035: A Blueprint for Net-zero. In December 2019, Repsol was the first to pledge to become carbon neutral by 2050. By January 2020, the IEA stressed that the driving question for the industry was “should today’s oil and gas companies be viewed only as part of the problem, or could they also be crucial in solving it?”

When COVID-19 hit, the picture changed dramatically. With much of the world on lockdown, demand plummeted. Oil prices even went briefly negative in some US oil futures contracts and investment went into free-fall. With global COVID-19 case numbers still rising in late 2020, with many countries facing a second wave of the virus, it’s clear that the economic impacts of the pandemic will not disappear overnight. Latest numbers from the IEA point to a 35% decline in upstream oil and gas spending in 2020 compared to 2019, compared to an 18% decline in energy investment overall.

And yet in the midst of this turmoil, with the whole oil and gas sector under severe financial stress, more major European integrated oil and gas companies (such as BP, Eni, OMV, Shell, Total and Equinor) have joined Repsol in pledging to achieve net-zero carbon emissions by 2050.

At a time when it might have been tempting to retreat from the climate change debate and focus on shoring up near-term financial performance, these multinationals appear to have decided to step up rather than step back, albeit with varying levels of ambition and depth.

Is this time different?

This is not the first time that oil and gas companies have put forward transformational ambitions. Nearly 20 years ago, BP rebranded to “beyond petroleum”, pledging to control emissions and become leaders in promoting environmental sustainability. Investments did follow, but by 2013 most of BP’s renewable energy assets had been sold off. So, will this time be different?

There does appear to be more momentum than ever before. At least two of the recent commitments – those of BP and ENI – crucially include Scope 3 emissions. Scope 3 emissions encompass indirect emissions across a company’s value chain, including from the use of their products. For an oil and gas company, this implies a radical change of business model, well beyond reducing emissions from the companies’ own operations.

BP in particular made two startling announcements as the pandemic started to take hold. First, the firm cut its long-term oil price forecast for oil by 30%, leading to a write-down of assets by between USD 13 and USD 17.5 billion. Perhaps more surprising than the write-down itself was the reason given: that the pandemic will “accelerate the pace of transition to a lower-carbon economy and energy system”. Second, BP has begun planning with expectations of a carbon price of USD 100 per tonne by 2030 – a big step up from the current USD 40.

And yet key questions remain about the nature of the pledges and how they will be achieved. What does the “net” actually entail – what level of residual emissions, and what kind of offsets or carbon removal? How will firms go about reinventing themselves? Does this mean shifting towards renewable electricity (an entirely different business model), producing low-carbon fuels, eliminating methane emissions (currently around 15% of all energy-related GHG emissions), developing carbon capture, utilisation and storage (CCUS), or all of the above and more? Crucially, when will the necessary investment be forthcoming? As recently as 2019, 99% of firms’ capital investment was flowing into core oil and gas projects – a far cry from reinvention. Even among those firms that have net-zero plans for mid-century, most of them do not include intermediate targets. Clarity on some of these questions may come from the forthcoming Science-based Targets methodology for the oil and gas sector.

Further, the footprint of oil and gas majors that have announced ambitious climate plans remains small relative to overall production. In particular, nationally owned oil companies have been much slower to join the party. But these state-owned firms account for over half of global production (and even greater share of reserves) and control a larger proportion of low-cost oil, making them more likely to continue producing in a low oil price world. That said, October 2020 saw a promising sign of change with Malaysia’s Petronas becoming the first NOC to pledge net zero by 2050.

Why now?

While the devil is in the detail of these pledges, it is clear that they imply a transition to energy firms with little resemblance to those of today, requiring new skills, new business models, and new technologies. What is driving firms to think about such reinvention now, even at a time of severe financial difficulty?

Investor activism is part of the picture, as environmentally conscious stakeholders such as Climate Action 100+ and socially active pension funds like the Church of England have raised pressure for reform. Reputation management, staff retention and anticipation of future regulation are also at play. But changes are also being driven by the market and the need to ensure access to finance in a world where oil is worth less as demand peaks, carbon prices rise, climate risk disclosure becomes the norm, and high-emission and high-cost projects face real risks of becoming stranded assets.

Market signals are already picking up on this new reality. In early October, NextEra, the biggest wind energy producer in the US and one of the biggest solar energy producers, overtook ExxonMobil as the most valuable US energy company by market capitalisation. More broadly, renewables have been more resilient than fossil fuels in the face of the Covid-19 crisis and renewable energy firms in Germany, France, the UK and US have outperformed oil & gas stocks since the onset of the COVID 19 crisis, as well as over the previous ten years.

Enter “green recovery”

To put these pledges in context, the challenge of heading off dangerous climate change is still immense. Because of COVID-19, global emissions are expected to drop steeply in 2020. But to stand a good chance of keeping the increase in global temperature to 1.5 °C above pre-industrial levels, emissions will need to continue to decline at nearly the same rate – around 8% – every year from now until 2030. The global economy simply cannot pick up as it was before.

In that light, the OECD and others have highlighted the potential for stimulus packages to accelerate the transition to net-zero emissions as part of a “green recovery”. Stimulus measures so far announced do include meaningful support for the green transition – in particular in the EU – but much more needs to be done. The OECD reports that while at least 30 countries among the OECD its key partners have included measures to support the transition to greener economies as part of their recovery programmes, many are also planning measures that will likely have direct or indirectly negative impacts on the environment. According to Energy Policy Tracker estimates, G20 countries have so far committed USD 234.73 billion in support to fossil fuel energy versus USD 151.29 billion for clean energy.

In that light, the slew of pledges from the oil and gas sector helps to highlight an opportunity as governments continue to refine recovery packages. What better signal that a green recovery is good for growth and jobs than for some of the world’s most profitable companies to voluntarily pledge their own transformation? Governments can take heed by continuing to reorient energy stimulus towards clean energy, while actively supporting the needed transition in skills and workforce.

Further reading:

OECD Policy Responses to Coronavirus (COVID-19): Making the green recovery work for jobs, income and growth

OECD website focusing on green recovery"	https://oecd-environment-focus.blog/2020/12/02/is-the-covid-19-crisis-spurring-a-transition-to-net-zero-emissions-in-the-oil-and-gas-sector/	"Is the COVID-19 crisis spurring a transition to net-zero emissions in the oil and gas sector?
By Andrew Prag and Guy Halpern, OECD Environment Directorate
Shutterstock.com / pan demin
Almost all economic sectors have suffered due to the evolving COVID-19 crisis. For the oil and gas industry, already battered on one side by low prices due to an oil price war between Russia and Saudi Arabia, and on the other by the push to decarbonise the global economy, the crisis hit at an especially challenging time.
Nevertheless, an increasing number of international oil and gas companies have set out seemingly ambitious goals to transition to “net-zero” carbon emissions by 2050. What is behind these new announcements? Are they just greenwashing, or do they represent a genuine intent to transform firms in the face of the accelerating energy transition? What do the commitments mean for achieving the Paris Agreement goals, and can they help to convince governments to be bolder and to really deliver on their plans for a “green recovery” after COVID-19?
The oil and gas sector before and during the COVID-19 crisis
Well before COVID-19, there were clear signs that change was coming to some parts of the oil and gas industry. In 2014, CEOs of 12 major firms came together form the Oil and Gas Climate Initiative, later pledging support for the Paris Agreement. In September 2019, coinciding with the UN Climate Summit in New York, the UK Offshore Oil and Gas association (OGUK) published Roadmap to 2035: A Blueprint for Net-zero. In December 2019, Repsol was the first to pledge to become carbon neutral by 2050. By January 2020, the IEA stressed that the driving question for the industry was “should today’s oil and gas companies be viewed only as part of the problem, or could they also be crucial in solving it?”
When COVID-19 hit, the picture changed dramatically. With much of the world on lockdown, demand plummeted. Oil prices even went briefly negative in some US oil futures contracts and investment went into free-fall. With global COVID-19 case numbers still rising in late 2020, with many countries facing a second wave of the virus, it’s clear that the economic impacts of the pandemic will not disappear overnight. Latest numbers from the IEA point to a 35% decline in upstream oil and gas spending in 2020 compared to 2019, compared to an 18% decline in energy investment overall.
And yet in the midst of this turmoil, with the whole oil and gas sector under severe financial stress, more major European integrated oil and gas companies (such as BP, Eni, OMV, Shell, Total and Equinor) have joined Repsol in pledging to achieve net-zero carbon emissions by 2050.
At a time when it might have been tempting to retreat from the climate change debate and focus on shoring up near-term financial performance, these multinationals appear to have decided to step up rather than step back, albeit with varying levels of ambition and depth.
Is this time different?
This is not the first time that oil and gas companies have put forward transformational ambitions. Nearly 20 years ago, BP rebranded to “beyond petroleum”, pledging to control emissions and become leaders in promoting environmental sustainability. Investments did follow, but by 2013 most of BP’s renewable energy assets had been sold off. So, will this time be different?
There does appear to be more momentum than ever before. At least two of the recent commitments – those of BP and ENI – crucially include Scope 3 emissions. Scope 3 emissions encompass indirect emissions across a company’s value chain, including from the use of their products. For an oil and gas company, this implies a radical change of business model, well beyond reducing emissions from the companies’ own operations.
BP in particular made two startling announcements as the pandemic started to take hold. First, the firm cut its long-term oil price forecast for oil by 30%, leading to a write-down of assets by between USD 13 and USD 17.5 billion. Perhaps more surprising than the write-down itself was the reason given: that the pandemic will “accelerate the pace of transition to a lower-carbon economy and energy system”. Second, BP has begun planning with expectations of a carbon price of USD 100 per tonne by 2030 – a big step up from the current USD 40.
And yet key questions remain about the nature of the pledges and how they will be achieved. What does the “net” actually entail – what level of residual emissions, and what kind of offsets or carbon removal? How will firms go about reinventing themselves? Does this mean shifting towards renewable electricity (an entirely different business model), producing low-carbon fuels, eliminating methane emissions (currently around 15% of all energy-related GHG emissions), developing carbon capture, utilisation and storage (CCUS), or all of the above and more? Crucially, when will the necessary investment be forthcoming? As recently as 2019, 99% of firms’ capital investment was flowing into core oil and gas projects – a far cry from reinvention. Even among those firms that have net-zero plans for mid-century, most of them do not include intermediate targets. Clarity on some of these questions may come from the forthcoming Science-based Targets methodology for the oil and gas sector.
Further, the footprint of oil and gas majors that have announced ambitious climate plans remains small relative to overall production. In particular, nationally owned oil companies have been much slower to join the party. But these state-owned firms account for over half of global production (and even greater share of reserves) and control a larger proportion of low-cost oil, making them more likely to continue producing in a low oil price world. That said, October 2020 saw a promising sign of change with Malaysia’s Petronas becoming the first NOC to pledge net zero by 2050.
Why now?
While the devil is in the detail of these pledges, it is clear that they imply a transition to energy firms with little resemblance to those of today, requiring new skills, new business models, and new technologies. What is driving firms to think about such reinvention now, even at a time of severe financial difficulty?
Investor activism is part of the picture, as environmentally conscious stakeholders such as Climate Action 100+ and socially active pension funds like the Church of England have raised pressure for reform. Reputation management, staff retention and anticipation of future regulation are also at play. But changes are also being driven by the market and the need to ensure access to finance in a world where oil is worth less as demand peaks, carbon prices rise, climate risk disclosure becomes the norm, and high-emission and high-cost projects face real risks of becoming stranded assets.
Market signals are already picking up on this new reality. In early October, NextEra, the biggest wind energy producer in the US and one of the biggest solar energy producers, overtook ExxonMobil as the most valuable US energy company by market capitalisation. More broadly, renewables have been more resilient than fossil fuels in the face of the Covid-19 crisis and renewable energy firms in Germany, France, the UK and US have outperformed oil & gas stocks since the onset of the COVID 19 crisis, as well as over the previous ten years.
Enter “green recovery”
To put these pledges in context, the challenge of heading off dangerous climate change is still immense. Because of COVID-19, global emissions are expected to drop steeply in 2020. But to stand a good chance of keeping the increase in global temperature to 1.5 °C above pre-industrial levels, emissions will need to continue to decline at nearly the same rate – around 8% – every year from now until 2030. The global economy simply cannot pick up as it was before.
In that light, the OECD and others have highlighted the potential for stimulus packages to accelerate the transition to net-zero emissions as part of a “green recovery”. Stimulus measures so far announced do include meaningful support for the green transition – in particular in the EU – but much more needs to be done. The OECD reports that while at least 30 countries among the OECD its key partners have included measures to support the transition to greener economies as part of their recovery programmes, many are also planning measures that will likely have direct or indirectly negative impacts on the environment. According to Energy Policy Tracker estimates, G20 countries have so far committed USD 234.73 billion in support to fossil fuel energy versus USD 151.29 billion for clean energy.
In that light, the slew of pledges from the oil and gas sector helps to highlight an opportunity as governments continue to refine recovery packages. What better signal that a green recovery is good for growth and jobs than for some of the world’s most profitable companies to voluntarily pledge their own transformation? Governments can take heed by continuing to reorient energy stimulus towards clean energy, while actively supporting the needed transition in skills and workforce.
Further reading:
OECD Policy Responses to Coronavirus (COVID-19): Making the green recovery work for jobs, income and growth
OECD website focusing on green recovery"	9277
environment	[]	2017-07-21 00:00:00	"Renewable-energy technologies are critically important, both in addressing the risks of climate change and achieving Sustainable Development Goal number 7 (SDG7,) relative to affordable and clean energy. They have also become increasingly cost-competitive: the capital cost of utility-scale solar photovoltaic (PV) energy has fallen by more than 60 per cent since 2010, and that of onshore wind energy by 20 per cent.

There is no shortage of capital available globally to finance renewable-energy projects. The financial sector encompasses more than €100 trillion of assets. So how is it that investment in renewable energy is not flowing faster?

The trillion-dollar question is how we can shift incentives and strengthen the right conditions to make solar, wind and other renewable power more attractive to investors. To respect the Paris Agreement’s goal of limiting temperature rise to well below 2°C, annual investment in renewable energy needs to increase by 150 per cent between now and 2050.

New OECD research shows that incoherent policies, misalignments in electricity markets and cumbersome and risky investment conditions are among the main factors holding back investment and innovation in renewable energy in advanced and emerging countries.

In order to meet renewable energy deployment goals, policy makers need to strengthen investment conditions, from investment policy to competition, trade and financial market policy. And most importantly, specific policy incentives and climate policies should not be considered in isolation from the broader environment for investment and innovation in renewable energy.

Creating a supportive framework for renewables

At the policy level, scaling up investment in deployed renewables requires designing targeted incentives such as: feed-in tariffs (i.e. a guaranteed minimum price per unit of renewable power generated); renewable energy certificates (a certificate proving that one unit of electricity was generated from a renewables source, which can be sold separately from the underlying physical electricity associated with a renewable-based generation source); and public tenders (which include public competitive bidding or auctions for a set capacity of renewable power).

Feed-in tariffs and certificates in particular have driven investment in advanced countries, leading, for example, to an 11 per cent increase in renewables investment for each additional unit of feed-in tariff, in USD/KWh. Auctions and tenders have supported renewables investment in emerging markets (OECD analysis shows that, historically, increasing the capacity of a tender by 1 MW leads to a 0.1 per cent increase in renewables investment flows).

Explicit carbon prices (using carbon taxes or emissions trading schemes) have driven investment in renewables in the European Union and in emerging economies, and across OECD and G20 countries in solar energy. But at the same time, pressure from fossil-fuel subsidies in the electricity sector has also deterred renewables investment in emerging economies.

Incompatible incentives are worrying on a number of fronts, not only for investment in deployed renewables but also innovation in earlier-stage renewables technologies. An example: feed-in tariffs stimulate renewable-energy patents, yet policy across OECD countries and emerging economies has been shifting away from FiTs toward public tenders, to adjust to changing market conditions, control the deployment of large-scale renewables, and reduce costs for consumers.

Also, government spending in research, development and demonstration (RD&D) for low-carbon technologies is at historic lows. This has negative implications for innovation; OECD research shows that public RD&D expenditures have thus far played an important role in stimulating patenting in renewables technologies.

In addition to aligning our incentives, we need to take advantage of the fact that some climate mitigation policies enhance the positive effects of other policies when they are combined. For example, setting carbon prices while providing public RD&D spending in renewables technologies has worked well for mobilising renewables investment in emerging markets. In OECD countries, Denmark has become a leader in renewables technologies, including by providing integrated, sector-wide policy support to RD&D and deployment of renewables.

Next, we have to make the investment environment in renewable energy – especially solar and wind energy – far more attractive, and also make it easier to do business, with improvements in the following areas:

Investment policy and investment facilitation (property registration, corruption perception, regulatory quality, licensing and permitting systems),

Competition and trade policy (direct control of the state over enterprises, ease of trading across borders)

Financial access (access to domestic credit for the private sector)

Finally, we need to work at making sure that the broader investment environment isn’t at odds with low-carbon investment. For example, the implementation of Basel III banking regulations – though they are important – also may have had the unintended consequence of constraining access to debt financing for capital-intensive renewable projects. Investment in renewable energy and other low-carbon technologies needs to take place on a far greater scale if we are to achieve the ambition of the Paris Agreement and the Sustainable Development Goals. Evidence-based research and stakeholder co-operation are needed to help policy makers design effective public policies that facilitate the transition to a low-carbon economy. The OECD stands ready to support these critical goals, as part of the OECD Centre on Green Finance and Investment.

♣♣♣

Notes:

This blog post is based on the author’s report The empirics of enabling investment and innovation in renewable energy, co-authored with Dirk Röttgers and Pralhad Burli, OECD Environment Working Papers, No. 123, May 2017.

The post gives the views of its authors, not the position of LSE Business Review or the London School of Economics.

Featured image credit: Solar panels, by skeeze, under a CC0 licence

When you leave a comment, you’re agreeing to our Comment Policy.

Geraldine Ang works as a policy analyst and economist on green finance and investment and climate change in Paris at the Organisation for Economic Co-operation and Development (OECD), which she joined in 2011. She undertakes research and policy analysis to help governments from advanced, emerging and developing countries mobilise private investment in green infrastructure. She has co-authored several reports. Geraldine has contributed to several high-level events, such as the climate conference COP21 in Paris in December 2015. Geraldine holds a Master’s of Public Administration from Columbia University’s School of International and Public Affairs (2011) and a Master in Management from HEC Paris (2006)."	https://blogs.lse.ac.uk/businessreview/2017/07/21/whats-holding-back-investment-and-innovation-in-renewable-energy/	"Renewable-energy technologies are critically important, both in addressing the risks of climate change and achieving Sustainable Development Goal number 7 (SDG7,) relative to affordable and clean energy. They have also become increasingly cost-competitive: the capital cost of utility-scale solar photovoltaic (PV) energy has fallen by more than 60 per cent since 2010, and that of onshore wind energy by 20 per cent.
There is no shortage of capital available globally to finance renewable-energy projects. The financial sector encompasses more than €100 trillion of assets. So how is it that investment in renewable energy is not flowing faster?
The trillion-dollar question is how we can shift incentives and strengthen the right conditions to make solar, wind and other renewable power more attractive to investors. To respect the Paris Agreement’s goal of limiting temperature rise to well below 2°C, annual investment in renewable energy needs to increase by 150 per cent between now and 2050.
New OECD research shows that incoherent policies, misalignments in electricity markets and cumbersome and risky investment conditions are among the main factors holding back investment and innovation in renewable energy in advanced and emerging countries.
In order to meet renewable energy deployment goals, policy makers need to strengthen investment conditions, from investment policy to competition, trade and financial market policy. And most importantly, specific policy incentives and climate policies should not be considered in isolation from the broader environment for investment and innovation in renewable energy.
Creating a supportive framework for renewables
At the policy level, scaling up investment in deployed renewables requires designing targeted incentives such as: feed-in tariffs (i.e. a guaranteed minimum price per unit of renewable power generated); renewable energy certificates (a certificate proving that one unit of electricity was generated from a renewables source, which can be sold separately from the underlying physical electricity associated with a renewable-based generation source); and public tenders (which include public competitive bidding or auctions for a set capacity of renewable power).
Feed-in tariffs and certificates in particular have driven investment in advanced countries, leading, for example, to an 11 per cent increase in renewables investment for each additional unit of feed-in tariff, in USD/KWh. Auctions and tenders have supported renewables investment in emerging markets (OECD analysis shows that, historically, increasing the capacity of a tender by 1 MW leads to a 0.1 per cent increase in renewables investment flows).
Explicit carbon prices (using carbon taxes or emissions trading schemes) have driven investment in renewables in the European Union and in emerging economies, and across OECD and G20 countries in solar energy. But at the same time, pressure from fossil-fuel subsidies in the electricity sector has also deterred renewables investment in emerging economies.
Incompatible incentives are worrying on a number of fronts, not only for investment in deployed renewables but also innovation in earlier-stage renewables technologies. An example: feed-in tariffs stimulate renewable-energy patents, yet policy across OECD countries and emerging economies has been shifting away from FiTs toward public tenders, to adjust to changing market conditions, control the deployment of large-scale renewables, and reduce costs for consumers.
Also, government spending in research, development and demonstration (RD&D) for low-carbon technologies is at historic lows. This has negative implications for innovation; OECD research shows that public RD&D expenditures have thus far played an important role in stimulating patenting in renewables technologies.
In addition to aligning our incentives, we need to take advantage of the fact that some climate mitigation policies enhance the positive effects of other policies when they are combined. For example, setting carbon prices while providing public RD&D spending in renewables technologies has worked well for mobilising renewables investment in emerging markets. In OECD countries, Denmark has become a leader in renewables technologies, including by providing integrated, sector-wide policy support to RD&D and deployment of renewables.
Next, we have to make the investment environment in renewable energy – especially solar and wind energy – far more attractive, and also make it easier to do business, with improvements in the following areas:
Investment policy and investment facilitation (property registration, corruption perception, regulatory quality, licensing and permitting systems),
Competition and trade policy (direct control of the state over enterprises, ease of trading across borders)
Financial access (access to domestic credit for the private sector)
Finally, we need to work at making sure that the broader investment environment isn’t at odds with low-carbon investment. For example, the implementation of Basel III banking regulations – though they are important – also may have had the unintended consequence of constraining access to debt financing for capital-intensive renewable projects. Investment in renewable energy and other low-carbon technologies needs to take place on a far greater scale if we are to achieve the ambition of the Paris Agreement and the Sustainable Development Goals. Evidence-based research and stakeholder co-operation are needed to help policy makers design effective public policies that facilitate the transition to a low-carbon economy. The OECD stands ready to support these critical goals, as part of the OECD Centre on Green Finance and Investment.
♣♣♣
Notes:
This blog post is based on the author’s report The empirics of enabling investment and innovation in renewable energy, co-authored with Dirk Röttgers and Pralhad Burli, OECD Environment Working Papers, No. 123, May 2017.
The post gives the views of its authors, not the position of LSE Business Review or the London School of Economics.
Featured image credit: Solar panels, by skeeze, under a CC0 licence
When you leave a comment, you’re agreeing to our Comment Policy.
Geraldine Ang works as a policy analyst and economist on green finance and investment and climate change in Paris at the Organisation for Economic Co-operation and Development (OECD), which she joined in 2011. She undertakes research and policy analysis to help governments from advanced, emerging and developing countries mobilise private investment in green infrastructure. She has co-authored several reports. Geraldine has contributed to several high-level events, such as the climate conference COP21 in Paris in December 2015. Geraldine holds a Master’s of Public Administration from Columbia University’s School of International and Public Affairs (2011) and a Master in Management from HEC Paris (2006)."	6924
environment	[]	2017-04-19 00:00:00	"Hideki Takada and Rob Youngman, OECD Environment Directorate

We know decarbonisation will require a massive shift of investment away from fossil fuel and into such areas as renewable energy, energy efficiency in buildings and industry, electric vehicles and public transport. A key challenge for policy makers is to understand how to make best use of available policy levers to help accelerate this shift towards low-carbon investment. This includes facilitating the financing of low-carbon investment, including financing through equity investment or – on the debt side – through bank loans and bonds.

Green bonds have gained considerable prominence in recent years as one way to finance the transition to a low-carbon economy. These bonds are an instrument which is used to finance green projects that deliver environmental benefits. The green bond market is still young – it got started only ten years ago – but has experienced rapid growth. With growing market appetite for such bonds, annual issuance rose from just USD 3 billion in 2011 to USD 95 billion in 2016. Many initial green bond issuances were made by public finance institutions such as the European Investment Bank and the World Bank.

Green bonds have become increasingly popular amongst banks, corporates, and national and local governments to finance green projects. In 2016 Apple issued a USD 1.5 billion green bond backing renewable energy for data centres, energy efficiency and green materials, becoming the first technology company to issue a green bond. Other landmark issuances in 2016 included Poland’s sovereign issuance – making it the first country to issue green bonds to fund projects that address climate change. Last year also saw the first municipal green bond issuance in Latin America (Mexico City), which raised USD 50 million to pay for energy-efficient lighting, transit upgrades and water infrastructure. This year, in January, the French government announced the largest sovereign green bond issuance to date – EUR 7 billion – to fund the energy transition.

Why do green bonds trigger such interest?

Bond finance is a natural fit for low-carbon investments such as renewable energy infrastructure, which is characterised by high up-front capital costs and long-dated income streams. They also can offer several benefits to both bond issuers and investors. For example, by issuing green bonds, bond issuers diversify and expand their funding sources by attracting investors who would not normally purchase their bonds. “Over-subscription” of green bonds – i.e. cases where demand exceeds the amount of bonds being issued – can also provide benefits. For example, excess demand for the French sovereign green bond issuance (EUR 23 billion versus the EUR 7 billion actually issued) allowed the government to raise several times more capital than initially targeted. Issuers can also gain reputational benefits by highlighting their green activities. At the same time, green bonds can help investors satisfy ESG (environment, social and governance) objectives while also securing risk adjusted returns.

The new OECD report Mobilising Bond Markets for a Low-Carbon Transition, published today, takes a closer look at the importance of green bonds and policy actions to promote further growth of this market. The report also provides a unique quantitative framework for analysing potential bond market evolution and the contribution it can make to financing key low-carbon sectors: renewable energy, energy efficiency and low-emission vehicles. The analysis provides a projection of the four major markets (China, the European Union, Japan and the United States) between 2015 and 2035 under a two degree scenario identified by the International Energy Agency. The results of the analysis suggest that by 2035 green bonds have the potential to scale to USD 4.7-5.6 trillion in outstanding securities and USD 620-720 billion in annual issuance for these key three sectors in the four markets.

While these figures may seem large on an absolute basis, they are small (approximately 4%) relative to the scale of debt securities markets in general – in 2014 USD 19 trillion of bonds were issued in the four markets and USD 97 trillion of outstanding debt securities were held globally. In these deep pools of capital, there is plenty of room for the green bond market to grow.

The OECD report finds that bond markets have the potential to play a significant role in the transition to a low-carbon economy. Nevertheless, as the green bond market evolves, it faces a range of challenges and barriers. Greater transparency may be needed to avoid confusion, inefficiency and the risk of “greenwashing” where bonds are sold as “green bonds” but projects financed by those bonds do not deliver expected green benefits. Policy makers are faced with the challenge of developing green guidelines and standards and, in particular, defining international rules without imposing overly stringent requirements that could raise issuance costs. Striking a balance between securing market confidence and reducing green transaction costs will be critical and the right set of policies will be crucial. In addition, while the green bond market can facilitate the financing of projects, it cannot itself create a pipeline of bankable projects. Governments will need to set ambitious policies to ensure low-carbon investment needs are met. Ultimately, credible and consistent energy and climate policy and attractiveness of low-carbon projects will be the drivers of investment.

For a closer look at the potential contribution of the green bond market to the low-carbon transition and policy options see: Mobilising Bond Markets for a Low-Carbon Transition just released today.

Join us on 28 April at 13:30 CEST to discuss Green Finance and Investment at our next free OECD Green Talks LIVE webinar. For more information and to register: http://bit.ly/GreenTalks

wk1003mike/shutterstock

Useful links

Mobilising Bond Markets for a Low-Carbon Transition

Green Talks Live: Green finance and investment

Growth, Investment and the Low-Carbon Transition

Centre on Green Finance and Investment

Financing Climate Change Action

Green bonds: Country experiences, barriers and options"	http://oecdinsights.org/2017/04/19/can-green-bonds-fuel-the-low-carbon-transition/	"Hideki Takada and Rob Youngman, OECD Environment Directorate
We know decarbonisation will require a massive shift of investment away from fossil fuel and into such areas as renewable energy, energy efficiency in buildings and industry, electric vehicles and public transport. A key challenge for policy makers is to understand how to make best use of available policy levers to help accelerate this shift towards low-carbon investment. This includes facilitating the financing of low-carbon investment, including financing through equity investment or – on the debt side – through bank loans and bonds.
Green bonds have gained considerable prominence in recent years as one way to finance the transition to a low-carbon economy. These bonds are an instrument which is used to finance green projects that deliver environmental benefits. The green bond market is still young – it got started only ten years ago – but has experienced rapid growth. With growing market appetite for such bonds, annual issuance rose from just USD 3 billion in 2011 to USD 95 billion in 2016. Many initial green bond issuances were made by public finance institutions such as the European Investment Bank and the World Bank.
Green bonds have become increasingly popular amongst banks, corporates, and national and local governments to finance green projects. In 2016 Apple issued a USD 1.5 billion green bond backing renewable energy for data centres, energy efficiency and green materials, becoming the first technology company to issue a green bond. Other landmark issuances in 2016 included Poland’s sovereign issuance – making it the first country to issue green bonds to fund projects that address climate change. Last year also saw the first municipal green bond issuance in Latin America (Mexico City), which raised USD 50 million to pay for energy-efficient lighting, transit upgrades and water infrastructure. This year, in January, the French government announced the largest sovereign green bond issuance to date – EUR 7 billion – to fund the energy transition.
Why do green bonds trigger such interest?
Bond finance is a natural fit for low-carbon investments such as renewable energy infrastructure, which is characterised by high up-front capital costs and long-dated income streams. They also can offer several benefits to both bond issuers and investors. For example, by issuing green bonds, bond issuers diversify and expand their funding sources by attracting investors who would not normally purchase their bonds. “Over-subscription” of green bonds – i.e. cases where demand exceeds the amount of bonds being issued – can also provide benefits. For example, excess demand for the French sovereign green bond issuance (EUR 23 billion versus the EUR 7 billion actually issued) allowed the government to raise several times more capital than initially targeted. Issuers can also gain reputational benefits by highlighting their green activities. At the same time, green bonds can help investors satisfy ESG (environment, social and governance) objectives while also securing risk adjusted returns.
The new OECD report Mobilising Bond Markets for a Low-Carbon Transition, published today, takes a closer look at the importance of green bonds and policy actions to promote further growth of this market. The report also provides a unique quantitative framework for analysing potential bond market evolution and the contribution it can make to financing key low-carbon sectors: renewable energy, energy efficiency and low-emission vehicles. The analysis provides a projection of the four major markets (China, the European Union, Japan and the United States) between 2015 and 2035 under a two degree scenario identified by the International Energy Agency. The results of the analysis suggest that by 2035 green bonds have the potential to scale to USD 4.7-5.6 trillion in outstanding securities and USD 620-720 billion in annual issuance for these key three sectors in the four markets.
While these figures may seem large on an absolute basis, they are small (approximately 4%) relative to the scale of debt securities markets in general – in 2014 USD 19 trillion of bonds were issued in the four markets and USD 97 trillion of outstanding debt securities were held globally. In these deep pools of capital, there is plenty of room for the green bond market to grow.
The OECD report finds that bond markets have the potential to play a significant role in the transition to a low-carbon economy. Nevertheless, as the green bond market evolves, it faces a range of challenges and barriers. Greater transparency may be needed to avoid confusion, inefficiency and the risk of “greenwashing” where bonds are sold as “green bonds” but projects financed by those bonds do not deliver expected green benefits. Policy makers are faced with the challenge of developing green guidelines and standards and, in particular, defining international rules without imposing overly stringent requirements that could raise issuance costs. Striking a balance between securing market confidence and reducing green transaction costs will be critical and the right set of policies will be crucial. In addition, while the green bond market can facilitate the financing of projects, it cannot itself create a pipeline of bankable projects. Governments will need to set ambitious policies to ensure low-carbon investment needs are met. Ultimately, credible and consistent energy and climate policy and attractiveness of low-carbon projects will be the drivers of investment.
For a closer look at the potential contribution of the green bond market to the low-carbon transition and policy options see: Mobilising Bond Markets for a Low-Carbon Transition just released today.
Join us on 28 April at 13:30 CEST to discuss Green Finance and Investment at our next free OECD Green Talks LIVE webinar. For more information and to register: http://bit.ly/GreenTalks
wk1003mike/shutterstock
Useful links
Mobilising Bond Markets for a Low-Carbon Transition
Green Talks Live: Green finance and investment
Growth, Investment and the Low-Carbon Transition
Centre on Green Finance and Investment
Financing Climate Change Action
Green bonds: Country experiences, barriers and options"	6233
environment	[]	2016-11-04 00:00:00	"Maha Skah, OECD Environment Directorate

Are you a city-dweller, concerned about the challenges of urbanisation, resilience and inclusiveness?

Cities and urban areas represent unrivalled concentrations of people, economic growth, commercial networks, and innovation – and have the potential to make a significant contribution to the transition towards a low-carbon world. A starting point would be to explore multi-level governance solutions that allow cities to continue developing in a sustainable manner. But to get there, we must first understand the issues at the core of decision and policy-making between national and subnational levels of government. Tackling the underlying issues is essential to make green growth a reality for all cities, big and small.

The OECD’s fifth Green Growth and Sustainable Development (GGSD) Forum will take place on 9-10 November in Paris. Over the years, it has proven to be a unique space for collaboration and discussion on interconnected policy areas that matter to people, our planet, and our economies. This year, discussions will focus on how cities and national governments can achieve urban green growth by enacting appropriate spatial planning and land-use policies.

Urban populations today are growing at an unprecedented rate with more than half of the global population currently living in towns and cities. Green growth is not just needed at the city level to reconcile sustained economic growth with the urgent imperative to fight climate change – it is also a prerequisite to get ready for the additional three billion urbanites expected by the year 2050. Discussions at the Forum will address how different countries and cities, including rapidly urbanising emerging countries, can design and adopt specific urban green growth models that work for them. The latest OECD publication Green Growth in Bandung under the project Urban Green Growth in Dynamic Asia identifies opportunities for urban green growth in areas such as land use and transport, among others.

The GGSD Forum takes place at an opportune time. Growing recognition by city mayors in recent years has prompted the creation of numerous multi-stakeholder coalitions that address sustainability in urban environments, such as the Covenant of Mayors for Climate & Energy and C40 Cities. Recent discussions at the Habitat III Summit in Quito examined how countries can implement the New Urban Agenda, a far-reaching global agreement to achieve the urbanisation and sustainability targets of the Sustainable Development Goals endorsed by world leaders last year.

With over 90% of all urban areas located in coastal areas, cities are under escalating pressure to deal with the risks of rising sea-levels and destruction due to flooding and powerful storms that also cause financial disruptions to city budget cycles. Most recently, hurricane Matthew was a painful reminder that urban areas continue to be highly vulnerable to the distressing consequences of climate-related natural disasters. One can and should ask what we have learned about improving the resilience of urban infrastructure, and how it can contribute to green growth. In one of the Forum’s parallel sessions co-organised with the World Bank, the conversation will focus on how cities and local governments can be encouraged to invest in greener and more resilient infrastructure projects. Countries need to look towards developing resilient infrastructure to cope with the multiple challenges of climate change, increasing population density and urban sprawl.

The OECD’s GGSD Forum is also an opportunity to bring challenges to the table and tackle them head on. Speakers and panellists from leading research institutions, elected officials, and multidisciplinary experts are encouraged to identify knowledge gaps and suggest recommendations for future work. Bringing together experts from different areas can help authorities at different levels of government determine the best way to implement the green growth agenda, while simultaneously addressing the various economic, social and environmental implications of spatial planning. Cities need to adopt or strengthen a green growth model that takes into account specific local circumstances; however, there is much to be gained from the sharing of expertise among cities within a region. A synthesis report of the Urban Green Growth in Dynamic Asia project to be launched at the Forum on 10 November will examine how urban green growth can be achieved in rapidly expanding Asian economies.

Most importantly, this Forum is about exploring opportunities for local actions that can make a significant contribution to green growth. This includes designing urban green growth policies that range from land-use regulation and planning, taxation, transport, energy efficiency, waste, water management to public procurement. Similarly important is to select adequate green growth indicators and monitoring systems at the subnational levels to track progress. Many cities are aware that there is scope to reduce their energy consumption and overall share of global CO2 emissions by replicating best practices. The GGSD Forum will show that implementing these policies is not only desirable and possible, but that it needs to start now.

Useful links

The 2016 OECD Green Growth and Sustainable Development (GGSD) Forum will focus on:

Inclusive green growth and sustainable development in land policy use.

Urban sprawl.

Innovative approaches to green growth and inclusiveness challenges in cities.

Resilient infrastructure.

Tracking progress on urban green growth and Sustainable Development Goals (SDGs)

Inclusive cities.

Impact of tax policies on land use outcomes."	https://oecdinsights.org/2016/11/04/urban-green-growth-is-about-asking-the-right-questions-at-the-right-time/	"Maha Skah, OECD Environment Directorate
Are you a city-dweller, concerned about the challenges of urbanisation, resilience and inclusiveness?
Cities and urban areas represent unrivalled concentrations of people, economic growth, commercial networks, and innovation – and have the potential to make a significant contribution to the transition towards a low-carbon world. A starting point would be to explore multi-level governance solutions that allow cities to continue developing in a sustainable manner. But to get there, we must first understand the issues at the core of decision and policy-making between national and subnational levels of government. Tackling the underlying issues is essential to make green growth a reality for all cities, big and small.
The OECD’s fifth Green Growth and Sustainable Development (GGSD) Forum will take place on 9-10 November in Paris. Over the years, it has proven to be a unique space for collaboration and discussion on interconnected policy areas that matter to people, our planet, and our economies. This year, discussions will focus on how cities and national governments can achieve urban green growth by enacting appropriate spatial planning and land-use policies.
Urban populations today are growing at an unprecedented rate with more than half of the global population currently living in towns and cities. Green growth is not just needed at the city level to reconcile sustained economic growth with the urgent imperative to fight climate change – it is also a prerequisite to get ready for the additional three billion urbanites expected by the year 2050. Discussions at the Forum will address how different countries and cities, including rapidly urbanising emerging countries, can design and adopt specific urban green growth models that work for them. The latest OECD publication Green Growth in Bandung under the project Urban Green Growth in Dynamic Asia identifies opportunities for urban green growth in areas such as land use and transport, among others.
The GGSD Forum takes place at an opportune time. Growing recognition by city mayors in recent years has prompted the creation of numerous multi-stakeholder coalitions that address sustainability in urban environments, such as the Covenant of Mayors for Climate & Energy and C40 Cities. Recent discussions at the Habitat III Summit in Quito examined how countries can implement the New Urban Agenda, a far-reaching global agreement to achieve the urbanisation and sustainability targets of the Sustainable Development Goals endorsed by world leaders last year.
With over 90% of all urban areas located in coastal areas, cities are under escalating pressure to deal with the risks of rising sea-levels and destruction due to flooding and powerful storms that also cause financial disruptions to city budget cycles. Most recently, hurricane Matthew was a painful reminder that urban areas continue to be highly vulnerable to the distressing consequences of climate-related natural disasters. One can and should ask what we have learned about improving the resilience of urban infrastructure, and how it can contribute to green growth. In one of the Forum’s parallel sessions co-organised with the World Bank, the conversation will focus on how cities and local governments can be encouraged to invest in greener and more resilient infrastructure projects. Countries need to look towards developing resilient infrastructure to cope with the multiple challenges of climate change, increasing population density and urban sprawl.
The OECD’s GGSD Forum is also an opportunity to bring challenges to the table and tackle them head on. Speakers and panellists from leading research institutions, elected officials, and multidisciplinary experts are encouraged to identify knowledge gaps and suggest recommendations for future work. Bringing together experts from different areas can help authorities at different levels of government determine the best way to implement the green growth agenda, while simultaneously addressing the various economic, social and environmental implications of spatial planning. Cities need to adopt or strengthen a green growth model that takes into account specific local circumstances; however, there is much to be gained from the sharing of expertise among cities within a region. A synthesis report of the Urban Green Growth in Dynamic Asia project to be launched at the Forum on 10 November will examine how urban green growth can be achieved in rapidly expanding Asian economies.
Most importantly, this Forum is about exploring opportunities for local actions that can make a significant contribution to green growth. This includes designing urban green growth policies that range from land-use regulation and planning, taxation, transport, energy efficiency, waste, water management to public procurement. Similarly important is to select adequate green growth indicators and monitoring systems at the subnational levels to track progress. Many cities are aware that there is scope to reduce their energy consumption and overall share of global CO2 emissions by replicating best practices. The GGSD Forum will show that implementing these policies is not only desirable and possible, but that it needs to start now.
Useful links
The 2016 OECD Green Growth and Sustainable Development (GGSD) Forum will focus on:
Inclusive green growth and sustainable development in land policy use.
Urban sprawl.
Innovative approaches to green growth and inclusiveness challenges in cities.
Resilient infrastructure.
Tracking progress on urban green growth and Sustainable Development Goals (SDGs)
Inclusive cities.
Impact of tax policies on land use outcomes."	5689
technology	[]	2021-05-21 00:00:00	"Scientists at the University of Colorado Boulder have tapped into a poltergeist-like property of electrons to design devices that can capture excess heat from their environment -- and turn it into usable electricity.

The researchers have described their new ""optical rectennas"" in a paper published today in the journal Nature Communications. These devices, which are too small to see with the naked eye, are roughly 100 times more efficient than similar tools used for energy harvesting. And they achieve that feat through a mysterious process called ""resonant tunneling"" -- in which electrons pass through solid matter without spending any energy.

""They go in like ghosts,"" said lead author Amina Belkadi, who recently earned her PhD from the Department of Electrical, Computer and Energy Engineering (ECEE).

Rectennas (short for ""rectifying antennas""), she explained, work a bit like car radio antennas. But instead of picking up radio waves and turning them into tunes, optical rectennas absorb light and heat and convert it into power.

They're also potential game changers in the world of renewable energy. Working rectennas could, theoretically, harvest the heat coming from factory smokestacks or bakery ovens that would otherwise go to waste. Some scientists have even proposed mounting these devices on airships that would fly high above the planet's surface to capture the energy radiating from Earth to outer space.

But, so far, rectennas haven't been able to reach the efficiencies needed to meet those goals. Until now, perhaps. In the new study, Belkadi and her colleagues have designed the first-ever rectennas that are capable of generating power.

advertisement

""We demonstrate for the first time electrons undergoing resonant tunneling in an energy-harvesting optical rectenna,"" she said. ""Until now, it was only a theoretical possibility.""

Study coauthor Garret Moddel, professor of ECEE, said that the study is a major advance for this technology.

""This innovation makes a significant step toward making rectennas more practical,"" he said. ""Right now, the efficiency is really low, but it's going to increase.""

An unbeatable problem

It's a development that Moddel, who has literally written the book on these devices, has been looking forward to for a long time. Rectennas have been around since 1964 when an engineer named William C. Brown used microwaves to power a small helicopter. They're relatively simple tools, made up of an antenna, which absorbs radiation, and a diode, which converts that energy into DC currents.

advertisement

""It's like a radio receiver that picks up light in the form of electromagnetic waves,"" he said.

The problem, however, is that to capture thermal radiation and not just microwaves, rectennas need to be incredibly small -- many times thinner than a human hair. And that can cause a range of problems. The smaller an electrical device is, for example, the higher its resistance becomes, which can shrink the power output of a rectenna.

""You need this device to have very low resistance, but it also needs to be really responsive to light,"" Belkadi said. ""Anything you do to make the device better in one way would make the other worse.""

For decades, in other words, optical rectennas seemed like a no-win scenario. That is until Belkadi and her colleagues, who include postdoctoral researcher Ayendra Weerakkody, landed on a solution: Why not sidestep that obstacle entirely?

A ghostly solution

The team's approach relies on a strange property of the quantum realm.

Belkadi explained that in a traditional rectenna, electrons must pass through an insulator in order to generate power. These insulators add a lot of resistance to the devices, reducing the amount of electricity that engineers can get out.

In the latest study, however, the researchers decided to add two insulators to their devices, not just one. That addition had the counterintuitive effect of creating an energetic phenomenon called a quantum ""well."" If electrons hit this well with just the right energy, they can use it to tunnel through the two insulators -- experiencing no resistance in the process. It's not unlike a ghost drifting through a wall unperturbed. A graduate student in Moddel's research group had previously theorized that such spectral behavior could be possible in optical rectennas, but, until now, no one had been able to prove it.

""If you choose your materials right and get them at the right thickness, then it creates this sort of energy level where electrons see no resistance,"" Belkadi said. ""They just go zooming through.""

And that means more power. To test the spooky effect, Belkadi and her colleagues arrayed a network of about 250,000 rectennas, which are shaped like tiny bowties, onto a hot plate in the lab. Then they cranked up the heat.

The devices were able to capture less than 1% of the heat produced by the hot plate. But Belkadi thinks that those numbers are only going to go up.

""If we use different materials or change our insulators, then we may be able to make that well deeper,"" she said. ""The deeper the well is, the more electrons can pass all the way through.""

Moddel is looking forward to the day when rectennas sit on top of everything from solar panels on the ground to lighter-than-air vehicles in the air: ""If you can capture heat radiating into deep space, then you can get power anytime, anywhere."""	https://www.sciencedaily.com/releases/2021/05/210518114809.htm	"Scientists at the University of Colorado Boulder have tapped into a poltergeist-like property of electrons to design devices that can capture excess heat from their environment -- and turn it into usable electricity.
The researchers have described their new ""optical rectennas"" in a paper published today in the journal Nature Communications. These devices, which are too small to see with the naked eye, are roughly 100 times more efficient than similar tools used for energy harvesting. And they achieve that feat through a mysterious process called ""resonant tunneling"" -- in which electrons pass through solid matter without spending any energy.
""They go in like ghosts,"" said lead author Amina Belkadi, who recently earned her PhD from the Department of Electrical, Computer and Energy Engineering (ECEE).
Rectennas (short for ""rectifying antennas""), she explained, work a bit like car radio antennas. But instead of picking up radio waves and turning them into tunes, optical rectennas absorb light and heat and convert it into power.
They're also potential game changers in the world of renewable energy. Working rectennas could, theoretically, harvest the heat coming from factory smokestacks or bakery ovens that would otherwise go to waste. Some scientists have even proposed mounting these devices on airships that would fly high above the planet's surface to capture the energy radiating from Earth to outer space.
But, so far, rectennas haven't been able to reach the efficiencies needed to meet those goals. Until now, perhaps. In the new study, Belkadi and her colleagues have designed the first-ever rectennas that are capable of generating power.
advertisement
""We demonstrate for the first time electrons undergoing resonant tunneling in an energy-harvesting optical rectenna,"" she said. ""Until now, it was only a theoretical possibility.""
Study coauthor Garret Moddel, professor of ECEE, said that the study is a major advance for this technology.
""This innovation makes a significant step toward making rectennas more practical,"" he said. ""Right now, the efficiency is really low, but it's going to increase.""
An unbeatable problem
It's a development that Moddel, who has literally written the book on these devices, has been looking forward to for a long time. Rectennas have been around since 1964 when an engineer named William C. Brown used microwaves to power a small helicopter. They're relatively simple tools, made up of an antenna, which absorbs radiation, and a diode, which converts that energy into DC currents.
advertisement
""It's like a radio receiver that picks up light in the form of electromagnetic waves,"" he said.
The problem, however, is that to capture thermal radiation and not just microwaves, rectennas need to be incredibly small -- many times thinner than a human hair. And that can cause a range of problems. The smaller an electrical device is, for example, the higher its resistance becomes, which can shrink the power output of a rectenna.
""You need this device to have very low resistance, but it also needs to be really responsive to light,"" Belkadi said. ""Anything you do to make the device better in one way would make the other worse.""
For decades, in other words, optical rectennas seemed like a no-win scenario. That is until Belkadi and her colleagues, who include postdoctoral researcher Ayendra Weerakkody, landed on a solution: Why not sidestep that obstacle entirely?
A ghostly solution
The team's approach relies on a strange property of the quantum realm.
Belkadi explained that in a traditional rectenna, electrons must pass through an insulator in order to generate power. These insulators add a lot of resistance to the devices, reducing the amount of electricity that engineers can get out.
In the latest study, however, the researchers decided to add two insulators to their devices, not just one. That addition had the counterintuitive effect of creating an energetic phenomenon called a quantum ""well."" If electrons hit this well with just the right energy, they can use it to tunnel through the two insulators -- experiencing no resistance in the process. It's not unlike a ghost drifting through a wall unperturbed. A graduate student in Moddel's research group had previously theorized that such spectral behavior could be possible in optical rectennas, but, until now, no one had been able to prove it.
""If you choose your materials right and get them at the right thickness, then it creates this sort of energy level where electrons see no resistance,"" Belkadi said. ""They just go zooming through.""
And that means more power. To test the spooky effect, Belkadi and her colleagues arrayed a network of about 250,000 rectennas, which are shaped like tiny bowties, onto a hot plate in the lab. Then they cranked up the heat.
The devices were able to capture less than 1% of the heat produced by the hot plate. But Belkadi thinks that those numbers are only going to go up.
""If we use different materials or change our insulators, then we may be able to make that well deeper,"" she said. ""The deeper the well is, the more electrons can pass all the way through.""
Moddel is looking forward to the day when rectennas sit on top of everything from solar panels on the ground to lighter-than-air vehicles in the air: ""If you can capture heat radiating into deep space, then you can get power anytime, anywhere."""	5380
technology	[]	2021-05-21 00:00:00	"With the rise of the digital age, the amount of WiFi sources to transmit information wirelessly between devices has grown exponentially. This results in the widespread use of the 2.4GHz radio frequency that WiFi uses, with excess signals available to be tapped for alternative uses.

To harness this under-utilised source of energy, a research team from the National University of Singapore (NUS) and Japan's Tohoku University (TU) has developed a technology that uses tiny smart devices known as spin-torque oscillators (STOs) to harvest and convert wireless radio frequencies into energy to power small electronics. In their study, the researchers had successfully harvested energy using WiFi-band signals to power a light-emitting diode (LED) wirelessly, and without using any battery.

""We are surrounded by WiFi signals, but when we are not using them to access the Internet, they are inactive, and this is a huge waste. Our latest result is a step towards turning readily-available 2.4GHz radio waves into a green source of energy, hence reducing the need for batteries to power electronics that we use regularly. In this way, small electric gadgets and sensors can be powered wirelessly by using radio frequency waves as part of the Internet of Things. With the advent of smart homes and cities, our work could give rise to energy-efficient applications in communication, computing, and neuromorphic systems,"" said Professor Yang Hyunsoo from the NUS Department of Electrical and Computer Engineering, who spearheaded the project.

The research was carried out in collaboration with the research team of Professor Guo Yong Xin, who is also from the NUS Department of Electrical and Computer Engineering, as well as Professor Shunsuke Fukami and his team from TU. The results were published in Nature Communications on 18 May 2021.

Converting WiFi signals into usable energy

Spin-torque oscillators are a class of emerging devices that generate microwaves, and have applications in wireless communication systems. However, the application of STOs is hindered due to a low output power and broad linewidth.

advertisement

While mutual synchronisation of multiple STOs is a way to overcome this problem, current schemes, such as short-range magnetic coupling between multiple STOs, have spatial restrictions. On the other hand, long-range electrical synchronisation using vortex oscillators is limited in frequency responses of only a few hundred MHz. It also requires dedicated current sources for the individual STOs, which can complicate the overall on-chip implementation.

To overcome the spatial and low frequency limitations, the research team came up with an array in which eight STOs are connected in series. Using this array, the 2.4 GHz electromagnetic radio waves that WiFi uses was converted into a direct voltage signal, which was then transmitted to a capacitor to light up a 1.6-volt LED. When the capacitor was charged for five seconds, it was able to light up the same LED for one minute after the wireless power was switched off.

In their study, the researchers also highlighted the importance of electrical topology for designing on-chip STO systems, and compared the series design with the parallel one. They found that the parallel configuration is more useful for wireless transmission due to better time-domain stability, spectral noise behaviour, and control over impedance mismatch. On the other hand, series connections have an advantage for energy harvesting due to the additive effect of the diode-voltage from STOs.

Commenting on the significance of their results, Dr Raghav Sharma, the first author of the paper, shared, ""Aside from coming up with an STO array for wireless transmission and energy harvesting, our work also demonstrated control over the synchronising state of coupled STOs using injection locking from an external radio-frequency source. These results are important for prospective applications of synchronised STOs, such as fast-speed neuromorphic computing.""

Next steps

To enhance the energy harvesting ability of their technology, the researchers are looking to increase the number of STOs in the array they had designed. In addition, they are planning to test their energy harvesters for wirelessly charging other useful electronic devices and sensors.

The research team also hopes to work with industry partners to explore the development of on-chip STOs for self-sustained smart systems, which can open up possibilities for wireless charging and wireless signal detection systems."	https://www.sciencedaily.com/releases/2021/05/210518114153.htm	"With the rise of the digital age, the amount of WiFi sources to transmit information wirelessly between devices has grown exponentially. This results in the widespread use of the 2.4GHz radio frequency that WiFi uses, with excess signals available to be tapped for alternative uses.
To harness this under-utilised source of energy, a research team from the National University of Singapore (NUS) and Japan's Tohoku University (TU) has developed a technology that uses tiny smart devices known as spin-torque oscillators (STOs) to harvest and convert wireless radio frequencies into energy to power small electronics. In their study, the researchers had successfully harvested energy using WiFi-band signals to power a light-emitting diode (LED) wirelessly, and without using any battery.
""We are surrounded by WiFi signals, but when we are not using them to access the Internet, they are inactive, and this is a huge waste. Our latest result is a step towards turning readily-available 2.4GHz radio waves into a green source of energy, hence reducing the need for batteries to power electronics that we use regularly. In this way, small electric gadgets and sensors can be powered wirelessly by using radio frequency waves as part of the Internet of Things. With the advent of smart homes and cities, our work could give rise to energy-efficient applications in communication, computing, and neuromorphic systems,"" said Professor Yang Hyunsoo from the NUS Department of Electrical and Computer Engineering, who spearheaded the project.
The research was carried out in collaboration with the research team of Professor Guo Yong Xin, who is also from the NUS Department of Electrical and Computer Engineering, as well as Professor Shunsuke Fukami and his team from TU. The results were published in Nature Communications on 18 May 2021.
Converting WiFi signals into usable energy
Spin-torque oscillators are a class of emerging devices that generate microwaves, and have applications in wireless communication systems. However, the application of STOs is hindered due to a low output power and broad linewidth.
advertisement
While mutual synchronisation of multiple STOs is a way to overcome this problem, current schemes, such as short-range magnetic coupling between multiple STOs, have spatial restrictions. On the other hand, long-range electrical synchronisation using vortex oscillators is limited in frequency responses of only a few hundred MHz. It also requires dedicated current sources for the individual STOs, which can complicate the overall on-chip implementation.
To overcome the spatial and low frequency limitations, the research team came up with an array in which eight STOs are connected in series. Using this array, the 2.4 GHz electromagnetic radio waves that WiFi uses was converted into a direct voltage signal, which was then transmitted to a capacitor to light up a 1.6-volt LED. When the capacitor was charged for five seconds, it was able to light up the same LED for one minute after the wireless power was switched off.
In their study, the researchers also highlighted the importance of electrical topology for designing on-chip STO systems, and compared the series design with the parallel one. They found that the parallel configuration is more useful for wireless transmission due to better time-domain stability, spectral noise behaviour, and control over impedance mismatch. On the other hand, series connections have an advantage for energy harvesting due to the additive effect of the diode-voltage from STOs.
Commenting on the significance of their results, Dr Raghav Sharma, the first author of the paper, shared, ""Aside from coming up with an STO array for wireless transmission and energy harvesting, our work also demonstrated control over the synchronising state of coupled STOs using injection locking from an external radio-frequency source. These results are important for prospective applications of synchronised STOs, such as fast-speed neuromorphic computing.""
Next steps
To enhance the energy harvesting ability of their technology, the researchers are looking to increase the number of STOs in the array they had designed. In addition, they are planning to test their energy harvesters for wirelessly charging other useful electronic devices and sensors.
The research team also hopes to work with industry partners to explore the development of on-chip STOs for self-sustained smart systems, which can open up possibilities for wireless charging and wireless signal detection systems."	4533
technology	[]	2021-05-21 00:00:00	"Spintronics is an emerging technology for manufacturing electronic devices that take advantage of electron spin and its associated magnetic properties, instead of using the electrical charge of an electron, to carry information. Antiferromagnetic materials are attracting attention in spintronics, with the expectation of spin operations with higher stability. Unlike ferromagnetic materials, in which atoms align along the same direction like in the typical refrigerator magnets, magnetic atoms inside antiferromagnets have antiparallel spin alignments that cancel out the net magnetization.

Scientists have worked on controlling the alignment of magnetic atoms within antiferromagnetic materials to create magnetic switches. Conventionally, this has been done using a 'field-cooling' procedure, which heats and then cools a magnetic system containing an antiferromagnet, while applying an external magnetic field. However, this process is inefficient for use in many micro- or nano- structured spintronics devices because the spatial resolution of the process itself is not high enough to be applied in a micro- or nano-scale devices.

""We discovered that we can control the antiferromagnetic state by simultaneously applying mechanical vibration and a magnetic field,"" says Jung-Il Hong of DGIST's Spin Nanotech Laboratory. ""The process can replace the conventional heating and cooling approach, which is both inconvenient and harmful to the magnetic material. We hope our new procedure will facilitate the integration of antiferromagnetic materials into spintronics-based micro- and nano-devices.""

Hong and his colleagues combined two layers: a cobalt-iron-boron ferromagnetic film on top of an iridium manganese antiferromagnetic film. The layers were grown on piezoelectric ceramic substrates. Combined application of mechanical vibration and a magnetic field allowed the scientists to control the alignments of magnetic spins repeatedly along any direction desired.

The team aims to continue the search and development of new magnetic phases beyond conventionally classified magnetic materials. ""Historically, new material discovery has led to the development of new technologies,"" says Hong. ""We want our research work to be a seed for new technologies."""	https://www.sciencedaily.com/releases/2021/05/210518114209.htm	"Spintronics is an emerging technology for manufacturing electronic devices that take advantage of electron spin and its associated magnetic properties, instead of using the electrical charge of an electron, to carry information. Antiferromagnetic materials are attracting attention in spintronics, with the expectation of spin operations with higher stability. Unlike ferromagnetic materials, in which atoms align along the same direction like in the typical refrigerator magnets, magnetic atoms inside antiferromagnets have antiparallel spin alignments that cancel out the net magnetization.
Scientists have worked on controlling the alignment of magnetic atoms within antiferromagnetic materials to create magnetic switches. Conventionally, this has been done using a 'field-cooling' procedure, which heats and then cools a magnetic system containing an antiferromagnet, while applying an external magnetic field. However, this process is inefficient for use in many micro- or nano- structured spintronics devices because the spatial resolution of the process itself is not high enough to be applied in a micro- or nano-scale devices.
""We discovered that we can control the antiferromagnetic state by simultaneously applying mechanical vibration and a magnetic field,"" says Jung-Il Hong of DGIST's Spin Nanotech Laboratory. ""The process can replace the conventional heating and cooling approach, which is both inconvenient and harmful to the magnetic material. We hope our new procedure will facilitate the integration of antiferromagnetic materials into spintronics-based micro- and nano-devices.""
Hong and his colleagues combined two layers: a cobalt-iron-boron ferromagnetic film on top of an iridium manganese antiferromagnetic film. The layers were grown on piezoelectric ceramic substrates. Combined application of mechanical vibration and a magnetic field allowed the scientists to control the alignments of magnetic spins repeatedly along any direction desired.
The team aims to continue the search and development of new magnetic phases beyond conventionally classified magnetic materials. ""Historically, new material discovery has led to the development of new technologies,"" says Hong. ""We want our research work to be a seed for new technologies."""	2261
technology	['Carmen Drahl', 'Jake Buehler', 'Maria Temming', 'Charles Q. Choi', 'Tina Hesman Saey', 'Laura Sanders', 'Susan Milius', 'Carolyn Gramling', 'Jonathan Lambert', 'Emily Conover']	2020-08-19 18:00:00-04:00	"A robot beetle goes the distance on its own thanks to a methanol-fueled micromuscle.

Scientists envision that swarms of robotic insects could assist search-and-rescue operations (SN: 5/19/16). But tight spaces are out of reach for robots that must be tethered to an energy source. The new bot, described August 19 in Science Robotics, carries its liquid fuel inside its body.

“I realized the critical issue was power,” says Néstor O. Pérez-Arancibia. His team at the University of Southern California in Los Angeles turned to methanol because in a given mass, it packs over 10 times the energy as tiny batteries.

To turn methanol into motion, the researchers coated a nickel-titanium alloy wire with platinum. The alloy contracts like a muscle when heated, and extends once cool. The platinum generates heat by combusting any methanol vapor that comes in contact with it. By varying the exposure to fuel in a periodic pattern, the temperature varies and the micromuscle accordions. That motion causes the bot’s forelegs to rear up. When the legs scooch back again, the body drags forward.

This beetle bot may weigh only as much as three grains of rice but to crawl it needs a tiny yet mighty source of fuel. Instead of tethering it to a power source, scientists stowed methanol inside the robot’s body, where it powers an artificial micromuscle. That self-contained fuel could be a crucial advance in creating bots that can swarm into tight spaces during search-and-rescue missions.

Excluding fuel, the beetle bot weighs about as much as three grains of rice, on par with live insects. It crawls on flat surfaces while carrying up to 2.6 times its weight. It tackles inclines steeper than the toughest treadmill setting. And it can run for over one hour, Pérez-Arancibia says. With a battery — even a state-of-the-art one — it would run for a few seconds at best, he estimates.

There’s room for improvement: The beetle is slower than comparable robots and can’t be steered. Next-generation prototypes will use the same artificial muscle principle with a speedier, more maneuverable design and a different fuel.

Flying robots are his ultimate goal. Specifically? “We want to do butterflies,” he says."	https://www.sciencenews.org/article/methanol-fuel-beetle-robot	"A robot beetle goes the distance on its own thanks to a methanol-fueled micromuscle.
Scientists envision that swarms of robotic insects could assist search-and-rescue operations (SN: 5/19/16). But tight spaces are out of reach for robots that must be tethered to an energy source. The new bot, described August 19 in Science Robotics, carries its liquid fuel inside its body.
“I realized the critical issue was power,” says Néstor O. Pérez-Arancibia. His team at the University of Southern California in Los Angeles turned to methanol because in a given mass, it packs over 10 times the energy as tiny batteries.
To turn methanol into motion, the researchers coated a nickel-titanium alloy wire with platinum. The alloy contracts like a muscle when heated, and extends once cool. The platinum generates heat by combusting any methanol vapor that comes in contact with it. By varying the exposure to fuel in a periodic pattern, the temperature varies and the micromuscle accordions. That motion causes the bot’s forelegs to rear up. When the legs scooch back again, the body drags forward.
This beetle bot may weigh only as much as three grains of rice but to crawl it needs a tiny yet mighty source of fuel. Instead of tethering it to a power source, scientists stowed methanol inside the robot’s body, where it powers an artificial micromuscle. That self-contained fuel could be a crucial advance in creating bots that can swarm into tight spaces during search-and-rescue missions.
Excluding fuel, the beetle bot weighs about as much as three grains of rice, on par with live insects. It crawls on flat surfaces while carrying up to 2.6 times its weight. It tackles inclines steeper than the toughest treadmill setting. And it can run for over one hour, Pérez-Arancibia says. With a battery — even a state-of-the-art one — it would run for a few seconds at best, he estimates.
There’s room for improvement: The beetle is slower than comparable robots and can’t be steered. Next-generation prototypes will use the same artificial muscle principle with a speedier, more maneuverable design and a different fuel.
Flying robots are his ultimate goal. Specifically? “We want to do butterflies,” he says."	2199
technology	['Oliver Burkeman']	2009-10-23 00:00:00	"Towards the end of the summer of 1969 – a few weeks after the moon landings, a few days after Woodstock, and a month before the first broadcast of Monty Python's Flying Circus – a large grey metal box was delivered to the office of Leonard Kleinrock, a professor at the University of California in Los Angeles. It was the same size and shape as a household refrigerator, and outwardly, at least, it had about as much charm. But Kleinrock was thrilled: a photograph from the time shows him standing beside it, in requisite late-60s brown tie and brown trousers, beaming like a proud father.

Had he tried to explain his excitement to anyone but his closest colleagues, they probably wouldn't have understood. The few outsiders who knew of the box's existence couldn't even get its name right: it was an IMP, or ""interface message processor"", but the year before, when a Boston company had won the contract to build it, its local senator, Ted Kennedy, sent a telegram praising its ecumenical spirit in creating the first ""interfaith message processor"". Needless to say, though, the box that arrived outside Kleinrock's office wasn't a machine capable of fostering understanding among the great religions of the world. It was much more important than that.

It's impossible to say for certain when the internet began, mainly because nobody can agree on what, precisely, the internet is. (This is only partly a philosophical question: it is also a matter of egos, since several of the people who made key contributions are anxious to claim the credit.) But 29 October 1969 – 40 years ago next week – has a strong claim for being, as Kleinrock puts it today, ""the day the infant internet uttered its first words"". At 10.30pm, as Kleinrock's fellow professors and students crowded around, a computer was connected to the IMP, which made contact with a second IMP, attached to a second computer, several hundred miles away at the Stanford Research Institute, and an undergraduate named Charley Kline tapped out a message. Samuel Morse, sending the first telegraph message 125 years previously, chose the portentous phrase: ""What hath God wrought?"" But Kline's task was to log in remotely from LA to the Stanford machine, and there was no opportunity for portentousness: his instructions were to type the command LOGIN.

To say that the rest is history is the emptiest of cliches – but trying to express the magnitude of what began that day, and what has happened in the decades since, is an undertaking that quickly exposes the limits of language. It's interesting to compare how much has changed in computing and the internet since 1969 with, say, how much has changed in world politics. Consider even the briefest summary of how much has happened on the global stage since 1969: the Vietnam war ended; the cold war escalated then declined; the Berlin Wall fell; communism collapsed; Islamic fundamentalism surged. And yet nothing has quite the power to make people in their 30s, 40s or 50s feel very old indeed as reflecting upon the growth of the internet and the world wide web. Twelve years after Charley Kline's first message on the Arpanet, as it was then known, there were still only 213 computers on the network; but 14 years after that, 16 million people were online, and email was beginning to change the world; the first really usable web browser wasn't launched until 1993, but by 1995 we had Amazon, by 1998 Google, and by 2001, Wikipedia, at which point there were 513 million people online. Today the figure is more like 1.7 billion.

Unless you are 15 years old or younger, you have lived through the dotcom bubble and bust, the birth of Friends Reunited and Craigslist and eBay and Facebook and Twitter, blogging, the browser wars, Google Earth, filesharing controversies, the transformation of the record industry, political campaigning, activism and campaigning, the media, publishing, consumer banking, the pornography industry, travel agencies, dating and retail; and unless you're a specialist, you've probably only been following the most attention-grabbing developments. Here's one of countless statistics that are liable to induce feelings akin to vertigo: on New Year's Day 1994 – only yesterday, in other words – there were an estimated 623 websites. In total. On the whole internet. ""This isn't a matter of ego or crowing,"" says Steve Crocker, who was present that day at UCLA in 1969, ""but there has not been, in the entire history of mankind, anything that has changed so dramatically as computer communications, in terms of the rate of change.""

Looking back now, Kleinrock and Crocker are both struck by how, as young computer scientists, they were simultaneously aware that they were involved in something momentous and, at the same time, merely addressing a fairly mundane technical problem. On the one hand, they were there because of the Russian Sputnik satellite launch, in 1957, which panicked the American defence establishment, prompting Eisenhower to channel millions of dollars into scientific research, and establishing Arpa, the Advanced Research Projects Agency, to try to win the arms technology race. The idea was ""that we would not get surprised again,"" said Robert Taylor, the Arpa scientist who secured the money for the Arpanet, persuading the agency's head to give him a million dollars that had been earmarked for ballistic missile research. With another pioneer of the early internet, JCR Licklider, Taylor co-wrote the paper, ""The Computer As A Communication Device"", which hinted at what was to come. ""In a few years, men will be able to communicate more effectively through a machine than face to face,"" they declared. ""That is rather a startling thing to say, but it is our conclusion.""

On the other hand, the breakthrough accomplished that night in 1969 was a decidedly down-to-earth one. The Arpanet was not, in itself, intended as some kind of secret weapon to put the Soviets in their place: it was simply a way to enable researchers to access computers remotely, because computers were still vast and expensive, and the scientists needed a way to share resources. (The notion that the network was designed so that it would survive a nuclear attack is an urban myth, though some of those involved sometimes used that argument to obtain funding.) The technical problem solved by the IMPs wasn't very exciting, either. It was already possible to link computers by telephone lines, but it was glacially slow, and every computer in the network had to be connected, by a dedicated line, to every other computer, which meant you couldn't connect more than a handful of machines without everything becoming monstrously complex and costly. The solution, called ""packet switching"" – which owed its existence to the work of a British physicist, Donald Davies – involved breaking data down into blocks that could be routed around any part of the network that happened to be free, before getting reassembled at the other end.

""I thought this was important, but I didn't really think it was as challenging as what I thought of as the 'real research',"" says Crocker, a genial Californian, now 65, who went on to play a key role in the expansion of the internet. ""I was particularly fascinated, in those days, by artificial intelligence, and by trying to understand how people think. I thought that was a much more substantial and respectable research topic than merely connecting up a few machines. That was certainly useful, but it wasn't art.""

Still, Kleinrock recalls a tangible sense of excitement that night as Kline sat down at the SDS Sigma 7 computer, connected to the IMP, and at the same time made telephone contact with his opposite number at Stanford. As his colleagues watched, he typed the letter L, to begin the word LOGIN.

""Have you got the L?"" he asked, down the phone line. ""Got the L,"" the voice at Stanford responded.

Kline typed an O. ""Have you got the O?""

""Got the O,"" Stanford replied.

Kline typed a G, at which point the system crashed, and the connection was lost. The G didn't make it through, which meant that, quite by accident, the first message ever transmitted across the nascent internet turned out, after all, to be fittingly biblical:

""LO.""

Frenzied visions of a global conscious brain

One of the most intriguing things about the growth of the internet is this: to a select group of technological thinkers, the surprise wasn't how quickly it spread across the world, remaking business, culture and politics – but that it took so long to get off the ground. Even when computers were mainly run on punch-cards and paper tape, there were whispers that it was inevitable that they would one day work collectively, in a network, rather than individually. (Tracing the origins of online culture even further back is some people's idea of an entertaining game: there are those who will tell you that the Talmud, the book of Jewish law, contains a form of hypertext, the linking-and-clicking structure at the heart of the web.) In 1945, the American presidential science adviser, Vannevar Bush, was already imagining the ""memex"", a device in which ""an individual stores all his books, records, and communications"", which would be linked to each other by ""a mesh of associative trails"", like weblinks. Others had frenzied visions of the world's machines turning into a kind of conscious brain. And in 1946, an astonishingly complete vision of the future appeared in the magazine Astounding Science Fiction. In a story entitled A Logic Named Joe, the author Murray Leinster envisioned a world in which every home was equipped with a tabletop box that he called a ""logic"":

""You got a logic in your house. It looks like a vision receiver used to, only it's got keys instead of dials and you punch the keys for what you wanna get . . . you punch 'Sally Hancock's Phone' an' the screen blinks an' sputters an' you're hooked up with the logic in her house an' if somebody answers you got a vision-phone connection. But besides that, if you punch for the weather forecast [or] who was mistress of the White House durin' Garfield's administration . . . that comes on the screen too. The relays in the tank do it. The tank is a big buildin' full of all the facts in creation . . . hooked in with all the other tanks all over the country . . . The only thing it won't do is tell you exactly what your wife meant when she said, 'Oh, you think so, do you?' in that peculiar kinda voice ""

Despite all these predictions, though, the arrival of the internet in the shape we know it today was never a matter of inevitability. It was a crucial idiosyncracy of the Arpanet that its funding came from the American defence establishment – but that the millions ended up on university campuses, with researchers who embraced an anti-establishment ethic, and who in many cases were committedly leftwing; one computer scientist took great pleasure in wearing an anti-Vietnam badge to a briefing at the Pentagon. Instead of smothering their research in the utmost secrecy – as you might expect of a cold war project aimed at winning a technological battle against Moscow – they made public every step of their thinking, in documents known as Requests For Comments.

Deliberately or not, they helped encourage a vibrant culture of hobbyists on the fringes of academia – students and rank amateurs who built their own electronic bulletin-board systems and eventually FidoNet, a network to connect them to each other. An argument can be made that these unofficial tinkerings did as much to create the public internet as did the Arpanet. Well into the 90s, by the time the Arpanet had been replaced by NSFNet, a larger government-funded network, it was still the official position that only academic researchers, and those affiliated to them, were supposed to use the network. It was the hobbyists, making unofficial connections into the main system, who first opened the internet up to allcomers.

What made all of this possible, on a technical level, was simultaneously the dullest-sounding and most crucial development since Kleinrock's first message. This was the software known as TCP/IP, which made it possible for networks to connect to other networks, creating a ""network of networks"", capable of expanding virtually infinitely – which is another way of defining what the internet is. It's for this reason that the inventors of TCP/IP, Vint Cerf and Bob Kahn, are contenders for the title of fathers of the internet, although Kleinrock, understandably, disagrees. ""Let me use an analogy,"" he says. ""You would certainly not credit the birth of aviation to the invention of the jet engine. The Wright Brothers launched aviation. Jet engines greatly improved things.""

The spread of the internet across the Atlantic, through academia and eventually to the public, is a tale too intricate to recount here, though it bears mentioning that British Telecom and the British government didn't really want the internet at all: along with other European governments, they were in favour of a different networking technology, Open Systems Interconnect. Nevertheless, by July 1992, an Essex-born businessman named Cliff Stanford had opened Demon Internet, Britain's first commercial internet service provider. Officially, the public still wasn't meant to be connecting to the internet. ""But it was never a real problem,"" Stanford says today. ""The people trying to enforce that weren't working very hard to make it happen, and the people working to do the opposite were working much harder."" The French consulate in London was an early customer, paying Demon £10 a month instead of thousands of pounds to lease a private line to Paris from BT.

After a year or so, Demon had between 2,000 and 3,000 users, but they weren't always clear why they had signed up: it was as if they had sensed the direction of the future, in some inchoate fashion, but hadn't thought things through any further than that. ""The question we always got was: 'OK, I'm connected – what do I do now?'"" Stanford recalls. ""It was one of the most common questions on our support line. We would answer with 'Well, what do you want to do? Do you want to send an email?' 'Well, I don't know anyone with an email address.' People got connected, but they didn't know what was meant to happen next.""

Fortunately, a couple of years previously, a British scientist based at Cern, the physics laboratory outside Geneva, had begun to answer that question, and by 1993 his answer was beginning to be known to the general public. What happened next was the web.

The birth of the web

I sent my first email in 1994, not long after arriving at university, from a small, under-ventilated computer room that smelt strongly of sweat. Email had been in existence for decades by then – the @ symbol was introduced in 1971, and the first message, according to the programmer who sent it, Ray Tomlinson, was ""something like QWERTYUIOP"". (The test messages, Tomlinson has said, ""were entirely forgettable, and I have, therefore, forgotten them"".) But according to an unscientific poll of friends, family and colleagues, 1994 seems fairly typical: I was neither an early adopter nor a late one. A couple of years later I got my first mobile phone, which came with two batteries: a very large one, for normal use, and an extremely large one, for those occasions on which you might actually want a few hours of power. By the time I arrived at the Guardian, email was in use, but only as an add-on to the internal messaging system, operated via chunky beige terminals with green-on-black screens. It took for ever to find the @ symbol on the keyboard, and I don't remember anything like an inbox, a sent-mail folder, or attachments. I am 34 years old, but sometimes I feel like Methuselah.

I have no recollection of when I first used the world wide web, though it was almost certainly when people still called it the world wide web, or even W3, perhaps in the same breath as the phrase ""information superhighway"", made popular by Al Gore. (Or ""infobahn"": did any of us really, ever, call the internet the ""infobahn""?) For most of us, though, the web is in effect synonymous with the internet, even if we grasp that in technical terms that's inaccurate: the web is simply a system that sits on top of the internet, making it greatly easier to navigate the information there, and to use it as a medium of sharing and communication. But the distinction rarely seems relevant in everyday life now, which is why its inventor, Tim Berners-Lee, has his own legitimate claim to be the progenitor of the internet as we know it. The first ever website was his own, at CERN: info.cern.ch.

The idea that a network of computers might enable a specific new way of thinking about information, instead of just allowing people to access the data on each other's terminals, had been around for as long as the idea of the network itself: it's there in Vannevar Bush's memex, and Murray Leinster's logics. But the grandest expression of it was Project Xanadu, launched in 1960 by the American philosopher Ted Nelson, who imagined – and started to build – a vast repository for every piece of writing in existence, with everything connected to everything else according to a principle he called ""transclusion"". It was also, presciently, intended as a method for handling many of the problems that would come to plague the media in the age of the internet, automatically channelling small royalties back to the authors of anything that was linked. Xanadu was a mind-spinning vision – and at least according to an unflattering portrayal by Wired magazine in 1995, over which Nelson threatened to sue, led those attempting to create it into a rabbit-hole of confusion, backbiting and ""heart-slashing despair"". Nelson continues to develop Xanadu today, arguing that it is a vastly superior alternative to the web. ""WE FIGHT ON,"" the Xanadu website declares, sounding rather beleaguered, not least since the declaration is made on a website.

Web browsers crossed the border into mainstream use far more rapidly than had been the case with the internet itself: Mosaic launched in 1993 and Netscape followed soon after, though it was an embarrassingly long time before Microsoft realised the commercial necessity of getting involved at all. Amazon and eBay were online by 1995. And in 1998 came Google, offering a powerful new way to search the proliferating mass of information on the web. Until not too long before Google, it had been common for search or directory websites to boast about how much of the web's information they had indexed – the relic of a brief period, hilarious in hindsight, when a user might genuinely have hoped to check all the webpages that mentioned a given subject. Google, and others, saw that the key to the web's future would be helping users exclude almost everything on any given topic, restricting search results to the most relevant pages.

Without most of us quite noticing when it happened, the web went from being a strange new curiosity to a background condition of everyday life: I have no memory of there being an intermediate stage, when, say, half the information I needed on a particular topic could be found online, while the other half still required visits to libraries. ""I remember the first time I saw a web address on the side of a truck, and I thought, huh, OK, something's happening here,"" says Spike Ilacqua, who years beforehand had helped found The World, the first commercial internet service provider in the US. Finally, he stopped telling acquaintances that he worked in ""computers"", and started to say that he worked on ""the internet"", and nobody thought that was strange.

It is absurd – though also unavoidable here – to compact the whole of what happened from then onwards into a few sentences: the dotcom boom, the historically unprecedented dotcom bust, the growing ""digital divide"", and then the hugely significant flourishing, over the last seven years, of what became known as Web 2.0. It is only this latter period that has revealed the true capacity of the web for ""generativity"", for the publishing of blogs by anyone who could type, for podcasting and video-sharing, for the undermining of totalitarian regimes, for the use of sites such as Twitter and Facebook to create (and ruin) friendships, spread fashions and rumours, or organise political resistance. But you almost certainly know all this: it's part of what these days, in many parts of the world, we call ""just being alive"".

The most confounding thing of all is that in a few years' time, all this stupendous change will probably seem like not very much change at all. As Crocker points out, when you're dealing with exponential growth, the distance from A to B looks huge until you get to point C, whereupon the distance between A and B looks like almost nothing; when you get to point D, the distance between B and C looks similarly tiny. One day, presumably, everything that has happened in the last 40 years will look like early throat-clearings — mere preparations for whatever the internet is destined to become. We will be the equivalents of the late-60s computer engineers, in their horn-rimmed glasses, brown suits, and brown ties, strange, period-costume characters populating some dimly remembered past.

Will you remember when the web was something you accessed primarily via a computer? Will you remember when there were places you couldn't get a wireless connection? Will you remember when ""being on the web"" was still a distinct concept, something that described only a part of your life, instead of permeating all of it? Will you remember Google?"	https://www.theguardian.com/technology/2009/oct/23/internet-40-history-arpanet	"Towards the end of the summer of 1969 – a few weeks after the moon landings, a few days after Woodstock, and a month before the first broadcast of Monty Python's Flying Circus – a large grey metal box was delivered to the office of Leonard Kleinrock, a professor at the University of California in Los Angeles. It was the same size and shape as a household refrigerator, and outwardly, at least, it had about as much charm. But Kleinrock was thrilled: a photograph from the time shows him standing beside it, in requisite late-60s brown tie and brown trousers, beaming like a proud father.
Had he tried to explain his excitement to anyone but his closest colleagues, they probably wouldn't have understood. The few outsiders who knew of the box's existence couldn't even get its name right: it was an IMP, or ""interface message processor"", but the year before, when a Boston company had won the contract to build it, its local senator, Ted Kennedy, sent a telegram praising its ecumenical spirit in creating the first ""interfaith message processor"". Needless to say, though, the box that arrived outside Kleinrock's office wasn't a machine capable of fostering understanding among the great religions of the world. It was much more important than that.
It's impossible to say for certain when the internet began, mainly because nobody can agree on what, precisely, the internet is. (This is only partly a philosophical question: it is also a matter of egos, since several of the people who made key contributions are anxious to claim the credit.) But 29 October 1969 – 40 years ago next week – has a strong claim for being, as Kleinrock puts it today, ""the day the infant internet uttered its first words"". At 10.30pm, as Kleinrock's fellow professors and students crowded around, a computer was connected to the IMP, which made contact with a second IMP, attached to a second computer, several hundred miles away at the Stanford Research Institute, and an undergraduate named Charley Kline tapped out a message. Samuel Morse, sending the first telegraph message 125 years previously, chose the portentous phrase: ""What hath God wrought?"" But Kline's task was to log in remotely from LA to the Stanford machine, and there was no opportunity for portentousness: his instructions were to type the command LOGIN.
To say that the rest is history is the emptiest of cliches – but trying to express the magnitude of what began that day, and what has happened in the decades since, is an undertaking that quickly exposes the limits of language. It's interesting to compare how much has changed in computing and the internet since 1969 with, say, how much has changed in world politics. Consider even the briefest summary of how much has happened on the global stage since 1969: the Vietnam war ended; the cold war escalated then declined; the Berlin Wall fell; communism collapsed; Islamic fundamentalism surged. And yet nothing has quite the power to make people in their 30s, 40s or 50s feel very old indeed as reflecting upon the growth of the internet and the world wide web. Twelve years after Charley Kline's first message on the Arpanet, as it was then known, there were still only 213 computers on the network; but 14 years after that, 16 million people were online, and email was beginning to change the world; the first really usable web browser wasn't launched until 1993, but by 1995 we had Amazon, by 1998 Google, and by 2001, Wikipedia, at which point there were 513 million people online. Today the figure is more like 1.7 billion.
Unless you are 15 years old or younger, you have lived through the dotcom bubble and bust, the birth of Friends Reunited and Craigslist and eBay and Facebook and Twitter, blogging, the browser wars, Google Earth, filesharing controversies, the transformation of the record industry, political campaigning, activism and campaigning, the media, publishing, consumer banking, the pornography industry, travel agencies, dating and retail; and unless you're a specialist, you've probably only been following the most attention-grabbing developments. Here's one of countless statistics that are liable to induce feelings akin to vertigo: on New Year's Day 1994 – only yesterday, in other words – there were an estimated 623 websites. In total. On the whole internet. ""This isn't a matter of ego or crowing,"" says Steve Crocker, who was present that day at UCLA in 1969, ""but there has not been, in the entire history of mankind, anything that has changed so dramatically as computer communications, in terms of the rate of change.""
Looking back now, Kleinrock and Crocker are both struck by how, as young computer scientists, they were simultaneously aware that they were involved in something momentous and, at the same time, merely addressing a fairly mundane technical problem. On the one hand, they were there because of the Russian Sputnik satellite launch, in 1957, which panicked the American defence establishment, prompting Eisenhower to channel millions of dollars into scientific research, and establishing Arpa, the Advanced Research Projects Agency, to try to win the arms technology race. The idea was ""that we would not get surprised again,"" said Robert Taylor, the Arpa scientist who secured the money for the Arpanet, persuading the agency's head to give him a million dollars that had been earmarked for ballistic missile research. With another pioneer of the early internet, JCR Licklider, Taylor co-wrote the paper, ""The Computer As A Communication Device"", which hinted at what was to come. ""In a few years, men will be able to communicate more effectively through a machine than face to face,"" they declared. ""That is rather a startling thing to say, but it is our conclusion.""
On the other hand, the breakthrough accomplished that night in 1969 was a decidedly down-to-earth one. The Arpanet was not, in itself, intended as some kind of secret weapon to put the Soviets in their place: it was simply a way to enable researchers to access computers remotely, because computers were still vast and expensive, and the scientists needed a way to share resources. (The notion that the network was designed so that it would survive a nuclear attack is an urban myth, though some of those involved sometimes used that argument to obtain funding.) The technical problem solved by the IMPs wasn't very exciting, either. It was already possible to link computers by telephone lines, but it was glacially slow, and every computer in the network had to be connected, by a dedicated line, to every other computer, which meant you couldn't connect more than a handful of machines without everything becoming monstrously complex and costly. The solution, called ""packet switching"" – which owed its existence to the work of a British physicist, Donald Davies – involved breaking data down into blocks that could be routed around any part of the network that happened to be free, before getting reassembled at the other end.
""I thought this was important, but I didn't really think it was as challenging as what I thought of as the 'real research',"" says Crocker, a genial Californian, now 65, who went on to play a key role in the expansion of the internet. ""I was particularly fascinated, in those days, by artificial intelligence, and by trying to understand how people think. I thought that was a much more substantial and respectable research topic than merely connecting up a few machines. That was certainly useful, but it wasn't art.""
Still, Kleinrock recalls a tangible sense of excitement that night as Kline sat down at the SDS Sigma 7 computer, connected to the IMP, and at the same time made telephone contact with his opposite number at Stanford. As his colleagues watched, he typed the letter L, to begin the word LOGIN.
""Have you got the L?"" he asked, down the phone line. ""Got the L,"" the voice at Stanford responded.
Kline typed an O. ""Have you got the O?""
""Got the O,"" Stanford replied.
Kline typed a G, at which point the system crashed, and the connection was lost. The G didn't make it through, which meant that, quite by accident, the first message ever transmitted across the nascent internet turned out, after all, to be fittingly biblical:
""LO.""
Frenzied visions of a global conscious brain
One of the most intriguing things about the growth of the internet is this: to a select group of technological thinkers, the surprise wasn't how quickly it spread across the world, remaking business, culture and politics – but that it took so long to get off the ground. Even when computers were mainly run on punch-cards and paper tape, there were whispers that it was inevitable that they would one day work collectively, in a network, rather than individually. (Tracing the origins of online culture even further back is some people's idea of an entertaining game: there are those who will tell you that the Talmud, the book of Jewish law, contains a form of hypertext, the linking-and-clicking structure at the heart of the web.) In 1945, the American presidential science adviser, Vannevar Bush, was already imagining the ""memex"", a device in which ""an individual stores all his books, records, and communications"", which would be linked to each other by ""a mesh of associative trails"", like weblinks. Others had frenzied visions of the world's machines turning into a kind of conscious brain. And in 1946, an astonishingly complete vision of the future appeared in the magazine Astounding Science Fiction. In a story entitled A Logic Named Joe, the author Murray Leinster envisioned a world in which every home was equipped with a tabletop box that he called a ""logic"":
""You got a logic in your house. It looks like a vision receiver used to, only it's got keys instead of dials and you punch the keys for what you wanna get . . . you punch 'Sally Hancock's Phone' an' the screen blinks an' sputters an' you're hooked up with the logic in her house an' if somebody answers you got a vision-phone connection. But besides that, if you punch for the weather forecast [or] who was mistress of the White House durin' Garfield's administration . . . that comes on the screen too. The relays in the tank do it. The tank is a big buildin' full of all the facts in creation . . . hooked in with all the other tanks all over the country . . . The only thing it won't do is tell you exactly what your wife meant when she said, 'Oh, you think so, do you?' in that peculiar kinda voice ""
Despite all these predictions, though, the arrival of the internet in the shape we know it today was never a matter of inevitability. It was a crucial idiosyncracy of the Arpanet that its funding came from the American defence establishment – but that the millions ended up on university campuses, with researchers who embraced an anti-establishment ethic, and who in many cases were committedly leftwing; one computer scientist took great pleasure in wearing an anti-Vietnam badge to a briefing at the Pentagon. Instead of smothering their research in the utmost secrecy – as you might expect of a cold war project aimed at winning a technological battle against Moscow – they made public every step of their thinking, in documents known as Requests For Comments.
Deliberately or not, they helped encourage a vibrant culture of hobbyists on the fringes of academia – students and rank amateurs who built their own electronic bulletin-board systems and eventually FidoNet, a network to connect them to each other. An argument can be made that these unofficial tinkerings did as much to create the public internet as did the Arpanet. Well into the 90s, by the time the Arpanet had been replaced by NSFNet, a larger government-funded network, it was still the official position that only academic researchers, and those affiliated to them, were supposed to use the network. It was the hobbyists, making unofficial connections into the main system, who first opened the internet up to allcomers.
What made all of this possible, on a technical level, was simultaneously the dullest-sounding and most crucial development since Kleinrock's first message. This was the software known as TCP/IP, which made it possible for networks to connect to other networks, creating a ""network of networks"", capable of expanding virtually infinitely – which is another way of defining what the internet is. It's for this reason that the inventors of TCP/IP, Vint Cerf and Bob Kahn, are contenders for the title of fathers of the internet, although Kleinrock, understandably, disagrees. ""Let me use an analogy,"" he says. ""You would certainly not credit the birth of aviation to the invention of the jet engine. The Wright Brothers launched aviation. Jet engines greatly improved things.""
The spread of the internet across the Atlantic, through academia and eventually to the public, is a tale too intricate to recount here, though it bears mentioning that British Telecom and the British government didn't really want the internet at all: along with other European governments, they were in favour of a different networking technology, Open Systems Interconnect. Nevertheless, by July 1992, an Essex-born businessman named Cliff Stanford had opened Demon Internet, Britain's first commercial internet service provider. Officially, the public still wasn't meant to be connecting to the internet. ""But it was never a real problem,"" Stanford says today. ""The people trying to enforce that weren't working very hard to make it happen, and the people working to do the opposite were working much harder."" The French consulate in London was an early customer, paying Demon £10 a month instead of thousands of pounds to lease a private line to Paris from BT.
After a year or so, Demon had between 2,000 and 3,000 users, but they weren't always clear why they had signed up: it was as if they had sensed the direction of the future, in some inchoate fashion, but hadn't thought things through any further than that. ""The question we always got was: 'OK, I'm connected – what do I do now?'"" Stanford recalls. ""It was one of the most common questions on our support line. We would answer with 'Well, what do you want to do? Do you want to send an email?' 'Well, I don't know anyone with an email address.' People got connected, but they didn't know what was meant to happen next.""
Fortunately, a couple of years previously, a British scientist based at Cern, the physics laboratory outside Geneva, had begun to answer that question, and by 1993 his answer was beginning to be known to the general public. What happened next was the web.
The birth of the web
I sent my first email in 1994, not long after arriving at university, from a small, under-ventilated computer room that smelt strongly of sweat. Email had been in existence for decades by then – the @ symbol was introduced in 1971, and the first message, according to the programmer who sent it, Ray Tomlinson, was ""something like QWERTYUIOP"". (The test messages, Tomlinson has said, ""were entirely forgettable, and I have, therefore, forgotten them"".) But according to an unscientific poll of friends, family and colleagues, 1994 seems fairly typical: I was neither an early adopter nor a late one. A couple of years later I got my first mobile phone, which came with two batteries: a very large one, for normal use, and an extremely large one, for those occasions on which you might actually want a few hours of power. By the time I arrived at the Guardian, email was in use, but only as an add-on to the internal messaging system, operated via chunky beige terminals with green-on-black screens. It took for ever to find the @ symbol on the keyboard, and I don't remember anything like an inbox, a sent-mail folder, or attachments. I am 34 years old, but sometimes I feel like Methuselah.
I have no recollection of when I first used the world wide web, though it was almost certainly when people still called it the world wide web, or even W3, perhaps in the same breath as the phrase ""information superhighway"", made popular by Al Gore. (Or ""infobahn"": did any of us really, ever, call the internet the ""infobahn""?) For most of us, though, the web is in effect synonymous with the internet, even if we grasp that in technical terms that's inaccurate: the web is simply a system that sits on top of the internet, making it greatly easier to navigate the information there, and to use it as a medium of sharing and communication. But the distinction rarely seems relevant in everyday life now, which is why its inventor, Tim Berners-Lee, has his own legitimate claim to be the progenitor of the internet as we know it. The first ever website was his own, at CERN: info.cern.ch.
The idea that a network of computers might enable a specific new way of thinking about information, instead of just allowing people to access the data on each other's terminals, had been around for as long as the idea of the network itself: it's there in Vannevar Bush's memex, and Murray Leinster's logics. But the grandest expression of it was Project Xanadu, launched in 1960 by the American philosopher Ted Nelson, who imagined – and started to build – a vast repository for every piece of writing in existence, with everything connected to everything else according to a principle he called ""transclusion"". It was also, presciently, intended as a method for handling many of the problems that would come to plague the media in the age of the internet, automatically channelling small royalties back to the authors of anything that was linked. Xanadu was a mind-spinning vision – and at least according to an unflattering portrayal by Wired magazine in 1995, over which Nelson threatened to sue, led those attempting to create it into a rabbit-hole of confusion, backbiting and ""heart-slashing despair"". Nelson continues to develop Xanadu today, arguing that it is a vastly superior alternative to the web. ""WE FIGHT ON,"" the Xanadu website declares, sounding rather beleaguered, not least since the declaration is made on a website.
Web browsers crossed the border into mainstream use far more rapidly than had been the case with the internet itself: Mosaic launched in 1993 and Netscape followed soon after, though it was an embarrassingly long time before Microsoft realised the commercial necessity of getting involved at all. Amazon and eBay were online by 1995. And in 1998 came Google, offering a powerful new way to search the proliferating mass of information on the web. Until not too long before Google, it had been common for search or directory websites to boast about how much of the web's information they had indexed – the relic of a brief period, hilarious in hindsight, when a user might genuinely have hoped to check all the webpages that mentioned a given subject. Google, and others, saw that the key to the web's future would be helping users exclude almost everything on any given topic, restricting search results to the most relevant pages.
Without most of us quite noticing when it happened, the web went from being a strange new curiosity to a background condition of everyday life: I have no memory of there being an intermediate stage, when, say, half the information I needed on a particular topic could be found online, while the other half still required visits to libraries. ""I remember the first time I saw a web address on the side of a truck, and I thought, huh, OK, something's happening here,"" says Spike Ilacqua, who years beforehand had helped found The World, the first commercial internet service provider in the US. Finally, he stopped telling acquaintances that he worked in ""computers"", and started to say that he worked on ""the internet"", and nobody thought that was strange.
It is absurd – though also unavoidable here – to compact the whole of what happened from then onwards into a few sentences: the dotcom boom, the historically unprecedented dotcom bust, the growing ""digital divide"", and then the hugely significant flourishing, over the last seven years, of what became known as Web 2.0. It is only this latter period that has revealed the true capacity of the web for ""generativity"", for the publishing of blogs by anyone who could type, for podcasting and video-sharing, for the undermining of totalitarian regimes, for the use of sites such as Twitter and Facebook to create (and ruin) friendships, spread fashions and rumours, or organise political resistance. But you almost certainly know all this: it's part of what these days, in many parts of the world, we call ""just being alive"".
The most confounding thing of all is that in a few years' time, all this stupendous change will probably seem like not very much change at all. As Crocker points out, when you're dealing with exponential growth, the distance from A to B looks huge until you get to point C, whereupon the distance between A and B looks like almost nothing; when you get to point D, the distance between B and C looks similarly tiny. One day, presumably, everything that has happened in the last 40 years will look like early throat-clearings — mere preparations for whatever the internet is destined to become. We will be the equivalents of the late-60s computer engineers, in their horn-rimmed glasses, brown suits, and brown ties, strange, period-costume characters populating some dimly remembered past.
Will you remember when the web was something you accessed primarily via a computer? Will you remember when there were places you couldn't get a wireless connection? Will you remember when ""being on the web"" was still a distinct concept, something that described only a part of your life, instead of permeating all of it? Will you remember Google?"	21630
technology	['Malcolm Gladwell', 'Malcolm Gladwel']	2011-05-16 00:00:00	"In late 1979, a twenty-four-year-old entrepreneur paid a visit to a research center in Silicon Valley called Xerox PARC. He was the co-founder of a small computer startup down the road, in Cupertino. His name was Steve Jobs.

The mouse was conceived by the computer scientist Douglas Engelbart, developed by Xerox PARC, and made marketable by Apple. Illustration by PAUL ROGERS

Xerox PARC was the innovation arm of the Xerox Corporation. It was, and remains, on Coyote Hill Road, in Palo Alto, nestled in the foothills on the edge of town, in a long, low concrete building, with enormous terraces looking out over the jewels of Silicon Valley. To the northwest was Stanford University’s Hoover Tower. To the north was Hewlett-Packard’s sprawling campus. All around were scores of the other chip designers, software firms, venture capitalists, and hardware-makers. A visitor to PARC, taking in that view, could easily imagine that it was the computer world’s castle, lording over the valley below—and, at the time, this wasn’t far from the truth. In 1970, Xerox had assembled the world’s greatest computer engineers and programmers, and for the next ten years they had an unparalleled run of innovation and invention. If you were obsessed with the future in the seventies, you were obsessed with Xerox PARC—which was why the young Steve Jobs had driven to Coyote Hill Road.

Apple was already one of the hottest tech firms in the country. Everyone in the Valley wanted a piece of it. So Jobs proposed a deal: he would allow Xerox to buy a hundred thousand shares of his company for a million dollars—its highly anticipated I.P.O. was just a year away—if PARC would “open its kimono.” A lot of haggling ensued. Jobs was the fox, after all, and PARC was the henhouse. What would he be allowed to see? What wouldn’t he be allowed to see? Some at PARC thought that the whole idea was lunacy, but, in the end, Xerox went ahead with it. One PARC scientist recalls Jobs as “rambunctious”—a fresh-cheeked, caffeinated version of today’s austere digital emperor. He was given a couple of tours, and he ended up standing in front of a Xerox Alto, PARC’s prized personal computer.

An engineer named Larry Tesler conducted the demonstration. He moved the cursor across the screen with the aid of a “mouse.” Directing a conventional computer, in those days, meant typing in a command on the keyboard. Tesler just clicked on one of the icons on the screen. He opened and closed “windows,” deftly moving from one task to another. He wrote on an elegant word-processing program, and exchanged e-mails with other people at PARC, on the world’s first Ethernet network. Jobs had come with one of his software engineers, Bill Atkinson, and Atkinson moved in as close as he could, his nose almost touching the screen. “Jobs was pacing around the room, acting up the whole time,” Tesler recalled. “He was very excited. Then, when he began seeing the things I could do onscreen, he watched for about a minute and started jumping around the room, shouting, ‘Why aren’t you doing anything with this? This is the greatest thing. This is revolutionary!’ ”

Xerox began selling a successor to the Alto in 1981. It was slow and underpowered—and Xerox ultimately withdrew from personal computers altogether. Jobs, meanwhile, raced back to Apple, and demanded that the team working on the company’s next generation of personal computers change course. He wanted menus on the screen. He wanted windows. He wanted a mouse. The result was the Macintosh, perhaps the most famous product in the history of Silicon Valley.

“If Xerox had known what it had and had taken advantage of its real opportunities,” Jobs said, years later, “it could have been as big as I.B.M. plus Microsoft plus Xerox combined—and the largest high-technology company in the world.”

This is the legend of Xerox PARC. Jobs is the Biblical Jacob and Xerox is Esau, squandering his birthright for a pittance. In the past thirty years, the legend has been vindicated by history. Xerox, once the darling of the American high-technology community, slipped from its former dominance. Apple is now ascendant, and the demonstration in that room in Palo Alto has come to symbolize the vision and ruthlessness that separate true innovators from also-rans. As with all legends, however, the truth is a bit more complicated.

After Jobs returned from PARC, he met with a man named Dean Hovey, who was one of the founders of the industrial-design firm that would become known as IDEO. “Jobs went to Xerox PARC on a Wednesday or a Thursday, and I saw him on the Friday afternoon,” Hovey recalled. “I had a series of ideas that I wanted to bounce off him, and I barely got two words out of my mouth when he said, ‘No, no, no, you’ve got to do a mouse.’ I was, like, ‘What’s a mouse?’ I didn’t have a clue. So he explains it, and he says, ‘You know, [the Xerox mouse] is a mouse that cost three hundred dollars to build and it breaks within two weeks. Here’s your design spec: Our mouse needs to be manufacturable for less than fifteen bucks. It needs to not fail for a couple of years, and I want to be able to use it on Formica and my bluejeans.’ From that meeting, I went to Walgreens, which is still there, at the corner of Grant and El Camino in Mountain View, and I wandered around and bought all the underarm deodorants that I could find, because they had that ball in them. I bought a butter dish. That was the beginnings of the mouse.”

I spoke with Hovey in a ramshackle building in downtown Palo Alto, where his firm had started out. He had asked the current tenant if he could borrow his old office for the morning, just for the fun of telling the story of the Apple mouse in the place where it was invented. The room was the size of someone’s bedroom. It looked as if it had last been painted in the Coolidge Administration. Hovey, who is lean and healthy in a Northern California yoga-and-yogurt sort of way, sat uncomfortably at a rickety desk in a corner of the room. “Our first machine shop was literally out on the roof,” he said, pointing out the window to a little narrow strip of rooftop, covered in green outdoor carpeting. “We didn’t tell the planning commission. We went and got that clear corrugated stuff and put it across the top for a roof. We got out through the window.”

He had brought a big plastic bag full of the artifacts of that moment: diagrams scribbled on lined paper, dozens of differently sized plastic mouse shells, a spool of guitar wire, a tiny set of wheels from a toy train set, and the metal lid from a jar of Ralph’s preserves. He turned the lid over. It was filled with a waxlike substance, the middle of which had a round indentation, in the shape of a small ball. “It’s epoxy casting resin,” he said. “You pour it, and then I put Vaseline on a smooth steel ball, and set it in the resin, and it hardens around it.” He tucked the steel ball underneath the lid and rolled it around the tabletop. “It’s a kind of mouse.”

The hard part was that the roller ball needed to be connected to the housing of the mouse, so that it didn’t fall out, and so that it could transmit information about its movements to the cursor on the screen. But if the friction created by those connections was greater than the friction between the tabletop and the roller ball, the mouse would skip. And the more the mouse was used the more dust it would pick up off the tabletop, and the more it would skip. The Xerox PARC mouse was an elaborate affair, with an array of ball bearings supporting the roller ball. But there was too much friction on the top of the ball, and it couldn’t deal with dust and grime.

At first, Hovey set to work with various arrangements of ball bearings, but nothing quite worked. “This was the ‘aha’ moment,” Hovey said, placing his fingers loosely around the sides of the ball, so that they barely touched its surface. “So the ball’s sitting here. And it rolls. I attribute that not to the table but to the oldness of the building. The floor’s not level. So I started playing with it, and that’s when I realized: I want it to roll. I don’t want it to be supported by all kinds of ball bearings. I want to just barely touch it.”

The trick was to connect the ball to the rest of the mouse at the two points where there was the least friction—right where his fingertips had been, dead center on either side of the ball. “If it’s right at midpoint, there’s no force causing it to rotate. So it rolls.”

Hovey estimated their consulting fee at thirty-five dollars an hour; the whole project cost perhaps a hundred thousand dollars. “I originally pitched Apple on doing this mostly for royalties, as opposed to a consulting job,” he recalled. “I said, ‘I’m thinking fifty cents apiece,’ because I was thinking that they’d sell fifty thousand, maybe a hundred thousand of them.” He burst out laughing, because of how far off his estimates ended up being. “Steve’s pretty savvy. He said no. Maybe if I’d asked for a nickel, I would have been fine.”

Here is the first complicating fact about the Jobs visit. In the legend of Xerox PARC, Jobs stole the personal computer from Xerox. But the striking thing about Jobs’s instructions to Hovey is that he didn’t want to reproduce what he saw at PARC. “You know, there were disputes around the number of buttons—three buttons, two buttons, one-button mouse,” Hovey went on. “The mouse at Xerox had three buttons. But we came around to the fact that learning to mouse is a feat in and of itself, and to make it as simple as possible, with just one button, was pretty important.”

So was what Jobs took from Xerox the idea of the mouse? Not quite, because Xerox never owned the idea of the mouse. The PARC researchers got it from the computer scientist Douglas Engelbart, at Stanford Research Institute, fifteen minutes away on the other side of the university campus. Engelbart dreamed up the idea of moving the cursor around the screen with a stand-alone mechanical “animal” back in the mid- nineteen-sixties. His mouse was a bulky, rectangular affair, with what looked like steel roller-skate wheels. If you lined up Engelbart’s mouse, Xerox’s mouse, and Apple’s mouse, you would not see the serial reproduction of an object. You would see the evolution of a concept.

The same is true of the graphical user interface that so captured Jobs’s imagination. Xerox PARC’s innovation had been to replace the traditional computer command line with onscreen icons. But when you clicked on an icon you got a pop-up menu: this was the intermediary between the user’s intention and the computer’s response. Jobs’s software team took the graphical interface a giant step further. It emphasized “direct manipulation.” If you wanted to make a window bigger, you just pulled on its corner and made it bigger; if you wanted to move a window across the screen, you just grabbed it and moved it. The Apple designers also invented the menu bar, the pull-down menu, and the trash can—all features that radically simplified the original Xerox PARC idea.

The difference between direct and indirect manipulation—between three buttons and one button, three hundred dollars and fifteen dollars, and a roller ball supported by ball bearings and a free-rolling ball—is not trivial. It is the difference between something intended for experts, which is what Xerox PARC had in mind, and something that’s appropriate for a mass audience, which is what Apple had in mind. PARC was building a personal computer. Apple wanted to build a popular computer."	https://www.newyorker.com/magazine/2011/05/16/creation-myth	"In late 1979, a twenty-four-year-old entrepreneur paid a visit to a research center in Silicon Valley called Xerox PARC. He was the co-founder of a small computer startup down the road, in Cupertino. His name was Steve Jobs.
The mouse was conceived by the computer scientist Douglas Engelbart, developed by Xerox PARC, and made marketable by Apple. Illustration by PAUL ROGERS
Xerox PARC was the innovation arm of the Xerox Corporation. It was, and remains, on Coyote Hill Road, in Palo Alto, nestled in the foothills on the edge of town, in a long, low concrete building, with enormous terraces looking out over the jewels of Silicon Valley. To the northwest was Stanford University’s Hoover Tower. To the north was Hewlett-Packard’s sprawling campus. All around were scores of the other chip designers, software firms, venture capitalists, and hardware-makers. A visitor to PARC, taking in that view, could easily imagine that it was the computer world’s castle, lording over the valley below—and, at the time, this wasn’t far from the truth. In 1970, Xerox had assembled the world’s greatest computer engineers and programmers, and for the next ten years they had an unparalleled run of innovation and invention. If you were obsessed with the future in the seventies, you were obsessed with Xerox PARC—which was why the young Steve Jobs had driven to Coyote Hill Road.
Apple was already one of the hottest tech firms in the country. Everyone in the Valley wanted a piece of it. So Jobs proposed a deal: he would allow Xerox to buy a hundred thousand shares of his company for a million dollars—its highly anticipated I.P.O. was just a year away—if PARC would “open its kimono.” A lot of haggling ensued. Jobs was the fox, after all, and PARC was the henhouse. What would he be allowed to see? What wouldn’t he be allowed to see? Some at PARC thought that the whole idea was lunacy, but, in the end, Xerox went ahead with it. One PARC scientist recalls Jobs as “rambunctious”—a fresh-cheeked, caffeinated version of today’s austere digital emperor. He was given a couple of tours, and he ended up standing in front of a Xerox Alto, PARC’s prized personal computer.
An engineer named Larry Tesler conducted the demonstration. He moved the cursor across the screen with the aid of a “mouse.” Directing a conventional computer, in those days, meant typing in a command on the keyboard. Tesler just clicked on one of the icons on the screen. He opened and closed “windows,” deftly moving from one task to another. He wrote on an elegant word-processing program, and exchanged e-mails with other people at PARC, on the world’s first Ethernet network. Jobs had come with one of his software engineers, Bill Atkinson, and Atkinson moved in as close as he could, his nose almost touching the screen. “Jobs was pacing around the room, acting up the whole time,” Tesler recalled. “He was very excited. Then, when he began seeing the things I could do onscreen, he watched for about a minute and started jumping around the room, shouting, ‘Why aren’t you doing anything with this? This is the greatest thing. This is revolutionary!’ ”
Xerox began selling a successor to the Alto in 1981. It was slow and underpowered—and Xerox ultimately withdrew from personal computers altogether. Jobs, meanwhile, raced back to Apple, and demanded that the team working on the company’s next generation of personal computers change course. He wanted menus on the screen. He wanted windows. He wanted a mouse. The result was the Macintosh, perhaps the most famous product in the history of Silicon Valley.
“If Xerox had known what it had and had taken advantage of its real opportunities,” Jobs said, years later, “it could have been as big as I.B.M. plus Microsoft plus Xerox combined—and the largest high-technology company in the world.”
This is the legend of Xerox PARC. Jobs is the Biblical Jacob and Xerox is Esau, squandering his birthright for a pittance. In the past thirty years, the legend has been vindicated by history. Xerox, once the darling of the American high-technology community, slipped from its former dominance. Apple is now ascendant, and the demonstration in that room in Palo Alto has come to symbolize the vision and ruthlessness that separate true innovators from also-rans. As with all legends, however, the truth is a bit more complicated.
After Jobs returned from PARC, he met with a man named Dean Hovey, who was one of the founders of the industrial-design firm that would become known as IDEO. “Jobs went to Xerox PARC on a Wednesday or a Thursday, and I saw him on the Friday afternoon,” Hovey recalled. “I had a series of ideas that I wanted to bounce off him, and I barely got two words out of my mouth when he said, ‘No, no, no, you’ve got to do a mouse.’ I was, like, ‘What’s a mouse?’ I didn’t have a clue. So he explains it, and he says, ‘You know, [the Xerox mouse] is a mouse that cost three hundred dollars to build and it breaks within two weeks. Here’s your design spec: Our mouse needs to be manufacturable for less than fifteen bucks. It needs to not fail for a couple of years, and I want to be able to use it on Formica and my bluejeans.’ From that meeting, I went to Walgreens, which is still there, at the corner of Grant and El Camino in Mountain View, and I wandered around and bought all the underarm deodorants that I could find, because they had that ball in them. I bought a butter dish. That was the beginnings of the mouse.”
I spoke with Hovey in a ramshackle building in downtown Palo Alto, where his firm had started out. He had asked the current tenant if he could borrow his old office for the morning, just for the fun of telling the story of the Apple mouse in the place where it was invented. The room was the size of someone’s bedroom. It looked as if it had last been painted in the Coolidge Administration. Hovey, who is lean and healthy in a Northern California yoga-and-yogurt sort of way, sat uncomfortably at a rickety desk in a corner of the room. “Our first machine shop was literally out on the roof,” he said, pointing out the window to a little narrow strip of rooftop, covered in green outdoor carpeting. “We didn’t tell the planning commission. We went and got that clear corrugated stuff and put it across the top for a roof. We got out through the window.”
He had brought a big plastic bag full of the artifacts of that moment: diagrams scribbled on lined paper, dozens of differently sized plastic mouse shells, a spool of guitar wire, a tiny set of wheels from a toy train set, and the metal lid from a jar of Ralph’s preserves. He turned the lid over. It was filled with a waxlike substance, the middle of which had a round indentation, in the shape of a small ball. “It’s epoxy casting resin,” he said. “You pour it, and then I put Vaseline on a smooth steel ball, and set it in the resin, and it hardens around it.” He tucked the steel ball underneath the lid and rolled it around the tabletop. “It’s a kind of mouse.”
The hard part was that the roller ball needed to be connected to the housing of the mouse, so that it didn’t fall out, and so that it could transmit information about its movements to the cursor on the screen. But if the friction created by those connections was greater than the friction between the tabletop and the roller ball, the mouse would skip. And the more the mouse was used the more dust it would pick up off the tabletop, and the more it would skip. The Xerox PARC mouse was an elaborate affair, with an array of ball bearings supporting the roller ball. But there was too much friction on the top of the ball, and it couldn’t deal with dust and grime.
At first, Hovey set to work with various arrangements of ball bearings, but nothing quite worked. “This was the ‘aha’ moment,” Hovey said, placing his fingers loosely around the sides of the ball, so that they barely touched its surface. “So the ball’s sitting here. And it rolls. I attribute that not to the table but to the oldness of the building. The floor’s not level. So I started playing with it, and that’s when I realized: I want it to roll. I don’t want it to be supported by all kinds of ball bearings. I want to just barely touch it.”
The trick was to connect the ball to the rest of the mouse at the two points where there was the least friction—right where his fingertips had been, dead center on either side of the ball. “If it’s right at midpoint, there’s no force causing it to rotate. So it rolls.”
Hovey estimated their consulting fee at thirty-five dollars an hour; the whole project cost perhaps a hundred thousand dollars. “I originally pitched Apple on doing this mostly for royalties, as opposed to a consulting job,” he recalled. “I said, ‘I’m thinking fifty cents apiece,’ because I was thinking that they’d sell fifty thousand, maybe a hundred thousand of them.” He burst out laughing, because of how far off his estimates ended up being. “Steve’s pretty savvy. He said no. Maybe if I’d asked for a nickel, I would have been fine.”
Here is the first complicating fact about the Jobs visit. In the legend of Xerox PARC, Jobs stole the personal computer from Xerox. But the striking thing about Jobs’s instructions to Hovey is that he didn’t want to reproduce what he saw at PARC. “You know, there were disputes around the number of buttons—three buttons, two buttons, one-button mouse,” Hovey went on. “The mouse at Xerox had three buttons. But we came around to the fact that learning to mouse is a feat in and of itself, and to make it as simple as possible, with just one button, was pretty important.”
So was what Jobs took from Xerox the idea of the mouse? Not quite, because Xerox never owned the idea of the mouse. The PARC researchers got it from the computer scientist Douglas Engelbart, at Stanford Research Institute, fifteen minutes away on the other side of the university campus. Engelbart dreamed up the idea of moving the cursor around the screen with a stand-alone mechanical “animal” back in the mid- nineteen-sixties. His mouse was a bulky, rectangular affair, with what looked like steel roller-skate wheels. If you lined up Engelbart’s mouse, Xerox’s mouse, and Apple’s mouse, you would not see the serial reproduction of an object. You would see the evolution of a concept.
The same is true of the graphical user interface that so captured Jobs’s imagination. Xerox PARC’s innovation had been to replace the traditional computer command line with onscreen icons. But when you clicked on an icon you got a pop-up menu: this was the intermediary between the user’s intention and the computer’s response. Jobs’s software team took the graphical interface a giant step further. It emphasized “direct manipulation.” If you wanted to make a window bigger, you just pulled on its corner and made it bigger; if you wanted to move a window across the screen, you just grabbed it and moved it. The Apple designers also invented the menu bar, the pull-down menu, and the trash can—all features that radically simplified the original Xerox PARC idea.
The difference between direct and indirect manipulation—between three buttons and one button, three hundred dollars and fifteen dollars, and a roller ball supported by ball bearings and a free-rolling ball—is not trivial. It is the difference between something intended for experts, which is what Xerox PARC had in mind, and something that’s appropriate for a mass audience, which is what Apple had in mind. PARC was building a personal computer. Apple wanted to build a popular computer."	11523
technology	[]		"Will Leitch, senior writer at Sports On Earth, culture writer for Bloomberg Politics, contributing editor at New York magazine and founder of Deadspin, is doing his yearly fill-in for Drew Magary on today's Thursday Afternoon NFL Dick Joke Jamboroo. (Here is 2011's version, and here's 2012's and here's 2013's.) Leitch has written four books. Find more of his business at his Twitter feed and his official site.

Advertisement

In 2012, actor Rob Schneider, famous for something or other, spoke to a California television station about AB 2109, a California bill that required parents to get a physician's approval to opt out of vaccinating their children (something no sentient physician would ever approve). I only came across this interview recently. It is amazing.

You can almost follow along with Schneider's browser history as he continues to ramble on; there's the mom message board, there's the InfoWars THINGS THEY DON'T WANT YOU TO KNOW thread, there's the blog of the doctor with the degree-by-mail who is the only one willing to tell parents the truth. You can tell Schneider spent all night preparing for this interview, jotting down the words he wanted to emphasize, ""efficacy,"" ""toxicity,"" ""Nuremberg laws,"" ""forced sterilization."" He even ends with ""people have to stand up and get educated. Know all the facts.""

In this four-minute clip, I think you can encapsulate the last two years of American culture.

To believe that vaccines cause autism, that doctors are involved in a massive conspiracy to attack our children to help their pharmaceutical pals play golf, that the government sees vaccinations as a step toward forced sterilization and eventual death panels, that anyone on earth should ever wear that hat ... the only way that believing these things is possible is by talking specifically, and solely, to people who already agree with you. To believe these things is to ignore every published bit of medical research, to make up theories that require an otherworldly level of conspiracy and coordination in a country that can't even figure out HTML, to shut out every possible tidbit of contradictory evidence. That you should vaccinate your child is so plainly obvious and thoroughly documented that believing otherwise requires total disengagement with anything in the world that contradicts your narrow worldview. You have to be actively trying to get it wrong.

G/O Media may get a commission Exclusive for new customers Caliper CBD $35 at Caliper Use the promo code KINJATEN

And tons of people still think you shouldn't vaccinate your children! Twenty percent of the country, according to a University of Chicago study, believe doctors know vaccination causes autism but force the shots on children anyway. Again: This is an impossible belief to hold. Now, your response might just be ""well, those people are idiots,"" and I suppose that's the simplest explanation.

But I think it's more than that. I think that they are convinced they're right because they are only talking to other people who are convinced they are right. They have blocked out opposing voices — because they can. If you are an anti-vaccine activist, you can read so much ""information"" supporting your position that, as far as you can tell, you are right. That's what Schneider's talking about up there, that ""get educated"" business. Schneider doesn't see these beliefs as theories, or even as ""beliefs:"" He sees them as stone cold facts. On something about which he is so obviously wrong.

Advertisement

So here's my question: If 20 percent of the country can be so wrong on something so clearly incorrect (and harmful) as child vaccination, primarily because they can choose their evidence over your evidence ... what hope do any of us have? Because the rest of the world is helluva lot more complicated and confusing than whether or not to vaccinate your damn kid.

Chris Rock, when he was doing his big Truth Bomb press tour to promote Top Five, said something fascinating about the difference between President Bush and President Obama. He called Bush a ""cable network"" President; unlike Obama, he only catered to his subscribers. Rock also, astutely, points out ""whoever's the next president will do what Bush did.""

Advertisement

I'm not exactly certain that's true — I have no idea who the subscribers to, say, the Hillary Clinton cable channel are — but in the long term, there is zero doubt that he is right. You see this in every aspect of American life, from entertainment (where the only things anyone watches communally are sports, live musicals, or zombies attempting to eat the brains of thinly drawn caricatures) to politics (where the Republicans just won back the House the same way Bush beat Kerry, by appealing only to their base and not worrying about anyone else) to media (which is so fractured and desperate that it'll pump up whatever dumb Twitter shitstorm happens to be invading their feeds that afternoon, throw it on their front page, and pray; basically, outrage has become America's Assignment Desk). We are run by niche cultures right now. We've seen it from Gamergate to Sony Pictures to you name it. We don't have to build coalitions anymore; we just have to build a bigger coalition than you. We don't have to be right; we just have to be louder than the other guys. It's like that old joke about being chased by a bear.

This increased niche culture is a trademark of the web, and we used to think of it as a positive one; 20 years ago, if you didn't know any Quentin Tarantino or Woody Allen obsessives near you, you could go online and find them. (Theoretically.) The internet opened up a world that was truly revolutionary. But now, now that we're all online, and any novelty to this fact has worn off, the internet has closed that world. We now only have to interact with people who agree with us; if I use Twitter as my primary news source, as so many people do, I can carefully curate my feed to exclude anyone who disagrees with me about anything. (And if someone who slips in there who does, I can call them a horrible person.) Pauline Kael, the late film critic for the New Yorker, was once lambasted (unfairly, and inaccurately) for saying she couldn't believe Nixon was elected because she only personally knew one person who voted for him. But this is now accepted public policy. You don't have to find anyone to contradict you, if you don't want to.

Advertisement

This isn't just common practice now: This is how you win. The entire strategy for succeeding at anything, whether it's winning elections, selling a product or attracting visitors for your Website, revolves around pitching yourself as loudly as you can to those people on your side and turning those who disagree with you into the worst version of themselves, demonizing them into something subhuman and venal. Nuance is tossed out, even if you know a situation is desperately nuanced, in favor of quick points and splash; we've all become the New York Post.

This is simply how communication is done now. The idea of unifying anyone on anything is passé, old thinking, a waste of time. A horrible tragedy happens, and your first reaction, rather than taking a moment to mourn or quietly search for some grace and peace, is instead to start screaming and claiming that those with whom you disagree have blood on their hands. You are rewarded with this by the top slot on the news, a video that goes viral, and everyone on your side applauding you. And when you accept that's all you want to do—to turn away from the fundamental complexity at the heart of the human experience—you find you have no reason to return: After all, every time you say something loudly and strongly enough, the people who agree with you tell you how great you are. Those who disagree? Fuck the haters. Sic 'em, guys.

Advertisement

It can be so demoralizing, so exhausting, to watch this day after day after day. We have begun to shout at those with whom we disagree as if they are terrible drivers and we're within the safety of our own cars; they're the anonymous, faceless monsters we shower with the worst possible motives, just because they happen to be in our way when we're in a hurry. Except they can hear us. And so can everyone else.

So one tries to find hope.

I tend to find it outside, where people, you know, are. Because we drop this act during those strange, disorienting times when we find ourselves in mixed company, lo, real life. The things we do online, or when we think someone is watching, we don't do these things in the real world. In regular, everyday life, we accept all the time that those who disagree with us exist; sometimes we even like them.

Advertisement

They're our families, they're our friends, they're our neighbors, they're the people we open the door for at the supermarket. They're human beings, idiot, scared, just-trying-to-hang-on human beings like every single one of the rest of us. The world is uncertain and terrifying; life is hard and bewildering and unpredictable. We allow for this in our daily interactions in a way we do not in our virtual conversations. We accept human frailty, that we do not share a cerebral cortex with every other person on the planet and therefore will not always see eye-to-eye on all matters. The world is a massive place. There are currently 7.28 billion people on the planet and every single one of them is different. This is a good thing: This is humbling. This is an acceptance that we're all stupid, that we're all overwhelmed, that we're all trying, dammit.

We all have friends and family who believe things we personally find abhorrent. They do things that drive us crazy. Their faults flash brightly above their heads every day. And none of these things matter. We still find a way to love them anyway. Not everybody is just like everybody else. This is a good thing. This might be the only thing.

Advertisement

The Games

All games in the Jamboroo are evaluated for sheer watchability on a scale of 1 to 5 Throwgasms.

Advertisement

Five Throwgasms

Lions at Packers. Whew. I swear to God, I write about four different pieces a day, every day, all year, and no single piece stresses me out more than the lead to my fill-in week for Drew at the Jamboroo every December. I have no idea why. It just wears me out. I'm already glad it's over and can just get to the football part down here. Anyway: Hi! I'm Will. I started this joint — back when ""Kinja"" was three Hungarian man-boys being pelted with Werther's Originals until they pedaled fast enough to reboot the Gawker server — though now I'm just the guy who pops up every once in a while with the pretentious movie reviews and the B- ratings. Don't worry, Drew will be back next week for your ALL CAPS GOODNESS; I'll be out of here soon. It's Christmas. Just be happy you're getting any free content at all. I am fairly certain I'm the only person other than LeBron James working today.

Advertisement

Bengals at Steelers. Like everybody else in the professional media, I wrote constantly about Stephen Colbert last week; in our backlash-to-the-backlash-before-anyone-has-even-lashed-in-the-first-place culture, by Friday, everyone was so sick of Colbert tributes they'd forgotten why they liked him in the first place. But just you wait: When David Letterman retires this May, it's going to be that times infinity times a googol. Literally every single person — even dead people! maybe even some farmyard animals! — is going to tell you about how much Letterman meant to them, how he was an inspiration to them, how their whole idea of comedy was forged by David Letterman. These pieces are already starting, so get yours in quick. No criticism from me, though: I will read every single one of these, and I'll probably write a few myself. I apologize for all of us in advance.

Panthers at Falcons. For all the talk about the travesty of an NFC South team finishing below .500 and still hosting a playoff game ... it is worth noting that whoever wins this game is almost certainly going to be favored over an 11-or-12-win Buzzsaw That Is The Arizona Cardinals team in the first round of the playoffs. The 49ers, who haven't won in more than a month, are six-point favorites over the Buzzsaw this week. But we'll get to that.

Advertisement

Chargers at Chiefs. Thank you all, by the way, for taking time out from your schedule of hopping from theater to theater to see The Interview and popping in to read this. I made a big fuss out of this myself, but I'd bet now that the movie is actually showing in theaters not owned by spineless weasels, people won't bother to go see it. It makes sense that this would be the cycle:

Stage One: I don't want to watch The Interview.

Stage Two: I can't watch The Interview?

Stage Three: I must watch The Interview!

Stage Four: Oh, I can watch The Interview now?

Stage Five: I don't want to watch The Interview.

Browns at Ravens. The worst part about this NFL season, now that it's almost over: That we didn't get one last cameo appearance from Rex Grossman, who turned down the Browns' offer to join the team for this final game. I miss the Sex Cannon. I'd have liked to have been able to say goodbye one last time.

Advertisement

Jaguars at Texans. Before I decided to once again spend 1,600 words sniffing my own rectum, I was originally going to start this column with a thought project: Who is the next Bill Cosby? I don't mean ""who will start systematically drugging and assaulting women over a 40 year period,"" because we all know that's obviously going to be Jonah Hill. I mean: What's the story that media sort of glossed over at some point in last 20 years, only to have it explode on social media when the person involved attempts to promote something? After all, for all the talk about Hannibal Buress, the first time I'd seen Cosby's crimes brought up this year was Tom Scocca's ""knock knock oh yeah Bill Cosby is a rapist"" post back in February. That's all it takes, you know. Just a little push. So what are we going to remember that we didn't investigate next? Seriously, I'd love to hear your suggestions. I'll get a reporter right on it, unless of course I really admire the work of the person who committed all those crimes.

Advertisement

Rams at Seahawks. After my Buzzsaw That Is The Arizona Cardinals won that hideous Thursday night game two weeks ago, SB Nation's Matt Ufford, a Seahawks fan, taunted me on Twitter:

Advertisement

I've known Matt a long time — here we are flanking a nearly dead Drew Magary in 2007 — so I was eager to slap him back. But my Twitter revenge would be a dish best served cold! So I decided not to respond in the moment, instead waiting until my Buzzsaw had shocked the world Sunday night by beating Seattle and clinching home field throughout the NFC (and NFL) playoffs. I was all set to go: ""@mattufford #garbageteam"" would ROCK MATT'S WORLD as he watched his Seahawks fall to the mighty Buzzsaw! That would show Matt! That'll learn him to insult my favorite football franchise over social media! YOU MESS WITH THE BULL YOU GET THE HORNS.

Cardinals at 49ers. You know, I didn't end up sending any Tweets Ufford's way.

Advertisement

Cowboys at Washington Football Team. And that's it! Those are all the games with playoff implications. And yet I'm still going to try to write about the rest of them. Or at least mention the matchup briefly before heading into something entirely different like I've been doing with this column up to this point.

Colts at Titans. Jack Kogod raised an important question yesterday:

Advertisement

This seems like a job for Mark Lisanti, but I'm sympathetic to the cause. I think I like @HofC's suggestion: ""Douchebruhs."" I'm surprised this assignation doesn't already exist.

Saints at Buccaneers. Hey, congratulations to Craggs, by the way! Though now I'm not sure whom to bug about the fact that Deadspin appears to have forgotten to do its Hall of Fame this year. Is this like when one of those asshole 90-year-old baseball writers leaves his ballot blank? Or is that just over now? [Ed note: We just did it. You happy?]

Advertisement

Eagles at Giants. Cutler on the Eagles. It has to happen. By the way, am I the only person who sees Cutler every time he looks at BoJack Horseman? I'm not sure why that connection has been made it my brain, but it has.

Bills at Patriots.



Advertisement

Jets at Dolphins. You know how sometimes, when a person is exposed to something so overwhelming and staggering that the brain shuts down and can no longer function? I'm fairly certain this is what collectively happened to everyone when the videos of Rex Ryan and his wife and feet all came out. It has been four years, and I'm still not sure we've quite recovered. I mean, that is pretty much the most insane thing I've ever come across in sports. It feels like it should lead every conversation and newscast, still, to this day. ""More police outrage in Missouri, but first ... REX RYAN FILMED HIS WIFE'S FOOT FETISH VIDEOS."" When Rex is fired or quits on Monday, let that be his lasting memory. Let that be everyone's lasting memory.

Bears at Vikings. I'd thought this was a goodbye to games at Minnesota's TCF Bank Stadium, but no: The new Minnesota Stadium isn't going to be ready until 2016. That's a super long time to build a stadium, isn't it? Two full years at a fill-in stadium is quite a lease. That's almost as long as the Rams will stay at the Edward Jones Dome, I think.

Advertisement

Raiders at Broncos. Whew. Last one. Uh ... Rex Ryan's wife's feet!

Pregame Song That Makes Me Want To Run Through A Goddamn Brick Wall

R.I.P., Joe Cocker. Being a rock star in 1970 must have been amazing. Living 44 years after this is just a bonus, really.

Advertisement

Suicide Pick Of The Week

Last week's picks of Green Bay, Buffalo and Jacksonville went 2-1, making Drew 35-13 for the year. Time again to pick three teams for your suicide pool and one thing that makes you want to commit suicide. This week's picks are Baltimore, Houston, Seattle, and what we're going to do to Cuba's coast within the next 10 years. I have no doubt that by 2024, that place will be 40 percent Applebee's, 50 percent golf course and 10 percent Dutch Masters. Also, Havana will have a December 23 bowl game, and Gonzaga will play NJIT in a post-Thanksgiving tournament. And Jay Leno will do a yearly set.

Advertisement

This Week In Terrifying Animal News

ROBOT DRONE SHARK.

Fantasy Player Who Deserves To Die A Slow, Painful Death

Andrew Luck. Clearly.

Fire This Asshole!

It's a bit of an honor to get to do this section on Week 17, considering this site, and most others, will be spending all day Monday and Tuesday chronicling all the coach firings. I feel like the last person to see these guys alive.

Advertisement

Rex Ryan (foot fetishist)



Marc Trestman (nerd)



Mike Smith (unless they win)



Jim Harbaugh (within seconds)



Ken Whisenhunt (may bring in Max Hall)



Ron Rivera

Tom Coughlin



Dennis Allen-FIRED!!

Gus Bradley

Great Moments In Poop History

Sorry, my stomach remains too weak. I'm fairly certain I've read every word in every single Jamboroo that Drew has ever written ... other than the poop stories. I skip all of them. I'm like the guy who reads every story in the New Yorker other than the fiction. Except the exact opposite of that.

Advertisement

By the way: I just spent about 20 fruitless minutes trying to find the first-ever Jamboroo, I believe from 2007. The earliest I could find was this one. I know I'm the only person who cares, but searching Deadspin archives is freaking impossible.

Gametime Snack Of The Week

Advertisement

As is tradition: Bagel Bites, roughly 72 percent of my meals from 1998-2001. I once fell asleep while they were in the oven and woke up the next morning with, essentially, 15 hockey pucks. I might still have them around somewhere.

Gametime Cheap Beer Of The Week

Budweiser! But just so I can show this commercial again.

Sunday Afternoon Movie Of The Week For 49ers Fans

American Sniper. I wrote about this movie already for Bloomberg Politics, but seriously, it's excellent. There's no nuance, subtlety or balance ... which, above rants aside, is sometimes kind of nice. Grierson and I are doing our top 10 movies of 2014 next week — no B- movies here! — and this will be on mine.

Advertisement

Gratuitous Simpsons Quote

""Don' t be alarmed, Apu. Just go about your daily routine like I'm not wearing the hat.""

Advertisement

Enjoy the games, everyone. Thanks to Drew as always for having me. Go egg some nog.

Illustration by Jim Cooke.

"	https://deadspin.com/a-nation-of-echo-chambers-how-the-internet-closed-off-1674576436	"Will Leitch, senior writer at Sports On Earth, culture writer for Bloomberg Politics, contributing editor at New York magazine and founder of Deadspin, is doing his yearly fill-in for Drew Magary on today's Thursday Afternoon NFL Dick Joke Jamboroo. (Here is 2011's version, and here's 2012's and here's 2013's.) Leitch has written four books. Find more of his business at his Twitter feed and his official site.
Advertisement
In 2012, actor Rob Schneider, famous for something or other, spoke to a California television station about AB 2109, a California bill that required parents to get a physician's approval to opt out of vaccinating their children (something no sentient physician would ever approve). I only came across this interview recently. It is amazing.
You can almost follow along with Schneider's browser history as he continues to ramble on; there's the mom message board, there's the InfoWars THINGS THEY DON'T WANT YOU TO KNOW thread, there's the blog of the doctor with the degree-by-mail who is the only one willing to tell parents the truth. You can tell Schneider spent all night preparing for this interview, jotting down the words he wanted to emphasize, ""efficacy,"" ""toxicity,"" ""Nuremberg laws,"" ""forced sterilization."" He even ends with ""people have to stand up and get educated. Know all the facts.""
In this four-minute clip, I think you can encapsulate the last two years of American culture.
To believe that vaccines cause autism, that doctors are involved in a massive conspiracy to attack our children to help their pharmaceutical pals play golf, that the government sees vaccinations as a step toward forced sterilization and eventual death panels, that anyone on earth should ever wear that hat ... the only way that believing these things is possible is by talking specifically, and solely, to people who already agree with you. To believe these things is to ignore every published bit of medical research, to make up theories that require an otherworldly level of conspiracy and coordination in a country that can't even figure out HTML, to shut out every possible tidbit of contradictory evidence. That you should vaccinate your child is so plainly obvious and thoroughly documented that believing otherwise requires total disengagement with anything in the world that contradicts your narrow worldview. You have to be actively trying to get it wrong.
G/O Media may get a commission Exclusive for new customers Caliper CBD $35 at Caliper Use the promo code KINJATEN
And tons of people still think you shouldn't vaccinate your children! Twenty percent of the country, according to a University of Chicago study, believe doctors know vaccination causes autism but force the shots on children anyway. Again: This is an impossible belief to hold. Now, your response might just be ""well, those people are idiots,"" and I suppose that's the simplest explanation.
But I think it's more than that. I think that they are convinced they're right because they are only talking to other people who are convinced they are right. They have blocked out opposing voices — because they can. If you are an anti-vaccine activist, you can read so much ""information"" supporting your position that, as far as you can tell, you are right. That's what Schneider's talking about up there, that ""get educated"" business. Schneider doesn't see these beliefs as theories, or even as ""beliefs:"" He sees them as stone cold facts. On something about which he is so obviously wrong.
Advertisement
So here's my question: If 20 percent of the country can be so wrong on something so clearly incorrect (and harmful) as child vaccination, primarily because they can choose their evidence over your evidence ... what hope do any of us have? Because the rest of the world is helluva lot more complicated and confusing than whether or not to vaccinate your damn kid.
Chris Rock, when he was doing his big Truth Bomb press tour to promote Top Five, said something fascinating about the difference between President Bush and President Obama. He called Bush a ""cable network"" President; unlike Obama, he only catered to his subscribers. Rock also, astutely, points out ""whoever's the next president will do what Bush did.""
Advertisement
I'm not exactly certain that's true — I have no idea who the subscribers to, say, the Hillary Clinton cable channel are — but in the long term, there is zero doubt that he is right. You see this in every aspect of American life, from entertainment (where the only things anyone watches communally are sports, live musicals, or zombies attempting to eat the brains of thinly drawn caricatures) to politics (where the Republicans just won back the House the same way Bush beat Kerry, by appealing only to their base and not worrying about anyone else) to media (which is so fractured and desperate that it'll pump up whatever dumb Twitter shitstorm happens to be invading their feeds that afternoon, throw it on their front page, and pray; basically, outrage has become America's Assignment Desk). We are run by niche cultures right now. We've seen it from Gamergate to Sony Pictures to you name it. We don't have to build coalitions anymore; we just have to build a bigger coalition than you. We don't have to be right; we just have to be louder than the other guys. It's like that old joke about being chased by a bear.
This increased niche culture is a trademark of the web, and we used to think of it as a positive one; 20 years ago, if you didn't know any Quentin Tarantino or Woody Allen obsessives near you, you could go online and find them. (Theoretically.) The internet opened up a world that was truly revolutionary. But now, now that we're all online, and any novelty to this fact has worn off, the internet has closed that world. We now only have to interact with people who agree with us; if I use Twitter as my primary news source, as so many people do, I can carefully curate my feed to exclude anyone who disagrees with me about anything. (And if someone who slips in there who does, I can call them a horrible person.) Pauline Kael, the late film critic for the New Yorker, was once lambasted (unfairly, and inaccurately) for saying she couldn't believe Nixon was elected because she only personally knew one person who voted for him. But this is now accepted public policy. You don't have to find anyone to contradict you, if you don't want to.
Advertisement
This isn't just common practice now: This is how you win. The entire strategy for succeeding at anything, whether it's winning elections, selling a product or attracting visitors for your Website, revolves around pitching yourself as loudly as you can to those people on your side and turning those who disagree with you into the worst version of themselves, demonizing them into something subhuman and venal. Nuance is tossed out, even if you know a situation is desperately nuanced, in favor of quick points and splash; we've all become the New York Post.
This is simply how communication is done now. The idea of unifying anyone on anything is passé, old thinking, a waste of time. A horrible tragedy happens, and your first reaction, rather than taking a moment to mourn or quietly search for some grace and peace, is instead to start screaming and claiming that those with whom you disagree have blood on their hands. You are rewarded with this by the top slot on the news, a video that goes viral, and everyone on your side applauding you. And when you accept that's all you want to do—to turn away from the fundamental complexity at the heart of the human experience—you find you have no reason to return: After all, every time you say something loudly and strongly enough, the people who agree with you tell you how great you are. Those who disagree? Fuck the haters. Sic 'em, guys.
Advertisement
It can be so demoralizing, so exhausting, to watch this day after day after day. We have begun to shout at those with whom we disagree as if they are terrible drivers and we're within the safety of our own cars; they're the anonymous, faceless monsters we shower with the worst possible motives, just because they happen to be in our way when we're in a hurry. Except they can hear us. And so can everyone else.
So one tries to find hope.
I tend to find it outside, where people, you know, are. Because we drop this act during those strange, disorienting times when we find ourselves in mixed company, lo, real life. The things we do online, or when we think someone is watching, we don't do these things in the real world. In regular, everyday life, we accept all the time that those who disagree with us exist; sometimes we even like them.
Advertisement
They're our families, they're our friends, they're our neighbors, they're the people we open the door for at the supermarket. They're human beings, idiot, scared, just-trying-to-hang-on human beings like every single one of the rest of us. The world is uncertain and terrifying; life is hard and bewildering and unpredictable. We allow for this in our daily interactions in a way we do not in our virtual conversations. We accept human frailty, that we do not share a cerebral cortex with every other person on the planet and therefore will not always see eye-to-eye on all matters. The world is a massive place. There are currently 7.28 billion people on the planet and every single one of them is different. This is a good thing: This is humbling. This is an acceptance that we're all stupid, that we're all overwhelmed, that we're all trying, dammit.
We all have friends and family who believe things we personally find abhorrent. They do things that drive us crazy. Their faults flash brightly above their heads every day. And none of these things matter. We still find a way to love them anyway. Not everybody is just like everybody else. This is a good thing. This might be the only thing.
Advertisement
The Games
All games in the Jamboroo are evaluated for sheer watchability on a scale of 1 to 5 Throwgasms.
Advertisement
Five Throwgasms
Lions at Packers. Whew. I swear to God, I write about four different pieces a day, every day, all year, and no single piece stresses me out more than the lead to my fill-in week for Drew at the Jamboroo every December. I have no idea why. It just wears me out. I'm already glad it's over and can just get to the football part down here. Anyway: Hi! I'm Will. I started this joint — back when ""Kinja"" was three Hungarian man-boys being pelted with Werther's Originals until they pedaled fast enough to reboot the Gawker server — though now I'm just the guy who pops up every once in a while with the pretentious movie reviews and the B- ratings. Don't worry, Drew will be back next week for your ALL CAPS GOODNESS; I'll be out of here soon. It's Christmas. Just be happy you're getting any free content at all. I am fairly certain I'm the only person other than LeBron James working today.
Advertisement
Bengals at Steelers. Like everybody else in the professional media, I wrote constantly about Stephen Colbert last week; in our backlash-to-the-backlash-before-anyone-has-even-lashed-in-the-first-place culture, by Friday, everyone was so sick of Colbert tributes they'd forgotten why they liked him in the first place. But just you wait: When David Letterman retires this May, it's going to be that times infinity times a googol. Literally every single person — even dead people! maybe even some farmyard animals! — is going to tell you about how much Letterman meant to them, how he was an inspiration to them, how their whole idea of comedy was forged by David Letterman. These pieces are already starting, so get yours in quick. No criticism from me, though: I will read every single one of these, and I'll probably write a few myself. I apologize for all of us in advance.
Panthers at Falcons. For all the talk about the travesty of an NFC South team finishing below .500 and still hosting a playoff game ... it is worth noting that whoever wins this game is almost certainly going to be favored over an 11-or-12-win Buzzsaw That Is The Arizona Cardinals team in the first round of the playoffs. The 49ers, who haven't won in more than a month, are six-point favorites over the Buzzsaw this week. But we'll get to that.
Advertisement
Chargers at Chiefs. Thank you all, by the way, for taking time out from your schedule of hopping from theater to theater to see The Interview and popping in to read this. I made a big fuss out of this myself, but I'd bet now that the movie is actually showing in theaters not owned by spineless weasels, people won't bother to go see it. It makes sense that this would be the cycle:
Stage One: I don't want to watch The Interview.
Stage Two: I can't watch The Interview?
Stage Three: I must watch The Interview!
Stage Four: Oh, I can watch The Interview now?
Stage Five: I don't want to watch The Interview.
Browns at Ravens. The worst part about this NFL season, now that it's almost over: That we didn't get one last cameo appearance from Rex Grossman, who turned down the Browns' offer to join the team for this final game. I miss the Sex Cannon. I'd have liked to have been able to say goodbye one last time.
Advertisement
Jaguars at Texans. Before I decided to once again spend 1,600 words sniffing my own rectum, I was originally going to start this column with a thought project: Who is the next Bill Cosby? I don't mean ""who will start systematically drugging and assaulting women over a 40 year period,"" because we all know that's obviously going to be Jonah Hill. I mean: What's the story that media sort of glossed over at some point in last 20 years, only to have it explode on social media when the person involved attempts to promote something? After all, for all the talk about Hannibal Buress, the first time I'd seen Cosby's crimes brought up this year was Tom Scocca's ""knock knock oh yeah Bill Cosby is a rapist"" post back in February. That's all it takes, you know. Just a little push. So what are we going to remember that we didn't investigate next? Seriously, I'd love to hear your suggestions. I'll get a reporter right on it, unless of course I really admire the work of the person who committed all those crimes.
Advertisement
Rams at Seahawks. After my Buzzsaw That Is The Arizona Cardinals won that hideous Thursday night game two weeks ago, SB Nation's Matt Ufford, a Seahawks fan, taunted me on Twitter:
Advertisement
I've known Matt a long time — here we are flanking a nearly dead Drew Magary in 2007 — so I was eager to slap him back. But my Twitter revenge would be a dish best served cold! So I decided not to respond in the moment, instead waiting until my Buzzsaw had shocked the world Sunday night by beating Seattle and clinching home field throughout the NFC (and NFL) playoffs. I was all set to go: ""@mattufford #garbageteam"" would ROCK MATT'S WORLD as he watched his Seahawks fall to the mighty Buzzsaw! That would show Matt! That'll learn him to insult my favorite football franchise over social media! YOU MESS WITH THE BULL YOU GET THE HORNS.
Cardinals at 49ers. You know, I didn't end up sending any Tweets Ufford's way.
Advertisement
Cowboys at Washington Football Team. And that's it! Those are all the games with playoff implications. And yet I'm still going to try to write about the rest of them. Or at least mention the matchup briefly before heading into something entirely different like I've been doing with this column up to this point.
Colts at Titans. Jack Kogod raised an important question yesterday:
Advertisement
This seems like a job for Mark Lisanti, but I'm sympathetic to the cause. I think I like @HofC's suggestion: ""Douchebruhs."" I'm surprised this assignation doesn't already exist.
Saints at Buccaneers. Hey, congratulations to Craggs, by the way! Though now I'm not sure whom to bug about the fact that Deadspin appears to have forgotten to do its Hall of Fame this year. Is this like when one of those asshole 90-year-old baseball writers leaves his ballot blank? Or is that just over now? [Ed note: We just did it. You happy?]
Advertisement
Eagles at Giants. Cutler on the Eagles. It has to happen. By the way, am I the only person who sees Cutler every time he looks at BoJack Horseman? I'm not sure why that connection has been made it my brain, but it has.
Bills at Patriots.
Advertisement
Jets at Dolphins. You know how sometimes, when a person is exposed to something so overwhelming and staggering that the brain shuts down and can no longer function? I'm fairly certain this is what collectively happened to everyone when the videos of Rex Ryan and his wife and feet all came out. It has been four years, and I'm still not sure we've quite recovered. I mean, that is pretty much the most insane thing I've ever come across in sports. It feels like it should lead every conversation and newscast, still, to this day. ""More police outrage in Missouri, but first ... REX RYAN FILMED HIS WIFE'S FOOT FETISH VIDEOS."" When Rex is fired or quits on Monday, let that be his lasting memory. Let that be everyone's lasting memory.
Bears at Vikings. I'd thought this was a goodbye to games at Minnesota's TCF Bank Stadium, but no: The new Minnesota Stadium isn't going to be ready until 2016. That's a super long time to build a stadium, isn't it? Two full years at a fill-in stadium is quite a lease. That's almost as long as the Rams will stay at the Edward Jones Dome, I think.
Advertisement
Raiders at Broncos. Whew. Last one. Uh ... Rex Ryan's wife's feet!
Pregame Song That Makes Me Want To Run Through A Goddamn Brick Wall
R.I.P., Joe Cocker. Being a rock star in 1970 must have been amazing. Living 44 years after this is just a bonus, really.
Advertisement
Suicide Pick Of The Week
Last week's picks of Green Bay, Buffalo and Jacksonville went 2-1, making Drew 35-13 for the year. Time again to pick three teams for your suicide pool and one thing that makes you want to commit suicide. This week's picks are Baltimore, Houston, Seattle, and what we're going to do to Cuba's coast within the next 10 years. I have no doubt that by 2024, that place will be 40 percent Applebee's, 50 percent golf course and 10 percent Dutch Masters. Also, Havana will have a December 23 bowl game, and Gonzaga will play NJIT in a post-Thanksgiving tournament. And Jay Leno will do a yearly set.
Advertisement
This Week In Terrifying Animal News
ROBOT DRONE SHARK.
Fantasy Player Who Deserves To Die A Slow, Painful Death
Andrew Luck. Clearly.
Fire This Asshole!
It's a bit of an honor to get to do this section on Week 17, considering this site, and most others, will be spending all day Monday and Tuesday chronicling all the coach firings. I feel like the last person to see these guys alive.
Advertisement
Rex Ryan (foot fetishist)
Marc Trestman (nerd)
Mike Smith (unless they win)
Jim Harbaugh (within seconds)
Ken Whisenhunt (may bring in Max Hall)
Ron Rivera
Tom Coughlin
Dennis Allen-FIRED!!
Gus Bradley
Great Moments In Poop History
Sorry, my stomach remains too weak. I'm fairly certain I've read every word in every single Jamboroo that Drew has ever written ... other than the poop stories. I skip all of them. I'm like the guy who reads every story in the New Yorker other than the fiction. Except the exact opposite of that.
Advertisement
By the way: I just spent about 20 fruitless minutes trying to find the first-ever Jamboroo, I believe from 2007. The earliest I could find was this one. I know I'm the only person who cares, but searching Deadspin archives is freaking impossible.
Gametime Snack Of The Week
Advertisement
As is tradition: Bagel Bites, roughly 72 percent of my meals from 1998-2001. I once fell asleep while they were in the oven and woke up the next morning with, essentially, 15 hockey pucks. I might still have them around somewhere.
Gametime Cheap Beer Of The Week
Budweiser! But just so I can show this commercial again.
Sunday Afternoon Movie Of The Week For 49ers Fans
American Sniper. I wrote about this movie already for Bloomberg Politics, but seriously, it's excellent. There's no nuance, subtlety or balance ... which, above rants aside, is sometimes kind of nice. Grierson and I are doing our top 10 movies of 2014 next week — no B- movies here! — and this will be on mine.
Advertisement
Gratuitous Simpsons Quote
""Don' t be alarmed, Apu. Just go about your daily routine like I'm not wearing the hat.""
Advertisement
Enjoy the games, everyone. Thanks to Drew as always for having me. Go egg some nog.
Illustration by Jim Cooke.
"	20489
technology	['Elizabeth Kolbert', 'Elizabeth Kolber', 'Nathan Helle']	2017-08-28 00:00:00	"On the night of November 7, 1876, Rutherford B. Hayes’s wife, Lucy, took to her bed with a headache. The returns from the Presidential election were trickling in, and the Hayeses, who had been spending the evening in their parlor, in Columbus, Ohio, were dismayed. Hayes himself remained up until midnight; then he, too, retired, convinced that his Democratic opponent, Samuel J. Tilden, would become the next President.

Illustration by Nishant Choksi

Hayes had indeed lost the popular vote, by more than two hundred and fifty thousand ballots. And he might have lost the Electoral College as well had it not been for the machinations of journalists working in the shady corners of what’s been called “the Victorian Internet.”

Chief among the plotters was an Ohioan named William Henry Smith. Smith ran the western arm of the Associated Press, and in this way controlled the bulk of the copy that ran in many small-town newspapers. The Western A.P. operated in tight affiliation—some would say collusion—with Western Union, which exercised a near-monopoly over the nation’s telegraph lines. Early in the campaign, Smith decided that he would employ any means necessary to assure a victory for Hayes, who, at the time, was serving a third term as Ohio’s governor. In the run-up to the Republican National Convention, Smith orchestrated the release of damaging information about the Governor’s rivals. Then he had the Western A.P. blare Hayes’s campaign statements and mute Tilden’s. At one point, an unflattering piece about Hayes appeared in the Chicago Times, a Democratic paper. (The piece claimed that Hayes, who had been a general in the Union Army, had accepted money from a soldier to give to the man’s family, but had failed to pass it on when the soldier died.) The A.P. flooded the wires with articles discrediting the story.

Once the votes had been counted, attention shifted to South Carolina, Florida, and Louisiana—states where the results were disputed. Both parties dispatched emissaries to the three states to try to influence the Electoral College outcome. Telegrams sent by Tilden’s representatives were passed on to Smith, courtesy of Western Union. Smith, in turn, shared the contents of these dispatches with the Hayes forces. This proto-hack of the Democrats’ private communications gave the Republicans an obvious edge. Meanwhile, the A.P. sought and distributed legal opinions supporting Hayes. (Outraged Tilden supporters took to calling it the “Hayesociated Press.”) As Democrats watched what they considered to be the theft of the election, they fell into a funk.

“They are full of passion and want to do something desperate but hardly know how to,” one observer noted. Two days before Hayes was inaugurated, on March 5, 1877, the New York Sun appeared with a black border on the front page. “These are days of humiliation, shame and mourning for every patriotic American,” the paper’s editor wrote.

History, Mark Twain is supposed to have said, doesn’t repeat itself, but it does rhyme. Once again, the President of the United States is a Republican who lost the popular vote. Once again, he was abetted by shadowy agents who manipulated the news. And once again Democrats are in a finger-pointing funk.

Journalists, congressional committees, and a special counsel are probing the details of what happened last fall. But two new books contend that the large lines of the problem are already clear. As in the eighteen-seventies, we are in the midst of a technological revolution that has altered the flow of information. Now, as then, just a few companies have taken control, and this concentration of power—which Americans have acquiesced to without ever really intending to, simply by clicking away—is subverting our democracy.

Thirty years ago, almost no one used the Internet for anything. Today, just about everybody uses it for everything. Even as the Web has grown, however, it has narrowed. Google now controls nearly ninety per cent of search advertising, Facebook almost eighty per cent of mobile social traffic, and Amazon about seventy-five per cent of e-book sales. Such dominance, Jonathan Taplin argues, in “Move Fast and Break Things: How Facebook, Google, and Amazon Cornered Culture and Undermined Democracy” (Little, Brown), is essentially monopolistic. In his account, the new monopolies are even more powerful than the old ones, which tended to be limited to a single product or service. Carnegie, Taplin suggests, would have been envious of the reach of Mark Zuckerberg and Jeff Bezos.

Taplin, who until recently directed the Annenberg Innovation Lab, at the University of Southern California, started out as a tour manager. He worked with Judy Collins, Bob Dylan, and the Band, and also with George Harrison, on the Concert for Bangladesh. In “Move Fast and Break Things,” Taplin draws extensively on this experience to illustrate the damage, both deliberate and collateral, that Big Tech is wreaking.

Consider the case of Levon Helm. He was the drummer for the Band, and, though he never got rich off his music, well into middle age he was supported by royalties. In 1999, he was diagnosed with throat cancer. That same year, Napster came along, followed by YouTube, in 2005. Helm’s royalty income, which had run to about a hundred thousand dollars a year, according to Taplin, dropped “to almost nothing.” When Helm died, in 2012, millions of people were still listening to the Band’s music, but hardly any of them were paying for it. (In the years between the founding of Napster and Helm’s death, total consumer spending on recorded music in the United States dropped by roughly seventy per cent.) Friends had to stage a benefit for Helm’s widow so that she could hold on to their house.

Google entered and more or less immediately took over the music business when it acquired YouTube, in 2006, for $1.65 billion in stock. As Taplin notes, just about “every single tune in the world is available on YouTube as a simple audio file (most of them posted by users).” Many of these files are illegal, but to Google this is inconsequential. Under the Digital Media Copyright Act, signed into law by President Bill Clinton shortly after Google went live, Internet service providers aren’t liable for copyright infringement as long as they “expeditiously” take down or block access to the material once they’re notified of a problem. Musicians are constantly filing “takedown” notices—in just the first twelve weeks of last year, Google received such notices for more than two hundred million links—but, often, after one link is taken down, the song goes right back up at another one. In the fall of 2011, legislation aimed at curbing online copyright infringement, the Stop Online Piracy Act, was introduced. It had bipartisan support in Congress, and backing from such disparate groups as the National District Attorneys Association, the National League of Cities, the Association of Talent Agencies, and the International Brotherhood of Teamsters. In January, 2012, the bill seemed headed toward passage, when Google decided to flex its market-concentrated muscles. In place of its usual colorful logo, the company posted on its search page a black rectangle along with the message “Tell Congress: Please don’t censor the web!” The resulting traffic overwhelmed congressional Web sites, and support for the bill evaporated. (Senator Marco Rubio, of Florida, who had been one of the bill’s co-sponsors, denounced it on Facebook.)

Google itself doesn’t pirate music; it doesn’t have to. It’s selling the traffic—and, just as significant, the data about the traffic. Like the Koch brothers, Taplin observes, Google is “in the extraction industry.” Its business model is “to extract as much personal data from as many people in the world at the lowest possible price and to resell that data to as many companies as possible at the highest possible price.” And so Google profits from just about everything: cat videos, beheadings, alt-right rants, the Band performing “The Weight” at Woodstock, in 1969.

“I wasn’t always so skeptical,” Franklin Foer announces at the start of “World Without Mind: The Existential Threat of Big Tech” (Penguin Press). Franklin, the eldest of the three famous Foer brothers, is a journalist, and he began his career, in the mid-nineties, working for Slate, which had then just been founded by Microsoft. The experience, Foer writes, was “exhilarating.” Later, he became the editor of The New Republic. The magazine was on the brink of ruin when, in 2012, it was purchased by Chris Hughes, a co-founder of Facebook, whose personal fortune was estimated at half a billion dollars.

Foer saw Hughes as a “savior,” who could provide, in addition to cash, “an insider’s knowledge of social media” and “a millennial imprimatur.” The two men set out to revitalize the magazine, hiring high-priced talent and redesigning the Web site. Foer recounts that he became so consumed with monitoring traffic to the magazine’s site, using a tool called Chartbeat, that he checked it even while standing at the urinal.

The era of good feeling didn’t last. In the fall of 2014, Foer heard that Hughes had hired someone to replace him, and that this shadow editor was “lunching around New York offering jobs at The New Republic.” Before Hughes had a chance to fire him, Foer quit, and most of the magazine’s editorial staff left with him. “World Without Mind” is a reflection on Foer’s experiences and on the larger forces reshaping American arts and letters, or what’s nowadays often called “content.”

“I hope this book doesn’t come across as fueled by anger, but I don’t want to deny my anger either,” he writes. “The tech companies are destroying something precious. . . . They have eroded the integrity of institutions—media, publishing—that supply the intellectual material that provokes thought and guides democracy. Their most precious asset is our most precious asset, our attention, and they have abused it.”

“He’s at that awkward age—too old to be cute, but not dead yet.” Facebook

Twitter

Email

Shopping

Much of Foer’s anger, like Taplin’s, is directed at piracy. “Once an underground, amateur pastime,” he writes, “the bootlegging of intellectual property” has become “an accepted business practice.” He points to the Huffington Post, since shortened to HuffPost, which rose to prominence largely by aggregating—or, if you prefer, pilfering—content from publications like the Times and the Washington Post. Then there’s Google Books. Google set out to scan every book in creation and make the volumes available online, without bothering to consult the copyright holders. (The project has been hobbled by lawsuits.) Newspapers and magazines (including this one) have tried to disrupt the disrupters by placing articles behind paywalls, but, Foer contends, in the contest against Big Tech publishers can’t win; the lineup is too lopsided. “When newspapers and magazines require subscriptions to access their pieces, Google and Facebook tend to bury them,” he writes. “Articles protected by stringent paywalls almost never have the popularity that algorithms reward with prominence.”"	https://www.newyorker.com/magazine/2017/08/28/who-owns-the-internet	"On the night of November 7, 1876, Rutherford B. Hayes’s wife, Lucy, took to her bed with a headache. The returns from the Presidential election were trickling in, and the Hayeses, who had been spending the evening in their parlor, in Columbus, Ohio, were dismayed. Hayes himself remained up until midnight; then he, too, retired, convinced that his Democratic opponent, Samuel J. Tilden, would become the next President.
Illustration by Nishant Choksi
Hayes had indeed lost the popular vote, by more than two hundred and fifty thousand ballots. And he might have lost the Electoral College as well had it not been for the machinations of journalists working in the shady corners of what’s been called “the Victorian Internet.”
Chief among the plotters was an Ohioan named William Henry Smith. Smith ran the western arm of the Associated Press, and in this way controlled the bulk of the copy that ran in many small-town newspapers. The Western A.P. operated in tight affiliation—some would say collusion—with Western Union, which exercised a near-monopoly over the nation’s telegraph lines. Early in the campaign, Smith decided that he would employ any means necessary to assure a victory for Hayes, who, at the time, was serving a third term as Ohio’s governor. In the run-up to the Republican National Convention, Smith orchestrated the release of damaging information about the Governor’s rivals. Then he had the Western A.P. blare Hayes’s campaign statements and mute Tilden’s. At one point, an unflattering piece about Hayes appeared in the Chicago Times, a Democratic paper. (The piece claimed that Hayes, who had been a general in the Union Army, had accepted money from a soldier to give to the man’s family, but had failed to pass it on when the soldier died.) The A.P. flooded the wires with articles discrediting the story.
Once the votes had been counted, attention shifted to South Carolina, Florida, and Louisiana—states where the results were disputed. Both parties dispatched emissaries to the three states to try to influence the Electoral College outcome. Telegrams sent by Tilden’s representatives were passed on to Smith, courtesy of Western Union. Smith, in turn, shared the contents of these dispatches with the Hayes forces. This proto-hack of the Democrats’ private communications gave the Republicans an obvious edge. Meanwhile, the A.P. sought and distributed legal opinions supporting Hayes. (Outraged Tilden supporters took to calling it the “Hayesociated Press.”) As Democrats watched what they considered to be the theft of the election, they fell into a funk.
“They are full of passion and want to do something desperate but hardly know how to,” one observer noted. Two days before Hayes was inaugurated, on March 5, 1877, the New York Sun appeared with a black border on the front page. “These are days of humiliation, shame and mourning for every patriotic American,” the paper’s editor wrote.
History, Mark Twain is supposed to have said, doesn’t repeat itself, but it does rhyme. Once again, the President of the United States is a Republican who lost the popular vote. Once again, he was abetted by shadowy agents who manipulated the news. And once again Democrats are in a finger-pointing funk.
Journalists, congressional committees, and a special counsel are probing the details of what happened last fall. But two new books contend that the large lines of the problem are already clear. As in the eighteen-seventies, we are in the midst of a technological revolution that has altered the flow of information. Now, as then, just a few companies have taken control, and this concentration of power—which Americans have acquiesced to without ever really intending to, simply by clicking away—is subverting our democracy.
Thirty years ago, almost no one used the Internet for anything. Today, just about everybody uses it for everything. Even as the Web has grown, however, it has narrowed. Google now controls nearly ninety per cent of search advertising, Facebook almost eighty per cent of mobile social traffic, and Amazon about seventy-five per cent of e-book sales. Such dominance, Jonathan Taplin argues, in “Move Fast and Break Things: How Facebook, Google, and Amazon Cornered Culture and Undermined Democracy” (Little, Brown), is essentially monopolistic. In his account, the new monopolies are even more powerful than the old ones, which tended to be limited to a single product or service. Carnegie, Taplin suggests, would have been envious of the reach of Mark Zuckerberg and Jeff Bezos.
Taplin, who until recently directed the Annenberg Innovation Lab, at the University of Southern California, started out as a tour manager. He worked with Judy Collins, Bob Dylan, and the Band, and also with George Harrison, on the Concert for Bangladesh. In “Move Fast and Break Things,” Taplin draws extensively on this experience to illustrate the damage, both deliberate and collateral, that Big Tech is wreaking.
Consider the case of Levon Helm. He was the drummer for the Band, and, though he never got rich off his music, well into middle age he was supported by royalties. In 1999, he was diagnosed with throat cancer. That same year, Napster came along, followed by YouTube, in 2005. Helm’s royalty income, which had run to about a hundred thousand dollars a year, according to Taplin, dropped “to almost nothing.” When Helm died, in 2012, millions of people were still listening to the Band’s music, but hardly any of them were paying for it. (In the years between the founding of Napster and Helm’s death, total consumer spending on recorded music in the United States dropped by roughly seventy per cent.) Friends had to stage a benefit for Helm’s widow so that she could hold on to their house.
Google entered and more or less immediately took over the music business when it acquired YouTube, in 2006, for $1.65 billion in stock. As Taplin notes, just about “every single tune in the world is available on YouTube as a simple audio file (most of them posted by users).” Many of these files are illegal, but to Google this is inconsequential. Under the Digital Media Copyright Act, signed into law by President Bill Clinton shortly after Google went live, Internet service providers aren’t liable for copyright infringement as long as they “expeditiously” take down or block access to the material once they’re notified of a problem. Musicians are constantly filing “takedown” notices—in just the first twelve weeks of last year, Google received such notices for more than two hundred million links—but, often, after one link is taken down, the song goes right back up at another one. In the fall of 2011, legislation aimed at curbing online copyright infringement, the Stop Online Piracy Act, was introduced. It had bipartisan support in Congress, and backing from such disparate groups as the National District Attorneys Association, the National League of Cities, the Association of Talent Agencies, and the International Brotherhood of Teamsters. In January, 2012, the bill seemed headed toward passage, when Google decided to flex its market-concentrated muscles. In place of its usual colorful logo, the company posted on its search page a black rectangle along with the message “Tell Congress: Please don’t censor the web!” The resulting traffic overwhelmed congressional Web sites, and support for the bill evaporated. (Senator Marco Rubio, of Florida, who had been one of the bill’s co-sponsors, denounced it on Facebook.)
Google itself doesn’t pirate music; it doesn’t have to. It’s selling the traffic—and, just as significant, the data about the traffic. Like the Koch brothers, Taplin observes, Google is “in the extraction industry.” Its business model is “to extract as much personal data from as many people in the world at the lowest possible price and to resell that data to as many companies as possible at the highest possible price.” And so Google profits from just about everything: cat videos, beheadings, alt-right rants, the Band performing “The Weight” at Woodstock, in 1969.
“I wasn’t always so skeptical,” Franklin Foer announces at the start of “World Without Mind: The Existential Threat of Big Tech” (Penguin Press). Franklin, the eldest of the three famous Foer brothers, is a journalist, and he began his career, in the mid-nineties, working for Slate, which had then just been founded by Microsoft. The experience, Foer writes, was “exhilarating.” Later, he became the editor of The New Republic. The magazine was on the brink of ruin when, in 2012, it was purchased by Chris Hughes, a co-founder of Facebook, whose personal fortune was estimated at half a billion dollars.
Foer saw Hughes as a “savior,” who could provide, in addition to cash, “an insider’s knowledge of social media” and “a millennial imprimatur.” The two men set out to revitalize the magazine, hiring high-priced talent and redesigning the Web site. Foer recounts that he became so consumed with monitoring traffic to the magazine’s site, using a tool called Chartbeat, that he checked it even while standing at the urinal.
The era of good feeling didn’t last. In the fall of 2014, Foer heard that Hughes had hired someone to replace him, and that this shadow editor was “lunching around New York offering jobs at The New Republic.” Before Hughes had a chance to fire him, Foer quit, and most of the magazine’s editorial staff left with him. “World Without Mind” is a reflection on Foer’s experiences and on the larger forces reshaping American arts and letters, or what’s nowadays often called “content.”
“I hope this book doesn’t come across as fueled by anger, but I don’t want to deny my anger either,” he writes. “The tech companies are destroying something precious. . . . They have eroded the integrity of institutions—media, publishing—that supply the intellectual material that provokes thought and guides democracy. Their most precious asset is our most precious asset, our attention, and they have abused it.”
“He’s at that awkward age—too old to be cute, but not dead yet.” Facebook
Twitter
Email
Shopping
Much of Foer’s anger, like Taplin’s, is directed at piracy. “Once an underground, amateur pastime,” he writes, “the bootlegging of intellectual property” has become “an accepted business practice.” He points to the Huffington Post, since shortened to HuffPost, which rose to prominence largely by aggregating—or, if you prefer, pilfering—content from publications like the Times and the Washington Post. Then there’s Google Books. Google set out to scan every book in creation and make the volumes available online, without bothering to consult the copyright holders. (The project has been hobbled by lawsuits.) Newspapers and magazines (including this one) have tried to disrupt the disrupters by placing articles behind paywalls, but, Foer contends, in the contest against Big Tech publishers can’t win; the lineup is too lopsided. “When newspapers and magazines require subscriptions to access their pieces, Google and Facebook tend to bury them,” he writes. “Articles protected by stringent paywalls almost never have the popularity that algorithms reward with prominence.”"	11147
health	['Amy Wallace', 'Amy Wallac']		"This isn’t a religious dispute, like the debate over creationism and intelligent design. It’s a challenge to traditional science that crosses party, class, and religious lines.

So what has this award-winning 58-year-old scientist done to elicit such venom? He boldly states — in speeches, in journal articles, and in his 2008 book Autism's False Prophets — that vaccines do not cause autism or autoimmune disease or any of the other chronic conditions that have been blamed on them. He supports this assertion with meticulous evidence. And he calls to account those who promote bogus treatments for autism — treatments that he says not only don't work but often cause harm.

As a result, Offit has become the main target of a grassroots movement that opposes the systematic vaccination of children and the laws that require it. McCarthy, an actress and a former Playboy centerfold whose son has been diagnosed with autism, is the best-known leader of the movement, but she is joined by legions of well-organized supporters and sympathizers.

This isn't a religious dispute, like the debate over creationism and intelligent design. It's a challenge to traditional science that crosses party, class, and religious lines. It is partly a reaction to Big Pharma's blunders and PR missteps, from Vioxx to illegal marketing ploys, which have encouraged a distrust of experts. It is also, ironically, a product of the era of instant communication and easy access to information. The doubters and deniers are empowered by the Internet (online, nobody knows you're not a doctor) and helped by the mainstream media, which has an interest in pumping up bad science to create a ""debate"" where there should be none.

In the center of the fray is Paul Offit. ""People describe me as a vaccine advocate,"" he says. ""I see myself as a science advocate."" But in this battle — and make no mistake, he says, it's a pitched and heated battle — ""science alone isn't enough ... People are getting hurt. The parent who reads what Jenny McCarthy says and thinks, 'Well, maybe I shouldn't get this vaccine,' and their child dies of Hib meningitis,"" he says, shaking his head. ""It's such a fundamental failure on our part that we haven't convinced that parent.""

Consider: In certain parts of the US, vaccination rates have dropped so low that occurrences of some children's diseases are approaching pre-vaccine levels for the first time ever. And the number of people who choose not to vaccinate their children (so-called philosophical exemptions are available in about 20 states, including Pennsylvania, Texas, and much of the West) continues to rise. In states where such opting out is allowed, 2.6 percent of parents did so last year, up from 1 percent in 1991, according to the CDC. In some communities, like California's affluent Marin County, just north of San Francisco, non-vaccination rates are approaching 6 percent (counterintuitively, higher rates of non-vaccination often correspond with higher levels of education and wealth).

Science loses ground to pseudo-science because the latter seems to offer more comfort.

That may not sound like much, but a recent study by the Los Angeles Times indicates that the impact can be devastating. The Times found that even though only about 2 percent of California's kindergartners are unvaccinated (10,000 kids, or about twice the number as in 1997), they tend to be clustered, disproportionately increasing the risk of an outbreak of such largely eradicated diseases as measles, mumps, and pertussis (whooping cough). The clustering means almost 10 percent of elementary schools statewide may already be at risk.

In May, The New England Journal of Medicine laid the blame for clusters of disease outbreaks throughout the US squarely at the feet of declining vaccination rates, while nonprofit health care provider Kaiser Permanente reported that unvaccinated children were 23 times more likely to get pertussis, a highly contagious bacterial disease that causes violent coughing and is potentially lethal to infants. In the June issue of the journal Pediatrics, Jason Glanz, an epidemiologist at Kaiser's Institute for Health Research, revealed that the number of reported pertussis cases jumped from 1,000 in 1976 to 26,000 in 2004. A disease that vaccines made rare, in other words, is making a comeback. ""This study helps dispel one of the commonly held beliefs among vaccine-refusing parents: that their children are not at risk for vaccine-preventable diseases,"" Glanz says."	https://www.wired.com/2009/10/ff-waronscience/	"This isn’t a religious dispute, like the debate over creationism and intelligent design. It’s a challenge to traditional science that crosses party, class, and religious lines.
So what has this award-winning 58-year-old scientist done to elicit such venom? He boldly states — in speeches, in journal articles, and in his 2008 book Autism's False Prophets — that vaccines do not cause autism or autoimmune disease or any of the other chronic conditions that have been blamed on them. He supports this assertion with meticulous evidence. And he calls to account those who promote bogus treatments for autism — treatments that he says not only don't work but often cause harm.
As a result, Offit has become the main target of a grassroots movement that opposes the systematic vaccination of children and the laws that require it. McCarthy, an actress and a former Playboy centerfold whose son has been diagnosed with autism, is the best-known leader of the movement, but she is joined by legions of well-organized supporters and sympathizers.
This isn't a religious dispute, like the debate over creationism and intelligent design. It's a challenge to traditional science that crosses party, class, and religious lines. It is partly a reaction to Big Pharma's blunders and PR missteps, from Vioxx to illegal marketing ploys, which have encouraged a distrust of experts. It is also, ironically, a product of the era of instant communication and easy access to information. The doubters and deniers are empowered by the Internet (online, nobody knows you're not a doctor) and helped by the mainstream media, which has an interest in pumping up bad science to create a ""debate"" where there should be none.
In the center of the fray is Paul Offit. ""People describe me as a vaccine advocate,"" he says. ""I see myself as a science advocate."" But in this battle — and make no mistake, he says, it's a pitched and heated battle — ""science alone isn't enough ... People are getting hurt. The parent who reads what Jenny McCarthy says and thinks, 'Well, maybe I shouldn't get this vaccine,' and their child dies of Hib meningitis,"" he says, shaking his head. ""It's such a fundamental failure on our part that we haven't convinced that parent.""
Consider: In certain parts of the US, vaccination rates have dropped so low that occurrences of some children's diseases are approaching pre-vaccine levels for the first time ever. And the number of people who choose not to vaccinate their children (so-called philosophical exemptions are available in about 20 states, including Pennsylvania, Texas, and much of the West) continues to rise. In states where such opting out is allowed, 2.6 percent of parents did so last year, up from 1 percent in 1991, according to the CDC. In some communities, like California's affluent Marin County, just north of San Francisco, non-vaccination rates are approaching 6 percent (counterintuitively, higher rates of non-vaccination often correspond with higher levels of education and wealth).
Science loses ground to pseudo-science because the latter seems to offer more comfort.
That may not sound like much, but a recent study by the Los Angeles Times indicates that the impact can be devastating. The Times found that even though only about 2 percent of California's kindergartners are unvaccinated (10,000 kids, or about twice the number as in 1997), they tend to be clustered, disproportionately increasing the risk of an outbreak of such largely eradicated diseases as measles, mumps, and pertussis (whooping cough). The clustering means almost 10 percent of elementary schools statewide may already be at risk.
In May, The New England Journal of Medicine laid the blame for clusters of disease outbreaks throughout the US squarely at the feet of declining vaccination rates, while nonprofit health care provider Kaiser Permanente reported that unvaccinated children were 23 times more likely to get pertussis, a highly contagious bacterial disease that causes violent coughing and is potentially lethal to infants. In the June issue of the journal Pediatrics, Jason Glanz, an epidemiologist at Kaiser's Institute for Health Research, revealed that the number of reported pertussis cases jumped from 1,000 in 1976 to 26,000 in 2004. A disease that vaccines made rare, in other words, is making a comeback. ""This study helps dispel one of the commonly held beliefs among vaccine-refusing parents: that their children are not at risk for vaccine-preventable diseases,"" Glanz says."	4500
health	['Marcia Angell']	2011-06-23 00:00:00	"An advertisement for Prozac, from The American Journal of Psychiatry, 1995

It seems that Americans are in the midst of a raging epidemic of mental illness, at least as judged by the increase in the numbers treated for it. The tally of those who are so disabled by mental disorders that they qualify for Supplemental Security Income (SSI) or Social Security Disability Insurance (SSDI) increased nearly two and a half times between 1987 and 2007—from one in 184 Americans to one in seventy-six. For children, the rise is even more startling—a thirty-five-fold increase in the same two decades. Mental illness is now the leading cause of disability in children, well ahead of physical disabilities like cerebral palsy or Down syndrome, for which the federal programs were created.

A large survey of randomly selected adults, sponsored by the National Institute of Mental Health (NIMH) and conducted between 2001 and 2003, found that an astonishing 46 percent met criteria established by the American Psychiatric Association (APA) for having had at least one mental illness within four broad categories at some time in their lives. The categories were “anxiety disorders,” including, among other subcategories, phobias and post-traumatic stress disorder (PTSD); “mood disorders,” including major depression and bipolar disorders; “impulse-control disorders,” including various behavioral problems and attention-deficit/hyperactivity disorder (ADHD); and “substance use disorders,” including alcohol and drug abuse. Most met criteria for more than one diagnosis. Of a subgroup affected within the previous year, a third were under treatment—up from a fifth in a similar survey ten years earlier.

Nowadays treatment by medical doctors nearly always means psychoactive drugs, that is, drugs that affect the mental state. In fact, most psychiatrists treat only with drugs, and refer patients to psychologists or social workers if they believe psychotherapy is also warranted. The shift from “talk therapy” to drugs as the dominant mode of treatment coincides with the emergence over the past four decades of the theory that mental illness is caused primarily by chemical imbalances in the brain that can be corrected by specific drugs. That theory became broadly accepted, by the media and the public as well as by the medical profession, after Prozac came to market in 1987 and was intensively promoted as a corrective for a deficiency of serotonin in the brain. The number of people treated for depression tripled in the following ten years, and about 10 percent of Americans over age six now take antidepressants. The increased use of drugs to treat psychosis is even more dramatic. The new generation of antipsychotics, such as Risperdal, Zyprexa, and Seroquel, has replaced cholesterol-lowering agents as the top-selling class of drugs in the US.

What is going on here? Is the prevalence of mental illness really that high and still climbing? Particularly if these disorders are biologically determined and not a result of environmental influences, is it plausible to suppose that such an increase is real? Or are we learning to recognize and diagnose mental disorders that were always there? On the other hand, are we simply expanding the criteria for mental illness so that nearly everyone has one? And what about the drugs that are now the mainstay of treatment? Do they work? If they do, shouldn’t we expect the prevalence of mental illness to be declining, not rising?

These are the questions, among others, that concern the authors of the three provocative books under review here. They come at the questions from different backgrounds—Irving Kirsch is a psychologist at the University of Hull in the UK, Robert Whitaker a journalist and previously the author of a history of the treatment of mental illness called Mad in America (2001), and Daniel Carlat a psychiatrist who practices in a Boston suburb and publishes a newsletter and blog about his profession.

The authors emphasize different aspects of the epidemic of mental illness. Kirsch is concerned with whether antidepressants work. Whitaker, who has written an angrier book, takes on the entire spectrum of mental illness and asks whether psychoactive drugs create worse problems than they solve. Carlat, who writes more in sorrow than in anger, looks mainly at how his profession has allied itself with, and is manipulated by, the pharmaceutical industry. But despite their differences, all three are in remarkable agreement on some important matters, and they have documented their views well.

First, they agree on the disturbing extent to which the companies that sell psychoactive drugs—through various forms of marketing, both legal and illegal, and what many people would describe as bribery—have come to determine what constitutes a mental illness and how the disorders should be diagnosed and treated. This is a subject to which I’ll return.

Second, none of the three authors subscribes to the popular theory that mental illness is caused by a chemical imbalance in the brain. As Whitaker tells the story, that theory had its genesis shortly after psychoactive drugs were introduced in the 1950s. The first was Thorazine (chlorpromazine), which was launched in 1954 as a “major tranquilizer” and quickly found widespread use in mental hospitals to calm psychotic patients, mainly those with schizophrenia. Thorazine was followed the next year by Miltown (meprobamate), sold as a “minor tranquilizer” to treat anxiety in outpatients. And in 1957, Marsilid (iproniazid) came on the market as a “psychic energizer” to treat depression.

Advertisement

In the space of three short years, then, drugs had become available to treat what at that time were regarded as the three major categories of mental illness—psychosis, anxiety, and depression—and the face of psychiatry was totally transformed. These drugs, however, had not initially been developed to treat mental illness. They had been derived from drugs meant to treat infections, and were found only serendipitously to alter the mental state. At first, no one had any idea how they worked. They simply blunted disturbing mental symptoms. But over the next decade, researchers found that these drugs, and the newer psychoactive drugs that quickly followed, affected the levels of certain chemicals in the brain.

Some brief—and necessarily quite simplified—background: the brain contains billions of nerve cells, called neurons, arrayed in immensely complicated networks and communicating with one another constantly. The typical neuron has multiple filamentous extensions, one called an axon and the others called dendrites, through which it sends and receives signals from other neurons. For one neuron to communicate with another, however, the signal must be transmitted across the tiny space separating them, called a synapse. To accomplish that, the axon of the sending neuron releases a chemical, called a neurotransmitter, into the synapse. The neurotransmitter crosses the synapse and attaches to receptors on the second neuron, often a dendrite, thereby activating or inhibiting the receiving cell. Axons have multiple terminals, so each neuron has multiple synapses. Afterward, the neurotransmitter is either reabsorbed by the first neuron or metabolized by enzymes so that the status quo ante is restored. There are exceptions and variations to this story, but that is the usual way neurons communicate with one another.

When it was found that psychoactive drugs affect neurotransmitter levels in the brain, as evidenced mainly by the levels of their breakdown products in the spinal fluid, the theory arose that the cause of mental illness is an abnormality in the brain’s concentration of these chemicals that is specifically countered by the appropriate drug. For example, because Thorazine was found to lower dopamine levels in the brain, it was postulated that psychoses like schizophrenia are caused by too much dopamine. Or later, because certain antidepressants increase levels of the neurotransmitter serotonin in the brain, it was postulated that depression is caused by too little serotonin. (These antidepressants, like Prozac or Celexa, are called selective serotonin reuptake inhibitors (SSRIs) because they prevent the reabsorption of serotonin by the neurons that release it, so that more remains in the synapses to activate other neurons.) Thus, instead of developing a drug to treat an abnormality, an abnormality was postulated to fit a drug.

That was a great leap in logic, as all three authors point out. It was entirely possible that drugs that affected neurotransmitter levels could relieve symptoms even if neurotransmitters had nothing to do with the illness in the first place (and even possible that they relieved symptoms through some other mode of action entirely). As Carlat puts it, “By this same logic one could argue that the cause of all pain conditions is a deficiency of opiates, since narcotic pain medications activate opiate receptors in the brain.” Or similarly, one could argue that fevers are caused by too little aspirin.

But the main problem with the theory is that after decades of trying to prove it, researchers have still come up empty-handed. All three authors document the failure of scientists to find good evidence in its favor. Neurotransmitter function seems to be normal in people with mental illness before treatment. In Whitaker’s words:

Prior to treatment, patients diagnosed with schizophrenia, depression, and other psychiatric disorders do not suffer from any known “chemical imbalance.” However, once a person is put on a psychiatric medication, which, in one manner or another, throws a wrench into the usual mechanics of a neuronal pathway, his or her brain begins to function…abnormally.

Carlat refers to the chemical imbalance theory as a “myth” (which he calls “convenient” because it destigmatizes mental illness), and Kirsch, whose book focuses on depression, sums up this way: “It now seems beyond question that the traditional account of depression as a chemical imbalance in the brain is simply wrong.” Why the theory persists despite the lack of evidence is a subject I’ll come to.

Do the drugs work? After all, regardless of the theory, that is the practical question. In his spare, remarkably engrossing book, The Emperor’s New Drugs, Kirsch describes his fifteen-year scientific quest to answer that question about antidepressants. When he began his work in 1995, his main interest was in the effects of placebos. To study them, he and a colleague reviewed thirty-eight published clinical trials that compared various treatments for depression with placebos, or compared psychotherapy with no treatment. Most such trials last for six to eight weeks, and during that time, patients tend to improve somewhat even without any treatment. But Kirsch found that placebos were three times as effective as no treatment. That didn’t particularly surprise him. What did surprise him was the fact that antidepressants were only marginally better than placebos. As judged by scales used to measure depression, placebos were 75 percent as effective as antidepressants. Kirsch then decided to repeat his study by examining a more complete and standardized data set.

Advertisement

The data he used were obtained from the US Food and Drug Administration (FDA) instead of the published literature. When drug companies seek approval from the FDA to market a new drug, they must submit to the agency all clinical trials they have sponsored. The trials are usually double-blind and placebo-controlled, that is, the participating patients are randomly assigned to either drug or placebo, and neither they nor their doctors know which they have been assigned. The patients are told only that they will receive an active drug or a placebo, and they are also told of any side effects they might experience. If two trials show that the drug is more effective than a placebo, the drug is generally approved. But companies may sponsor as many trials as they like, most of which could be negative—that is, fail to show effectiveness. All they need is two positive ones. (The results of trials of the same drug can differ for many reasons, including the way the trial is designed and conducted, its size, and the types of patients studied.)

Edward Gorey Charitable Trust

For obvious reasons, drug companies make very sure that their positive studies are published in medical journals and doctors know about them, while the negative ones often languish unseen within the FDA, which regards them as proprietary and therefore confidential. This practice greatly biases the medical literature, medical education, and treatment decisions.

Kirsch and his colleagues used the Freedom of Information Act to obtain FDA reviews of all placebo-controlled clinical trials, whether positive or negative, submitted for the initial approval of the six most widely used antidepressant drugs approved between 1987 and 1999—Prozac, Paxil, Zoloft, Celexa, Serzone, and Effexor. This was a better data set than the one used in his previous study, not only because it included negative studies but because the FDA sets uniform quality standards for the trials it reviews and not all of the published research in Kirsch’s earlier study had been submitted to the FDA as part of a drug approval application.

Altogether, there were forty-two trials of the six drugs. Most of them were negative. Overall, placebos were 82 percent as effective as the drugs, as measured by the Hamilton Depression Scale (HAM-D), a widely used score of symptoms of depression. The average difference between drug and placebo was only 1.8 points on the HAM-D, a difference that, while statistically significant, was clinically meaningless. The results were much the same for all six drugs: they were all equally unimpressive. Yet because the positive studies were extensively publicized, while the negative ones were hidden, the public and the medical profession came to believe that these drugs were highly effective antidepressants.

Kirsch was also struck by another unexpected finding. In his earlier study and in work by others, he observed that even treatments that were not considered to be antidepressants—such as synthetic thyroid hormone, opiates, sedatives, stimulants, and some herbal remedies—were as effective as antidepressants in alleviating the symptoms of depression. Kirsch writes, “When administered as antidepressants, drugs that increase, decrease or have no effect on serotonin all relieve depression to about the same degree.” What all these “effective” drugs had in common was that they produced side effects, which participating patients had been told they might experience.

It is important that clinical trials, particularly those dealing with subjective conditions like depression, remain double-blind, with neither patients nor doctors knowing whether or not they are getting a placebo. That prevents both patients and doctors from imagining improvements that are not there, something that is more likely if they believe the agent being administered is an active drug instead of a placebo. Faced with his findings that nearly any pill with side effects was slightly more effective in treating depression than an inert placebo, Kirsch speculated that the presence of side effects in individuals receiving drugs enabled them to guess correctly that they were getting active treatment—and this was borne out by interviews with patients and doctors—which made them more likely to report improvement. He suggests that the reason antidepressants appear to work better in relieving severe depression than in less severe cases is that patients with severe symptoms are likely to be on higher doses and therefore experience more side effects.

To further investigate whether side effects bias responses, Kirsch looked at some trials that employed “active” placebos instead of inert ones. An active placebo is one that itself produces side effects, such as atropine—a drug that selectively blocks the action of certain types of nerve fibers. Although not an antidepressant, atropine causes, among other things, a noticeably dry mouth. In trials using atropine as the placebo, there was no difference between the antidepressant and the active placebo. Everyone had side effects of one type or another, and everyone reported the same level of improvement. Kirsch reported a number of other odd findings in clinical trials of antidepressants, including the fact that there is no dose-response curve—that is, high doses worked no better than low ones—which is extremely unlikely for truly effective drugs. “Putting all this together,” writes Kirsch,

leads to the conclusion that the relatively small difference between drugs and placebos might not be a real drug effect at all. Instead, it might be an enhanced placebo effect, produced by the fact that some patients have broken [the] blind and have come to realize whether they were given drug or placebo. If this is the case, then there is no real antidepressant drug effect at all. Rather than comparing placebo to drug, we have been comparing “regular” placebos to “extra-strength” placebos.

That is a startling conclusion that flies in the face of widely accepted medical opinion, but Kirsch reaches it in a careful, logical way. Psychiatrists who use antidepressants—and that’s most of them—and patients who take them might insist that they know from clinical experience that the drugs work. But anecdotes are known to be a treacherous way to evaluate medical treatments, since they are so subject to bias; they can suggest hypotheses to be studied, but they cannot prove them. That is why the development of the double-blind, randomized, placebo-controlled clinical trial in the middle of the past century was such an important advance in medical science. Anecdotes about leeches or laetrile or megadoses of vitamin C, or any number of other popular treatments, could not stand up to the scrutiny of well-designed trials. Kirsch is a faithful proponent of the scientific method, and his voice therefore brings a welcome objectivity to a subject often swayed by anecdotes, emotions, or, as we will see, self-interest.

Whitaker’s book is broader and more polemical. He considers all mental illness, not just depression. Whereas Kirsch concludes that antidepressants are probably no more effective than placebos, Whitaker concludes that they and most of the other psychoactive drugs are not only ineffective but harmful. He begins by observing that even as drug treatment for mental illness has skyrocketed, so has the prevalence of the conditions treated:

The number of disabled mentally ill has risen dramatically since 1955, and during the past two decades, a period when the prescribing of psychiatric medications has exploded, the number of adults and children disabled by mental illness has risen at a mind-boggling rate. Thus we arrive at an obvious question, even though it is heretical in kind: Could our drug-based paradigm of care, in some unforeseen way, be fueling this modern-day plague?

Moreover, Whitaker contends, the natural history of mental illness has changed. Whereas conditions such as schizophrenia and depression were once mainly self-limited or episodic, with each episode usually lasting no more than six months and interspersed with long periods of normalcy, the conditions are now chronic and lifelong. Whitaker believes that this might be because drugs, even those that relieve symptoms in the short term, cause long-term mental harms that continue after the underlying illness would have naturally resolved.

The evidence he marshals for this theory varies in quality. He doesn’t sufficiently acknowledge the difficulty of studying the natural history of any illness over a fifty-some-year time span during which many circumstances have changed, in addition to drug use. It is even more difficult to compare long-term outcomes in treated versus untreated patients, since treatment may be more likely in those with more severe disease at the outset. Nevertheless, Whitaker’s evidence is suggestive, if not conclusive.

If psychoactive drugs do cause harm, as Whitaker contends, what is the mechanism? The answer, he believes, lies in their effects on neurotransmitters. It is well understood that psychoactive drugs disturb neurotransmitter function, even if that was not the cause of the illness in the first place. Whitaker describes a chain of effects. When, for example, an SSRI antidepressant like Celexa increases serotonin levels in synapses, it stimulates compensatory changes through a process called negative feedback. In response to the high levels of serotonin, the neurons that secrete it (presynaptic neurons) release less of it, and the postsynaptic neurons become desensitized to it. In effect, the brain is trying to nullify the drug’s effects. The same is true for drugs that block neurotransmitters, except in reverse. For example, most antipsychotic drugs block dopamine, but the presynaptic neurons compensate by releasing more of it, and the postsynaptic neurons take it up more avidly. (This explanation is necessarily oversimplified, since many psychoactive drugs affect more than one of the many neurotransmitters.)

With long-term use of psychoactive drugs, the result is, in the words of Steve Hyman, a former director of the NIMH and until recently provost of Harvard University, “substantial and long-lasting alterations in neural function.” As quoted by Whitaker, the brain, Hyman wrote, begins to function in a manner “qualitatively as well as quantitatively different from the normal state.” After several weeks on psychoactive drugs, the brain’s compensatory efforts begin to fail, and side effects emerge that reflect the mechanism of action of the drugs. For example, the SSRIs may cause episodes of mania, because of the excess of serotonin. Antipsychotics cause side effects that resemble Parkinson’s disease, because of the depletion of dopamine (which is also depleted in Parkinson’s disease). As side effects emerge, they are often treated by other drugs, and many patients end up on a cocktail of psychoactive drugs prescribed for a cocktail of diagnoses. The episodes of mania caused by antidepressants may lead to a new diagnosis of “bipolar disorder” and treatment with a “mood stabilizer,” such as Depokote (an anticonvulsant) plus one of the newer antipsychotic drugs. And so on.

Some patients take as many as six psychoactive drugs daily. One well- respected researcher, Nancy Andreasen, and her colleagues published evidence that the use of antipsychotic drugs is associated with shrinkage of the brain, and that the effect is directly related to the dose and duration of treatment. As Andreasen explained to The New York Times, “The prefrontal cortex doesn’t get the input it needs and is being shut down by drugs. That reduces the psychotic symptoms. It also causes the prefrontal cortex to slowly atrophy.”*

Getting off the drugs is exceedingly difficult, according to Whitaker, because when they are withdrawn the compensatory mechanisms are left unopposed. When Celexa is withdrawn, serotonin levels fall precipitously because the presynaptic neurons are not releasing normal amounts and the postsynaptic neurons no longer have enough receptors for it. Similarly, when an antipsychotic is withdrawn, dopamine levels may skyrocket. The symptoms produced by withdrawing psychoactive drugs are often confused with relapses of the original disorder, which can lead psychiatrists to resume drug treatment, perhaps at higher doses.

Unlike the cool Kirsch, Whitaker is outraged by what he sees as an iatrogenic (i.e., inadvertent and medically introduced) epidemic of brain dysfunction, particularly that caused by the widespread use of the newer (“atypical”) antipsychotics, such as Zyprexa, which cause serious side effects. Here is what he calls his “quick thought experiment”:

Imagine that a virus suddenly appears in our society that makes people sleep twelve, fourteen hours a day. Those infected with it move about somewhat slowly and seem emotionally disengaged. Many gain huge amounts of weight—twenty, forty, sixty, and even one hundred pounds. Often their blood sugar levels soar, and so do their cholesterol levels. A number of those struck by the mysterious illness—including young children and teenagers—become diabetic in fairly short order…. The federal government gives hundreds of millions of dollars to scientists at the best universities to decipher the inner workings of this virus, and they report that the reason it causes such global dysfunction is that it blocks a multitude of neurotransmitter receptors in the brain—dopaminergic, serotonergic, muscarinic, adrenergic, and histaminergic. All of those neuronal pathways in the brain are compromised. Meanwhile, MRI studies find that over a period of several years, the virus shrinks the cerebral cortex, and this shrinkage is tied to cognitive decline. A terrified public clamors for a cure.

Now such an illness has in fact hit millions of American children and adults. We have just described the effects of Eli Lilly’s best-selling antipsychotic, Zyprexa.

If psychoactive drugs are useless, as Kirsch believes about antidepressants, or worse than useless, as Whitaker believes, why are they so widely prescribed by psychiatrists and regarded by the public and the profession as something akin to wonder drugs? Why is the current against which Kirsch and Whitaker and, as we will see, Carlat are swimming so powerful? I discuss these questions in Part II of this review.

—This is the first part of a two-part article."	https://www.nybooks.com/articles/2011/06/23/epidemic-mental-illness-why/	"An advertisement for Prozac, from The American Journal of Psychiatry, 1995
It seems that Americans are in the midst of a raging epidemic of mental illness, at least as judged by the increase in the numbers treated for it. The tally of those who are so disabled by mental disorders that they qualify for Supplemental Security Income (SSI) or Social Security Disability Insurance (SSDI) increased nearly two and a half times between 1987 and 2007—from one in 184 Americans to one in seventy-six. For children, the rise is even more startling—a thirty-five-fold increase in the same two decades. Mental illness is now the leading cause of disability in children, well ahead of physical disabilities like cerebral palsy or Down syndrome, for which the federal programs were created.
A large survey of randomly selected adults, sponsored by the National Institute of Mental Health (NIMH) and conducted between 2001 and 2003, found that an astonishing 46 percent met criteria established by the American Psychiatric Association (APA) for having had at least one mental illness within four broad categories at some time in their lives. The categories were “anxiety disorders,” including, among other subcategories, phobias and post-traumatic stress disorder (PTSD); “mood disorders,” including major depression and bipolar disorders; “impulse-control disorders,” including various behavioral problems and attention-deficit/hyperactivity disorder (ADHD); and “substance use disorders,” including alcohol and drug abuse. Most met criteria for more than one diagnosis. Of a subgroup affected within the previous year, a third were under treatment—up from a fifth in a similar survey ten years earlier.
Nowadays treatment by medical doctors nearly always means psychoactive drugs, that is, drugs that affect the mental state. In fact, most psychiatrists treat only with drugs, and refer patients to psychologists or social workers if they believe psychotherapy is also warranted. The shift from “talk therapy” to drugs as the dominant mode of treatment coincides with the emergence over the past four decades of the theory that mental illness is caused primarily by chemical imbalances in the brain that can be corrected by specific drugs. That theory became broadly accepted, by the media and the public as well as by the medical profession, after Prozac came to market in 1987 and was intensively promoted as a corrective for a deficiency of serotonin in the brain. The number of people treated for depression tripled in the following ten years, and about 10 percent of Americans over age six now take antidepressants. The increased use of drugs to treat psychosis is even more dramatic. The new generation of antipsychotics, such as Risperdal, Zyprexa, and Seroquel, has replaced cholesterol-lowering agents as the top-selling class of drugs in the US.
What is going on here? Is the prevalence of mental illness really that high and still climbing? Particularly if these disorders are biologically determined and not a result of environmental influences, is it plausible to suppose that such an increase is real? Or are we learning to recognize and diagnose mental disorders that were always there? On the other hand, are we simply expanding the criteria for mental illness so that nearly everyone has one? And what about the drugs that are now the mainstay of treatment? Do they work? If they do, shouldn’t we expect the prevalence of mental illness to be declining, not rising?
These are the questions, among others, that concern the authors of the three provocative books under review here. They come at the questions from different backgrounds—Irving Kirsch is a psychologist at the University of Hull in the UK, Robert Whitaker a journalist and previously the author of a history of the treatment of mental illness called Mad in America (2001), and Daniel Carlat a psychiatrist who practices in a Boston suburb and publishes a newsletter and blog about his profession.
The authors emphasize different aspects of the epidemic of mental illness. Kirsch is concerned with whether antidepressants work. Whitaker, who has written an angrier book, takes on the entire spectrum of mental illness and asks whether psychoactive drugs create worse problems than they solve. Carlat, who writes more in sorrow than in anger, looks mainly at how his profession has allied itself with, and is manipulated by, the pharmaceutical industry. But despite their differences, all three are in remarkable agreement on some important matters, and they have documented their views well.
First, they agree on the disturbing extent to which the companies that sell psychoactive drugs—through various forms of marketing, both legal and illegal, and what many people would describe as bribery—have come to determine what constitutes a mental illness and how the disorders should be diagnosed and treated. This is a subject to which I’ll return.
Second, none of the three authors subscribes to the popular theory that mental illness is caused by a chemical imbalance in the brain. As Whitaker tells the story, that theory had its genesis shortly after psychoactive drugs were introduced in the 1950s. The first was Thorazine (chlorpromazine), which was launched in 1954 as a “major tranquilizer” and quickly found widespread use in mental hospitals to calm psychotic patients, mainly those with schizophrenia. Thorazine was followed the next year by Miltown (meprobamate), sold as a “minor tranquilizer” to treat anxiety in outpatients. And in 1957, Marsilid (iproniazid) came on the market as a “psychic energizer” to treat depression.
Advertisement
In the space of three short years, then, drugs had become available to treat what at that time were regarded as the three major categories of mental illness—psychosis, anxiety, and depression—and the face of psychiatry was totally transformed. These drugs, however, had not initially been developed to treat mental illness. They had been derived from drugs meant to treat infections, and were found only serendipitously to alter the mental state. At first, no one had any idea how they worked. They simply blunted disturbing mental symptoms. But over the next decade, researchers found that these drugs, and the newer psychoactive drugs that quickly followed, affected the levels of certain chemicals in the brain.
Some brief—and necessarily quite simplified—background: the brain contains billions of nerve cells, called neurons, arrayed in immensely complicated networks and communicating with one another constantly. The typical neuron has multiple filamentous extensions, one called an axon and the others called dendrites, through which it sends and receives signals from other neurons. For one neuron to communicate with another, however, the signal must be transmitted across the tiny space separating them, called a synapse. To accomplish that, the axon of the sending neuron releases a chemical, called a neurotransmitter, into the synapse. The neurotransmitter crosses the synapse and attaches to receptors on the second neuron, often a dendrite, thereby activating or inhibiting the receiving cell. Axons have multiple terminals, so each neuron has multiple synapses. Afterward, the neurotransmitter is either reabsorbed by the first neuron or metabolized by enzymes so that the status quo ante is restored. There are exceptions and variations to this story, but that is the usual way neurons communicate with one another.
When it was found that psychoactive drugs affect neurotransmitter levels in the brain, as evidenced mainly by the levels of their breakdown products in the spinal fluid, the theory arose that the cause of mental illness is an abnormality in the brain’s concentration of these chemicals that is specifically countered by the appropriate drug. For example, because Thorazine was found to lower dopamine levels in the brain, it was postulated that psychoses like schizophrenia are caused by too much dopamine. Or later, because certain antidepressants increase levels of the neurotransmitter serotonin in the brain, it was postulated that depression is caused by too little serotonin. (These antidepressants, like Prozac or Celexa, are called selective serotonin reuptake inhibitors (SSRIs) because they prevent the reabsorption of serotonin by the neurons that release it, so that more remains in the synapses to activate other neurons.) Thus, instead of developing a drug to treat an abnormality, an abnormality was postulated to fit a drug.
That was a great leap in logic, as all three authors point out. It was entirely possible that drugs that affected neurotransmitter levels could relieve symptoms even if neurotransmitters had nothing to do with the illness in the first place (and even possible that they relieved symptoms through some other mode of action entirely). As Carlat puts it, “By this same logic one could argue that the cause of all pain conditions is a deficiency of opiates, since narcotic pain medications activate opiate receptors in the brain.” Or similarly, one could argue that fevers are caused by too little aspirin.
But the main problem with the theory is that after decades of trying to prove it, researchers have still come up empty-handed. All three authors document the failure of scientists to find good evidence in its favor. Neurotransmitter function seems to be normal in people with mental illness before treatment. In Whitaker’s words:
Prior to treatment, patients diagnosed with schizophrenia, depression, and other psychiatric disorders do not suffer from any known “chemical imbalance.” However, once a person is put on a psychiatric medication, which, in one manner or another, throws a wrench into the usual mechanics of a neuronal pathway, his or her brain begins to function…abnormally.
Carlat refers to the chemical imbalance theory as a “myth” (which he calls “convenient” because it destigmatizes mental illness), and Kirsch, whose book focuses on depression, sums up this way: “It now seems beyond question that the traditional account of depression as a chemical imbalance in the brain is simply wrong.” Why the theory persists despite the lack of evidence is a subject I’ll come to.
Do the drugs work? After all, regardless of the theory, that is the practical question. In his spare, remarkably engrossing book, The Emperor’s New Drugs, Kirsch describes his fifteen-year scientific quest to answer that question about antidepressants. When he began his work in 1995, his main interest was in the effects of placebos. To study them, he and a colleague reviewed thirty-eight published clinical trials that compared various treatments for depression with placebos, or compared psychotherapy with no treatment. Most such trials last for six to eight weeks, and during that time, patients tend to improve somewhat even without any treatment. But Kirsch found that placebos were three times as effective as no treatment. That didn’t particularly surprise him. What did surprise him was the fact that antidepressants were only marginally better than placebos. As judged by scales used to measure depression, placebos were 75 percent as effective as antidepressants. Kirsch then decided to repeat his study by examining a more complete and standardized data set.
Advertisement
The data he used were obtained from the US Food and Drug Administration (FDA) instead of the published literature. When drug companies seek approval from the FDA to market a new drug, they must submit to the agency all clinical trials they have sponsored. The trials are usually double-blind and placebo-controlled, that is, the participating patients are randomly assigned to either drug or placebo, and neither they nor their doctors know which they have been assigned. The patients are told only that they will receive an active drug or a placebo, and they are also told of any side effects they might experience. If two trials show that the drug is more effective than a placebo, the drug is generally approved. But companies may sponsor as many trials as they like, most of which could be negative—that is, fail to show effectiveness. All they need is two positive ones. (The results of trials of the same drug can differ for many reasons, including the way the trial is designed and conducted, its size, and the types of patients studied.)
Edward Gorey Charitable Trust
For obvious reasons, drug companies make very sure that their positive studies are published in medical journals and doctors know about them, while the negative ones often languish unseen within the FDA, which regards them as proprietary and therefore confidential. This practice greatly biases the medical literature, medical education, and treatment decisions.
Kirsch and his colleagues used the Freedom of Information Act to obtain FDA reviews of all placebo-controlled clinical trials, whether positive or negative, submitted for the initial approval of the six most widely used antidepressant drugs approved between 1987 and 1999—Prozac, Paxil, Zoloft, Celexa, Serzone, and Effexor. This was a better data set than the one used in his previous study, not only because it included negative studies but because the FDA sets uniform quality standards for the trials it reviews and not all of the published research in Kirsch’s earlier study had been submitted to the FDA as part of a drug approval application.
Altogether, there were forty-two trials of the six drugs. Most of them were negative. Overall, placebos were 82 percent as effective as the drugs, as measured by the Hamilton Depression Scale (HAM-D), a widely used score of symptoms of depression. The average difference between drug and placebo was only 1.8 points on the HAM-D, a difference that, while statistically significant, was clinically meaningless. The results were much the same for all six drugs: they were all equally unimpressive. Yet because the positive studies were extensively publicized, while the negative ones were hidden, the public and the medical profession came to believe that these drugs were highly effective antidepressants.
Kirsch was also struck by another unexpected finding. In his earlier study and in work by others, he observed that even treatments that were not considered to be antidepressants—such as synthetic thyroid hormone, opiates, sedatives, stimulants, and some herbal remedies—were as effective as antidepressants in alleviating the symptoms of depression. Kirsch writes, “When administered as antidepressants, drugs that increase, decrease or have no effect on serotonin all relieve depression to about the same degree.” What all these “effective” drugs had in common was that they produced side effects, which participating patients had been told they might experience.
It is important that clinical trials, particularly those dealing with subjective conditions like depression, remain double-blind, with neither patients nor doctors knowing whether or not they are getting a placebo. That prevents both patients and doctors from imagining improvements that are not there, something that is more likely if they believe the agent being administered is an active drug instead of a placebo. Faced with his findings that nearly any pill with side effects was slightly more effective in treating depression than an inert placebo, Kirsch speculated that the presence of side effects in individuals receiving drugs enabled them to guess correctly that they were getting active treatment—and this was borne out by interviews with patients and doctors—which made them more likely to report improvement. He suggests that the reason antidepressants appear to work better in relieving severe depression than in less severe cases is that patients with severe symptoms are likely to be on higher doses and therefore experience more side effects.
To further investigate whether side effects bias responses, Kirsch looked at some trials that employed “active” placebos instead of inert ones. An active placebo is one that itself produces side effects, such as atropine—a drug that selectively blocks the action of certain types of nerve fibers. Although not an antidepressant, atropine causes, among other things, a noticeably dry mouth. In trials using atropine as the placebo, there was no difference between the antidepressant and the active placebo. Everyone had side effects of one type or another, and everyone reported the same level of improvement. Kirsch reported a number of other odd findings in clinical trials of antidepressants, including the fact that there is no dose-response curve—that is, high doses worked no better than low ones—which is extremely unlikely for truly effective drugs. “Putting all this together,” writes Kirsch,
leads to the conclusion that the relatively small difference between drugs and placebos might not be a real drug effect at all. Instead, it might be an enhanced placebo effect, produced by the fact that some patients have broken [the] blind and have come to realize whether they were given drug or placebo. If this is the case, then there is no real antidepressant drug effect at all. Rather than comparing placebo to drug, we have been comparing “regular” placebos to “extra-strength” placebos.
That is a startling conclusion that flies in the face of widely accepted medical opinion, but Kirsch reaches it in a careful, logical way. Psychiatrists who use antidepressants—and that’s most of them—and patients who take them might insist that they know from clinical experience that the drugs work. But anecdotes are known to be a treacherous way to evaluate medical treatments, since they are so subject to bias; they can suggest hypotheses to be studied, but they cannot prove them. That is why the development of the double-blind, randomized, placebo-controlled clinical trial in the middle of the past century was such an important advance in medical science. Anecdotes about leeches or laetrile or megadoses of vitamin C, or any number of other popular treatments, could not stand up to the scrutiny of well-designed trials. Kirsch is a faithful proponent of the scientific method, and his voice therefore brings a welcome objectivity to a subject often swayed by anecdotes, emotions, or, as we will see, self-interest.
Whitaker’s book is broader and more polemical. He considers all mental illness, not just depression. Whereas Kirsch concludes that antidepressants are probably no more effective than placebos, Whitaker concludes that they and most of the other psychoactive drugs are not only ineffective but harmful. He begins by observing that even as drug treatment for mental illness has skyrocketed, so has the prevalence of the conditions treated:
The number of disabled mentally ill has risen dramatically since 1955, and during the past two decades, a period when the prescribing of psychiatric medications has exploded, the number of adults and children disabled by mental illness has risen at a mind-boggling rate. Thus we arrive at an obvious question, even though it is heretical in kind: Could our drug-based paradigm of care, in some unforeseen way, be fueling this modern-day plague?
Moreover, Whitaker contends, the natural history of mental illness has changed. Whereas conditions such as schizophrenia and depression were once mainly self-limited or episodic, with each episode usually lasting no more than six months and interspersed with long periods of normalcy, the conditions are now chronic and lifelong. Whitaker believes that this might be because drugs, even those that relieve symptoms in the short term, cause long-term mental harms that continue after the underlying illness would have naturally resolved.
The evidence he marshals for this theory varies in quality. He doesn’t sufficiently acknowledge the difficulty of studying the natural history of any illness over a fifty-some-year time span during which many circumstances have changed, in addition to drug use. It is even more difficult to compare long-term outcomes in treated versus untreated patients, since treatment may be more likely in those with more severe disease at the outset. Nevertheless, Whitaker’s evidence is suggestive, if not conclusive.
If psychoactive drugs do cause harm, as Whitaker contends, what is the mechanism? The answer, he believes, lies in their effects on neurotransmitters. It is well understood that psychoactive drugs disturb neurotransmitter function, even if that was not the cause of the illness in the first place. Whitaker describes a chain of effects. When, for example, an SSRI antidepressant like Celexa increases serotonin levels in synapses, it stimulates compensatory changes through a process called negative feedback. In response to the high levels of serotonin, the neurons that secrete it (presynaptic neurons) release less of it, and the postsynaptic neurons become desensitized to it. In effect, the brain is trying to nullify the drug’s effects. The same is true for drugs that block neurotransmitters, except in reverse. For example, most antipsychotic drugs block dopamine, but the presynaptic neurons compensate by releasing more of it, and the postsynaptic neurons take it up more avidly. (This explanation is necessarily oversimplified, since many psychoactive drugs affect more than one of the many neurotransmitters.)
With long-term use of psychoactive drugs, the result is, in the words of Steve Hyman, a former director of the NIMH and until recently provost of Harvard University, “substantial and long-lasting alterations in neural function.” As quoted by Whitaker, the brain, Hyman wrote, begins to function in a manner “qualitatively as well as quantitatively different from the normal state.” After several weeks on psychoactive drugs, the brain’s compensatory efforts begin to fail, and side effects emerge that reflect the mechanism of action of the drugs. For example, the SSRIs may cause episodes of mania, because of the excess of serotonin. Antipsychotics cause side effects that resemble Parkinson’s disease, because of the depletion of dopamine (which is also depleted in Parkinson’s disease). As side effects emerge, they are often treated by other drugs, and many patients end up on a cocktail of psychoactive drugs prescribed for a cocktail of diagnoses. The episodes of mania caused by antidepressants may lead to a new diagnosis of “bipolar disorder” and treatment with a “mood stabilizer,” such as Depokote (an anticonvulsant) plus one of the newer antipsychotic drugs. And so on.
Some patients take as many as six psychoactive drugs daily. One well- respected researcher, Nancy Andreasen, and her colleagues published evidence that the use of antipsychotic drugs is associated with shrinkage of the brain, and that the effect is directly related to the dose and duration of treatment. As Andreasen explained to The New York Times, “The prefrontal cortex doesn’t get the input it needs and is being shut down by drugs. That reduces the psychotic symptoms. It also causes the prefrontal cortex to slowly atrophy.”*
Getting off the drugs is exceedingly difficult, according to Whitaker, because when they are withdrawn the compensatory mechanisms are left unopposed. When Celexa is withdrawn, serotonin levels fall precipitously because the presynaptic neurons are not releasing normal amounts and the postsynaptic neurons no longer have enough receptors for it. Similarly, when an antipsychotic is withdrawn, dopamine levels may skyrocket. The symptoms produced by withdrawing psychoactive drugs are often confused with relapses of the original disorder, which can lead psychiatrists to resume drug treatment, perhaps at higher doses.
Unlike the cool Kirsch, Whitaker is outraged by what he sees as an iatrogenic (i.e., inadvertent and medically introduced) epidemic of brain dysfunction, particularly that caused by the widespread use of the newer (“atypical”) antipsychotics, such as Zyprexa, which cause serious side effects. Here is what he calls his “quick thought experiment”:
Imagine that a virus suddenly appears in our society that makes people sleep twelve, fourteen hours a day. Those infected with it move about somewhat slowly and seem emotionally disengaged. Many gain huge amounts of weight—twenty, forty, sixty, and even one hundred pounds. Often their blood sugar levels soar, and so do their cholesterol levels. A number of those struck by the mysterious illness—including young children and teenagers—become diabetic in fairly short order…. The federal government gives hundreds of millions of dollars to scientists at the best universities to decipher the inner workings of this virus, and they report that the reason it causes such global dysfunction is that it blocks a multitude of neurotransmitter receptors in the brain—dopaminergic, serotonergic, muscarinic, adrenergic, and histaminergic. All of those neuronal pathways in the brain are compromised. Meanwhile, MRI studies find that over a period of several years, the virus shrinks the cerebral cortex, and this shrinkage is tied to cognitive decline. A terrified public clamors for a cure.
Now such an illness has in fact hit millions of American children and adults. We have just described the effects of Eli Lilly’s best-selling antipsychotic, Zyprexa.
If psychoactive drugs are useless, as Kirsch believes about antidepressants, or worse than useless, as Whitaker believes, why are they so widely prescribed by psychiatrists and regarded by the public and the profession as something akin to wonder drugs? Why is the current against which Kirsch and Whitaker and, as we will see, Carlat are swimming so powerful? I discuss these questions in Part II of this review.
—This is the first part of a two-part article."	25665
health	['Peter Wilson']	2019-02-28 00:00:00	"For more than a century we’ve counted on calories to tell us what will make us fat. Peter Wilson says it’s time to bury the world’s most misleading measure

The first time that Salvador Camacho thought he was going to die he was sitting in his father’s Chrysler sedan with a friend listening to music. The 22-year-old engineering student was parked near his home in the central Mexican city of Toluca and in the fading evening light he didn’t notice two tattooed men approach. Tori Amos’s hit, “Bliss”, had just started playing when the gang members pointed guns at the young men.

So began a 24-hour ordeal. Strong willed and solidly built, Camacho was singled out as the more stubborn of the pair. He was blindfolded and beaten. One robber eventually threw him to the ground, put a gun to the back of his head and told him it was time to die. He passed out, waking in a field with his hands tied behind his back, almost naked.

Camacho survived but, traumatised, he sank into depression. Soon he was drinking heavily and binge eating. His weight ballooned from a trim 70kg to 103kg.

That led to his second near-death experience, eight years later, in 2007. He remembers waking up and blinking at bright lights: he was being wheeled on a stretcher into a hospital emergency ward, with an attack of severe arrhythmia, or irregular heart beat. “A cardiologist told me that if I didn’t lose weight and get my health under control I would be dead in five years,” he says.

That second crisis forced Camacho belatedly to deal with the trauma of the first. To help with what he now understands was post-traumatic stress disorder, he started having counselling and taking anti-depressants and anti-anxiety drugs. To address his physical health, he tried to lose weight. This effort propelled him to the centre of one of the most fraught scientific debates of our age: the calorie wars, a fierce disagreement about diet and weight control.

Today, more than a decade after his cardiologist’s stark warning, Camacho lives in the Swiss city of Basel. He is relaxed and confident, except when two topics come up. When he recounts his kidnapping his gaze drops, his smile vanishes and he becomes noticeably quieter, although he says his panic attacks have virtually disappeared. The other touchy topic is weight control, which causes him to shake his head in anger at what he and millions of other dieters have gone through. “It’s just ridiculous,” he says with exasperation and a touch of venom. “People are living with real pain and guilt and all they get is advice that is confused or just plain wrong.”

The guidance that Camacho’s doctors gave him, along with a string of nutritionists and his own online research, was unanimous. It would be familiar to the millions of people who have ever tried to diet. “Everybody tells you that to lose weight you have to eat less and move more,” he says, “and the way to do that is to count your calories.”

At his heaviest, Camacho’s body-mass index – the ratio of his height to his weight – reached 35.6, well above the 30 mark that doctors define as clinically obese. Most government guidelines indicated that, as a man, he needed 2,500 calories a day to maintain his weight (the target for women is 2,000). Nutritionists told Camacho that if he ate fewer than 2,000 calories a day, a weekly “deficit” of 3,500 would mean that he would lose 0.5kg a week.

With a desk job as a planning engineer in a Mexican hospital, he knew it would take real discipline to trim his pudgy frame. But as his kidnappers had quickly realised, he is an unusually determined character. He began getting up before dawn each day to run 10km. He also started accounting for every morsel of food he consumed.

“I filled in Excel spreadsheets every night, every week and every month listing everything I ate. It became a real obsession for me,” says Camacho. Out went the Burger King Whoppers, fried tacos packed with pork and cheese, and tortas (Mexican sandwiches filled with meat, refried beans, avocado and peppers). Out too went his usual steady flow of beer and wine. In came carefully measured low-fat cheese and turkey sandwiches, salads, canned peach juice, Gatorade and Coke Zero, with three Special-K low-calorie diet bars a day.

“I was always tired and hungry and I would get really moody and distracted,” he says. “I was thinking about food all the time.” He was constantly told that if he got the maths right – consuming fewer calories than he burned each day – the results would soon show. “I really did everything you are supposed to do,” he insists with the tone of a schoolboy who completed his homework yet still failed a big test. He bought a battery of exercise monitoring devices to measure how many calories he was expending on his runs. “I was told to exercise for at least 45 minutes at least four or five times a week. I actually ran for more than an hour every day.” He kept to low-fat, low-calorie food for three years. It simply didn’t work. At one point he lost about 10kg but his weight rebounded, though he still restricted his calories.

Dieters the world over will be familiar with Camacho’s frustrations. Most studies show that more than 80% of people regain any lost weight in the long term. And like him, when we fail, most of us assume that we are too lazy or greedy – that we are at fault.

As a general rule it is true that if you eat vastly fewer calories than you burn, you’ll get slimmer (and if you consume far more, you’ll get fatter). But the myriad faddy diets flogged to us each year belie the simplicity of the formula that Camacho was given. The calorie as a scientific measurement is not in dispute. But calculating the exact calorific content of food is far harder than the confidently precise numbers displayed on food packets suggest. Two items of food with identical calorific values may be digested in very different ways. Each body processes calories differently. Even for a single individual, the time of day that you eat matters. The more we probe, the more we realise that tallying calories will do little to help us control our weight or even maintain a healthy diet: the beguiling simplicity of counting calories in and calories out is dangerously flawed.

The calorie is ubiquitous in daily life. It takes top billing on the information label of most packaged food and drinks. Ever more restaurants list the number of calories in each dish on their menus. Counting the calories we expend has become just as standard. Gym equipment, fitness devices around our wrists, even our phones tell us how many calories we have supposedly burned in a single exercise session or over the course of a day.

It wasn’t always thus. For centuries, scientists assumed that it was the mass of food consumed that was significant. In the late 16th century an Italian physician named Santorio Sanctorius invented a “weighing chair”, dangling from a giant scale, in which he sat at regular intervals to weigh himself, everything he ate and drank, and all the faeces and urine he produced. Despite 30 years of compulsive chair dangling, Sanctorius answered few of his own questions about the impact that his consumption had on his body.

Only later did the focus shift to the energy different foodstuffs contained. In the 18th century Antoine Lavoisier, a French aristocrat, worked out that burning a candle required a gas from the air – which he named oxygen – to fuel the flame and release heat and other gases. He applied the same principle to food, concluding that it fuels the body like a slow-burning fire. He built a calorimeter, a device big enough to hold a guinea pig, and measured the heat the creature generated to estimate how much energy it was producing. Unfortunately the French revolution – specifically the guillotine – cut short his thinking on the subject. But he had started something. Other scientists later constructed “bomb calori­meters” in which they burned food to measure the heat – and thus the potential energy – released from it.

The calorie – which comes from “calor”, the Latin for “heat” – was originally used to measure the efficiency of steam engines: one calorie is the energy required to heat 1kg of water by one degree Celsius. Only in the 1860s did German scientists begin using it to calculate the energy in food. It was an American agricultural chemist, Wilbur Atwater, who popularised the idea that it could be used to measure both the energy contained in food and the energy the body expended on things like muscular work, tissue repair and powering the organs. In 1887, after a trip to Germany, he wrote a series of wildly popular articles in Century, an American magazine, suggesting that “food is to the body what fuel is to the fire.” He introduced the public to the notion of “macronutrients” – carbohydrates, protein and fat – so called because the body needs a lot of them.

Today many of us want to monitor our calorie consumption in order to lose or maintain our weight. Atwater, the son of a Methodist minister, was motivated by the opposite concern: at a time when malnutrition was widespread, he sought to help poor people find the most cost-effective items to fill themselves up.

To see how much energy different macronutrients provided to the body, he fed samples of an “average” American diet of that era – which he believed to be heavy in molasses cookies, barley meal and chicken gizzards – to a group of male students in a basement at Wesleyan University in Middletown, Connecticut. For up to 12 days at a time a volunteer would eat, sleep and lift weights while sealed inside a six-foot-high chamber measuring four feet wide by seven feet deep. The energy in each meal was calculated by burning identical foods in a bomb calorimeter.

The walls were filled with water, and changes in its temperature allowed Atwater to calculate how much energy the students’ bodies were generating. His team collected the students’ faeces and burned that too, to see how much energy had been left in the body in the digestion process.

This was pioneering stuff for the 1890s. Atwater eventually concluded that a gram of either carbohydrate or protein made an average of four calories of energy available to the body, and a gram of fat offered an average of 8.9 calories, a figure later rounded up to nine calories for convenience. We now know far more about the workings of the human body: Atwater was right that some of a meal’s potential energy was excreted, but had no idea that some was also used to digest the meal itself, and that the body expends different amounts of energy depending on the food. Yet more than a century after igniting the faeces of Wesleyan students, the numbers Atwater calculated for each macro­nutrient remain the standard for measuring the calories in any given food stuff. Those experiments were the basis of Salvador Camacho’s daily calorific arithmetic.

Atwater transformed the way the public thought about food, with his simple belief that “a calorie is a calorie”. He counselled the poor against eating too many leafy green vegetables because they weren’t sufficiently dense in energy. By his account, it made no difference whether calories came from chocolate or spinach: if the body absorbed more energy than it used, then it would store the excess as body fat, causing you to put on weight.

That idea captured the public imagination. In 1918 the first book was published in America based on the notion that a healthy diet was no more complicated than the simple addition and subtraction of calories. “You may eat just what you like – candy, pie, cake, fat meat, butter, cream but count your calories!” wrote Lulu Hunt Peters in “Diet and Health”. “Now that you know you can have the things you like, proceed to make your menus containing very little of them.” The book sold millions.

By the 1930s the calorie had become entrenched in both the public mind and government policy. Its exclusive focus on the energy content of food, rather than its vitamin content, say, went virtually unchallenged. Rising incomes and greater female participation in the workforce meant that by the 1960s people were eating out more often or buying prepared food, so they wanted more information about what they were consuming. Nutritional information on foodstuffs was widespread but haphazard; many items carried outlandish claims about their health benefits. Labelling became standardised and mandatory in America only in 1990.

The emphasis and use of this information shifted too. By the late 1960s, obesity was becoming a pressing health concern as people became more sedentary and started eating highly processed foods and lots of sugar. As the number of people who needed to lose weight grew, changing diets became the focus of attention.

So began the war on fat, in which Atwater’s calorie calculations were an unwitting ally. Because counting calories was seen as an objective arbiter of the health qualities of a foodstuff, it seemed logical that the most calorie-laden part of any food item – fat – must be bad for you. By this measure, dishes low in calories, but rich in sugar and carbohydrates, seemed healthier. People were increasingly willing to blame fat for many of the health ills of modern life, helped along by the sugar lobby: in 2016, a researcher at the University of California uncovered documents from 1967 showing that sugar companies secretly funded studies at Harvard University designed to blame fat for the growing obesity epidemic. That the dietary “fat” found in olive oil, bacon and butter is branded with the same word as the unwanted flesh around our middles made it all the easier to demonise.

A US Senate committee report in 1977 recommended a low-fat, low-cholesterol diet for all, and other governments followed suit. The food industry responded with enthusiasm, removing fat, the most calorie-dense of macronutrients, from food items and replacing it with sugar, starch and salt. As a bonus, the thousands of new cheap and tasty “low-cal” and “low-fat” products which Camacho used to diet tended to have longer shelf lives and higher profit margins.

But this didn’t lead to the expected improvements in public health. Instead, it coincided almost exactly with the most dramatic rise in obesity in human history. Between 1975 and 2016 obesity almost tripled worldwide, according to the World Health Organisation ( WHO ): nearly 40% of over-18s – some 1.9bn adults – are now overweight. That contributed to a rapid rise in cardiovascular diseases (mainly heart disease and stroke) which became the leading cause of death worldwide. Rates of type-2 diabetes, which is often linked to lifestyle and diet, have more than doubled since 1980.

It wasn’t only wealthy countries that saw such trends. In Mexico, middle-class urban families such as Camacho’s got fatter too. As a child Camacho was fit and loved playing football. But at the age of ten, in 1988, he was one of many young Mexicans who started stacking on weight as increasing trade with America saw cheap sweets and fizzy drinks flood the shops, a process known as the “Coca-colonisation” of Mexico. “There were suddenly all these flavours you had never tasted, with chocolates, candies and Dr Pepper,” Camacho remembers: “Overnight I got fat.” When his uncles teased him about his bulging waistline, he cut back on sweets and stayed in good shape until his kidnapping 12 years later. Other Mexicans just kept bulking up. In 2013 Mexico overtook America as the most obese country in the world.

To combat this trend, governments worldwide have enshrined calorie-counting in policy. The WHO attributes the “fundamental cause” of obesity worldwide to “an energy imbalance between calories consumed and calories expended”. Governments the world over persist in offering the same advice: count and cut calories. This has infiltrated ever more areas of life. In 2018 the American government ordered food chains and vending machines to provide calorie details on their menus, to help consumers make “informed and healthful decisions”. Australia and Britain are headed in similar directions. Government bodies advise dieters to record their meals in a calorie journal to lose weight. The experimental efforts of a 19th-century scientist stand barely changed – and are barely questioned.

Millions of dieters give up when their calorie-counting is unsuccessful. Camacho was more stubborn than most. He took photos of his meals to record his intake more accurately, and would log into his calorie spreadsheets from his phone. He thought about every morsel he ate. And he bought a proliferation of gadgets to track his calorie output. But he still didn’t lose much weight.

One problem was that his sums were based on the idea that calorie counts are accurate. Food producers give impressively specific readings: a slice of Camacho’s favourite Domino’s double pepperoni pizza is supposedly 248 calories (not 247 nor 249). Yet the number of calories listed on food packets and menus are routinely wrong.

Susan Roberts, a nutritionist at Tufts University in Boston, has found that labels on American packaged foods miss their true calorie counts by an average of 8%. American government regulations allow such labels to understate calories by up to 20% (to ensure that consumers are not short-changed in terms of how much nutrition they receive). The information on some processed frozen foods misstates their calorific content by as much as 70%.

That isn’t the only problem. Calorie counts are based on how much heat a foodstuff gives off when it burns in an oven. But the human body is far more complex than an oven. When food is burned in a laboratory it surrenders its calories within seconds. By contrast, the real-life journey from dinner plate to toilet bowl takes on average about a day, but can range from eight to 80 hours depending on the person. A calorie of carbohydrate and a calorie of protein both have the same amount of stored energy, so they perform identically in an oven. But put those calories into real bodies and they behave quite differently. And we are still learning new insights: American researchers discovered last year that, for more than a century, we’ve been exaggerating by about 20% the number of calories we absorb from almonds.

The process of storing fat – the “weight” many people seek to lose – is influenced by dozens of other factors. Apart from calories, our genes, the trillions of bacteria that live in our gut, food preparation and sleep affect how we process food. Academic discussions of food and nutrition are littered with references to huge bodies of research that still need to be conducted. “No other field of science or medicine sees such a lack of rigorous studies,” says Tim Spector, a professor of genetic epidemiology at Kings College in London. “We can create synthetic DNA and clone animals but we still know incredibly little about the stuff that keeps us alive.”

What we do know, however, suggests that counting calories is very crude and often misleading. Think of a burger, the kind of food that Camacho eschewed during his early efforts to lose weight. Take a bite and the saliva in your mouth starts to break it down, a process that continues when you swallow, transporting the morsel towards your stomach and beyond to be churned further. The digestive process transforms the protein, carbohydrates and fat in the burger into their basic compounds so that they are tiny enough to be absorbed into the bloodstream via the small intestine to fuel and repair the trillions of cells in the body. But the basic molecules from each macronutrient play very different roles within the body.

All carbohydrates break down into sugars, which are the body’s main fuel source. But the speed at which your body gets its fuel from food can be as important as the amount of fuel. Simple carbohydrates are swiftly absorbed into the bloodstream, providing a fast shot of energy: the body absorbs the sugar from a can of fizzy drink at a rate of 30 calories a minute, compared with two calories a minute from complex carbohydrates such as potatoes or rice. That matters, because a sudden hit of sugar prompts the rapid release of insulin, a hormone that carries the sugar out of the bloodstream and into the body’s cells. Problems arise when there is too much sugar in the blood. The liver can store some of the excess, but any that remains is stashed as fat. So consuming large quantities of sugar is the fastest way to create body fat. And, once the insulin has done its work, blood-sugar levels slump, which tends to leave you hungry, as well as plumper.

Getting fat is a consequence of civilisation. Our ancestors would have enjoyed a heavy hit of sugar perhaps four times a year, when a new season produced fresh fruit. Many now enjoy that kind of sugar kick every day. The average person in the developed world consumes 20 times as much sugar as people did even during Atwater’s time.

But it is a different story when you eat complex carbohydrates such as cereals. These are strung together from simple carbohydrates, so they also break down into sugar, but because they do so more slowly, your blood-sugar levels remain steadier. The fruit juices that Camacho was encouraged to drink contained fewer calories than one of his wholegrain buns but the bread delivered less of a sugar hit and left him feeling satiated for longer.

Other macronutrients have different functions. Protein, the dominant component of meat, fish and dairy products, acts as the main building block for bone, skin, hair and other body tissues. In the absence of sufficient quantities of carbohydrates it can also serve as fuel for the body. But since it is broken down more slowly than carbohydrates, protein is less likely to be converted to body fat.

Fat is a different matter again. It should leave you feeling fuller for longer, because your body splits it into tiny fatty acids more slowly than it processes carbohydrates or protein. We all need fat to make hormones and to protect our nerves (a bit like plastic coating protects an electric wire). Over millennia, fat has also been a crucial way for humans to store energy, allowing us to survive periods of famine. Nowadays, even without the risk of starvation, our bodies are programmed to store excess fuel in case we run out of food. No wonder a single measure – the energy content – can’t capture such complexity.

Our fixation with counting calories assumes both that all calories are equal and that all bodies respond to calories in identical ways: Camacho was told that, since he was a man, he needed 2,500 calories a day to maintain his weight. Yet a growing body of research shows that when different people consume the same meal, the impact on each person’s blood sugar and fat formation will vary according to their genes, lifestyles and unique mix of gut bacteria.

Research published this year showed that a certain set of genes is found more often in overweight people than in skinny ones, suggesting that some people have to work harder than others to stay thin (a fact that many of us already felt intuitively to be true). Differences in gut microbiomes can alter how people process food. A study of 800 Israelis in 2015 found that the rise in their blood-sugar levels varied by a factor of four in response to identical food.

Some people’s intestines are 50% longer than others: those with shorter ones absorb fewer calories, which means that they excrete more of the energy in food, putting on less weight.

The response of your own body may also change depending on when you eat. Lose weight and your body will try to regain it, slowing down your metabolism and even reducing the energy you spend on fidgeting and twitching your muscles. Even your eating and sleeping schedules can be important. Going without a full night’s sleep may spur your body to create more fatty tissue, which casts a grim light on Camacho’s years of early-morning exertion. You may put on more weight eating small amounts over 12-15 hours than eating the same food in three distinct meals over a shorter period.

There’s a further weakness in the calorie-counting system: the amount of energy we absorb from food depends on how we prepare it. Chopping and grinding food essentially does part of the work of digestion, making more calories available to your body by ripping apart cell walls before you eat it. That effect is magnified when you add heat: cooking increases the proportion of food digested in the stomach and small intestine, from 50% to 95%. The digestible calories in beef rises by 15% on cooking, and in sweet potato some 40% (the exact change depends on whether it is boiled, roasted or microwaved). So significant is this impact that Richard Wrangham, a primatologist at Harvard University, reckons that cooking was necessary for human evolution. It enabled the neurological expansion that created Homo sapiens: powering the brain consumes about a fifth of a person’s metabolic energy each day (cooking also means we didn’t need to spend all day chewing, unlike chimps).

The difficulty in counting accurately doesn’t stop there. The calorie load of carbohydrate-heavy items such as rice, pasta, bread and potatoes can be slashed simply by cooking, chilling and reheating them. As starch molecules cool they form new structures that are harder to digest. You absorb fewer calories eating toast that has been left to go cold, or leftover spaghetti, than if they were freshly made. Scientists in Sri Lanka discovered in 2015 that they could more than halve the calories potentially absorbed from rice by adding coconut oil during cooking and then cooling the rice. This made the starch less digestible so the body may take on fewer calories (they have yet to test on human beings the precise effects of rice cooked in this way). That’s a bad thing if you’re malnourished, but a boon if you’re trying to lose weight.

Different parts of a vegetable or fruit may be absorbed differently too: older leaves are tougher, for example. The starchy interior of sweetcorn kernels is easily digested but the cellulose husk is impossible to break down and passes through the body untouched. Just think about that moment when you look into the toilet bowl after eating sweetcorn.

As with so many dieters, Camacho’s efforts to accurately track his calories “in” were doomed. But so too were his attempts to track his calories “out”. The message from many public authorities and food producers, especially fast-food companies that sponsor sports events, is that even the unhealthiest foods will not make you fat if you do your part by taking plenty of exercise. Exercise does, of course, have clear health benefits. But unless you’re a professional athlete, it plays a smaller part in weight control than most people believe. As much as 75% of the average person’s daily energy expenditure comes not through exercise but from ordinary daily activities and from keeping your body functioning by digesting food, powering organs and maintaining a regular body temperature. Even drinking iced water – which delivers no energy – forces the body to burn calories to maintain its preferred temperature, making it the only known case of consuming something with “negative” calories. A popular expression in English tells us not to “compare apples and oranges” and assume them to be the same: yet calories put pizzas and oranges, or apples and ice cream, on the same scale, and deems them equal.

After three years of dedicated calorie-counting Camacho changed tack. While recovering from running the 2010 marathon in San Diego he took up Crossfit training, an exercise regime that includes high-intensity training and weightlifting. There he met people using a very different method to control their weight. Like him, they exercised regularly. But rather than limiting their calories, they ate natural foods, what Camacho calls “stuff from a real plant, not an industrial plant”.

Fed up with feeling like a hungry failure, he decided to give it a go. He ditched his heavily processed low-calorie products and focused on the quality of his food rather than quantity. He stopped feeling ravenous all the time. “It sounds simple but I decided to listen to my body and eat whenever I was hungry but only when I was hungry, and to eat real food, not food ‘products’,” he says. He went back to items that he’d long banned himself from eating. He had his first rasher of bacon in three years and enjoyed cheese, whole-fat milk and steaks.

He immediately felt less hungry and happier. More surprising, he quickly began to lose his extra fat. “I was sleeping so much better and within a couple of months I stopped the depression and anxiety medication,” he says. “I went from always feeling guilty and angry and afraid to feeling in control of myself and actually proud of my own body. Suddenly I could enjoy eating and drinking again.”

The weight stayed off and in 2012 he moved to Heidelberg in Germany, a world away from the hectic streets of Mexico, to study for a masters degree in public health. “The idea hit me that I could combine my own experience with academic work to try to help other people overcome these various barriers that I had found.” After his masters he embarked on a doctorate on how to tackle obesity in Mexico.

Today he is married to a German scholar, Erica Gunther, who has studied food systems around the world. Their diet includes things he used to shun, such as egg yolks, olive oil and nuts. Two days a week the couple stick to vegetarian meals but otherwise he devours steak, kidneys, liver and some of his favourite Mexican dishes – barbacoa (lamb), carnitas (pork) and tacos with grilled meat.

His wife enjoys making a traditional Mexican sweet pastry called pan de muerto (bread of death). “Before I would have run an extra two hours to compensate for eating that but now I don’t care, I just make sure it is a treat, not an everyday thing.” Having spent years trying to forgo alcohol, he has a glass or two of wine several times a week, and goes for a beer with friends from his gym.

Sweating through three or four workouts a week, he is as well-muscled as a professional rugby player. A stable 80kg, he has very little body fat, though he is still considered overweight by the body-mass-index charts, which rate many beefed-up professional athletes as too heavy. The only relapse of anxiety he suffers nowadays happens when he hears Tori Amos singing “Bliss” – the song playing when he was kidnapped – which he says “is a real pity because it’s a great song”.

Today Camacho could be described as a calorie dissident, one of a small but growing number of academics and scientists who say that the persistence of calorie-counting compounds the obesity epidemic, rather than remedying it. Counting calories has disrupted our ability to eat the right amount of food, he says, and has steered us towards poor choices. In 2017 he wrote an academic paper that was one of the most savage attacks on the calorie system published in a peer-reviewed journal. “I’m actually embarrassed at what I used to believe,” he says. “I was doing everything I could to follow the official advice but it was totally wrong and I feel stupid for never even questioning it.”

Given the vast evidence that calorie-counting is imprecise at best, and contributes to rising obesity at worst, why has it persisted?

The simplicity of calorie-counting explains its appeal. Metrics that tell consumers the extent to which foods have been processed, or whether they will suppress hunger, are harder to understand. Faced with the calorie juggernaut, none has gained wide acceptance.

The scientific and health establishment knows that the current system is flawed. A senior adviser to the UN ’s Food and Agriculture Organisation warned in 2002 that the Atwater “factors” of 4-4-9 at the heart of the calorie-counting system were “a gross oversimplification” and so inaccurate that they could mislead consumers into choosing unhealthy products because they understate the calories in some carbohydrates. The organisation said it would give “further consideration” to overhauling the system but 17 years later there is little momentum for change. It even rejected the idea of harmonising the many methods that are used in different countries – a label in Australia can give a different count from one in America for the same product.

Officials at the WHO also acknowledge the problems of the current system, but say it is so entrenched in consumer behaviour, public policy and industry standards that it would be too expensive and disruptive to make big changes. The experiments that Atwater conducted a century ago, without calculators or computers, have never been repeated even though our understanding of how our bodies work is vastly improved. There is little funding or enthusiasm for such work. As Susan Roberts at Tufts University says, collecting and analysing faeces “is the worst research job in the world”.

The calorie system, says Camacho, lets food producers off the hook: “They can say, ‘We’re not responsible for the unhealthy products we sell, we just have to list the calories and leave it to you to manage your own weight’.” Camacho and other calorie dissidents argue that sugar and highly processed carbohydrates play havoc with people’s hormonal systems. Higher insulin levels mean more energy is converted into fat tissues leaving less available to fuel the rest of the body. That in turn drives hunger and overeating. In other words the constant hunger and fatigue suffered by Camacho and other dieters may be symptoms of being overweight, rather than the cause of the problem. Yet much of the food industry defends the status quo too. To change how we assess the energy and health values of food would undermine the business model of many companies.

The only major organisation to shift the emphasis beyond calories is one dedicated to helping its customers slim down: Weight Watchers. In 2001 the world’s best-known dieting firm introduced a points system that moved away from focusing exclusively on calories to also classifying foods according to their sugar and saturated fat content, and their impact on appetite. Chris Stirk, the firm’s general manager in Britain, says the organisation made the change because relying on calories to lose weight is “outdated”: “Science evolves daily, monthly, yearly, let alone since the 1800s.”

Many of us know instinctively that not all calories are the same. A lollipop and an apple may contain similar numbers of calories but the apple is clearly better for us. But after a lifetime of hearing about the calorie and its role in supposedly foolproof diet advice we could be forgiven for being confused about how best to eat. It’s time to lay it to rest.■"	https://www.economist.com/1843/2019/02/28/death-of-the-calorie	"For more than a century we’ve counted on calories to tell us what will make us fat. Peter Wilson says it’s time to bury the world’s most misleading measure
The first time that Salvador Camacho thought he was going to die he was sitting in his father’s Chrysler sedan with a friend listening to music. The 22-year-old engineering student was parked near his home in the central Mexican city of Toluca and in the fading evening light he didn’t notice two tattooed men approach. Tori Amos’s hit, “Bliss”, had just started playing when the gang members pointed guns at the young men.
So began a 24-hour ordeal. Strong willed and solidly built, Camacho was singled out as the more stubborn of the pair. He was blindfolded and beaten. One robber eventually threw him to the ground, put a gun to the back of his head and told him it was time to die. He passed out, waking in a field with his hands tied behind his back, almost naked.
Camacho survived but, traumatised, he sank into depression. Soon he was drinking heavily and binge eating. His weight ballooned from a trim 70kg to 103kg.
That led to his second near-death experience, eight years later, in 2007. He remembers waking up and blinking at bright lights: he was being wheeled on a stretcher into a hospital emergency ward, with an attack of severe arrhythmia, or irregular heart beat. “A cardiologist told me that if I didn’t lose weight and get my health under control I would be dead in five years,” he says.
That second crisis forced Camacho belatedly to deal with the trauma of the first. To help with what he now understands was post-traumatic stress disorder, he started having counselling and taking anti-depressants and anti-anxiety drugs. To address his physical health, he tried to lose weight. This effort propelled him to the centre of one of the most fraught scientific debates of our age: the calorie wars, a fierce disagreement about diet and weight control.
Today, more than a decade after his cardiologist’s stark warning, Camacho lives in the Swiss city of Basel. He is relaxed and confident, except when two topics come up. When he recounts his kidnapping his gaze drops, his smile vanishes and he becomes noticeably quieter, although he says his panic attacks have virtually disappeared. The other touchy topic is weight control, which causes him to shake his head in anger at what he and millions of other dieters have gone through. “It’s just ridiculous,” he says with exasperation and a touch of venom. “People are living with real pain and guilt and all they get is advice that is confused or just plain wrong.”
The guidance that Camacho’s doctors gave him, along with a string of nutritionists and his own online research, was unanimous. It would be familiar to the millions of people who have ever tried to diet. “Everybody tells you that to lose weight you have to eat less and move more,” he says, “and the way to do that is to count your calories.”
At his heaviest, Camacho’s body-mass index – the ratio of his height to his weight – reached 35.6, well above the 30 mark that doctors define as clinically obese. Most government guidelines indicated that, as a man, he needed 2,500 calories a day to maintain his weight (the target for women is 2,000). Nutritionists told Camacho that if he ate fewer than 2,000 calories a day, a weekly “deficit” of 3,500 would mean that he would lose 0.5kg a week.
With a desk job as a planning engineer in a Mexican hospital, he knew it would take real discipline to trim his pudgy frame. But as his kidnappers had quickly realised, he is an unusually determined character. He began getting up before dawn each day to run 10km. He also started accounting for every morsel of food he consumed.
“I filled in Excel spreadsheets every night, every week and every month listing everything I ate. It became a real obsession for me,” says Camacho. Out went the Burger King Whoppers, fried tacos packed with pork and cheese, and tortas (Mexican sandwiches filled with meat, refried beans, avocado and peppers). Out too went his usual steady flow of beer and wine. In came carefully measured low-fat cheese and turkey sandwiches, salads, canned peach juice, Gatorade and Coke Zero, with three Special-K low-calorie diet bars a day.
“I was always tired and hungry and I would get really moody and distracted,” he says. “I was thinking about food all the time.” He was constantly told that if he got the maths right – consuming fewer calories than he burned each day – the results would soon show. “I really did everything you are supposed to do,” he insists with the tone of a schoolboy who completed his homework yet still failed a big test. He bought a battery of exercise monitoring devices to measure how many calories he was expending on his runs. “I was told to exercise for at least 45 minutes at least four or five times a week. I actually ran for more than an hour every day.” He kept to low-fat, low-calorie food for three years. It simply didn’t work. At one point he lost about 10kg but his weight rebounded, though he still restricted his calories.
Dieters the world over will be familiar with Camacho’s frustrations. Most studies show that more than 80% of people regain any lost weight in the long term. And like him, when we fail, most of us assume that we are too lazy or greedy – that we are at fault.
As a general rule it is true that if you eat vastly fewer calories than you burn, you’ll get slimmer (and if you consume far more, you’ll get fatter). But the myriad faddy diets flogged to us each year belie the simplicity of the formula that Camacho was given. The calorie as a scientific measurement is not in dispute. But calculating the exact calorific content of food is far harder than the confidently precise numbers displayed on food packets suggest. Two items of food with identical calorific values may be digested in very different ways. Each body processes calories differently. Even for a single individual, the time of day that you eat matters. The more we probe, the more we realise that tallying calories will do little to help us control our weight or even maintain a healthy diet: the beguiling simplicity of counting calories in and calories out is dangerously flawed.
The calorie is ubiquitous in daily life. It takes top billing on the information label of most packaged food and drinks. Ever more restaurants list the number of calories in each dish on their menus. Counting the calories we expend has become just as standard. Gym equipment, fitness devices around our wrists, even our phones tell us how many calories we have supposedly burned in a single exercise session or over the course of a day.
It wasn’t always thus. For centuries, scientists assumed that it was the mass of food consumed that was significant. In the late 16th century an Italian physician named Santorio Sanctorius invented a “weighing chair”, dangling from a giant scale, in which he sat at regular intervals to weigh himself, everything he ate and drank, and all the faeces and urine he produced. Despite 30 years of compulsive chair dangling, Sanctorius answered few of his own questions about the impact that his consumption had on his body.
Only later did the focus shift to the energy different foodstuffs contained. In the 18th century Antoine Lavoisier, a French aristocrat, worked out that burning a candle required a gas from the air – which he named oxygen – to fuel the flame and release heat and other gases. He applied the same principle to food, concluding that it fuels the body like a slow-burning fire. He built a calorimeter, a device big enough to hold a guinea pig, and measured the heat the creature generated to estimate how much energy it was producing. Unfortunately the French revolution – specifically the guillotine – cut short his thinking on the subject. But he had started something. Other scientists later constructed “bomb calori­meters” in which they burned food to measure the heat – and thus the potential energy – released from it.
The calorie – which comes from “calor”, the Latin for “heat” – was originally used to measure the efficiency of steam engines: one calorie is the energy required to heat 1kg of water by one degree Celsius. Only in the 1860s did German scientists begin using it to calculate the energy in food. It was an American agricultural chemist, Wilbur Atwater, who popularised the idea that it could be used to measure both the energy contained in food and the energy the body expended on things like muscular work, tissue repair and powering the organs. In 1887, after a trip to Germany, he wrote a series of wildly popular articles in Century, an American magazine, suggesting that “food is to the body what fuel is to the fire.” He introduced the public to the notion of “macronutrients” – carbohydrates, protein and fat – so called because the body needs a lot of them.
Today many of us want to monitor our calorie consumption in order to lose or maintain our weight. Atwater, the son of a Methodist minister, was motivated by the opposite concern: at a time when malnutrition was widespread, he sought to help poor people find the most cost-effective items to fill themselves up.
To see how much energy different macronutrients provided to the body, he fed samples of an “average” American diet of that era – which he believed to be heavy in molasses cookies, barley meal and chicken gizzards – to a group of male students in a basement at Wesleyan University in Middletown, Connecticut. For up to 12 days at a time a volunteer would eat, sleep and lift weights while sealed inside a six-foot-high chamber measuring four feet wide by seven feet deep. The energy in each meal was calculated by burning identical foods in a bomb calorimeter.
The walls were filled with water, and changes in its temperature allowed Atwater to calculate how much energy the students’ bodies were generating. His team collected the students’ faeces and burned that too, to see how much energy had been left in the body in the digestion process.
This was pioneering stuff for the 1890s. Atwater eventually concluded that a gram of either carbohydrate or protein made an average of four calories of energy available to the body, and a gram of fat offered an average of 8.9 calories, a figure later rounded up to nine calories for convenience. We now know far more about the workings of the human body: Atwater was right that some of a meal’s potential energy was excreted, but had no idea that some was also used to digest the meal itself, and that the body expends different amounts of energy depending on the food. Yet more than a century after igniting the faeces of Wesleyan students, the numbers Atwater calculated for each macro­nutrient remain the standard for measuring the calories in any given food stuff. Those experiments were the basis of Salvador Camacho’s daily calorific arithmetic.
Atwater transformed the way the public thought about food, with his simple belief that “a calorie is a calorie”. He counselled the poor against eating too many leafy green vegetables because they weren’t sufficiently dense in energy. By his account, it made no difference whether calories came from chocolate or spinach: if the body absorbed more energy than it used, then it would store the excess as body fat, causing you to put on weight.
That idea captured the public imagination. In 1918 the first book was published in America based on the notion that a healthy diet was no more complicated than the simple addition and subtraction of calories. “You may eat just what you like – candy, pie, cake, fat meat, butter, cream but count your calories!” wrote Lulu Hunt Peters in “Diet and Health”. “Now that you know you can have the things you like, proceed to make your menus containing very little of them.” The book sold millions.
By the 1930s the calorie had become entrenched in both the public mind and government policy. Its exclusive focus on the energy content of food, rather than its vitamin content, say, went virtually unchallenged. Rising incomes and greater female participation in the workforce meant that by the 1960s people were eating out more often or buying prepared food, so they wanted more information about what they were consuming. Nutritional information on foodstuffs was widespread but haphazard; many items carried outlandish claims about their health benefits. Labelling became standardised and mandatory in America only in 1990.
The emphasis and use of this information shifted too. By the late 1960s, obesity was becoming a pressing health concern as people became more sedentary and started eating highly processed foods and lots of sugar. As the number of people who needed to lose weight grew, changing diets became the focus of attention.
So began the war on fat, in which Atwater’s calorie calculations were an unwitting ally. Because counting calories was seen as an objective arbiter of the health qualities of a foodstuff, it seemed logical that the most calorie-laden part of any food item – fat – must be bad for you. By this measure, dishes low in calories, but rich in sugar and carbohydrates, seemed healthier. People were increasingly willing to blame fat for many of the health ills of modern life, helped along by the sugar lobby: in 2016, a researcher at the University of California uncovered documents from 1967 showing that sugar companies secretly funded studies at Harvard University designed to blame fat for the growing obesity epidemic. That the dietary “fat” found in olive oil, bacon and butter is branded with the same word as the unwanted flesh around our middles made it all the easier to demonise.
A US Senate committee report in 1977 recommended a low-fat, low-cholesterol diet for all, and other governments followed suit. The food industry responded with enthusiasm, removing fat, the most calorie-dense of macronutrients, from food items and replacing it with sugar, starch and salt. As a bonus, the thousands of new cheap and tasty “low-cal” and “low-fat” products which Camacho used to diet tended to have longer shelf lives and higher profit margins.
But this didn’t lead to the expected improvements in public health. Instead, it coincided almost exactly with the most dramatic rise in obesity in human history. Between 1975 and 2016 obesity almost tripled worldwide, according to the World Health Organisation ( WHO ): nearly 40% of over-18s – some 1.9bn adults – are now overweight. That contributed to a rapid rise in cardiovascular diseases (mainly heart disease and stroke) which became the leading cause of death worldwide. Rates of type-2 diabetes, which is often linked to lifestyle and diet, have more than doubled since 1980.
It wasn’t only wealthy countries that saw such trends. In Mexico, middle-class urban families such as Camacho’s got fatter too. As a child Camacho was fit and loved playing football. But at the age of ten, in 1988, he was one of many young Mexicans who started stacking on weight as increasing trade with America saw cheap sweets and fizzy drinks flood the shops, a process known as the “Coca-colonisation” of Mexico. “There were suddenly all these flavours you had never tasted, with chocolates, candies and Dr Pepper,” Camacho remembers: “Overnight I got fat.” When his uncles teased him about his bulging waistline, he cut back on sweets and stayed in good shape until his kidnapping 12 years later. Other Mexicans just kept bulking up. In 2013 Mexico overtook America as the most obese country in the world.
To combat this trend, governments worldwide have enshrined calorie-counting in policy. The WHO attributes the “fundamental cause” of obesity worldwide to “an energy imbalance between calories consumed and calories expended”. Governments the world over persist in offering the same advice: count and cut calories. This has infiltrated ever more areas of life. In 2018 the American government ordered food chains and vending machines to provide calorie details on their menus, to help consumers make “informed and healthful decisions”. Australia and Britain are headed in similar directions. Government bodies advise dieters to record their meals in a calorie journal to lose weight. The experimental efforts of a 19th-century scientist stand barely changed – and are barely questioned.
Millions of dieters give up when their calorie-counting is unsuccessful. Camacho was more stubborn than most. He took photos of his meals to record his intake more accurately, and would log into his calorie spreadsheets from his phone. He thought about every morsel he ate. And he bought a proliferation of gadgets to track his calorie output. But he still didn’t lose much weight.
One problem was that his sums were based on the idea that calorie counts are accurate. Food producers give impressively specific readings: a slice of Camacho’s favourite Domino’s double pepperoni pizza is supposedly 248 calories (not 247 nor 249). Yet the number of calories listed on food packets and menus are routinely wrong.
Susan Roberts, a nutritionist at Tufts University in Boston, has found that labels on American packaged foods miss their true calorie counts by an average of 8%. American government regulations allow such labels to understate calories by up to 20% (to ensure that consumers are not short-changed in terms of how much nutrition they receive). The information on some processed frozen foods misstates their calorific content by as much as 70%.
That isn’t the only problem. Calorie counts are based on how much heat a foodstuff gives off when it burns in an oven. But the human body is far more complex than an oven. When food is burned in a laboratory it surrenders its calories within seconds. By contrast, the real-life journey from dinner plate to toilet bowl takes on average about a day, but can range from eight to 80 hours depending on the person. A calorie of carbohydrate and a calorie of protein both have the same amount of stored energy, so they perform identically in an oven. But put those calories into real bodies and they behave quite differently. And we are still learning new insights: American researchers discovered last year that, for more than a century, we’ve been exaggerating by about 20% the number of calories we absorb from almonds.
The process of storing fat – the “weight” many people seek to lose – is influenced by dozens of other factors. Apart from calories, our genes, the trillions of bacteria that live in our gut, food preparation and sleep affect how we process food. Academic discussions of food and nutrition are littered with references to huge bodies of research that still need to be conducted. “No other field of science or medicine sees such a lack of rigorous studies,” says Tim Spector, a professor of genetic epidemiology at Kings College in London. “We can create synthetic DNA and clone animals but we still know incredibly little about the stuff that keeps us alive.”
What we do know, however, suggests that counting calories is very crude and often misleading. Think of a burger, the kind of food that Camacho eschewed during his early efforts to lose weight. Take a bite and the saliva in your mouth starts to break it down, a process that continues when you swallow, transporting the morsel towards your stomach and beyond to be churned further. The digestive process transforms the protein, carbohydrates and fat in the burger into their basic compounds so that they are tiny enough to be absorbed into the bloodstream via the small intestine to fuel and repair the trillions of cells in the body. But the basic molecules from each macronutrient play very different roles within the body.
All carbohydrates break down into sugars, which are the body’s main fuel source. But the speed at which your body gets its fuel from food can be as important as the amount of fuel. Simple carbohydrates are swiftly absorbed into the bloodstream, providing a fast shot of energy: the body absorbs the sugar from a can of fizzy drink at a rate of 30 calories a minute, compared with two calories a minute from complex carbohydrates such as potatoes or rice. That matters, because a sudden hit of sugar prompts the rapid release of insulin, a hormone that carries the sugar out of the bloodstream and into the body’s cells. Problems arise when there is too much sugar in the blood. The liver can store some of the excess, but any that remains is stashed as fat. So consuming large quantities of sugar is the fastest way to create body fat. And, once the insulin has done its work, blood-sugar levels slump, which tends to leave you hungry, as well as plumper.
Getting fat is a consequence of civilisation. Our ancestors would have enjoyed a heavy hit of sugar perhaps four times a year, when a new season produced fresh fruit. Many now enjoy that kind of sugar kick every day. The average person in the developed world consumes 20 times as much sugar as people did even during Atwater’s time.
But it is a different story when you eat complex carbohydrates such as cereals. These are strung together from simple carbohydrates, so they also break down into sugar, but because they do so more slowly, your blood-sugar levels remain steadier. The fruit juices that Camacho was encouraged to drink contained fewer calories than one of his wholegrain buns but the bread delivered less of a sugar hit and left him feeling satiated for longer.
Other macronutrients have different functions. Protein, the dominant component of meat, fish and dairy products, acts as the main building block for bone, skin, hair and other body tissues. In the absence of sufficient quantities of carbohydrates it can also serve as fuel for the body. But since it is broken down more slowly than carbohydrates, protein is less likely to be converted to body fat.
Fat is a different matter again. It should leave you feeling fuller for longer, because your body splits it into tiny fatty acids more slowly than it processes carbohydrates or protein. We all need fat to make hormones and to protect our nerves (a bit like plastic coating protects an electric wire). Over millennia, fat has also been a crucial way for humans to store energy, allowing us to survive periods of famine. Nowadays, even without the risk of starvation, our bodies are programmed to store excess fuel in case we run out of food. No wonder a single measure – the energy content – can’t capture such complexity.
Our fixation with counting calories assumes both that all calories are equal and that all bodies respond to calories in identical ways: Camacho was told that, since he was a man, he needed 2,500 calories a day to maintain his weight. Yet a growing body of research shows that when different people consume the same meal, the impact on each person’s blood sugar and fat formation will vary according to their genes, lifestyles and unique mix of gut bacteria.
Research published this year showed that a certain set of genes is found more often in overweight people than in skinny ones, suggesting that some people have to work harder than others to stay thin (a fact that many of us already felt intuitively to be true). Differences in gut microbiomes can alter how people process food. A study of 800 Israelis in 2015 found that the rise in their blood-sugar levels varied by a factor of four in response to identical food.
Some people’s intestines are 50% longer than others: those with shorter ones absorb fewer calories, which means that they excrete more of the energy in food, putting on less weight.
The response of your own body may also change depending on when you eat. Lose weight and your body will try to regain it, slowing down your metabolism and even reducing the energy you spend on fidgeting and twitching your muscles. Even your eating and sleeping schedules can be important. Going without a full night’s sleep may spur your body to create more fatty tissue, which casts a grim light on Camacho’s years of early-morning exertion. You may put on more weight eating small amounts over 12-15 hours than eating the same food in three distinct meals over a shorter period.
There’s a further weakness in the calorie-counting system: the amount of energy we absorb from food depends on how we prepare it. Chopping and grinding food essentially does part of the work of digestion, making more calories available to your body by ripping apart cell walls before you eat it. That effect is magnified when you add heat: cooking increases the proportion of food digested in the stomach and small intestine, from 50% to 95%. The digestible calories in beef rises by 15% on cooking, and in sweet potato some 40% (the exact change depends on whether it is boiled, roasted or microwaved). So significant is this impact that Richard Wrangham, a primatologist at Harvard University, reckons that cooking was necessary for human evolution. It enabled the neurological expansion that created Homo sapiens: powering the brain consumes about a fifth of a person’s metabolic energy each day (cooking also means we didn’t need to spend all day chewing, unlike chimps).
The difficulty in counting accurately doesn’t stop there. The calorie load of carbohydrate-heavy items such as rice, pasta, bread and potatoes can be slashed simply by cooking, chilling and reheating them. As starch molecules cool they form new structures that are harder to digest. You absorb fewer calories eating toast that has been left to go cold, or leftover spaghetti, than if they were freshly made. Scientists in Sri Lanka discovered in 2015 that they could more than halve the calories potentially absorbed from rice by adding coconut oil during cooking and then cooling the rice. This made the starch less digestible so the body may take on fewer calories (they have yet to test on human beings the precise effects of rice cooked in this way). That’s a bad thing if you’re malnourished, but a boon if you’re trying to lose weight.
Different parts of a vegetable or fruit may be absorbed differently too: older leaves are tougher, for example. The starchy interior of sweetcorn kernels is easily digested but the cellulose husk is impossible to break down and passes through the body untouched. Just think about that moment when you look into the toilet bowl after eating sweetcorn.
As with so many dieters, Camacho’s efforts to accurately track his calories “in” were doomed. But so too were his attempts to track his calories “out”. The message from many public authorities and food producers, especially fast-food companies that sponsor sports events, is that even the unhealthiest foods will not make you fat if you do your part by taking plenty of exercise. Exercise does, of course, have clear health benefits. But unless you’re a professional athlete, it plays a smaller part in weight control than most people believe. As much as 75% of the average person’s daily energy expenditure comes not through exercise but from ordinary daily activities and from keeping your body functioning by digesting food, powering organs and maintaining a regular body temperature. Even drinking iced water – which delivers no energy – forces the body to burn calories to maintain its preferred temperature, making it the only known case of consuming something with “negative” calories. A popular expression in English tells us not to “compare apples and oranges” and assume them to be the same: yet calories put pizzas and oranges, or apples and ice cream, on the same scale, and deems them equal.
After three years of dedicated calorie-counting Camacho changed tack. While recovering from running the 2010 marathon in San Diego he took up Crossfit training, an exercise regime that includes high-intensity training and weightlifting. There he met people using a very different method to control their weight. Like him, they exercised regularly. But rather than limiting their calories, they ate natural foods, what Camacho calls “stuff from a real plant, not an industrial plant”.
Fed up with feeling like a hungry failure, he decided to give it a go. He ditched his heavily processed low-calorie products and focused on the quality of his food rather than quantity. He stopped feeling ravenous all the time. “It sounds simple but I decided to listen to my body and eat whenever I was hungry but only when I was hungry, and to eat real food, not food ‘products’,” he says. He went back to items that he’d long banned himself from eating. He had his first rasher of bacon in three years and enjoyed cheese, whole-fat milk and steaks.
He immediately felt less hungry and happier. More surprising, he quickly began to lose his extra fat. “I was sleeping so much better and within a couple of months I stopped the depression and anxiety medication,” he says. “I went from always feeling guilty and angry and afraid to feeling in control of myself and actually proud of my own body. Suddenly I could enjoy eating and drinking again.”
The weight stayed off and in 2012 he moved to Heidelberg in Germany, a world away from the hectic streets of Mexico, to study for a masters degree in public health. “The idea hit me that I could combine my own experience with academic work to try to help other people overcome these various barriers that I had found.” After his masters he embarked on a doctorate on how to tackle obesity in Mexico.
Today he is married to a German scholar, Erica Gunther, who has studied food systems around the world. Their diet includes things he used to shun, such as egg yolks, olive oil and nuts. Two days a week the couple stick to vegetarian meals but otherwise he devours steak, kidneys, liver and some of his favourite Mexican dishes – barbacoa (lamb), carnitas (pork) and tacos with grilled meat.
His wife enjoys making a traditional Mexican sweet pastry called pan de muerto (bread of death). “Before I would have run an extra two hours to compensate for eating that but now I don’t care, I just make sure it is a treat, not an everyday thing.” Having spent years trying to forgo alcohol, he has a glass or two of wine several times a week, and goes for a beer with friends from his gym.
Sweating through three or four workouts a week, he is as well-muscled as a professional rugby player. A stable 80kg, he has very little body fat, though he is still considered overweight by the body-mass-index charts, which rate many beefed-up professional athletes as too heavy. The only relapse of anxiety he suffers nowadays happens when he hears Tori Amos singing “Bliss” – the song playing when he was kidnapped – which he says “is a real pity because it’s a great song”.
Today Camacho could be described as a calorie dissident, one of a small but growing number of academics and scientists who say that the persistence of calorie-counting compounds the obesity epidemic, rather than remedying it. Counting calories has disrupted our ability to eat the right amount of food, he says, and has steered us towards poor choices. In 2017 he wrote an academic paper that was one of the most savage attacks on the calorie system published in a peer-reviewed journal. “I’m actually embarrassed at what I used to believe,” he says. “I was doing everything I could to follow the official advice but it was totally wrong and I feel stupid for never even questioning it.”
Given the vast evidence that calorie-counting is imprecise at best, and contributes to rising obesity at worst, why has it persisted?
The simplicity of calorie-counting explains its appeal. Metrics that tell consumers the extent to which foods have been processed, or whether they will suppress hunger, are harder to understand. Faced with the calorie juggernaut, none has gained wide acceptance.
The scientific and health establishment knows that the current system is flawed. A senior adviser to the UN ’s Food and Agriculture Organisation warned in 2002 that the Atwater “factors” of 4-4-9 at the heart of the calorie-counting system were “a gross oversimplification” and so inaccurate that they could mislead consumers into choosing unhealthy products because they understate the calories in some carbohydrates. The organisation said it would give “further consideration” to overhauling the system but 17 years later there is little momentum for change. It even rejected the idea of harmonising the many methods that are used in different countries – a label in Australia can give a different count from one in America for the same product.
Officials at the WHO also acknowledge the problems of the current system, but say it is so entrenched in consumer behaviour, public policy and industry standards that it would be too expensive and disruptive to make big changes. The experiments that Atwater conducted a century ago, without calculators or computers, have never been repeated even though our understanding of how our bodies work is vastly improved. There is little funding or enthusiasm for such work. As Susan Roberts at Tufts University says, collecting and analysing faeces “is the worst research job in the world”.
The calorie system, says Camacho, lets food producers off the hook: “They can say, ‘We’re not responsible for the unhealthy products we sell, we just have to list the calories and leave it to you to manage your own weight’.” Camacho and other calorie dissidents argue that sugar and highly processed carbohydrates play havoc with people’s hormonal systems. Higher insulin levels mean more energy is converted into fat tissues leaving less available to fuel the rest of the body. That in turn drives hunger and overeating. In other words the constant hunger and fatigue suffered by Camacho and other dieters may be symptoms of being overweight, rather than the cause of the problem. Yet much of the food industry defends the status quo too. To change how we assess the energy and health values of food would undermine the business model of many companies.
The only major organisation to shift the emphasis beyond calories is one dedicated to helping its customers slim down: Weight Watchers. In 2001 the world’s best-known dieting firm introduced a points system that moved away from focusing exclusively on calories to also classifying foods according to their sugar and saturated fat content, and their impact on appetite. Chris Stirk, the firm’s general manager in Britain, says the organisation made the change because relying on calories to lose weight is “outdated”: “Science evolves daily, monthly, yearly, let alone since the 1800s.”
Many of us know instinctively that not all calories are the same. A lollipop and an apple may contain similar numbers of calories but the apple is clearly better for us. But after a lifetime of hearing about the calorie and its role in supposedly foolproof diet advice we could be forgiven for being confused about how best to eat. It’s time to lay it to rest.■"	34754
health	['Wendy Orent', 'Brandon Kohrt', 'Catherine Stinson', 'Douglas Boin', 'Jacob Stegenga']		"The latest epidemic to terrify the Western world is Ebola, a virus that has killed hundreds in Africa in 2014 alone. No wonder there was so much worry when two infected health care workers from the United States were transported home from Liberia for treatment – why bring this plague to the US, exposing the rest of the country as well? But the truth is that Ebola, murderous though it is, doesn’t have what it takes to produce a pandemic, a worldwide outbreak of infectious disease. It spreads only through intimate contact with infected body fluids; to avoid Ebola, just refrain from touching sweat, blood or the bodies of the sick or dead.

Yet no logic can quell our pandemic paranoia, which first infected the zeitgeist with the publication of Laurie Garrett’s The Coming Plague (1994) and Richard Preston’s Hot Zone (1995). These books suggested that human incursion into rainforests and jungles would stir deadly viruses in wait; perturb nature and she nails you in the end. By the late 1990s, we were deep into the biological weapons scare, pumping billions of dollars in worldwide government funding to fight evil, lab-made disease. As if this weren’t enough, the panic caused from 2004 to 2007 by reports of the H5N1 or bird flu virus etched the prospect of a cross-species Andromeda strain in the Western mind.

The fear seems confirmed by historical memory: after all, plagues have killed a lot of people, and deadly diseases litter history like black confetti. The Antonine Plague, attributed to measles or smallpox in the year 165 CE, killed the Roman Emperor Marcus Aurelius and millions of his subjects. The Justinian Plague, caused by the deadly bacterial pathogen Yersinia pestis, spread from North Africa across the Mediterranean Sea to Constantinople and other cities along the Mediterranean. By 542, infected rats and fleas had carried the infection as far north as Rennes in France and into the heart of Germany. Millions died.

Then there was the Black Death of 1348-50, also caused by Yersinia pestis, but this time spread by human fleas and from human lung to human lung, through the air. The plague spread along the Silk Road to what is now Afghanistan, India, Persia, Constantinople, and thence across the Mediterranean to Italy and the rest of Europe, killing tens of millions worldwide. Of all the past pandemics, the 1918 influenza (also known as the Spanish flu) is now considered the über-threat, the rod by which all other pandemics are measured. It killed 40 million people around the globe.

It was the great Australian virologist Frank Macfarlane Burnet who argued that the deadliest diseases were those newly introduced into the human species. It seemed to make sense: the parasite that kills its host is a dead parasite since, without the host, the germ has no way to survive and spread. According to this argument, new germs that erupt into our species will be potential triggers for pandemics, while germs that have a long history in a host species will have evolved to be relatively benign.

Many health experts take the notion further, contending that any coming plague will come from human intrusion into the natural world. One risk, they suggest, comes when hungry people in Africa and elsewhere forge deep into forests and jungles to hunt ‘bushmeat’ – rodents, rabbits, monkeys, apes – with exposure to dangerous pathogens the unhappy result. Those pathogens move silently among wild animals, but can also explode with terrifying ferocity among people when humans venture where they shouldn’t. According to the same line of thought, another proposed risk would result when birds spread a new pandemic strain to chickens in factory farms and, ultimately, to us.

But there’s something in these scenarios that’s not entirely logical. There is nothing new in the intimate contact between animals and people. Our hominid ancestors lived on wildlife before we ever evolved into Homo sapiens: that’s why anthropologists call them hunter-gatherers, a term that still applies to some modern peoples, including bushmeat hunters in West Africa. After domesticating animals, we lived close beside them, keeping cows, pigs and chickens in farmyards and even within households for thousands of years. Pandemics arise out of more than mere contact between human beings and animals: from an evolutionary point of view, there is a missing step between animal pathogen and human pandemic that’s been almost completely overlooked in these terrifying but entirely speculative ideas.

According to the evolutionary epidemiologist Paul W Ewald of the University of Louisville, the most dangerous infectious diseases are almost always not animal diseases freshly broken into the human species, but diseases adapted to humanity over time: smallpox, malaria, tuberculosis, leprosy, typhus, yellow fever, polio. In order to adapt to the human species, a germ needs to cycle among people – from person to person to person. In each iteration, the strains best adapted to transmission will be the ones that spread. So natural selection will push circulating strains towards more and more effective transmission, and therefore towards increasing adaptation to human hosts. This process necessarily takes place among people.

A pathogen must force its new human host to act as a germ dispensary system: sneezing, coughing, spewing germ-laden particles in the air

That is why the idea that highly pathogenic animal viruses such as H5N1 bird flu cycling among chickens, for example, can suddenly mutate into a lethal, transmissible human virus makes little evolutionary sense. Adaptation to a new host is a precise and delicate business. To be a human-adapted germ means having the ability to infect human cells, to overcome human immunity, and to manipulate the human host into shedding the germ.

A pathogen must force its new human host to act as a germ dispensary system: sneezing, coughing, spewing germ-laden particles in the air, or passing them through diarrhoea. To make us sneeze, a cold or flu virus somehow piggybacks on our evolved tendency to sneeze and cough to rid ourselves of irritants, whether that’s pollen or virus. Sneezing out virus might help you reduce your viral load and recover more quickly, but it also ensures that your co-workers go home with a dose of what ails you. A virus that does not cause symptoms is much less likely to transmit. On the other hand, if you are flattened immediately, the pathogen could be too virulent, immobilising you so thoroughly that you can’t get out to spread the disease. (Ebola is a standout example here.)

But once they evolve the knack for human-to-human transmission, these same killer pathogens can cause devastating human disease. The usual suspects are well-known: smallpox, which survives in the external environment; cholera, which spreads in water; malaria, carried by insects; and Staphylococcus aureus, spread by unwitting hospital attendants and known today in deadliest, most antibiotic-resistant form as MRSA.

One mysterious ancient outbreak, the Great Plague of Athens, shows how deadly epidemics unroll in time. The Plague – said to have been caused by typhus, measles, small pox or Ebola, depending on whom you ask – exploded in Athens in the summer of 430 BCE, during the early days of the Peloponnesian War, a 27-year struggle between Athens and Sparta over hegemony in the Hellenic world. Pericles, the de facto leader of Athens, who pushed for war, developed a defensive strategy that proved fatal, to him and to as many as a third of Athenian citizens. He insisted on bringing all citizens – people who lived in the towns and rural areas outside the walled city – into Athens, leaving the rest of the city-state to be ravaged by the invading Spartans. The Athenian Long Walls ran down to the separate ports of Piraeus and Phaleron, each of which lay about four miles from the City of Athens proper. Thus sealed off, fronting only on the sea, Athenians could shelter safely, Pericles argued, until the Peloponnesian War was won.

The normal population of the city was around 150,000. Scholars estimate that 200,000 to 250,000 farmers and townsmen and their families came streaming in, bringing everything they could carry with them – down to the woodwork on their farmhouse walls. But Pericles had made no provision for the newcomers, who were used to their country manors, their quiet towns, their open fields. A few had homes or relatives within the walls. But most had nowhere to go, and huddled in stifling huts, or in tents flung up in the narrow spaces between the walls. The crowded encampments were ripe for virulent infection.

Physicians and attendants died quickly, and the only people who could care for the sick were survivors immune to further infection

That infection came from the sea, the portal that Pericles had left open to feed the Athenian people and bring in enough money to keep the war effort alive. The Athenian historian Thucydides tells us that the disease originated in Ethiopia, travelled northward into Egypt and Libya, moved across the Mediterranean to the island of Lemnos, and on to Athens. ‘There was no record of the disease being so virulent anywhere else or causing so many deaths at it did in Athens,’ wrote Thucydides, and we have no reason to doubt him. This is a critical bit of information. It tells us that the Great Plague evolved: it became much more lethal in the great refugee camp that wartime Athens had become. Thucydides continued:

People in perfect health suddenly began to have burning feelings in the head; their eyes became red and inflamed; inside their mouths there was bleeding from the throat and tongue, and the breath became unnatural and unpleasant. The next symptoms were sneezing and hoarseness of voice, and before long the pain settled on the chest and was accompanied by coughing. Next the stomach was affected with stomach-aches and with vomitings of every kind of bile that has been given a name by the medical profession, and this being accompanied by great pain and difficulty. In most cases there were attacks of ineffectual retching, producing violent spasms; this sometimes ended with this stage of the disease, but sometimes continued long afterward.

Thucydides goes on to describe rashes, delirium, mental confusion and, among the survivors, blackened, dead tissue on the extremities, and sometimes blindness. This was a highly virulent, transmissible, systemic disease. Physicians and attendants died quickly, and the only people who could care for the sick were survivors immune to further infection.

The Athenian plague shows how a disease of mild to moderate virulence can heat up in what we can only call a ‘disease factory’ – a place where the sick are trapped together with the well, causing infection to spread like wildfire. If the Athenian infection was typhus, the most likely culprit, it would normally be spread by the human body louse and its infected faeces. There must have been body lice among the Athenian squatters. Usually just scratching lice bites, thus digging louse faeces inadvertently into the skin, causes transmission. But in crowded conditions, transmission of typhus can be more direct. For instance, in the Serbian prison camps of the First World War, where typhus was rampant, transmission through the air – possibly caused by aerosolised louse faeces – was known to occur. The right conditions, in other words, can cause the short-term evolution of a louse-borne disease into something explosive.

As Ewald has shown, a similar evolutionary process gave rise to the 1918 flu, propelled by the trench-warfare system of the Western Front. In the spring of 1918, a first wave of mild flu broke out in the US, and spread across the country to the troop ships loading for Europe. From those ships, the infection spread into the trenches, where it swiftly heated up to deadly virulence in the disease factory conditions of the Front: the trenches, the trains, the trucks moving the wounded and the sick together, where virus from people immobilised by illness was able, over and over, to infect the well. The whole system was a giant viral delivery service. The disease left the Western Front by several ports and exploded across the planet, killing about 2.5 per cent of those it infected – and it infected hundreds of millions.

This predatory influenza was not caused by some random combination of bird flu genes, as the new plague paranoia predicts. Both the Athenian Plague and the 1918 flu evolved in predictable Darwinian fashion. Germs that ravage the body more swiftly and effectively will outcompete milder strains. If those lethal strains have repeated access to fresh hosts, the brakes on virulence are off, and deadly disease evolves and spreads.

Looking at epidemics and pandemics through this evolutionary lens makes it clear that the most important condition necessary for the evolution of virulent, transmissible disease is the existence of a human disease factory. Without social conditions that allow the evolution of virulent, transmissible disease, deadly outbreaks are unlikely to emerge.

Deadliness itself isn’t that uncommon: SARS, or severe acute respiratory syndrome, which terrorised China in 2002 and 2003, killed 10 per cent of its victims; Ebola kills 60-90 per cent; untreated rabies kills close to 100 per cent, as does untreated pneumonic plague, caused by Yersinia pestis, the bacterial agent of the Black Death, the worst pandemic in human history. But to be both deadly and efficiently transmissible requires exacting circumstances. Even the Black Death – thought to come originally from Central Asian marmots – must have evolved in a chain of human-to-human transmission to become as lethally and effective as it was.

Fighting existing pathogens is more urgent than hunting for possible new ones – less exciting, but more likely to ease real suffering in the world

So what is wrong with listening to the drumbeat, to the endless calls to protect ourselves against the coming plague – against Ebola from Africa and bird flu from Asia? Is it possible that a huge pandemic could erupt from some as-yet unknown pathogen? Is apocalypse lurking out there, among rats or monkeys, or bats or flying squirrels or birds? The Black Death shows that you can never say never: there might be an animal pathogen out there that, under the right circumstances, can evolve and maintain both virulence and transmissibility among humans as well as animals.

The Central African monkeypox virus (so called because it was first identified in macaques in 1958) has dangerous attributes: like the smallpox virus, it is often deadly, and it’s also a ‘sit-and-wait pathogen’, in Ewald’s terms – highly durable in the outside environment. Yet the evolution of monkeypox into a human disease such as smallpox seems, at this point, unlikely: some strains of monkeypox have transmitted from person to person for several iterations, but the chains of transmission have easily been broken, and the evolutionary process stopped in its tracks.

Instead, people continue to die of human-adapted disease. Malaria kills more than 1 million children annually. Tuberculosis, in its ugliest, drug-resistant forms, is well-entrenched worldwide. Polio, despite our noblest efforts, continues to cripple and kill children. Fighting existing pathogens seems more urgent than hunting for possible new ones – less exciting, but more likely to ease real suffering in the world.

If the Great Plague of Athens tells us anything, it is to avoid social conditions that allow pathogens to evolve great virulence and transmissibility. Preventing disease factories – trench-like warfare conditions, crowded hospitals, enormous refugee camps – is our best protection. While alarmists among us wait for the plague to pounce out of the jungle, it is far more likely to come from inside us, our disease factories and our social world."	https://aeon.co/essays/the-next-pandemic-will-be-nothing-like-ebola	"The latest epidemic to terrify the Western world is Ebola, a virus that has killed hundreds in Africa in 2014 alone. No wonder there was so much worry when two infected health care workers from the United States were transported home from Liberia for treatment – why bring this plague to the US, exposing the rest of the country as well? But the truth is that Ebola, murderous though it is, doesn’t have what it takes to produce a pandemic, a worldwide outbreak of infectious disease. It spreads only through intimate contact with infected body fluids; to avoid Ebola, just refrain from touching sweat, blood or the bodies of the sick or dead.
Yet no logic can quell our pandemic paranoia, which first infected the zeitgeist with the publication of Laurie Garrett’s The Coming Plague (1994) and Richard Preston’s Hot Zone (1995). These books suggested that human incursion into rainforests and jungles would stir deadly viruses in wait; perturb nature and she nails you in the end. By the late 1990s, we were deep into the biological weapons scare, pumping billions of dollars in worldwide government funding to fight evil, lab-made disease. As if this weren’t enough, the panic caused from 2004 to 2007 by reports of the H5N1 or bird flu virus etched the prospect of a cross-species Andromeda strain in the Western mind.
The fear seems confirmed by historical memory: after all, plagues have killed a lot of people, and deadly diseases litter history like black confetti. The Antonine Plague, attributed to measles or smallpox in the year 165 CE, killed the Roman Emperor Marcus Aurelius and millions of his subjects. The Justinian Plague, caused by the deadly bacterial pathogen Yersinia pestis, spread from North Africa across the Mediterranean Sea to Constantinople and other cities along the Mediterranean. By 542, infected rats and fleas had carried the infection as far north as Rennes in France and into the heart of Germany. Millions died.
Then there was the Black Death of 1348-50, also caused by Yersinia pestis, but this time spread by human fleas and from human lung to human lung, through the air. The plague spread along the Silk Road to what is now Afghanistan, India, Persia, Constantinople, and thence across the Mediterranean to Italy and the rest of Europe, killing tens of millions worldwide. Of all the past pandemics, the 1918 influenza (also known as the Spanish flu) is now considered the über-threat, the rod by which all other pandemics are measured. It killed 40 million people around the globe.
It was the great Australian virologist Frank Macfarlane Burnet who argued that the deadliest diseases were those newly introduced into the human species. It seemed to make sense: the parasite that kills its host is a dead parasite since, without the host, the germ has no way to survive and spread. According to this argument, new germs that erupt into our species will be potential triggers for pandemics, while germs that have a long history in a host species will have evolved to be relatively benign.
Many health experts take the notion further, contending that any coming plague will come from human intrusion into the natural world. One risk, they suggest, comes when hungry people in Africa and elsewhere forge deep into forests and jungles to hunt ‘bushmeat’ – rodents, rabbits, monkeys, apes – with exposure to dangerous pathogens the unhappy result. Those pathogens move silently among wild animals, but can also explode with terrifying ferocity among people when humans venture where they shouldn’t. According to the same line of thought, another proposed risk would result when birds spread a new pandemic strain to chickens in factory farms and, ultimately, to us.
But there’s something in these scenarios that’s not entirely logical. There is nothing new in the intimate contact between animals and people. Our hominid ancestors lived on wildlife before we ever evolved into Homo sapiens: that’s why anthropologists call them hunter-gatherers, a term that still applies to some modern peoples, including bushmeat hunters in West Africa. After domesticating animals, we lived close beside them, keeping cows, pigs and chickens in farmyards and even within households for thousands of years. Pandemics arise out of more than mere contact between human beings and animals: from an evolutionary point of view, there is a missing step between animal pathogen and human pandemic that’s been almost completely overlooked in these terrifying but entirely speculative ideas.
According to the evolutionary epidemiologist Paul W Ewald of the University of Louisville, the most dangerous infectious diseases are almost always not animal diseases freshly broken into the human species, but diseases adapted to humanity over time: smallpox, malaria, tuberculosis, leprosy, typhus, yellow fever, polio. In order to adapt to the human species, a germ needs to cycle among people – from person to person to person. In each iteration, the strains best adapted to transmission will be the ones that spread. So natural selection will push circulating strains towards more and more effective transmission, and therefore towards increasing adaptation to human hosts. This process necessarily takes place among people.
A pathogen must force its new human host to act as a germ dispensary system: sneezing, coughing, spewing germ-laden particles in the air
That is why the idea that highly pathogenic animal viruses such as H5N1 bird flu cycling among chickens, for example, can suddenly mutate into a lethal, transmissible human virus makes little evolutionary sense. Adaptation to a new host is a precise and delicate business. To be a human-adapted germ means having the ability to infect human cells, to overcome human immunity, and to manipulate the human host into shedding the germ.
A pathogen must force its new human host to act as a germ dispensary system: sneezing, coughing, spewing germ-laden particles in the air, or passing them through diarrhoea. To make us sneeze, a cold or flu virus somehow piggybacks on our evolved tendency to sneeze and cough to rid ourselves of irritants, whether that’s pollen or virus. Sneezing out virus might help you reduce your viral load and recover more quickly, but it also ensures that your co-workers go home with a dose of what ails you. A virus that does not cause symptoms is much less likely to transmit. On the other hand, if you are flattened immediately, the pathogen could be too virulent, immobilising you so thoroughly that you can’t get out to spread the disease. (Ebola is a standout example here.)
But once they evolve the knack for human-to-human transmission, these same killer pathogens can cause devastating human disease. The usual suspects are well-known: smallpox, which survives in the external environment; cholera, which spreads in water; malaria, carried by insects; and Staphylococcus aureus, spread by unwitting hospital attendants and known today in deadliest, most antibiotic-resistant form as MRSA.
One mysterious ancient outbreak, the Great Plague of Athens, shows how deadly epidemics unroll in time. The Plague – said to have been caused by typhus, measles, small pox or Ebola, depending on whom you ask – exploded in Athens in the summer of 430 BCE, during the early days of the Peloponnesian War, a 27-year struggle between Athens and Sparta over hegemony in the Hellenic world. Pericles, the de facto leader of Athens, who pushed for war, developed a defensive strategy that proved fatal, to him and to as many as a third of Athenian citizens. He insisted on bringing all citizens – people who lived in the towns and rural areas outside the walled city – into Athens, leaving the rest of the city-state to be ravaged by the invading Spartans. The Athenian Long Walls ran down to the separate ports of Piraeus and Phaleron, each of which lay about four miles from the City of Athens proper. Thus sealed off, fronting only on the sea, Athenians could shelter safely, Pericles argued, until the Peloponnesian War was won.
The normal population of the city was around 150,000. Scholars estimate that 200,000 to 250,000 farmers and townsmen and their families came streaming in, bringing everything they could carry with them – down to the woodwork on their farmhouse walls. But Pericles had made no provision for the newcomers, who were used to their country manors, their quiet towns, their open fields. A few had homes or relatives within the walls. But most had nowhere to go, and huddled in stifling huts, or in tents flung up in the narrow spaces between the walls. The crowded encampments were ripe for virulent infection.
Physicians and attendants died quickly, and the only people who could care for the sick were survivors immune to further infection
That infection came from the sea, the portal that Pericles had left open to feed the Athenian people and bring in enough money to keep the war effort alive. The Athenian historian Thucydides tells us that the disease originated in Ethiopia, travelled northward into Egypt and Libya, moved across the Mediterranean to the island of Lemnos, and on to Athens. ‘There was no record of the disease being so virulent anywhere else or causing so many deaths at it did in Athens,’ wrote Thucydides, and we have no reason to doubt him. This is a critical bit of information. It tells us that the Great Plague evolved: it became much more lethal in the great refugee camp that wartime Athens had become. Thucydides continued:
People in perfect health suddenly began to have burning feelings in the head; their eyes became red and inflamed; inside their mouths there was bleeding from the throat and tongue, and the breath became unnatural and unpleasant. The next symptoms were sneezing and hoarseness of voice, and before long the pain settled on the chest and was accompanied by coughing. Next the stomach was affected with stomach-aches and with vomitings of every kind of bile that has been given a name by the medical profession, and this being accompanied by great pain and difficulty. In most cases there were attacks of ineffectual retching, producing violent spasms; this sometimes ended with this stage of the disease, but sometimes continued long afterward.
Thucydides goes on to describe rashes, delirium, mental confusion and, among the survivors, blackened, dead tissue on the extremities, and sometimes blindness. This was a highly virulent, transmissible, systemic disease. Physicians and attendants died quickly, and the only people who could care for the sick were survivors immune to further infection.
The Athenian plague shows how a disease of mild to moderate virulence can heat up in what we can only call a ‘disease factory’ – a place where the sick are trapped together with the well, causing infection to spread like wildfire. If the Athenian infection was typhus, the most likely culprit, it would normally be spread by the human body louse and its infected faeces. There must have been body lice among the Athenian squatters. Usually just scratching lice bites, thus digging louse faeces inadvertently into the skin, causes transmission. But in crowded conditions, transmission of typhus can be more direct. For instance, in the Serbian prison camps of the First World War, where typhus was rampant, transmission through the air – possibly caused by aerosolised louse faeces – was known to occur. The right conditions, in other words, can cause the short-term evolution of a louse-borne disease into something explosive.
As Ewald has shown, a similar evolutionary process gave rise to the 1918 flu, propelled by the trench-warfare system of the Western Front. In the spring of 1918, a first wave of mild flu broke out in the US, and spread across the country to the troop ships loading for Europe. From those ships, the infection spread into the trenches, where it swiftly heated up to deadly virulence in the disease factory conditions of the Front: the trenches, the trains, the trucks moving the wounded and the sick together, where virus from people immobilised by illness was able, over and over, to infect the well. The whole system was a giant viral delivery service. The disease left the Western Front by several ports and exploded across the planet, killing about 2.5 per cent of those it infected – and it infected hundreds of millions.
This predatory influenza was not caused by some random combination of bird flu genes, as the new plague paranoia predicts. Both the Athenian Plague and the 1918 flu evolved in predictable Darwinian fashion. Germs that ravage the body more swiftly and effectively will outcompete milder strains. If those lethal strains have repeated access to fresh hosts, the brakes on virulence are off, and deadly disease evolves and spreads.
Looking at epidemics and pandemics through this evolutionary lens makes it clear that the most important condition necessary for the evolution of virulent, transmissible disease is the existence of a human disease factory. Without social conditions that allow the evolution of virulent, transmissible disease, deadly outbreaks are unlikely to emerge.
Deadliness itself isn’t that uncommon: SARS, or severe acute respiratory syndrome, which terrorised China in 2002 and 2003, killed 10 per cent of its victims; Ebola kills 60-90 per cent; untreated rabies kills close to 100 per cent, as does untreated pneumonic plague, caused by Yersinia pestis, the bacterial agent of the Black Death, the worst pandemic in human history. But to be both deadly and efficiently transmissible requires exacting circumstances. Even the Black Death – thought to come originally from Central Asian marmots – must have evolved in a chain of human-to-human transmission to become as lethally and effective as it was.
Fighting existing pathogens is more urgent than hunting for possible new ones – less exciting, but more likely to ease real suffering in the world
So what is wrong with listening to the drumbeat, to the endless calls to protect ourselves against the coming plague – against Ebola from Africa and bird flu from Asia? Is it possible that a huge pandemic could erupt from some as-yet unknown pathogen? Is apocalypse lurking out there, among rats or monkeys, or bats or flying squirrels or birds? The Black Death shows that you can never say never: there might be an animal pathogen out there that, under the right circumstances, can evolve and maintain both virulence and transmissibility among humans as well as animals.
The Central African monkeypox virus (so called because it was first identified in macaques in 1958) has dangerous attributes: like the smallpox virus, it is often deadly, and it’s also a ‘sit-and-wait pathogen’, in Ewald’s terms – highly durable in the outside environment. Yet the evolution of monkeypox into a human disease such as smallpox seems, at this point, unlikely: some strains of monkeypox have transmitted from person to person for several iterations, but the chains of transmission have easily been broken, and the evolutionary process stopped in its tracks.
Instead, people continue to die of human-adapted disease. Malaria kills more than 1 million children annually. Tuberculosis, in its ugliest, drug-resistant forms, is well-entrenched worldwide. Polio, despite our noblest efforts, continues to cripple and kill children. Fighting existing pathogens seems more urgent than hunting for possible new ones – less exciting, but more likely to ease real suffering in the world.
If the Great Plague of Athens tells us anything, it is to avoid social conditions that allow pathogens to evolve great virulence and transmissibility. Preventing disease factories – trench-like warfare conditions, crowded hospitals, enormous refugee camps – is our best protection. While alarmists among us wait for the plague to pounce out of the jungle, it is far more likely to come from inside us, our disease factories and our social world."	15901
health	['Anthony Cirillo', 'National Academy Of Sports Medicine', 'Kristen Sturt']		"Making this key lifestyle tweak keeps you mobile as you age—but that’s not where the benefits end.

Here’s a startling fact: About 3 in 4 American adults don’t get the recommended amount of physical activity, according to the Centers for Disease Control and Prevention.

Even more sobering: Many adults don’t get any activity at all, aside from what they need to make it through the day. And as we age, more and more of us stop moving. Almost 23 percent of adults between age 18 and 44 are sedentary. For those 65 and older, it’s around 32 percent.

While you likely know that long-term inactivity weakens your bones and muscles, you may not realize that it can damage your heart and brain, too. This, in turn, raises your odds of dementia and heart disease, among other conditions, and can lead to early death.

But research suggests that getting exercise can help keep these organs healthy and delay or prevent their decline. And if you regularly work up a sweat over a number of years? All the better.

“You really need to think about ways to keep moving,” says Kevin Bohnsack, MD, a family medicine physician at Saint Joseph Mercy Health System in Ann Arbor, Michigan. “Everything that increases your overall activity can ward off that sedentary lifestyle,” he adds—along with the cardiac and cognitive problems that can come with it.

How exercise benefits the heart

As you progress through middle age, your heart gradually begins to weaken. Its walls get thicker and less flexible, and your arteries become stiffer. This raises your risk for high blood pressure (hypertension) and other heart problems, including heart attack and heart failure. And if you’re sedentary, that risk goes up even more.

When you exercise, your heart beats faster, increasing blood flow and supplying your body with necessary oxygen. The more you work out, the stronger your heart gets and the more elastic your blood vessels become. This helps you maintain a lower blood pressure and decreases your chances of developing many cardiovascular problems.

It’s aerobic exercise—also called cardio—that really does the trick. Research suggests that consistent, long-term moderate or vigorous cardio training may be most helpful, though any physical activity promotes good heart health. “It can be anything from running to biking to rowing,” says Dr. Bohnsack. “Anything that builds up that heart rate.”

Getting in shape benefits your heart in other ways, too, by helping neutralize risk factors linked to heart disease. Exercise is associated with:

A reduction in inflammation

An increase in HDL (“good” cholesterol) and decrease in LDL (“bad” cholesterol)

Maintaining a healthy weight and staving off obesity

And though more studies are needed, research increasingly shows that exercise can boost your heart health no matter your age. For example, for one small study published in March 2018 in the journal Circulation, 28 middle-aged men completed two years of high-intensity exercise training. Compared to a control group, scientists found the exercise reduced their cardiac stiffness and increased their bodies’ capacity for oxygen use—both of which may slash the risk for heart failure.

For another study published in the August 2018 issue of Journal of the American Heart Association, researchers gave heartrate and movement sensors to 1,600 British volunteers between the ages of 60 and 64. After five days, they found that more active people had fewer indicators of heart disease in their blood. Not too shabby, boomers.

How exercise benefits the brain

What’s good for your heart is generally good for your mind—and research shows breaking a sweat on a regular basis can boost brain health in several ways.

First, exercise is tied to improved cognition, which includes better memory, attention and executive function—things like controlling emotions and completing tasks. It can enhance the speed with which you process and react to information, too, along with your capacity to draw from your past knowledge and experiences.

Getting physical is also linked to slower age-related cognitive decline, where we gradually lose our thinking, focus and memory skills. “In other words,” says Bohnsack, “if you like where you are, it’s a good idea to continue to exercise because that may at least help you retain your current cognitive function.”

And though the jury is still out on whether it improves symptoms, exercise may help prevent or delay dementia, including Alzheimer’s disease. For example, one 2017 review in The Journals of Gerontology: Biological Sciences found that activity was associated with a lower risk of Alzheimer’s down the line. The link was strongest for people who purposely exercised in their spare time, rather than those who had physically active jobs. This suggests mental benefits may depend on your chosen activity, in addition to the time you put into it.

How does exercise do all this? Scientists aren’t completely sure. It’s thought that working out improves blood flow and oxygen delivery to the brain, helping it function better. Some research indicates it prevents shrinkage of the hippocampus—the part of the brain crucial for learning and remembering things. Experts also believe it stimulates chemical activity in the brain that could contribute to better cognition.

Finally, exercise may help lower your chances of developing other conditions connected to dementia, including cardiovascular disease.

When can you start?

No matter our age, pretty much all of us can gain from exercise. “There is evidence to suggest that doing more vigorous exercise earlier in life is more beneficial,” says Bohnsack, “but it’s never too late to start because everyone benefits from doing some sort of movement or physical activity.”

In addition to its rewards for the heart and brain, working out:

Boosts your mood and energy

Helps prevent injuries

Lowers your risk of other diseases associated with aging, like arthritis

Helps you remain independent

Government exercise guidelines recommend that adults shoot for 150 minutes or more of moderate-intensity or 75 minutes of vigorous-intensity aerobic activity weekly. Ideally, it should be spread across several days. Cardio activities like walking, biking, swimming, bowling, gardening and dancing are good options for older adults.

Your regimen should also incorporate some strength training, along with balance and flexibility moves. (Think yoga or tai chi.) They can help keep you mobile and reduce injuries—especially from falls, which are often catastrophic for older people’s health.

Ease into your routine

Of course, older adults should always speak with a healthcare professional (HCP) before beginning any new regimen, especially if you have a chronic condition, like heart disease. Your HCP can help you decide on a safe, effective routine attuned to your fitness level.

And remember: Even if it’s just a short walk, any exertion is better than none. “Taking steps during the day to do physical activities or movement can be just as beneficial as if you joined a gym,” says Bohnsack. To start, he suggests simple moves like doing squats at work or parking farther away from your office so you can log a few extra steps.

It may help to use an app like Sharecare (available for iOS and Android) to help you track your daily activity.

Whatever you do, Bohnsack says, you must decide if planting yourself on the sofa is worth your long-term brain and heart health: “As I emphasize to patients, ‘A rolling stone gathers no moss.’”

Medically reviewed in February 2019."	https://www.sharecare.com/health/aging-and-fitness/article/want-keep-heart-brain-young	"Making this key lifestyle tweak keeps you mobile as you age—but that’s not where the benefits end.
Here’s a startling fact: About 3 in 4 American adults don’t get the recommended amount of physical activity, according to the Centers for Disease Control and Prevention.
Even more sobering: Many adults don’t get any activity at all, aside from what they need to make it through the day. And as we age, more and more of us stop moving. Almost 23 percent of adults between age 18 and 44 are sedentary. For those 65 and older, it’s around 32 percent.
While you likely know that long-term inactivity weakens your bones and muscles, you may not realize that it can damage your heart and brain, too. This, in turn, raises your odds of dementia and heart disease, among other conditions, and can lead to early death.
But research suggests that getting exercise can help keep these organs healthy and delay or prevent their decline. And if you regularly work up a sweat over a number of years? All the better.
“You really need to think about ways to keep moving,” says Kevin Bohnsack, MD, a family medicine physician at Saint Joseph Mercy Health System in Ann Arbor, Michigan. “Everything that increases your overall activity can ward off that sedentary lifestyle,” he adds—along with the cardiac and cognitive problems that can come with it.
How exercise benefits the heart
As you progress through middle age, your heart gradually begins to weaken. Its walls get thicker and less flexible, and your arteries become stiffer. This raises your risk for high blood pressure (hypertension) and other heart problems, including heart attack and heart failure. And if you’re sedentary, that risk goes up even more.
When you exercise, your heart beats faster, increasing blood flow and supplying your body with necessary oxygen. The more you work out, the stronger your heart gets and the more elastic your blood vessels become. This helps you maintain a lower blood pressure and decreases your chances of developing many cardiovascular problems.
It’s aerobic exercise—also called cardio—that really does the trick. Research suggests that consistent, long-term moderate or vigorous cardio training may be most helpful, though any physical activity promotes good heart health. “It can be anything from running to biking to rowing,” says Dr. Bohnsack. “Anything that builds up that heart rate.”
Getting in shape benefits your heart in other ways, too, by helping neutralize risk factors linked to heart disease. Exercise is associated with:
A reduction in inflammation
An increase in HDL (“good” cholesterol) and decrease in LDL (“bad” cholesterol)
Maintaining a healthy weight and staving off obesity
And though more studies are needed, research increasingly shows that exercise can boost your heart health no matter your age. For example, for one small study published in March 2018 in the journal Circulation, 28 middle-aged men completed two years of high-intensity exercise training. Compared to a control group, scientists found the exercise reduced their cardiac stiffness and increased their bodies’ capacity for oxygen use—both of which may slash the risk for heart failure.
For another study published in the August 2018 issue of Journal of the American Heart Association, researchers gave heartrate and movement sensors to 1,600 British volunteers between the ages of 60 and 64. After five days, they found that more active people had fewer indicators of heart disease in their blood. Not too shabby, boomers.
How exercise benefits the brain
What’s good for your heart is generally good for your mind—and research shows breaking a sweat on a regular basis can boost brain health in several ways.
First, exercise is tied to improved cognition, which includes better memory, attention and executive function—things like controlling emotions and completing tasks. It can enhance the speed with which you process and react to information, too, along with your capacity to draw from your past knowledge and experiences.
Getting physical is also linked to slower age-related cognitive decline, where we gradually lose our thinking, focus and memory skills. “In other words,” says Bohnsack, “if you like where you are, it’s a good idea to continue to exercise because that may at least help you retain your current cognitive function.”
And though the jury is still out on whether it improves symptoms, exercise may help prevent or delay dementia, including Alzheimer’s disease. For example, one 2017 review in The Journals of Gerontology: Biological Sciences found that activity was associated with a lower risk of Alzheimer’s down the line. The link was strongest for people who purposely exercised in their spare time, rather than those who had physically active jobs. This suggests mental benefits may depend on your chosen activity, in addition to the time you put into it.
How does exercise do all this? Scientists aren’t completely sure. It’s thought that working out improves blood flow and oxygen delivery to the brain, helping it function better. Some research indicates it prevents shrinkage of the hippocampus—the part of the brain crucial for learning and remembering things. Experts also believe it stimulates chemical activity in the brain that could contribute to better cognition.
Finally, exercise may help lower your chances of developing other conditions connected to dementia, including cardiovascular disease.
When can you start?
No matter our age, pretty much all of us can gain from exercise. “There is evidence to suggest that doing more vigorous exercise earlier in life is more beneficial,” says Bohnsack, “but it’s never too late to start because everyone benefits from doing some sort of movement or physical activity.”
In addition to its rewards for the heart and brain, working out:
Boosts your mood and energy
Helps prevent injuries
Lowers your risk of other diseases associated with aging, like arthritis
Helps you remain independent
Government exercise guidelines recommend that adults shoot for 150 minutes or more of moderate-intensity or 75 minutes of vigorous-intensity aerobic activity weekly. Ideally, it should be spread across several days. Cardio activities like walking, biking, swimming, bowling, gardening and dancing are good options for older adults.
Your regimen should also incorporate some strength training, along with balance and flexibility moves. (Think yoga or tai chi.) They can help keep you mobile and reduce injuries—especially from falls, which are often catastrophic for older people’s health.
Ease into your routine
Of course, older adults should always speak with a healthcare professional (HCP) before beginning any new regimen, especially if you have a chronic condition, like heart disease. Your HCP can help you decide on a safe, effective routine attuned to your fitness level.
And remember: Even if it’s just a short walk, any exertion is better than none. “Taking steps during the day to do physical activities or movement can be just as beneficial as if you joined a gym,” says Bohnsack. To start, he suggests simple moves like doing squats at work or parking farther away from your office so you can log a few extra steps.
It may help to use an app like Sharecare (available for iOS and Android) to help you track your daily activity.
Whatever you do, Bohnsack says, you must decide if planting yourself on the sofa is worth your long-term brain and heart health: “As I emphasize to patients, ‘A rolling stone gathers no moss.’”
Medically reviewed in February 2019."	7522
health	['Shelley Webb']		"At age 84, Sister Madonna Buder (aka The Iron Nun), finished Ironman Canada. How’d she do it? Sister Iron says, “…all I was concentrating on [was] getting the job done.”

Single-mindedness is essential for endurance athletes—and also for you, as you walk 10,000 steps a day to strengthen your body and mind without the wear and tear of endurance sports. Plus, mental focus let’s you drive safely or read—and remember—a book on a crowded commuter train. But many folks say that over time, focus becomes difficult and they’re more easily distracted.

Well, researchers measured the brain activity of volunteers 18 to 88 as they watched a movie and reacted to distractions. Turns out, as you age you notice and react to ever-more diverse sensory stimuli and that can blur your focus. Aging, said the researchers, makes your “experience of the world…increasingly individualistic,” differing from both younger folks and your peers as well.

So how can you enjoy the benefits of expanding sensory awareness and hold onto the ability to focus sharply? Try this.

Reduce mind-clouding stress with mindful meditation 5 minutes every morning and evening. Do resistance training exercises for 30 minutes, three days a week. Practice attention training: Set up tasks that require you to tune into what you’re doing and tune out distractions (like reading while the TV is on or writing an email in a noisy room).

Now you’re ready to defy expectations, just like Sister Madonna.

Medically reviewed in January 2020."	https://www.sharecare.com/health/aging-mental-health/article/3-ways-to-maintain-focus-as-you-age	"At age 84, Sister Madonna Buder (aka The Iron Nun), finished Ironman Canada. How’d she do it? Sister Iron says, “…all I was concentrating on [was] getting the job done.”
Single-mindedness is essential for endurance athletes—and also for you, as you walk 10,000 steps a day to strengthen your body and mind without the wear and tear of endurance sports. Plus, mental focus let’s you drive safely or read—and remember—a book on a crowded commuter train. But many folks say that over time, focus becomes difficult and they’re more easily distracted.
Well, researchers measured the brain activity of volunteers 18 to 88 as they watched a movie and reacted to distractions. Turns out, as you age you notice and react to ever-more diverse sensory stimuli and that can blur your focus. Aging, said the researchers, makes your “experience of the world…increasingly individualistic,” differing from both younger folks and your peers as well.
So how can you enjoy the benefits of expanding sensory awareness and hold onto the ability to focus sharply? Try this.
Reduce mind-clouding stress with mindful meditation 5 minutes every morning and evening. Do resistance training exercises for 30 minutes, three days a week. Practice attention training: Set up tasks that require you to tune into what you’re doing and tune out distractions (like reading while the TV is on or writing an email in a noisy room).
Now you’re ready to defy expectations, just like Sister Madonna.
Medically reviewed in January 2020."	1496
health	['Dr. Michael Roizen', 'Discovery Health']		"Your home should make you more healthy, not less. Put your pollution radar to the test and answer these four questions.

Ahhhh, home at last. Time to unwind, kick off your shoes and take a deep breath of . . . pollution?

Yep. Indoor air—in homes and offices—can be more polluted than outdoor air. And the average home contains hundreds of sources of worrisome pollutants, from cleaning supplies to carpets.

So how concerned should you be?

Research suggests that you'd have to breathe in unusually high levels of household pollutants for a long time to suffer serious health effects. But if you're sensitive to chemicals and allergens, even low levels can trigger irritating reactions.

But your home should make you more healthy, not less. Put your pollution radar to the test, and answer these four questions:

1. Pungent formaldehyde can be found in which of these products? (More than one answer may be correct.)

a. Cosmetics and nail polish

b. Glues and adhesives

c. Pressed-wood products (plywood, particle board, and medium-density fiberboard)

d. Foam insulation materials

e. Mouthwash

f. Wallpaper

g. Wrinkle-resistant drapes, linens, and other fabrics

h. All of the above

The answer is ""All of the above.""

While formaldehyde is a ""probable"" carcinogen, experts say the effect of typical home levels on cancer risk is low. Still, who needs it? And if you or your kids are sensitive to the stuff, you want to clear the air of it.

What you can do:

Buy solid-wood products; antique furniture; glass; or metal, such as stainless steel. (If you do buy pressed-wood furniture or paneling, be sure it conforms to low-emission standards.) Agency stamps that certify such products include ANSI, HPMA, CPA, NPA, HPVA.

Pass on treated fabrics whenever possible.

Check personal-care product ingredients for formaldehyde, and toss 'em if it's shown.

If you suspect levels are high—your eyes, nose and throat are irritated; you have headaches; you are dizzy and nauseated—buy a test kit, or have a professional testing company test the air.

Related: Clean up indoor air with these plants.

2. Which of the following household products is likely to contain other volatile organic compounds (VOCs), or chemicals that become gas at room temperature?

a. Air fresheners

b. Tap water

c. Freshly dry-cleaned clothing

d. None of the above

e. All of the above

The answer is ""All of the above.""

VOC levels vary from home to home and room to room. If you painted a room in the past year, if you bought new carpet or furniture, or if anyone smokes in your house, you've inhaled VOCs. In fact, VOCs—also released by heavily chlorinated tap water and perchloroethylene (PCE) in your just-dry-cleaned sweater—are two to five times higher indoors than outside.

Related: Cigarette smoke contains over 4,000 chemicals. Here are a few you could be inhaling.

At high levels, you may feel dizzy, nauseated, tired and uncoordinated. Your ears, nose and throat may become irritated, and you may have a skin reaction. A study of young children suggests that high levels of VOCs in the home may be linked to asthma.

What you can do:

Buy floor models of furniture and appliances that have had time to ""off gas.""

Buy solid-wood, glass, or metal (stainless steel) products.

Let fresh air in rather than using an air freshener. Weather permitting, open doors and windows, and use fans to bring outside air in.

Keep temperature and humidity as low as you can while still feeling comfortable; it can help decrease ""off-gassing.""

Do indoor construction and painting when you can vacate the house for a while or when it's warm enough to open all the windows and doors.

Buy low-VOC, air-friendly cleaning products, or make your own. (Go to www.cleanaircounts.org for recipes.)

Use VOC-free or low-VOC paint.

Store materials with high VOC levels (solvents, most paint) in a garage or shed, not in the house.

Keep household cleaners tightly sealed when not in use.

If your dry-cleaned clothes have a chemical odor, ask the cleaner to dry them properly. If it happens again, try a different dry cleaner, preferably one that doesn't use perchloroethylene. Take cleaned clothes from the plastic wrap, and let outdoor air circulate around them for an hour.

If your water has a strong chlorine smell, open a window or turn on the exhaust fan when taking a hot shower or bath. Chloroform is a by-product of chlorinated water.

Have your home tested if you have sensitivity symptoms.

3. Radon is the second leading cause of lung cancer in America. Where does it come from?

a. Radiators

b. Fluorescent lighting

c. Soil and groundwater

d. Wood-burning stoves or fireplaces

e. All of the above

The correct answer is ""Soil and groundwater.""

Radon, a radioactive gas, is naturally released by minerals such as uranium and radium in well water, soil and rock in certain areas, and by some building materials made from earth or stone.

When radon is in the soil beneath your house, it wafts in through cracks in the foundation, gaps in floorboards and openings around pipes. Once inside, it can build up to dangerous levels. It's estimated that high radon levels can be found in 1 of every 15 homes in America; in some parts of the country, that number jumps to 1 in 3.

What you can do:

Check the indoor air at the lowest level of your home with a radon test kit. For an additional fee (usually $5 to $25), the kit maker will analyze the results. Or you can have a professional do it.

Retest every 2 to 5 years or if you do any renovations, put in a new heating system, or install central air conditioning.

If radon problems are detected, have them fixed promptly by a qualified radon mitigation contractor.

Related: Try these easy ways to reduce your exposure to toxins.

4. Dust mites, pollen, dust, mold, bacteria, insects and animal dander trigger allergic reactions and hay fever-like symptoms in sensitive people. Where do these irritants lurk in your home?

a. Carpets and rugs

b. Fluffy toys

c. Beds and bedding

d. Air-conditioning systems

e. All of the above

The correct answer is ""All of the above.""

These biological bad guys are everywhere, including on pets and in the bathroom.

If you have respiratory complaints; eye, nose and throat irritation; frequent headaches; or feel tired or dizzy, see your healthcare provider for allergy testing.

What you can do:

Dust and vacuum your home, including upholstered furniture, frequently.

Consider investing in a HEPA-filter vacuum, which improves air quality and reduces allergens.

Don't let moisture or humidity build up anywhere. Mold and dust mites flourish in warm, damp environments. If you spot mold, remove it immediately. Try to identify the source of the moisture and fix the problem.

Use exhaust fans in the bathroom and kitchen to help keep humidity levels low.

If you use an air conditioner, humidifier or dehumidifier, make sure they're well maintained and cleaned regularly.

Medically reviewed in April 2019."	https://www.sharecare.com/health/air-quality/article/is-indoor-air-quality-at-work-home-killing-you	"Your home should make you more healthy, not less. Put your pollution radar to the test and answer these four questions.
Ahhhh, home at last. Time to unwind, kick off your shoes and take a deep breath of . . . pollution?
Yep. Indoor air—in homes and offices—can be more polluted than outdoor air. And the average home contains hundreds of sources of worrisome pollutants, from cleaning supplies to carpets.
So how concerned should you be?
Research suggests that you'd have to breathe in unusually high levels of household pollutants for a long time to suffer serious health effects. But if you're sensitive to chemicals and allergens, even low levels can trigger irritating reactions.
But your home should make you more healthy, not less. Put your pollution radar to the test, and answer these four questions:
1. Pungent formaldehyde can be found in which of these products? (More than one answer may be correct.)
a. Cosmetics and nail polish
b. Glues and adhesives
c. Pressed-wood products (plywood, particle board, and medium-density fiberboard)
d. Foam insulation materials
e. Mouthwash
f. Wallpaper
g. Wrinkle-resistant drapes, linens, and other fabrics
h. All of the above
The answer is ""All of the above.""
While formaldehyde is a ""probable"" carcinogen, experts say the effect of typical home levels on cancer risk is low. Still, who needs it? And if you or your kids are sensitive to the stuff, you want to clear the air of it.
What you can do:
Buy solid-wood products; antique furniture; glass; or metal, such as stainless steel. (If you do buy pressed-wood furniture or paneling, be sure it conforms to low-emission standards.) Agency stamps that certify such products include ANSI, HPMA, CPA, NPA, HPVA.
Pass on treated fabrics whenever possible.
Check personal-care product ingredients for formaldehyde, and toss 'em if it's shown.
If you suspect levels are high—your eyes, nose and throat are irritated; you have headaches; you are dizzy and nauseated—buy a test kit, or have a professional testing company test the air.
Related: Clean up indoor air with these plants.
2. Which of the following household products is likely to contain other volatile organic compounds (VOCs), or chemicals that become gas at room temperature?
a. Air fresheners
b. Tap water
c. Freshly dry-cleaned clothing
d. None of the above
e. All of the above
The answer is ""All of the above.""
VOC levels vary from home to home and room to room. If you painted a room in the past year, if you bought new carpet or furniture, or if anyone smokes in your house, you've inhaled VOCs. In fact, VOCs—also released by heavily chlorinated tap water and perchloroethylene (PCE) in your just-dry-cleaned sweater—are two to five times higher indoors than outside.
Related: Cigarette smoke contains over 4,000 chemicals. Here are a few you could be inhaling.
At high levels, you may feel dizzy, nauseated, tired and uncoordinated. Your ears, nose and throat may become irritated, and you may have a skin reaction. A study of young children suggests that high levels of VOCs in the home may be linked to asthma.
What you can do:
Buy floor models of furniture and appliances that have had time to ""off gas.""
Buy solid-wood, glass, or metal (stainless steel) products.
Let fresh air in rather than using an air freshener. Weather permitting, open doors and windows, and use fans to bring outside air in.
Keep temperature and humidity as low as you can while still feeling comfortable; it can help decrease ""off-gassing.""
Do indoor construction and painting when you can vacate the house for a while or when it's warm enough to open all the windows and doors.
Buy low-VOC, air-friendly cleaning products, or make your own. (Go to www.cleanaircounts.org for recipes.)
Use VOC-free or low-VOC paint.
Store materials with high VOC levels (solvents, most paint) in a garage or shed, not in the house.
Keep household cleaners tightly sealed when not in use.
If your dry-cleaned clothes have a chemical odor, ask the cleaner to dry them properly. If it happens again, try a different dry cleaner, preferably one that doesn't use perchloroethylene. Take cleaned clothes from the plastic wrap, and let outdoor air circulate around them for an hour.
If your water has a strong chlorine smell, open a window or turn on the exhaust fan when taking a hot shower or bath. Chloroform is a by-product of chlorinated water.
Have your home tested if you have sensitivity symptoms.
3. Radon is the second leading cause of lung cancer in America. Where does it come from?
a. Radiators
b. Fluorescent lighting
c. Soil and groundwater
d. Wood-burning stoves or fireplaces
e. All of the above
The correct answer is ""Soil and groundwater.""
Radon, a radioactive gas, is naturally released by minerals such as uranium and radium in well water, soil and rock in certain areas, and by some building materials made from earth or stone.
When radon is in the soil beneath your house, it wafts in through cracks in the foundation, gaps in floorboards and openings around pipes. Once inside, it can build up to dangerous levels. It's estimated that high radon levels can be found in 1 of every 15 homes in America; in some parts of the country, that number jumps to 1 in 3.
What you can do:
Check the indoor air at the lowest level of your home with a radon test kit. For an additional fee (usually $5 to $25), the kit maker will analyze the results. Or you can have a professional do it.
Retest every 2 to 5 years or if you do any renovations, put in a new heating system, or install central air conditioning.
If radon problems are detected, have them fixed promptly by a qualified radon mitigation contractor.
Related: Try these easy ways to reduce your exposure to toxins.
4. Dust mites, pollen, dust, mold, bacteria, insects and animal dander trigger allergic reactions and hay fever-like symptoms in sensitive people. Where do these irritants lurk in your home?
a. Carpets and rugs
b. Fluffy toys
c. Beds and bedding
d. Air-conditioning systems
e. All of the above
The correct answer is ""All of the above.""
These biological bad guys are everywhere, including on pets and in the bathroom.
If you have respiratory complaints; eye, nose and throat irritation; frequent headaches; or feel tired or dizzy, see your healthcare provider for allergy testing.
What you can do:
Dust and vacuum your home, including upholstered furniture, frequently.
Consider investing in a HEPA-filter vacuum, which improves air quality and reduces allergens.
Don't let moisture or humidity build up anywhere. Mold and dust mites flourish in warm, damp environments. If you spot mold, remove it immediately. Try to identify the source of the moisture and fix the problem.
Use exhaust fans in the bathroom and kitchen to help keep humidity levels low.
If you use an air conditioner, humidifier or dehumidifier, make sure they're well maintained and cleaned regularly.
Medically reviewed in April 2019."	6912
health	['Taylor Lupo']		"Your airways are responsible for carrying air into and out of your lungs, but what happens when they don’t function properly? Asthma, a chronic condition that affects about 24 million Americans, constricts the airways, preventing air from reaching the lungs and causing uncomfortable and sometimes dangerous symptoms.

For some, asthma symptoms are an inconvenience, but for others it causes life-threatening attacks that can interfere with everyday life.

Asthma triggers:

Someone with asthma may develop sensitivities to things around them, which can trigger asthma symptoms or attacks. During an asthma attack, the muscles around the airways contract, reducing a person’s ability to breathe properly and causing symptoms like coughing and wheezing.

These “triggers” vary from person-to-person, but luckily, many of them can be avoided. Asthma triggers include:

Tobacco smoke

Respiratory infections

Exercise

A change in weather

Allergens (pollen, pets, mold, etc.)

Intense crying or laughing

Medications

Irritants (cleaning products, pollution, perfumes, etc.)

Some individuals experience asthma flare-ups in certain places, like in the gym, at work and outdoors. Exercise-induced asthma may happen more frequently in cold weather; occupational asthmatics are sensitive to chemical smells, gas or dust; and those with allergy-induced asthma should be wary of airborne substances like mold spores, pet dander and pollen.

Asthma symptoms:

There are several symptoms that are characteristic of an asthma attack, some of which may signal a serious medical emergency. Signs and symptoms of asthma include chest pain, shortness of breath, wheezing when exhaling, coughing attacks and trouble sleeping due to symptoms.

If your asthma symptoms become more frequent or bothersome, your difficulty breathing worsens or you need to use your quick-relief inhaler more often, it may be an indication that your asthma is getting worse.

Asthma can be life threatening, so seek emergency medical treatment if shortness of breath is rapidly worsening, there is no improvement after using your inhaler or you experience shortness of breath during minimal physical activity.

Diagnosis and treatment:

To diagnose asthma, your healthcare provider will review your symptoms, medical history and family history before performing a physical exam. During the exam, your provider will listen to your heart and lungs. Your provider may also order breathing, blood and allergy tests, and chest x-rays. The results should tell your healthcare provider whether or not you have asthma or can help identify other conditions that may be contributing to your symptoms.

The first step in controlling asthma is avoiding your triggers, but that’s not always possible. Individuals with asthma should work with their healthcare provider to find the best treatment regimen. Your provider may prescribe long term asthma control medications that are taken every day to control symptoms and prevent attacks, along with quick-relief medications taken for short-term relief of symptoms and possibly medications for allergy-induced asthma to reduce the body’s sensitivity to allergy triggers.

Medically reviewed in January 2019."	https://www.sharecare.com/health/asthma-respiratory-problems/article/what-you-need-to-know-about-asthma	"Your airways are responsible for carrying air into and out of your lungs, but what happens when they don’t function properly? Asthma, a chronic condition that affects about 24 million Americans, constricts the airways, preventing air from reaching the lungs and causing uncomfortable and sometimes dangerous symptoms.
For some, asthma symptoms are an inconvenience, but for others it causes life-threatening attacks that can interfere with everyday life.
Asthma triggers:
Someone with asthma may develop sensitivities to things around them, which can trigger asthma symptoms or attacks. During an asthma attack, the muscles around the airways contract, reducing a person’s ability to breathe properly and causing symptoms like coughing and wheezing.
These “triggers” vary from person-to-person, but luckily, many of them can be avoided. Asthma triggers include:
Tobacco smoke
Respiratory infections
Exercise
A change in weather
Allergens (pollen, pets, mold, etc.)
Intense crying or laughing
Medications
Irritants (cleaning products, pollution, perfumes, etc.)
Some individuals experience asthma flare-ups in certain places, like in the gym, at work and outdoors. Exercise-induced asthma may happen more frequently in cold weather; occupational asthmatics are sensitive to chemical smells, gas or dust; and those with allergy-induced asthma should be wary of airborne substances like mold spores, pet dander and pollen.
Asthma symptoms:
There are several symptoms that are characteristic of an asthma attack, some of which may signal a serious medical emergency. Signs and symptoms of asthma include chest pain, shortness of breath, wheezing when exhaling, coughing attacks and trouble sleeping due to symptoms.
If your asthma symptoms become more frequent or bothersome, your difficulty breathing worsens or you need to use your quick-relief inhaler more often, it may be an indication that your asthma is getting worse.
Asthma can be life threatening, so seek emergency medical treatment if shortness of breath is rapidly worsening, there is no improvement after using your inhaler or you experience shortness of breath during minimal physical activity.
Diagnosis and treatment:
To diagnose asthma, your healthcare provider will review your symptoms, medical history and family history before performing a physical exam. During the exam, your provider will listen to your heart and lungs. Your provider may also order breathing, blood and allergy tests, and chest x-rays. The results should tell your healthcare provider whether or not you have asthma or can help identify other conditions that may be contributing to your symptoms.
The first step in controlling asthma is avoiding your triggers, but that’s not always possible. Individuals with asthma should work with their healthcare provider to find the best treatment regimen. Your provider may prescribe long term asthma control medications that are taken every day to control symptoms and prevent attacks, along with quick-relief medications taken for short-term relief of symptoms and possibly medications for allergy-induced asthma to reduce the body’s sensitivity to allergy triggers.
Medically reviewed in January 2019."	3182
biology	[]	2021-05-21 00:00:00	"Base editing is a novel gene editing approach that can precisely change individual building blocks in a DNA sequence. By installing such a point mutation in a specific gene, an international research team led by the University of Zurich has succeeded in sustainably lowering high LDL cholesterol levels in the blood of mice and macaques. This opens up the possibility of curing patients with inherited metabolic liver diseases.

Lipoproteins are complex particles that deliver fat molecules to all tissues of the body through the blood system, supplying energy to the cells. One such lipoprotein, the low-density lipoprotein (LDL), can transport thousands of fat molecules, such as cholesterol, per particle. High levels of LDL in the blood are clinically associated with an increased risk of cardiovascular diseases. Since LDL can also carry cholesterol into smaller vessels and thus supply more distant tissues, it can increasingly block the artery lumen, which leads to atherosclerosis.

Introducing a single gene mutation blocks an enzyme

An international research team led by the University of Zurich (UZH) has now demonstrated that a novel precise gene editing approach can reduce high LDL cholesterol levels -- substantially and sustainably. The scientists introduced a single point mutation in the gene encoding for an enzyme called PCSK9. This protein is involved in the uptake of LDL cholesterol from the blood into the cells. ""The genetic change we induced in mice and macaques successfully blocked PCSK9, which led to a significant reduction of the LDL cholesterol concentrations in the blood. This provides a potential therapy for patients suffering from familial hypercholesterolemia, an inherited form of high cholesterol levels,"" says study leader Gerald Schwank, professor at the Institute for Pharmacology and Toxicology of UZH.

Adaption of RNA technology used in COVID-19 vaccines

The gene editing technology applied by the researchers uses what are known as base editors. These proteins can change individual bases of the DNA molecule -- a single ""letter"" of a genetic ""text"" -- into another. Adenine base editors, for example, convert an adenine (A) into a guanine (G). And base editors do this much more precisely than previous CRISPR-Cas nucleases, which function as molecular scissors. To control the delivery of the base editor tool into the liver of animals, the researchers adapted the RNA technology used in COVID-19 vaccines. However, instead of encapsulating an RNA encoding the spike protein of SARS-CoV2 into lipid nanoparticles, they encapsulated an RNA encoding for the adenine base editor.

Accurate, efficient and safe

The RNA-lipid nanoparticles formulations were introduced into the animals intravenously, leading to liver-specific uptake and transient production of the base editor tool by the cell machinery. ""Up to two-thirds of PCSK9 genes were edited in the mice and up to one-third in the non-human primates, leading to a significant reduction in LDL cholesterol levels,"" says Schwank. In addition, the scientists carefully assessed whether unspecific editing at undesired locations occurred, but found no indications of such off-target events.

RNA-based therapies for metabolic liver diseases

""Our study shows the feasibility of installing single nucleotide base changes in the liver of non-human primates with high efficiency and accuracy. Approximately 30 percent of all disease-causing hereditary mutations are single base mutations that can, in principle, be corrected with base editors,"" says Schwank. The new approach could therefore be used to treat a large number of patients suffering from inherited metabolic liver diseases, such as hypercholesterolemia, phenylketonuria or urea cycle disorders. Compared to conventional drugs, genome editing has the advantage that induced changes are sustainable. Thus, if a mutation is repaired in a sufficient number of cells, the patient will be permanently cured."	https://www.sciencedaily.com/releases/2021/05/210519120726.htm	"Base editing is a novel gene editing approach that can precisely change individual building blocks in a DNA sequence. By installing such a point mutation in a specific gene, an international research team led by the University of Zurich has succeeded in sustainably lowering high LDL cholesterol levels in the blood of mice and macaques. This opens up the possibility of curing patients with inherited metabolic liver diseases.
Lipoproteins are complex particles that deliver fat molecules to all tissues of the body through the blood system, supplying energy to the cells. One such lipoprotein, the low-density lipoprotein (LDL), can transport thousands of fat molecules, such as cholesterol, per particle. High levels of LDL in the blood are clinically associated with an increased risk of cardiovascular diseases. Since LDL can also carry cholesterol into smaller vessels and thus supply more distant tissues, it can increasingly block the artery lumen, which leads to atherosclerosis.
Introducing a single gene mutation blocks an enzyme
An international research team led by the University of Zurich (UZH) has now demonstrated that a novel precise gene editing approach can reduce high LDL cholesterol levels -- substantially and sustainably. The scientists introduced a single point mutation in the gene encoding for an enzyme called PCSK9. This protein is involved in the uptake of LDL cholesterol from the blood into the cells. ""The genetic change we induced in mice and macaques successfully blocked PCSK9, which led to a significant reduction of the LDL cholesterol concentrations in the blood. This provides a potential therapy for patients suffering from familial hypercholesterolemia, an inherited form of high cholesterol levels,"" says study leader Gerald Schwank, professor at the Institute for Pharmacology and Toxicology of UZH.
Adaption of RNA technology used in COVID-19 vaccines
The gene editing technology applied by the researchers uses what are known as base editors. These proteins can change individual bases of the DNA molecule -- a single ""letter"" of a genetic ""text"" -- into another. Adenine base editors, for example, convert an adenine (A) into a guanine (G). And base editors do this much more precisely than previous CRISPR-Cas nucleases, which function as molecular scissors. To control the delivery of the base editor tool into the liver of animals, the researchers adapted the RNA technology used in COVID-19 vaccines. However, instead of encapsulating an RNA encoding the spike protein of SARS-CoV2 into lipid nanoparticles, they encapsulated an RNA encoding for the adenine base editor.
Accurate, efficient and safe
The RNA-lipid nanoparticles formulations were introduced into the animals intravenously, leading to liver-specific uptake and transient production of the base editor tool by the cell machinery. ""Up to two-thirds of PCSK9 genes were edited in the mice and up to one-third in the non-human primates, leading to a significant reduction in LDL cholesterol levels,"" says Schwank. In addition, the scientists carefully assessed whether unspecific editing at undesired locations occurred, but found no indications of such off-target events.
RNA-based therapies for metabolic liver diseases
""Our study shows the feasibility of installing single nucleotide base changes in the liver of non-human primates with high efficiency and accuracy. Approximately 30 percent of all disease-causing hereditary mutations are single base mutations that can, in principle, be corrected with base editors,"" says Schwank. The new approach could therefore be used to treat a large number of patients suffering from inherited metabolic liver diseases, such as hypercholesterolemia, phenylketonuria or urea cycle disorders. Compared to conventional drugs, genome editing has the advantage that induced changes are sustainable. Thus, if a mutation is repaired in a sufficient number of cells, the patient will be permanently cured."	3954
biology	[]	2021-05-21 00:00:00	"For young plants, timing is just about everything. Now, scientists have found that herbivores, animals that consume plants, have a lot to say about evolution at this vulnerable life stage.

Once a plant seedling breaches the soil surface and begins to grow, a broad range of factors will determine whether it thrives or perishes.

Scientists have long perceived that natural selection favors early rising seeds. Seedlings that emerge early in the growing season should have a competitive advantage in monopolizing precious soil resources. Early growth also should mean more access to light, since early growers can block sunlight for seedlings that emerge later in the season.

Despite plenty of proof that germinating early is highly advantageous, many plants germinate later. Why?

University of California San Diego researchers found in a recent study that herbivores have a lot to say in determining how germination time contributes to plant growth.

Research published in the journal Evolution Letters by former UC San Diego graduate student Joseph Waterton and Division of Biological Sciences Professor Elsa Cleland has shown that certain vertebrate herbivores -- including mice, rabbits and birds -- play an underappreciated role in shaping natural selection in plant growth. Due to earlier seasonal growth patterns emerging from climate change, the new findings may factor into evolutionary responses to global environmental changes.

""Germination timing is a really important trait that influences the fitness of plants and their ability to survive and reproduce,"" said Waterton, who received his PhD in Biological Sciences and is now at Indiana University. ""Until this study we didn't understand the role herbivores played in the evolution of this trait -- we had very little idea of what shaped this trait besides aspects of the abiotic environment, such as climate.""

In field studies using two California grass species, Waterton and Cleland found that early emerging seedlings were more impacted by vertebrate herbivores compared to those that emerged later, likely because they were the first and biggest bits of greenery available on the landscape at the start of the growing season when not much else was growing. They found that such early season consumption by herbivores shrinks the benefit of early seedling emergence.

Conducted at the UC San Diego Biology Field Station in early 2018, the researchers' study examined native Stipa pulchra and non?native Bromus diandrus grasses, common California grass species with vastly different origins and growth strategies, in neighboring plots that either excluded or allowed access to vertebrate herbivores. Herbivores consistently weakened the success of early emergers in both grass species, the results showed.

With climate change forces shifting the timing of many plant life stages earlier in the year, the new study shows that herbivores are likely working as a counteracting force.

""Plants that germinate earlier than their neighbors tend to win out. But our study shows that herbivory early in the growing season can counteract the advantage of early germination,"" said Cleland, a professor in the Ecology, Behavior and Evolution Section. ""This is important because in order to persist and keep pace with climate change, many species will need to shift their seasonal timing. Our study shows that we can't accurately estimate the strength of natural selection on key traits if we don't account for realistic forces acting in nature, such as herbivory on plants.""

The study was funded by a graduate student researcher fellowship from the University of California's Institute for the Study of Ecological Effects of Climate Impacts (ISEECI) and a Jeanne M. Messier Memorial Fellowship."	https://www.sciencedaily.com/releases/2021/05/210519120805.htm	"For young plants, timing is just about everything. Now, scientists have found that herbivores, animals that consume plants, have a lot to say about evolution at this vulnerable life stage.
Once a plant seedling breaches the soil surface and begins to grow, a broad range of factors will determine whether it thrives or perishes.
Scientists have long perceived that natural selection favors early rising seeds. Seedlings that emerge early in the growing season should have a competitive advantage in monopolizing precious soil resources. Early growth also should mean more access to light, since early growers can block sunlight for seedlings that emerge later in the season.
Despite plenty of proof that germinating early is highly advantageous, many plants germinate later. Why?
University of California San Diego researchers found in a recent study that herbivores have a lot to say in determining how germination time contributes to plant growth.
Research published in the journal Evolution Letters by former UC San Diego graduate student Joseph Waterton and Division of Biological Sciences Professor Elsa Cleland has shown that certain vertebrate herbivores -- including mice, rabbits and birds -- play an underappreciated role in shaping natural selection in plant growth. Due to earlier seasonal growth patterns emerging from climate change, the new findings may factor into evolutionary responses to global environmental changes.
""Germination timing is a really important trait that influences the fitness of plants and their ability to survive and reproduce,"" said Waterton, who received his PhD in Biological Sciences and is now at Indiana University. ""Until this study we didn't understand the role herbivores played in the evolution of this trait -- we had very little idea of what shaped this trait besides aspects of the abiotic environment, such as climate.""
In field studies using two California grass species, Waterton and Cleland found that early emerging seedlings were more impacted by vertebrate herbivores compared to those that emerged later, likely because they were the first and biggest bits of greenery available on the landscape at the start of the growing season when not much else was growing. They found that such early season consumption by herbivores shrinks the benefit of early seedling emergence.
Conducted at the UC San Diego Biology Field Station in early 2018, the researchers' study examined native Stipa pulchra and non?native Bromus diandrus grasses, common California grass species with vastly different origins and growth strategies, in neighboring plots that either excluded or allowed access to vertebrate herbivores. Herbivores consistently weakened the success of early emergers in both grass species, the results showed.
With climate change forces shifting the timing of many plant life stages earlier in the year, the new study shows that herbivores are likely working as a counteracting force.
""Plants that germinate earlier than their neighbors tend to win out. But our study shows that herbivory early in the growing season can counteract the advantage of early germination,"" said Cleland, a professor in the Ecology, Behavior and Evolution Section. ""This is important because in order to persist and keep pace with climate change, many species will need to shift their seasonal timing. Our study shows that we can't accurately estimate the strength of natural selection on key traits if we don't account for realistic forces acting in nature, such as herbivory on plants.""
The study was funded by a graduate student researcher fellowship from the University of California's Institute for the Study of Ecological Effects of Climate Impacts (ISEECI) and a Jeanne M. Messier Memorial Fellowship."	3745
biology	[]	2021-05-21 00:00:00	"When scientists hunt for life, they often look for biosignatures, chemicals or phenomena that indicate the existence of present or past life. Yet it isn't necessarily the case that the signs of life on Earth are signs of life in other planetary environments. How do we find life in systems that do not resemble ours?

In groundbreaking new work, a team* led by Santa Fe Institute Professor Chris Kempes has developed a new ecological biosignature that could help scientists detect life in vastly different environments. Their work appears as part of a special issue of the Bulletin of Mathematical Biology collected in honor of renowned mathematical biologist James D. Murray.

The new research takes its starting point from the idea that stoichiometry, or chemical ratios, can serve as biosignatures. Since ""living systems display strikingly consistent ratios in their chemical make-up,"" Kempes explains, ""we can use stoichiometry to help us detect life."" Yet, as SFI Science Board member and contributor, Simon Levin, explains, ""the particular elemental ratios we see on Earth are the result of the particular conditions here, and a particular set of macromolecules like proteins and ribosomes, which have their own stoichiometry."" How can these elemental ratios be generalized beyond the life that we observe on our own planet?

The group solved this problem by building on two lawlike patterns, two scaling laws, that are entangled in elemental ratios we have observed on Earth. The first of these is that in individual cells, stoichiometry varies with cell size. In bacteria, for example, as cell size increases, protein concentrations decrease, and RNA concentrations increase. The second is that the abundance of cells in a given environment follows a power-law distribution. The third, which follows from integrating the first and second into a simple ecological model, is that the elemental abundance of particles to the elemental abundance in the environmental fluid is a function of particle size.

While the first of these (that elemental ratios shift with particle size) makes for a chemical biosignature, it is the third finding that makes for the new ecological biosignature. If we think of biosignatures not simply in terms of single chemicals or particles, and instead take account of the fluids in which particles appear, we see that the chemical abundances of living systems manifest themselves in mathematical ratios between the particle and environment. These general mathematical patterns may show up in coupled systems that differ significantly from Earth.

Ultimately, the theoretical framework is designed for application in future planetary missions. ""If we go to an ocean world and look at particles in context with their fluid, we can start to ask whether these particles are exhibiting a power-law that tells us that there is an intentional process, like life, making them,"" explains Heather Graham, Deputy Principal Investigator at NASA's Lab for Agnostic Biosignatures, of which she and Kempes are a part. To take this applied step, however, we need technology to size-sort particles, which, at the moment, we don't have for spaceflight. Yet the theory is ready, and when the technology lands on Earth, we can send it to icy oceans beyond our solar system with a promising new biosignature in hand."	https://www.sciencedaily.com/releases/2021/05/210520133738.htm	"When scientists hunt for life, they often look for biosignatures, chemicals or phenomena that indicate the existence of present or past life. Yet it isn't necessarily the case that the signs of life on Earth are signs of life in other planetary environments. How do we find life in systems that do not resemble ours?
In groundbreaking new work, a team* led by Santa Fe Institute Professor Chris Kempes has developed a new ecological biosignature that could help scientists detect life in vastly different environments. Their work appears as part of a special issue of the Bulletin of Mathematical Biology collected in honor of renowned mathematical biologist James D. Murray.
The new research takes its starting point from the idea that stoichiometry, or chemical ratios, can serve as biosignatures. Since ""living systems display strikingly consistent ratios in their chemical make-up,"" Kempes explains, ""we can use stoichiometry to help us detect life."" Yet, as SFI Science Board member and contributor, Simon Levin, explains, ""the particular elemental ratios we see on Earth are the result of the particular conditions here, and a particular set of macromolecules like proteins and ribosomes, which have their own stoichiometry."" How can these elemental ratios be generalized beyond the life that we observe on our own planet?
The group solved this problem by building on two lawlike patterns, two scaling laws, that are entangled in elemental ratios we have observed on Earth. The first of these is that in individual cells, stoichiometry varies with cell size. In bacteria, for example, as cell size increases, protein concentrations decrease, and RNA concentrations increase. The second is that the abundance of cells in a given environment follows a power-law distribution. The third, which follows from integrating the first and second into a simple ecological model, is that the elemental abundance of particles to the elemental abundance in the environmental fluid is a function of particle size.
While the first of these (that elemental ratios shift with particle size) makes for a chemical biosignature, it is the third finding that makes for the new ecological biosignature. If we think of biosignatures not simply in terms of single chemicals or particles, and instead take account of the fluids in which particles appear, we see that the chemical abundances of living systems manifest themselves in mathematical ratios between the particle and environment. These general mathematical patterns may show up in coupled systems that differ significantly from Earth.
Ultimately, the theoretical framework is designed for application in future planetary missions. ""If we go to an ocean world and look at particles in context with their fluid, we can start to ask whether these particles are exhibiting a power-law that tells us that there is an intentional process, like life, making them,"" explains Heather Graham, Deputy Principal Investigator at NASA's Lab for Agnostic Biosignatures, of which she and Kempes are a part. To take this applied step, however, we need technology to size-sort particles, which, at the moment, we don't have for spaceflight. Yet the theory is ready, and when the technology lands on Earth, we can send it to icy oceans beyond our solar system with a promising new biosignature in hand."	3324
biology	[]	2021-05-21 00:00:00	"Juvenile salmon migrating to the sea in the Sacramento River face a gauntlet of hazards in an environment drastically modified by humans, especially with respect to historical patterns of stream flow. Many studies have shown that survival rates of juvenile salmon improve as the amount of water flowing downstream increases, but ""more is better"" is not a useful guideline for agencies managing competing demands for the available water.

Now fisheries scientists have identified key thresholds in the relationship between stream flow and salmon survival that can serve as actionable targets for managing water resources in the Sacramento River. The new analysis, published May 19 in Ecosphere, revealed nonlinear effects in the flow-survival relationship, meaning it changes in stepwise fashion, with significant jumps in survival rates at two key steps.

A threshold defined in the paper as the ""historic mean"" flow of 10,712 cubic feet per second (cfs) provides an especially important target for resource managers, said first author Cyril Michel, a project scientist in the Institute of Marine Sciences at UC Santa Cruz.

""We see a substantial increase in salmon survival above that level, so if we can increase stream flow to that level for critical periods of the year, it would really benefit the salmon populations,"" Michel said.

The researchers analyzed migration survival data from 2,436 juvenile Chinook salmon tagged with acoustic transmitters and tracked in years with different water flows, from 2013 to 2019. After identifying the key thresholds, the team then used historical data on river flows and salmon migration patterns to run simulations of different management actions.

""We wanted to see how much the salmon populations would benefit if we had enacted flows to match that threshold of 10,712 cfs,"" Michel said. ""We found we could increase survival by a lot, sometimes doubling or tripling the survival rates in a given year, without having to spend too much water. It's a reasonable target that won't break the bank in most years.""

Juvenile salmon migrate out to sea in the spring, which was historically a period of high flows in the Sacramento River. Now, however, dams and water diversions combined with seasonal reductions in flows from tributaries result in spring flows that tend to be the lowest of the year.

advertisement

""Because of how we've plumbed the Central Valley, salmon now have to migrate out during low flows. So we're proposing to enact pulse flows in the spring to bring the river up to historical conditions for short periods of time,"" Michel said.

He noted that plans to implement pulse flows in the Sacramento River are now being developed under environmental permits renegotiated last year for the Central Valley Project, the massive federal water management project that includes the Sacramento River. An interagency group of scientists has agreed to use the 10,712 cfs threshold as the target for these pulse flows, Michel said.

""We're excited that this might actually happen this year or next, and we will be tracking survival rates to see how successful it is,"" he said. ""This could be a tool that is used for many years to come, with real benefits for salmon populations.""

Additional research on the optimal timing of pulse flows could improve their implementation, enabling adaptive management in response to environmental conditions, he added.

The study identified two other thresholds, defined as ""minimum"" (4,259 cfs) and ""high"" (22,872 cfs). Below the minimum threshold, only 3% of the tagged salmon survived the migration. Survival was 18.9% between minimum and historic mean, 50.8% between historic mean and high, and 35.3% above the high threshold.

The results suggest that the main mechanism behind the flow thresholds relates to how fast migrating fish are able to move through the river and get past the hazards along the way. Travel times for fish during flows between the historic mean and high thresholds were significantly shorter than for fish experiencing all other flows.

""In most years, it's best for juvenile salmon to rear in the upper river where it's safer, and then move through the lower river as quickly as possible to reduce their exposure to predators and other stressors,"" Michel explained."	https://www.sciencedaily.com/releases/2021/05/210520133711.htm	"Juvenile salmon migrating to the sea in the Sacramento River face a gauntlet of hazards in an environment drastically modified by humans, especially with respect to historical patterns of stream flow. Many studies have shown that survival rates of juvenile salmon improve as the amount of water flowing downstream increases, but ""more is better"" is not a useful guideline for agencies managing competing demands for the available water.
Now fisheries scientists have identified key thresholds in the relationship between stream flow and salmon survival that can serve as actionable targets for managing water resources in the Sacramento River. The new analysis, published May 19 in Ecosphere, revealed nonlinear effects in the flow-survival relationship, meaning it changes in stepwise fashion, with significant jumps in survival rates at two key steps.
A threshold defined in the paper as the ""historic mean"" flow of 10,712 cubic feet per second (cfs) provides an especially important target for resource managers, said first author Cyril Michel, a project scientist in the Institute of Marine Sciences at UC Santa Cruz.
""We see a substantial increase in salmon survival above that level, so if we can increase stream flow to that level for critical periods of the year, it would really benefit the salmon populations,"" Michel said.
The researchers analyzed migration survival data from 2,436 juvenile Chinook salmon tagged with acoustic transmitters and tracked in years with different water flows, from 2013 to 2019. After identifying the key thresholds, the team then used historical data on river flows and salmon migration patterns to run simulations of different management actions.
""We wanted to see how much the salmon populations would benefit if we had enacted flows to match that threshold of 10,712 cfs,"" Michel said. ""We found we could increase survival by a lot, sometimes doubling or tripling the survival rates in a given year, without having to spend too much water. It's a reasonable target that won't break the bank in most years.""
Juvenile salmon migrate out to sea in the spring, which was historically a period of high flows in the Sacramento River. Now, however, dams and water diversions combined with seasonal reductions in flows from tributaries result in spring flows that tend to be the lowest of the year.
advertisement
""Because of how we've plumbed the Central Valley, salmon now have to migrate out during low flows. So we're proposing to enact pulse flows in the spring to bring the river up to historical conditions for short periods of time,"" Michel said.
He noted that plans to implement pulse flows in the Sacramento River are now being developed under environmental permits renegotiated last year for the Central Valley Project, the massive federal water management project that includes the Sacramento River. An interagency group of scientists has agreed to use the 10,712 cfs threshold as the target for these pulse flows, Michel said.
""We're excited that this might actually happen this year or next, and we will be tracking survival rates to see how successful it is,"" he said. ""This could be a tool that is used for many years to come, with real benefits for salmon populations.""
Additional research on the optimal timing of pulse flows could improve their implementation, enabling adaptive management in response to environmental conditions, he added.
The study identified two other thresholds, defined as ""minimum"" (4,259 cfs) and ""high"" (22,872 cfs). Below the minimum threshold, only 3% of the tagged salmon survived the migration. Survival was 18.9% between minimum and historic mean, 50.8% between historic mean and high, and 35.3% above the high threshold.
The results suggest that the main mechanism behind the flow thresholds relates to how fast migrating fish are able to move through the river and get past the hazards along the way. Travel times for fish during flows between the historic mean and high thresholds were significantly shorter than for fish experiencing all other flows.
""In most years, it's best for juvenile salmon to rear in the upper river where it's safer, and then move through the lower river as quickly as possible to reduce their exposure to predators and other stressors,"" Michel explained."	4269
biology	[]	2021-05-21 00:00:00	"Open up Scott Roy's Twitter bio and you'll see a simple but revealing sentence: ""The more I learn the more I'm confused."" Now the rest of the scientific world can share in his confusion. The San Francisco State University associate professor of Biology's most recent research catalogues a strange and confounding system of genes in a tiny rodent that scientists have ignored for decades.

""This is basically the weirdest sex chromosome system known to science,"" Roy said. ""Nobody ordered this."" But he's serving it anyway.

The owner of those chromosomes is the creeping vole, a burrowing rodent native to the Pacific Northwest. Scientists have known since the '60s that the species had some odd genes: Their number of X and Y chromosomes (bundles of DNA that play a large role in determining sex) is off from what's expected in male and female mammals.

That finding caught Roy's eye when presented by a guest speaker at a San Francisco State seminar, and he realized that modern technology might be able to shed new light on the mysteries hiding in the voles' DNA. After working with collaborators to disentangle the voles' genetic history -- resulting in one of the most completely sequenced mammal genomes that exists, according to Roy -- the story only got stranger.

The team found that the X and Y chromosomes had fused somewhere in the rodents' past, and that the X chromosome in males started looking and acting like a Y chromosome. The numbers of X chromosomes in male and female voles changed too, along with smaller pieces of DNA getting swapped between them. The researchers published their results in Science on May 7.

Drastic genetic changes like these are exceptionally rare: The way genes determine sex in mammals has stayed mostly the same for about 180 million years, Roy explains. ""Mammals, with few exceptions, are kind of boring,"" he said. ""Previously we would have thought something like this is impossible.""

So how did the genes of this unassuming rodent end up so jumbled? It's not an easy question to answer, especially since evolution is bound to produce some strangeness simply by chance. Roy, however, is determined to figure out the ""why."" He suspects that what the team found in the vole's genome is something like the aftermath of an evolutionary battle for dominance between the X and Y chromosome.

The research couldn't have happened, Roy says, without collaborations with Oregon fish and wildlife biologists who had a creeping vole sample sitting in a lab freezer. He also teamed up with a group from Oklahoma State University when the two groups started chatting about creeping vole DNA sequences that were posted on the internet -- and both realized they were working on the same question.

Another key was working at a teaching-focused institution. Roy says he has the time to develop ideas with colleagues and students at SF State, and he can do research where he doesn't quite know what he'll find. ""This is a great example of non-hypothesis-based biology,"" Roy explained. ""The hypothesis was, 'This system is interesting. I bet if you looked into it some more, there'd be other interesting things.'""

It won't be the last time Roy's lab goes out on a limb. He and his collaborators plan to look into the genomes of other species related to the voles to chart the evolutionary path that led to this strange system. He'll also continue DNA sequencing curiosities across the tree of life.

""These bizarre systems give us a handhold to start to understand why the more common systems are the way they are and why our biology works as it does,"" he explained. By delving into the weirdest that nature has to offer, maybe we can come to understand ourselves better, too."	https://www.sciencedaily.com/releases/2021/05/210519162613.htm	"Open up Scott Roy's Twitter bio and you'll see a simple but revealing sentence: ""The more I learn the more I'm confused."" Now the rest of the scientific world can share in his confusion. The San Francisco State University associate professor of Biology's most recent research catalogues a strange and confounding system of genes in a tiny rodent that scientists have ignored for decades.
""This is basically the weirdest sex chromosome system known to science,"" Roy said. ""Nobody ordered this."" But he's serving it anyway.
The owner of those chromosomes is the creeping vole, a burrowing rodent native to the Pacific Northwest. Scientists have known since the '60s that the species had some odd genes: Their number of X and Y chromosomes (bundles of DNA that play a large role in determining sex) is off from what's expected in male and female mammals.
That finding caught Roy's eye when presented by a guest speaker at a San Francisco State seminar, and he realized that modern technology might be able to shed new light on the mysteries hiding in the voles' DNA. After working with collaborators to disentangle the voles' genetic history -- resulting in one of the most completely sequenced mammal genomes that exists, according to Roy -- the story only got stranger.
The team found that the X and Y chromosomes had fused somewhere in the rodents' past, and that the X chromosome in males started looking and acting like a Y chromosome. The numbers of X chromosomes in male and female voles changed too, along with smaller pieces of DNA getting swapped between them. The researchers published their results in Science on May 7.
Drastic genetic changes like these are exceptionally rare: The way genes determine sex in mammals has stayed mostly the same for about 180 million years, Roy explains. ""Mammals, with few exceptions, are kind of boring,"" he said. ""Previously we would have thought something like this is impossible.""
So how did the genes of this unassuming rodent end up so jumbled? It's not an easy question to answer, especially since evolution is bound to produce some strangeness simply by chance. Roy, however, is determined to figure out the ""why."" He suspects that what the team found in the vole's genome is something like the aftermath of an evolutionary battle for dominance between the X and Y chromosome.
The research couldn't have happened, Roy says, without collaborations with Oregon fish and wildlife biologists who had a creeping vole sample sitting in a lab freezer. He also teamed up with a group from Oklahoma State University when the two groups started chatting about creeping vole DNA sequences that were posted on the internet -- and both realized they were working on the same question.
Another key was working at a teaching-focused institution. Roy says he has the time to develop ideas with colleagues and students at SF State, and he can do research where he doesn't quite know what he'll find. ""This is a great example of non-hypothesis-based biology,"" Roy explained. ""The hypothesis was, 'This system is interesting. I bet if you looked into it some more, there'd be other interesting things.'""
It won't be the last time Roy's lab goes out on a limb. He and his collaborators plan to look into the genomes of other species related to the voles to chart the evolutionary path that led to this strange system. He'll also continue DNA sequencing curiosities across the tree of life.
""These bizarre systems give us a handhold to start to understand why the more common systems are the way they are and why our biology works as it does,"" he explained. By delving into the weirdest that nature has to offer, maybe we can come to understand ourselves better, too."	3697
biology	[]	2021-05-21 00:00:00	"Researchers have been able to prove for the first time that activation of distinct human endogenous retroviruses, which are part of our genome, impair brain development dramatically. This finding could help to advance research into therapies for neurodegenerative diseases. The study originated from an international collaboration led by Helmholtz Zentrum München.

Since our ancestors infected themselves with retroviruses millions of years ago, we have carried elements of these viruses in our genes -- known as human endogenous retroviruses, or HERVs for short. These viral elements have lost their ability to replicate and infect during evolution, but are an integral part of our genetic makeup. In fact, humans possess five times more HERVs in non-coding parts than coding genes. So far, strong focus has been devoted to the correlation of HERVs and the onset or progression of diseases. This is why HERV expression has been studied in samples of pathological origin. Although important, these studies do not provide conclusions about whether HERVs are the cause or the consequence of such disease.

Today, new technologies enable scientists to receive a deeper insight into the mechanisms of HERVs and their function. Together with her colleagues, virologist Michelle Vincendeau* has now succeeded for the first time in demonstrating the negative effects of HERV activation on human brain development.

HERV activation impairs brain development

Using CRISPR technology, the researchers activated a specific group of human endogenous retroviruses** in human embryonic stem cells and generated nerve cells (neurons). These viral elements in turn activated specific genes, including classical developmental factors, involved in brain development. As a result, cortical neurons, meaning the nerve cells in our cerebral cortex, lost their function entirely. They developed very differently from healthy neurons in this brain region -- with much a shorter axon (nerve cell extension) that were much less branched. Thus, activation of one specific HERV group impairs cortical neuron development and ultimately brain development.

Clinical relevance

Since neurodegenerative diseases are often associated with the activation of several HERV groups, the negative impact of HERV activation on cortical neuron development is an essential finding. It is already known that environmental factors such as viruses, bacteria, and UV light can activate distinct HERVs, thereby potentially contributing to disease onset. This knowledge, in turn, makes HERVs even more interesting for clinical application. Switching off distinct viral elements could open up a new field of research for the treatment of patients with neurodegenerative diseases. In a next step, the group at Helmholtz Zentrum München will study the impact of HERV deactivation in neurons in the context of disease.

New paths for basic research

In addition, the research findings provide important indications that epigenetic mechanisms keep viral elements under control in healthy brain development. Michelle Vincendeau even suspects a functional role for the controlled HERVs in normal brain development. ""We have carried these elements for about 40 to 70 million years. We assume that their presence is relevant to our natural processes, otherwise we would not have retained them for so long during evolution,"" Vincendeau says. Further basic research in this direction might reveal new functional roles for HERVs.

Notes:

* Michelle Vincendeau leads the research group for Human Endogenous Retroviruses at the Institute of Viorology at Helmholtz Zentrum München. Part of the data from the current study was generated in the context of her previous work at the Memorial Sloan Kettering Cancer Center in New York. For this paper, she also collaborated with researchers at the Technical University of Munich and the University of Saarland.

** HERV-K(HML-2)"	https://www.sciencedaily.com/releases/2021/05/210520133706.htm	"Researchers have been able to prove for the first time that activation of distinct human endogenous retroviruses, which are part of our genome, impair brain development dramatically. This finding could help to advance research into therapies for neurodegenerative diseases. The study originated from an international collaboration led by Helmholtz Zentrum München.
Since our ancestors infected themselves with retroviruses millions of years ago, we have carried elements of these viruses in our genes -- known as human endogenous retroviruses, or HERVs for short. These viral elements have lost their ability to replicate and infect during evolution, but are an integral part of our genetic makeup. In fact, humans possess five times more HERVs in non-coding parts than coding genes. So far, strong focus has been devoted to the correlation of HERVs and the onset or progression of diseases. This is why HERV expression has been studied in samples of pathological origin. Although important, these studies do not provide conclusions about whether HERVs are the cause or the consequence of such disease.
Today, new technologies enable scientists to receive a deeper insight into the mechanisms of HERVs and their function. Together with her colleagues, virologist Michelle Vincendeau* has now succeeded for the first time in demonstrating the negative effects of HERV activation on human brain development.
HERV activation impairs brain development
Using CRISPR technology, the researchers activated a specific group of human endogenous retroviruses** in human embryonic stem cells and generated nerve cells (neurons). These viral elements in turn activated specific genes, including classical developmental factors, involved in brain development. As a result, cortical neurons, meaning the nerve cells in our cerebral cortex, lost their function entirely. They developed very differently from healthy neurons in this brain region -- with much a shorter axon (nerve cell extension) that were much less branched. Thus, activation of one specific HERV group impairs cortical neuron development and ultimately brain development.
Clinical relevance
Since neurodegenerative diseases are often associated with the activation of several HERV groups, the negative impact of HERV activation on cortical neuron development is an essential finding. It is already known that environmental factors such as viruses, bacteria, and UV light can activate distinct HERVs, thereby potentially contributing to disease onset. This knowledge, in turn, makes HERVs even more interesting for clinical application. Switching off distinct viral elements could open up a new field of research for the treatment of patients with neurodegenerative diseases. In a next step, the group at Helmholtz Zentrum München will study the impact of HERV deactivation in neurons in the context of disease.
New paths for basic research
In addition, the research findings provide important indications that epigenetic mechanisms keep viral elements under control in healthy brain development. Michelle Vincendeau even suspects a functional role for the controlled HERVs in normal brain development. ""We have carried these elements for about 40 to 70 million years. We assume that their presence is relevant to our natural processes, otherwise we would not have retained them for so long during evolution,"" Vincendeau says. Further basic research in this direction might reveal new functional roles for HERVs.
Notes:
* Michelle Vincendeau leads the research group for Human Endogenous Retroviruses at the Institute of Viorology at Helmholtz Zentrum München. Part of the data from the current study was generated in the context of her previous work at the Memorial Sloan Kettering Cancer Center in New York. For this paper, she also collaborated with researchers at the Technical University of Munich and the University of Saarland.
** HERV-K(HML-2)"	3903
biology	[]	2021-05-21 00:00:00	"The field of mathematical topology is often described in terms of donuts and pretzels.

To most of us, the two differ in the way they taste or in their compatibility with morning coffee. But to a topologist, the only difference between the two is that one has a single hole and the other has three. There's no way to stretch or contort a donut to make it look like a pretzel -- at least not without ripping it or pasting different parts together, both of which are verboten in topology. The different number of holes make two shapes that are fundamentally, inexorably different.

In recent years, researchers have drawn on mathematical topology to help explain a range of phenomena like phase transitions in matter, aspects of Earth's climate and even how zebrafish form their iconic stripes. Now, a Brown University research team is working to use topology in yet another realm: training computers to classify how human cells organize into tissue-like architectures.

In a study published in the May 7 issue of the journal Soft Matter, the researchers demonstrate a machine learning technique that measures the topological traits of cell clusters. They showed that the system can accurately categorize cell clusters and infer the motility and adhesion of the cells that comprise them.

""You can think of this as topology-informed machine learning,"" said Dhananjay Bhaskar, a recent Ph.D. graduate who led the work. ""The hope is that this can help us to avoid some of the pitfalls that affect the accuracy of machine learning algorithms.""

Bhaskar developed the algorithm with Ian Y. Wong, an assistant professor in Brown's School of Engineering, and William Zhang, a Brown undergraduate.

advertisement

There's been a significant amount of work in recent years to use artificial intelligence as a means of analyzing big data with spatial information, such as medical imaging of patient tissues. Progress has been made in training these systems to classify accurately, ""but how they work is opaque and a little finicky,"" Wong said. ""Just like people, sometimes computers hallucinate. You can have a few pixels in the wrong place, and it can confuse the algorithm. So Dhananjay has been thinking about ways we might be able to make those analyses a little more robust.""

In developing this new system, Bhaskar took inspiration from modern art, specifically Pablo Picasso's ""Bull."" The series of 11 lithographs starts with a bull depicted in full detail. Each successive frame strips away a bit of detail, ending in a simple drawing capturing only the animal's fundamental attributes. By employing topology, Bhaskar thought he might be able to do something similar to understand the underlying form of tissue-like architectures.

The way in which cells migrate and interact depends on the physiology of the cells involved. For example, healthy tissues contain higher numbers of stationary epithelial cells. Processes like wound repair or cancer, however, often involve more mobile mesenchymal cells. Differences in physiology between the two cell types cause them to cluster together differently. Epithelial cells tend to aggregate into larger, more closely packed clusters. Mesenchymal cells tend to be more dispersed, with groups of cells branching off in different directions. But when assemblages contain a mix of both kinds of cells, it can be difficult to accurately analyze them.

The new algorithm uses a mathematical framework called persistent homology to examine microscope images of cell assemblages. Specifically, it looks at the topological patterns -- loops or holes -- that the cells form collectively. By looking at which patterns persist across different spatial resolutions, the algorithm determines which patterns are intrinsic to the image.

It starts by looking at the cells in their finest detail, determining which cells seem to be part of topological loops. Then it blurs the detail a bit by drawing a circle around each cell -- effectively making each cell a little larger -- to see which loops persist at that more coarse-grained scale and which get blurred out. The process is repeated until all the topological features eventually disappear. At the end, the algorithm produces a sort of bar code showing which loops persist across spatial scales. Those that are most persistent are stored as a simplified representation of the overall shape.

advertisement

As it turns out, those persistent topological objects can be used to categorize clusters of differing types of cells. After training their algorithm on computer-simulated cells programmed to behave like different types of cells, the team turned it loose on real experimental images of migratory cells. Those cells had been exposed to varying biochemical treatments so that some were more epithelial, some were more mesenchymal, and some were somewhere in between. The study showed that the topological algorithm was able to correctly classify different spatial patterns according to which biochemical treatment the cells had received.

""It was able to pull out all of these experimental treatments just by identifying these persistent topological loops,"" Wong said. ""We were kind of amazed at how well it did.""

The team hopes that one day the algorithm could be used in laboratory experiments to test drugs, helping to determine how different drugs can alter cell migration and adhesion. Eventually, it may also be used on medical images of tumors, potentially helping doctors to determine how malignant those tumors may be.

""We're looking for ways to catch subtleties that might not be apparent to the human eye,"" Wong said. ""We hope that this might be a human interpretable approach that complements existing machine learning approaches.""

The research was supported by National Cancer Institute's Innovative Molecular Analysis Technologies Program (R21CA212932) and the Brown University Data Science Initiative."	https://www.sciencedaily.com/releases/2021/05/210519114803.htm	"The field of mathematical topology is often described in terms of donuts and pretzels.
To most of us, the two differ in the way they taste or in their compatibility with morning coffee. But to a topologist, the only difference between the two is that one has a single hole and the other has three. There's no way to stretch or contort a donut to make it look like a pretzel -- at least not without ripping it or pasting different parts together, both of which are verboten in topology. The different number of holes make two shapes that are fundamentally, inexorably different.
In recent years, researchers have drawn on mathematical topology to help explain a range of phenomena like phase transitions in matter, aspects of Earth's climate and even how zebrafish form their iconic stripes. Now, a Brown University research team is working to use topology in yet another realm: training computers to classify how human cells organize into tissue-like architectures.
In a study published in the May 7 issue of the journal Soft Matter, the researchers demonstrate a machine learning technique that measures the topological traits of cell clusters. They showed that the system can accurately categorize cell clusters and infer the motility and adhesion of the cells that comprise them.
""You can think of this as topology-informed machine learning,"" said Dhananjay Bhaskar, a recent Ph.D. graduate who led the work. ""The hope is that this can help us to avoid some of the pitfalls that affect the accuracy of machine learning algorithms.""
Bhaskar developed the algorithm with Ian Y. Wong, an assistant professor in Brown's School of Engineering, and William Zhang, a Brown undergraduate.
advertisement
There's been a significant amount of work in recent years to use artificial intelligence as a means of analyzing big data with spatial information, such as medical imaging of patient tissues. Progress has been made in training these systems to classify accurately, ""but how they work is opaque and a little finicky,"" Wong said. ""Just like people, sometimes computers hallucinate. You can have a few pixels in the wrong place, and it can confuse the algorithm. So Dhananjay has been thinking about ways we might be able to make those analyses a little more robust.""
In developing this new system, Bhaskar took inspiration from modern art, specifically Pablo Picasso's ""Bull."" The series of 11 lithographs starts with a bull depicted in full detail. Each successive frame strips away a bit of detail, ending in a simple drawing capturing only the animal's fundamental attributes. By employing topology, Bhaskar thought he might be able to do something similar to understand the underlying form of tissue-like architectures.
The way in which cells migrate and interact depends on the physiology of the cells involved. For example, healthy tissues contain higher numbers of stationary epithelial cells. Processes like wound repair or cancer, however, often involve more mobile mesenchymal cells. Differences in physiology between the two cell types cause them to cluster together differently. Epithelial cells tend to aggregate into larger, more closely packed clusters. Mesenchymal cells tend to be more dispersed, with groups of cells branching off in different directions. But when assemblages contain a mix of both kinds of cells, it can be difficult to accurately analyze them.
The new algorithm uses a mathematical framework called persistent homology to examine microscope images of cell assemblages. Specifically, it looks at the topological patterns -- loops or holes -- that the cells form collectively. By looking at which patterns persist across different spatial resolutions, the algorithm determines which patterns are intrinsic to the image.
It starts by looking at the cells in their finest detail, determining which cells seem to be part of topological loops. Then it blurs the detail a bit by drawing a circle around each cell -- effectively making each cell a little larger -- to see which loops persist at that more coarse-grained scale and which get blurred out. The process is repeated until all the topological features eventually disappear. At the end, the algorithm produces a sort of bar code showing which loops persist across spatial scales. Those that are most persistent are stored as a simplified representation of the overall shape.
advertisement
As it turns out, those persistent topological objects can be used to categorize clusters of differing types of cells. After training their algorithm on computer-simulated cells programmed to behave like different types of cells, the team turned it loose on real experimental images of migratory cells. Those cells had been exposed to varying biochemical treatments so that some were more epithelial, some were more mesenchymal, and some were somewhere in between. The study showed that the topological algorithm was able to correctly classify different spatial patterns according to which biochemical treatment the cells had received.
""It was able to pull out all of these experimental treatments just by identifying these persistent topological loops,"" Wong said. ""We were kind of amazed at how well it did.""
The team hopes that one day the algorithm could be used in laboratory experiments to test drugs, helping to determine how different drugs can alter cell migration and adhesion. Eventually, it may also be used on medical images of tumors, potentially helping doctors to determine how malignant those tumors may be.
""We're looking for ways to catch subtleties that might not be apparent to the human eye,"" Wong said. ""We hope that this might be a human interpretable approach that complements existing machine learning approaches.""
The research was supported by National Cancer Institute's Innovative Molecular Analysis Technologies Program (R21CA212932) and the Brown University Data Science Initiative."	5885
chemistry	['Shi En Kim', 'Jake Buehler', 'Maria Temming', 'Charles Q. Choi', 'Tina Hesman Saey', 'Laura Sanders', 'Susan Milius', 'Carolyn Gramling', 'Jonathan Lambert', 'Emily Conover']	2021-05-03 13:27:17-04:00	"Chances are, most — if not all — of the produce in your kitchen is threatened by fungal diseases. The threat looms large for food staples of the world such as rice, wheat, potatoes and maize (SN: 9/22/05). Pathogenic fungi are also coming for our coffee, sugarcane, bananas and other economically important crops. Annually, fungal diseases destroy a third of all harvests and pose a dire threat to global food security.

To stop the spread of fungal diseases, farmers fumigate the soil with toxic chemicals that lay waste to the land, sparing not even the beneficial microbes teeming in the earth. Or they ply plants with fungicides. But fungicide use is effective only in the short run — until the pathogenic fungi evolve resistance against these synthetic chemicals.

Now, a new idea is taking root: Help plants stand their ground by giving them the tools to fight their own battles. A team led by Jason White, an environmental toxicologist at the Connecticut Agricultural Experiment Station in New Haven, is fortifying crops with nutrients fashioned into nanosized packages, which boost plants’ innate immunity against pathogenic fungi more efficiently than traditional plant feeding. Over the past few years, the researchers have devised various nanonutrient concoctions that boost the fungal resistance of soybeans, tomatoes, watermelons and, recently, eggplants, as reported in the April Plant Disease.

The concept “tackles the challenge at the origin rather than trying to put a Band-Aid on the [problem],” says Leanne Gilbertson, an environmental engineer at the University of Pittsburgh who was not involved in the research. White’s strategy provides plants with the nutrients they need to trigger enzyme production to guard against pathogenic attack. Without any synthetic chemicals introduced, the strategy sidesteps any opportunity for malignant fungi to develop resistances, she says.

Sign Up For the Latest from Science News Headlines and summaries of the latest Science News articles, delivered to your inbox Client key* E-mail* Go

The researchers’ nanomaterials approach is inspired by their earlier discovery that nanoparticles transported up from the roots of maize can loop back down from the leaves. The researchers dipped half of the root fibers of a single maize plant in a copper nanoparticle formulation and the other half in pure water. The copper showed up in the water-dipped roots, pointing to a roots-to-shoot-to-roots roundtrip, White and his colleagues reported in 2012 in Environmental Science & Technology. That finding suggested that nanoparticles can be applied directly to the leaves in the first place, even when the target destination was the roots.

Using the leaves as an entrance point gets around a perennial problem: Delivering dissolved nutrients through the soil is hardly efficient. Chemicals may break down in the soil, vaporize into the atmosphere or leach away. Only about 20 percent of watered nutrients eventually reach the target areas in a plant. “By using the nanoscale form, we can actually more effectively deliver [nutrients] where we want it and where the plant needs it,” White says.

To see if this approach could deliver nutrients specifically needed in defense against hostile fungi, White and colleagues carried out tests in eggplants and tomatoes. The team sprayed metallic nanoparticles onto the leaves and shoots of young plants, then infected the plants with pathogenic fungi. The nanoparticle-treated plants had elevated levels of nutritional metals in the roots and higher produce yields compared with the plants fed readily dissolved nutrients, the team reported in 2016 in Environmental Science: Nano.

The nanoparticles weren’t harming the fungi, the researchers found: The fungi still thrived amidst nanoparticles in the environment without the host plant present. Instead, the nanoparticles’ antifungal properties stem from providing plant nourishment — equivalent to humans taking nutritional supplements — that allows plants to mount an appropriate defense on demand.

What makes nanonutrients more potent than common fertilizers is the sweet spot in their sizes, which control how fast they dissolve, says Fabienne Schwab, an environmental chemist not involved in the research. Nanonutrients are thousands of times smaller than the diameter of human hair and thousands of times larger than readily dissolved nutrient salts. They have a large, exposed surface, so they dissolve more quickly than a heftier chunk of the same nutrient. Yet nanonutrients are big enough that that they don’t dissolve all at once: They can gradually release the nutrients over weeks. In contrast, readily dissolved nutrients give plants a temporary nutrient spike, akin to a sugar rush.

“When you use [nutrients] at the nanoscale, you can tune the solubility pretty much the way you like,” says Schwab, of the Adolphe Merkle Institute in Fribourg, Switzerland.

It’s not just the size that can be tuned — the shape, composition and surface chemistries can be modified to stimulate different levels of a plant’s responses. For instance, White and his collaborators found that nanometer-thin copper oxide sheets were better than spherical copper nanoparticles at preventing Fusarium virguliforme infection in soybeans. The key to their effectiveness lay in the nanosheets’ quicker release of charged copper atoms and stronger adhesion to leaf surfaces. The copper nanomaterials restored the soybean’s masses and photosynthesis rates to the levels of disease-free plants, the team reported in Nature Nanotechnology in 2020.

“It’s a very promising technology,” says Schwab, but she adds that there are other aspects to consider before its implementation. If agricultural nanotechnology is to achieve widespread use, it needs to observe environmental and safety regulations, as well as — perhaps even more challengingly — overcome consumer wariness. So far, White and his collaborators found no residual nanonutrients in their produce that would end up on the dining table of consumers. But other implications, such as the nanomaterials’ persistence in the environment and hazards posed to human handlers, have yet to be fully understood.

“People in general get nervous when you talk about nanotechnology and food,” says White. But he says his group isn’t using any exotic materials, whose health impacts remain complete enigmas. Instead “we’re using nutrients the plants need [that] they just can’t get enough of.”

White says he has eaten the eggplants, tomatoes and watermelons he’s grown for his research. And perhaps that’s the best reassurance consumers can get: a toxicologist trying the literal fruit of his labor."	https://www.sciencenews.org/article/nano-tech-nutrients-protect-plants-fungus-disease-food-agriculture	"Chances are, most — if not all — of the produce in your kitchen is threatened by fungal diseases. The threat looms large for food staples of the world such as rice, wheat, potatoes and maize (SN: 9/22/05). Pathogenic fungi are also coming for our coffee, sugarcane, bananas and other economically important crops. Annually, fungal diseases destroy a third of all harvests and pose a dire threat to global food security.
To stop the spread of fungal diseases, farmers fumigate the soil with toxic chemicals that lay waste to the land, sparing not even the beneficial microbes teeming in the earth. Or they ply plants with fungicides. But fungicide use is effective only in the short run — until the pathogenic fungi evolve resistance against these synthetic chemicals.
Now, a new idea is taking root: Help plants stand their ground by giving them the tools to fight their own battles. A team led by Jason White, an environmental toxicologist at the Connecticut Agricultural Experiment Station in New Haven, is fortifying crops with nutrients fashioned into nanosized packages, which boost plants’ innate immunity against pathogenic fungi more efficiently than traditional plant feeding. Over the past few years, the researchers have devised various nanonutrient concoctions that boost the fungal resistance of soybeans, tomatoes, watermelons and, recently, eggplants, as reported in the April Plant Disease.
The concept “tackles the challenge at the origin rather than trying to put a Band-Aid on the [problem],” says Leanne Gilbertson, an environmental engineer at the University of Pittsburgh who was not involved in the research. White’s strategy provides plants with the nutrients they need to trigger enzyme production to guard against pathogenic attack. Without any synthetic chemicals introduced, the strategy sidesteps any opportunity for malignant fungi to develop resistances, she says.
Sign Up For the Latest from Science News Headlines and summaries of the latest Science News articles, delivered to your inbox Client key* E-mail* Go
The researchers’ nanomaterials approach is inspired by their earlier discovery that nanoparticles transported up from the roots of maize can loop back down from the leaves. The researchers dipped half of the root fibers of a single maize plant in a copper nanoparticle formulation and the other half in pure water. The copper showed up in the water-dipped roots, pointing to a roots-to-shoot-to-roots roundtrip, White and his colleagues reported in 2012 in Environmental Science & Technology. That finding suggested that nanoparticles can be applied directly to the leaves in the first place, even when the target destination was the roots.
Using the leaves as an entrance point gets around a perennial problem: Delivering dissolved nutrients through the soil is hardly efficient. Chemicals may break down in the soil, vaporize into the atmosphere or leach away. Only about 20 percent of watered nutrients eventually reach the target areas in a plant. “By using the nanoscale form, we can actually more effectively deliver [nutrients] where we want it and where the plant needs it,” White says.
To see if this approach could deliver nutrients specifically needed in defense against hostile fungi, White and colleagues carried out tests in eggplants and tomatoes. The team sprayed metallic nanoparticles onto the leaves and shoots of young plants, then infected the plants with pathogenic fungi. The nanoparticle-treated plants had elevated levels of nutritional metals in the roots and higher produce yields compared with the plants fed readily dissolved nutrients, the team reported in 2016 in Environmental Science: Nano.
The nanoparticles weren’t harming the fungi, the researchers found: The fungi still thrived amidst nanoparticles in the environment without the host plant present. Instead, the nanoparticles’ antifungal properties stem from providing plant nourishment — equivalent to humans taking nutritional supplements — that allows plants to mount an appropriate defense on demand.
What makes nanonutrients more potent than common fertilizers is the sweet spot in their sizes, which control how fast they dissolve, says Fabienne Schwab, an environmental chemist not involved in the research. Nanonutrients are thousands of times smaller than the diameter of human hair and thousands of times larger than readily dissolved nutrient salts. They have a large, exposed surface, so they dissolve more quickly than a heftier chunk of the same nutrient. Yet nanonutrients are big enough that that they don’t dissolve all at once: They can gradually release the nutrients over weeks. In contrast, readily dissolved nutrients give plants a temporary nutrient spike, akin to a sugar rush.
“When you use [nutrients] at the nanoscale, you can tune the solubility pretty much the way you like,” says Schwab, of the Adolphe Merkle Institute in Fribourg, Switzerland.
It’s not just the size that can be tuned — the shape, composition and surface chemistries can be modified to stimulate different levels of a plant’s responses. For instance, White and his collaborators found that nanometer-thin copper oxide sheets were better than spherical copper nanoparticles at preventing Fusarium virguliforme infection in soybeans. The key to their effectiveness lay in the nanosheets’ quicker release of charged copper atoms and stronger adhesion to leaf surfaces. The copper nanomaterials restored the soybean’s masses and photosynthesis rates to the levels of disease-free plants, the team reported in Nature Nanotechnology in 2020.
“It’s a very promising technology,” says Schwab, but she adds that there are other aspects to consider before its implementation. If agricultural nanotechnology is to achieve widespread use, it needs to observe environmental and safety regulations, as well as — perhaps even more challengingly — overcome consumer wariness. So far, White and his collaborators found no residual nanonutrients in their produce that would end up on the dining table of consumers. But other implications, such as the nanomaterials’ persistence in the environment and hazards posed to human handlers, have yet to be fully understood.
“People in general get nervous when you talk about nanotechnology and food,” says White. But he says his group isn’t using any exotic materials, whose health impacts remain complete enigmas. Instead “we’re using nutrients the plants need [that] they just can’t get enough of.”
White says he has eaten the eggplants, tomatoes and watermelons he’s grown for his research. And perhaps that’s the best reassurance consumers can get: a toxicologist trying the literal fruit of his labor."	6664
chemistry	['Maria Temming', 'Jake Buehler', 'Charles Q. Choi', 'Tina Hesman Saey', 'Laura Sanders', 'Susan Milius', 'Carolyn Gramling', 'Jonathan Lambert', 'Emily Conover', 'Lisa Grossman']	2021-01-27 15:19:35-05:00	"It feels good to recycle. There’s a certain sense of accomplishment that comes from dutifully sorting soda bottles, plastic bags and yogurt cups from the rest of the garbage. The more plastic you put in that blue bin, the more you’re keeping out of landfills and the oceans, right?

Wrong. No matter how meticulous you are in cleaning and separating your plastics, most end up in the trash heap anyway.

Take flexible food packages. Those films contain several layers of different plastics. Because each plastic has to be recycled separately, those films are not recyclable. Grocery bags and shrink wrap are too flimsy, prone to getting tangled up with other materials on a conveyor belt. The polypropylene in yogurt cups and other items doesn’t usually get recycled either; recycling a hodgepodge of polypropylene produces a dark, smelly plastic that few manufacturers will use.

Only two kinds of plastic are commonly recycled in the United States: the kind in plastic soda bottles, polyethylene terephthalate, or PET; and the plastic found in milk jugs and detergent containers — high-density polyethylene, or HDPE. Together, those plastics make up only about a quarter of the world’s plastic trash, researchers reported in 2017 in Science Advances. And when those plastics are recycled, they aren’t good for much. Melting plastic down to recycle changes its consistency, so PET from bottles has to be mixed with brand-new plastic to make a sturdy final product. Recycling a mix of multicolored HDPE pieces creates a dark plastic good only for making products like park benches and waste bins, in which properties like color don’t matter much.

The difficulties of recycling plastic into anything manufacturers want to use is a big reason why the world is littered with so much plastic waste, says Eric Beckman, a chemical engineer at the University of Pittsburgh. In 2018 alone, the United States landfilled 27 million tons of plastic and recycled a mere 3 million, according to the U.S. Environmental Protection Agency. Low recycling rates aren’t just a problem in the United States. Of the 6.3 billion tons of plastic that have been discarded around the world, only about 9 percent has gotten recycled. Another 12 percent has been burned, and almost 80 percent has piled up on land or in waterways.

Good news/bad news The amount of plastic recycled in the United States has increased over the last few decades — but those levels still pale in comparison with the amount of plastic that goes into landfills. Plastic waste management, 1960–2018 E. Otwell E. Otwell Source: EPA

With plastic collecting everywhere from the top of Mount Everest to the bottom of the Mariana Trench, there’s an urgent need to reduce the amount of plastic that gets thrown away (SN: 1/16/21, p. 5). Some people propose replacing plastics with biodegradable materials, but those replacements are generally not as strong or cheap to make as plastics (SN: 6/22/19, p. 18). Since, realistically, plastic is not going away any time soon, chemists who understand the ins and outs of all this pesky plastic are working to make it easier to recycle and turn into higher-quality material that’s useful for more things.

“There’s not going to be a single technology that’s going to be the answer,” says Ed Daniels, senior project manager at the REMADE Institute in West Henrietta, N.Y., which funds research into new recycling techniques. Some projects are on the brink of breaking into industry; others are still just promising lab experiments. But all are focused on designing a future where any plastic that ends up in the recycling bin can have a second and third life in a new product.

Sign Up For the Latest from Science News Headlines and summaries of the latest Science News articles, delivered to your inbox Client key* E-mail* Go

Picking plastics apart

One of the biggest bottlenecks in plastic recycling is that every material has to get processed separately. “Most plastics are like oil and water,” says chemist Geoffrey Coates of Cornell University. They just don’t mix. Take, for example, a polyethylene detergent jug and its polypropylene cap. “If you melt those down, and I make a bottle out of that, and I squeeze it, it would basically crack down the side,” Coates says. “It’s crazy brittle. Totally worthless.”

That’s why the first destination for plastic recyclables is a material recovery facility, where people and machines do the sorting. Separated plastics can then be washed, shredded, melted and remolded. The system works well for simple items like soda bottles and milk jugs. But not for items like deodorant containers — where the bottle, crank and cap could all be made of different kinds of plastic. Food packaging films that contain several layers of different plastic are particularly tricky to take apart. Every year, 100 million tons of these multilayer films are produced worldwide. When thrown away, those plastics go to landfills, says chemical engineer George Huber of the University of Wisconsin–Madison.

At the Waste Management Material Recovery Facility in Elkridge, Md., workers sort trash moving past them on conveyor belts. Saul Loeb/AFP via Getty Images

To tackle that problem, Huber and colleagues devised a strategy for dealing with complex mixtures of plastics. The process uses a series of liquid solvents to dissolve individual plastic components off a product. The trick is choosing the right solvents to dissolve only one kind of plastic at a time, Huber says.

The team tested the technique on a packaging film that contained polyethylene and PET, as well as a plastic oxygen barrier made of ethylene vinyl alcohol, or EVOH, that keeps food fresh.

Stirring the film into a toluene solvent first dissolved the polyethylene layer. Dunking the remaining EVOH-PET film in a solvent called DMSO stripped off the EVOH. The researchers then plucked out the remaining PET film and recovered the other two plastics from their separate solvents by mixing in “antisolvent” chemicals. Those chemicals caused the plastic molecules that were dispersed in the liquids to bunch together into solid clumps that could be fished out.

This process recovered practically all of the plastic from the original film, the researchers reported last November in Science Advances. When tested on a jumble of polyethylene, PET and EVOH beads, the solvent washes recovered more than 95 percent of each material — hinting that these solvents could be used to strip plastic components off bulkier items than packaging films. So in theory, recovery facilities could use this technique to disassemble multiplastic deodorant containers and other products of various shapes and sizes.

Huber and colleagues next plan to look for solvents to dissolve more kinds of plastic, such as the polystyrene in Styrofoam. But it will take a lot more work to make this strategy efficient at sorting all the intricate plastic combinations in real-world recyclables.

False advertising Many plastic products are labeled with a number inside a triangle that symbolizes recycling. Yet, only plastics with 1 (polyethylene terephthalate) or 2 (high-density polyethylene) are widely recycled in the United States. The rest typically go to the landfill. PET

Water and soft drink bottles, salad domes, cookie trays, salad dressing and peanut butter containers HDPE

Milk and juice bottles, freezer bags, shampoo and detergent bottles PVC

Cosmetic containers, commercial cling wrap LDPE

Squeeze bottles, cling wrap, trash bags PP

Microwave dishes, ice cream tubs, yogurt containers, detergent bottle caps PS

CD cases, plastic disposable cups, plastic cutlery, video cases EPS

Foam polystyrene hot drink cups, food takeaway trays, protective packaging for fragile items Other

Water cooler bottles, flexible films, multimaterial packaging Source: Ellen MacArthur Foundation 2017

Making plastics mix

There may also be chemical shortcuts that allow multilayer films and other mixtures of plastics to be recycled as they are. Additives called compatibilizers help different melted-down plastics blend, so that unsorted materials can be treated as one. But there is no universal compatibilizer that allows every kind of plastic to be mixed together. And existing compatibilizers are not widely used because they are not very potent — and adding a lot of compatibilizer to a plastic blend gets expensive.

To boost viability, Coates and colleagues created a highly potent compatibilizer for polyethylene and polypropylene. Together, those two plastics make up more than half of the world’s plastic. The new compatibilizer molecule contains two segments of polyethylene, interspersed with two segments of polypropylene. Those alternating segments latch onto plastic molecules of the same kind in a mixture, bringing polyethylene and polypropylene together. It’s as if polyethylene were made of Legos, and polypropylene were made of Duplos, and the researchers made a special building block with connectors that fit both types of blocks.

Having two polyethylene and two polypropylene connectors for each compatibilizer molecule, rather than one, made this compatibilizer stronger than previous versions, Coates and colleagues reported in 2017 in Science. The first test of the new compatibilizer involved welding together strips of polyethylene and polypropylene. Ordinarily, the two materials easily peel apart. But with a layer of compatibilizer between them, the plastic strips broke, rather than the compatibilizer seal, when pulled apart.

In a second test, the researchers mixed the compatibilizer into a melted blend of polyethylene and polypropylene. It took only 1 percent compatibilizer to create a tough new plastic.

“These are crazy potent additives,” Coates says. Other compatibilizers had to be added at concentrations up to 10 percent to hold these two plastics together. The new compatibilizer is now the basis for Coates’ start-up, Intermix Performance Materials, based in Ithaca, N.Y.

Good as new

Even if every piece of plastic trash could easily be recycled, that still wouldn’t solve the world’s plastic problem. There are a couple major issues with how recycling currently works that severely limit the usability of recycled materials.

For one thing, recycled plastics inherit all the dyes, flame retardants and other additives that gave each original plastic piece its distinctive look and feel. “The plastic that you actually recover at the end of all this is really a very complex mixture,” says chemist Susannah Scott of the University of California, Santa Barbara. Few manufacturers can use plastic with a random mishmash of properties to make something new.

Plus, recycling breaks some of the chemical bonds in plastic molecules, affecting the strength and consistency of the material. Melting down and remolding plastic is sort of like reheating pizza in the microwave — you get out basically what you put in, just not as good. That limits the number of times plastic can be recycled before it has to be landfilled.

The solution to both problems could lie in a new kind of recycling process, called chemical recycling, which promises to make pure new plastic an infinite number of times. Chemical recycling involves taking plastics apart on the molecular level.

The molecules that make up plastics are called polymers, which are made of smaller monomers. Using heat and chemicals, it is possible to disassemble polymers into monomers, separate those building blocks from dyes and other contaminants, and piece the monomers back together into good-as-new plastic.

“Chemical recycling has really started to emerge as a force, I would say, within the last three or four years,” says University of Pittsburgh’s Beckman. But most chemical recycling techniques are too expensive or energy intensive for commercial use. “It’s not ready for prime time,” he says.

Different plastics require different chemical recycling processes, and some break down more easily than others. “The one that’s farthest along is PET,” Beckman says. “That polymer happens to be easy to take apart.” Several companies are developing methods to chemically recycle PET, including the French company Carbios.

Carbios is testing enzymes produced by microorganisms to break down PET. Researchers at the company described their work on one such enzyme last April in Nature. Microbes normally use the enzyme, called leaf-branch compost cutinase, to decompose the waxy coating on plant leaves. But the cutinase is also good at breaking PET down into its monomers: ethylene glycol and terephthalic acid.

Microbial help An enzyme naturally produced by microbes broke down about 50 percent of polyethylene terephthalate, or PET (blue line). A tweaked version of the enzyme broke down more than 80 percent of the plastic (black dotted line). Increasing the amount of the enzyme from 1 milligram per gram of PET to 3 milligrams made it even more efficient — breaking down about 90 percent of PET. PET breakdown by an enzyme E. Otwell E. Otwell Source: V. Tournier et al/Nature 2020

“The enzyme is like a molecular scissor,” says Alain Marty, chief scientific officer at Carbios. But because it evolved to decompose plant matter, not plastic, it’s not perfect. To make the enzyme better at snipping apart PET, “we redesigned what we call the active site of the enzyme,” Marty says. This involved swapping out some of the amino acids along that PET docking site for others.

When the researchers tested their mutant enzyme on colored plastic flakes from PET bottles, applying 3 milligrams of the enzyme per gram of PET, about 90 percent of the plastic broke down in about 10 hours. The original enzyme had maxed out at about 50 percent. Using the terephthalic acid monomers produced in that process, the researchers made new plastic bottles that were just as strong as the originals.

Carbios is now building a plant near Lyon, France, to start chemically recycling PET later this year.

Milder conditions

But other plastics, like polyethylene and polypropylene, are much harder to break down via chemical recycling. Taking apart polyethylene molecules, for instance, requires temperatures over 400° Celsius. At such high heat, the chemistry is chaotic. Plastic molecules break down randomly, generating a complex mixture of compounds that can be burned as fuel but not used to make new materials.

Scott, the UC Santa Barbara chemist, proposes partially breaking down these sturdy plastics in a more controlled way, under milder conditions, to make other kinds of useful molecules. She and colleagues recently came up with a way to transform polyethylene into alkylaromatic compounds, which can be used as biodegradable ingredients in shampoos, detergents and other products. The process involves placing polyethylene inside a reaction chamber set to 280° C, with a catalyst powder containing platinum nanoparticles.

Polyethylene is a long molecule, in which hydrogen atoms are connected to a carbon backbone that can be thousands of carbon atoms long. The platinum is good at breaking carbon-hydrogen bonds, Scott says. “When you do that, you generate hydrogen in the reactor, and the platinum catalyst can use the hydrogen to break the carbon-carbon bonds [in the molecule backbone]. So it actually chops the chain into smaller pieces.”

Since this reaction takes place at a relatively mild 280° C, it happens in an orderly fashion, snapping long polyethylene molecules into shorter chains that are each about 30 carbons long. Those fragments then arrange themselves into the six-sided ring structures characteristic of alkylaromatic compounds.

After 24 hours in the reaction chamber, “most of the products are liquids, and most of the liquids are alkylaromatics,” Scott says. In experiments, about 69 percent of the plastic in a low-density polyethylene bag was converted into liquid. About 55 percent of a high-density polyethylene bottle cap was transformed. The process produces hydrocarbon gases too, which could be used to generate heat to run the reaction at a recycling plant, Scott says.

For now, this is just a lab demo, and like many new recycling strategies, it’s still a long way off from commercialization. And no single upgrade to the recycling pipeline will rid the world of its growing mountains of plastic trash. “We’re going to need a suite of technologies to meet this challenge,” says Daniels, of the REMADE Institute. But each new technology — whether it’s focused on making plastics easier to recycle, or transforming them into more useful materials — could help."	https://www.sciencenews.org/article/chemistry-recycling-plastic-landfills-trash-materials	"It feels good to recycle. There’s a certain sense of accomplishment that comes from dutifully sorting soda bottles, plastic bags and yogurt cups from the rest of the garbage. The more plastic you put in that blue bin, the more you’re keeping out of landfills and the oceans, right?
Wrong. No matter how meticulous you are in cleaning and separating your plastics, most end up in the trash heap anyway.
Take flexible food packages. Those films contain several layers of different plastics. Because each plastic has to be recycled separately, those films are not recyclable. Grocery bags and shrink wrap are too flimsy, prone to getting tangled up with other materials on a conveyor belt. The polypropylene in yogurt cups and other items doesn’t usually get recycled either; recycling a hodgepodge of polypropylene produces a dark, smelly plastic that few manufacturers will use.
Only two kinds of plastic are commonly recycled in the United States: the kind in plastic soda bottles, polyethylene terephthalate, or PET; and the plastic found in milk jugs and detergent containers — high-density polyethylene, or HDPE. Together, those plastics make up only about a quarter of the world’s plastic trash, researchers reported in 2017 in Science Advances. And when those plastics are recycled, they aren’t good for much. Melting plastic down to recycle changes its consistency, so PET from bottles has to be mixed with brand-new plastic to make a sturdy final product. Recycling a mix of multicolored HDPE pieces creates a dark plastic good only for making products like park benches and waste bins, in which properties like color don’t matter much.
The difficulties of recycling plastic into anything manufacturers want to use is a big reason why the world is littered with so much plastic waste, says Eric Beckman, a chemical engineer at the University of Pittsburgh. In 2018 alone, the United States landfilled 27 million tons of plastic and recycled a mere 3 million, according to the U.S. Environmental Protection Agency. Low recycling rates aren’t just a problem in the United States. Of the 6.3 billion tons of plastic that have been discarded around the world, only about 9 percent has gotten recycled. Another 12 percent has been burned, and almost 80 percent has piled up on land or in waterways.
Good news/bad news The amount of plastic recycled in the United States has increased over the last few decades — but those levels still pale in comparison with the amount of plastic that goes into landfills. Plastic waste management, 1960–2018 E. Otwell E. Otwell Source: EPA
With plastic collecting everywhere from the top of Mount Everest to the bottom of the Mariana Trench, there’s an urgent need to reduce the amount of plastic that gets thrown away (SN: 1/16/21, p. 5). Some people propose replacing plastics with biodegradable materials, but those replacements are generally not as strong or cheap to make as plastics (SN: 6/22/19, p. 18). Since, realistically, plastic is not going away any time soon, chemists who understand the ins and outs of all this pesky plastic are working to make it easier to recycle and turn into higher-quality material that’s useful for more things.
“There’s not going to be a single technology that’s going to be the answer,” says Ed Daniels, senior project manager at the REMADE Institute in West Henrietta, N.Y., which funds research into new recycling techniques. Some projects are on the brink of breaking into industry; others are still just promising lab experiments. But all are focused on designing a future where any plastic that ends up in the recycling bin can have a second and third life in a new product.
Sign Up For the Latest from Science News Headlines and summaries of the latest Science News articles, delivered to your inbox Client key* E-mail* Go
Picking plastics apart
One of the biggest bottlenecks in plastic recycling is that every material has to get processed separately. “Most plastics are like oil and water,” says chemist Geoffrey Coates of Cornell University. They just don’t mix. Take, for example, a polyethylene detergent jug and its polypropylene cap. “If you melt those down, and I make a bottle out of that, and I squeeze it, it would basically crack down the side,” Coates says. “It’s crazy brittle. Totally worthless.”
That’s why the first destination for plastic recyclables is a material recovery facility, where people and machines do the sorting. Separated plastics can then be washed, shredded, melted and remolded. The system works well for simple items like soda bottles and milk jugs. But not for items like deodorant containers — where the bottle, crank and cap could all be made of different kinds of plastic. Food packaging films that contain several layers of different plastic are particularly tricky to take apart. Every year, 100 million tons of these multilayer films are produced worldwide. When thrown away, those plastics go to landfills, says chemical engineer George Huber of the University of Wisconsin–Madison.
At the Waste Management Material Recovery Facility in Elkridge, Md., workers sort trash moving past them on conveyor belts. Saul Loeb/AFP via Getty Images
To tackle that problem, Huber and colleagues devised a strategy for dealing with complex mixtures of plastics. The process uses a series of liquid solvents to dissolve individual plastic components off a product. The trick is choosing the right solvents to dissolve only one kind of plastic at a time, Huber says.
The team tested the technique on a packaging film that contained polyethylene and PET, as well as a plastic oxygen barrier made of ethylene vinyl alcohol, or EVOH, that keeps food fresh.
Stirring the film into a toluene solvent first dissolved the polyethylene layer. Dunking the remaining EVOH-PET film in a solvent called DMSO stripped off the EVOH. The researchers then plucked out the remaining PET film and recovered the other two plastics from their separate solvents by mixing in “antisolvent” chemicals. Those chemicals caused the plastic molecules that were dispersed in the liquids to bunch together into solid clumps that could be fished out.
This process recovered practically all of the plastic from the original film, the researchers reported last November in Science Advances. When tested on a jumble of polyethylene, PET and EVOH beads, the solvent washes recovered more than 95 percent of each material — hinting that these solvents could be used to strip plastic components off bulkier items than packaging films. So in theory, recovery facilities could use this technique to disassemble multiplastic deodorant containers and other products of various shapes and sizes.
Huber and colleagues next plan to look for solvents to dissolve more kinds of plastic, such as the polystyrene in Styrofoam. But it will take a lot more work to make this strategy efficient at sorting all the intricate plastic combinations in real-world recyclables.
False advertising Many plastic products are labeled with a number inside a triangle that symbolizes recycling. Yet, only plastics with 1 (polyethylene terephthalate) or 2 (high-density polyethylene) are widely recycled in the United States. The rest typically go to the landfill. PET
Water and soft drink bottles, salad domes, cookie trays, salad dressing and peanut butter containers HDPE
Milk and juice bottles, freezer bags, shampoo and detergent bottles PVC
Cosmetic containers, commercial cling wrap LDPE
Squeeze bottles, cling wrap, trash bags PP
Microwave dishes, ice cream tubs, yogurt containers, detergent bottle caps PS
CD cases, plastic disposable cups, plastic cutlery, video cases EPS
Foam polystyrene hot drink cups, food takeaway trays, protective packaging for fragile items Other
Water cooler bottles, flexible films, multimaterial packaging Source: Ellen MacArthur Foundation 2017
Making plastics mix
There may also be chemical shortcuts that allow multilayer films and other mixtures of plastics to be recycled as they are. Additives called compatibilizers help different melted-down plastics blend, so that unsorted materials can be treated as one. But there is no universal compatibilizer that allows every kind of plastic to be mixed together. And existing compatibilizers are not widely used because they are not very potent — and adding a lot of compatibilizer to a plastic blend gets expensive.
To boost viability, Coates and colleagues created a highly potent compatibilizer for polyethylene and polypropylene. Together, those two plastics make up more than half of the world’s plastic. The new compatibilizer molecule contains two segments of polyethylene, interspersed with two segments of polypropylene. Those alternating segments latch onto plastic molecules of the same kind in a mixture, bringing polyethylene and polypropylene together. It’s as if polyethylene were made of Legos, and polypropylene were made of Duplos, and the researchers made a special building block with connectors that fit both types of blocks.
Having two polyethylene and two polypropylene connectors for each compatibilizer molecule, rather than one, made this compatibilizer stronger than previous versions, Coates and colleagues reported in 2017 in Science. The first test of the new compatibilizer involved welding together strips of polyethylene and polypropylene. Ordinarily, the two materials easily peel apart. But with a layer of compatibilizer between them, the plastic strips broke, rather than the compatibilizer seal, when pulled apart.
In a second test, the researchers mixed the compatibilizer into a melted blend of polyethylene and polypropylene. It took only 1 percent compatibilizer to create a tough new plastic.
“These are crazy potent additives,” Coates says. Other compatibilizers had to be added at concentrations up to 10 percent to hold these two plastics together. The new compatibilizer is now the basis for Coates’ start-up, Intermix Performance Materials, based in Ithaca, N.Y.
Good as new
Even if every piece of plastic trash could easily be recycled, that still wouldn’t solve the world’s plastic problem. There are a couple major issues with how recycling currently works that severely limit the usability of recycled materials.
For one thing, recycled plastics inherit all the dyes, flame retardants and other additives that gave each original plastic piece its distinctive look and feel. “The plastic that you actually recover at the end of all this is really a very complex mixture,” says chemist Susannah Scott of the University of California, Santa Barbara. Few manufacturers can use plastic with a random mishmash of properties to make something new.
Plus, recycling breaks some of the chemical bonds in plastic molecules, affecting the strength and consistency of the material. Melting down and remolding plastic is sort of like reheating pizza in the microwave — you get out basically what you put in, just not as good. That limits the number of times plastic can be recycled before it has to be landfilled.
The solution to both problems could lie in a new kind of recycling process, called chemical recycling, which promises to make pure new plastic an infinite number of times. Chemical recycling involves taking plastics apart on the molecular level.
The molecules that make up plastics are called polymers, which are made of smaller monomers. Using heat and chemicals, it is possible to disassemble polymers into monomers, separate those building blocks from dyes and other contaminants, and piece the monomers back together into good-as-new plastic.
“Chemical recycling has really started to emerge as a force, I would say, within the last three or four years,” says University of Pittsburgh’s Beckman. But most chemical recycling techniques are too expensive or energy intensive for commercial use. “It’s not ready for prime time,” he says.
Different plastics require different chemical recycling processes, and some break down more easily than others. “The one that’s farthest along is PET,” Beckman says. “That polymer happens to be easy to take apart.” Several companies are developing methods to chemically recycle PET, including the French company Carbios.
Carbios is testing enzymes produced by microorganisms to break down PET. Researchers at the company described their work on one such enzyme last April in Nature. Microbes normally use the enzyme, called leaf-branch compost cutinase, to decompose the waxy coating on plant leaves. But the cutinase is also good at breaking PET down into its monomers: ethylene glycol and terephthalic acid.
Microbial help An enzyme naturally produced by microbes broke down about 50 percent of polyethylene terephthalate, or PET (blue line). A tweaked version of the enzyme broke down more than 80 percent of the plastic (black dotted line). Increasing the amount of the enzyme from 1 milligram per gram of PET to 3 milligrams made it even more efficient — breaking down about 90 percent of PET. PET breakdown by an enzyme E. Otwell E. Otwell Source: V. Tournier et al/Nature 2020
“The enzyme is like a molecular scissor,” says Alain Marty, chief scientific officer at Carbios. But because it evolved to decompose plant matter, not plastic, it’s not perfect. To make the enzyme better at snipping apart PET, “we redesigned what we call the active site of the enzyme,” Marty says. This involved swapping out some of the amino acids along that PET docking site for others.
When the researchers tested their mutant enzyme on colored plastic flakes from PET bottles, applying 3 milligrams of the enzyme per gram of PET, about 90 percent of the plastic broke down in about 10 hours. The original enzyme had maxed out at about 50 percent. Using the terephthalic acid monomers produced in that process, the researchers made new plastic bottles that were just as strong as the originals.
Carbios is now building a plant near Lyon, France, to start chemically recycling PET later this year.
Milder conditions
But other plastics, like polyethylene and polypropylene, are much harder to break down via chemical recycling. Taking apart polyethylene molecules, for instance, requires temperatures over 400° Celsius. At such high heat, the chemistry is chaotic. Plastic molecules break down randomly, generating a complex mixture of compounds that can be burned as fuel but not used to make new materials.
Scott, the UC Santa Barbara chemist, proposes partially breaking down these sturdy plastics in a more controlled way, under milder conditions, to make other kinds of useful molecules. She and colleagues recently came up with a way to transform polyethylene into alkylaromatic compounds, which can be used as biodegradable ingredients in shampoos, detergents and other products. The process involves placing polyethylene inside a reaction chamber set to 280° C, with a catalyst powder containing platinum nanoparticles.
Polyethylene is a long molecule, in which hydrogen atoms are connected to a carbon backbone that can be thousands of carbon atoms long. The platinum is good at breaking carbon-hydrogen bonds, Scott says. “When you do that, you generate hydrogen in the reactor, and the platinum catalyst can use the hydrogen to break the carbon-carbon bonds [in the molecule backbone]. So it actually chops the chain into smaller pieces.”
Since this reaction takes place at a relatively mild 280° C, it happens in an orderly fashion, snapping long polyethylene molecules into shorter chains that are each about 30 carbons long. Those fragments then arrange themselves into the six-sided ring structures characteristic of alkylaromatic compounds.
After 24 hours in the reaction chamber, “most of the products are liquids, and most of the liquids are alkylaromatics,” Scott says. In experiments, about 69 percent of the plastic in a low-density polyethylene bag was converted into liquid. About 55 percent of a high-density polyethylene bottle cap was transformed. The process produces hydrocarbon gases too, which could be used to generate heat to run the reaction at a recycling plant, Scott says.
For now, this is just a lab demo, and like many new recycling strategies, it’s still a long way off from commercialization. And no single upgrade to the recycling pipeline will rid the world of its growing mountains of plastic trash. “We’re going to need a suite of technologies to meet this challenge,” says Daniels, of the REMADE Institute. But each new technology — whether it’s focused on making plastics easier to recycle, or transforming them into more useful materials — could help."	16515
chemistry	['Emily Conover', 'Jake Buehler', 'Maria Temming', 'Charles Q. Choi', 'Tina Hesman Saey', 'Laura Sanders', 'Susan Milius', 'Carolyn Gramling', 'Jonathan Lambert', 'Lisa Grossman']	2021-01-07 21:16:22-05:00	"Chemistry students the world over are familiar with covalent bonds and hydrogen bonds. Now a study has revealed a strange variety of bond that acts like a hybrid of the two. Its properties raise questions about how chemical bonds are defined, chemists report in the Jan. 8 Science.

Hydrogen bonds are typically thought of as weak electrical attractions rather than true chemical bonds. Covalent bonds, on the other hand, are strong chemical bonds that hold together atoms within a molecule and result from electrons being shared among atoms. Now, researchers report that an unusually strong variety of hydrogen bond is in fact a hybrid, as it involves shared electrons, blurring the distinction between hydrogen and covalent bonds.

“Our understanding of chemical bonding, the way we teach it, is very much black and white,” says chemist Andrei Tokmakoff of the University of Chicago. The new study shows that “there’s actually a continuum.”

Tokmakoff and colleagues characterized the hybrid bond by observing groups of atoms called bifluoride ions, consisting of a single hydrogen atom sandwiched between a pair of fluorine atoms, in water. According to conventional wisdom, the hydrogen atom is bound to one fluorine by a covalent bond and to the other fluorine by a hydrogen bond.

Sign Up For the Latest from Science News Headlines and summaries of the latest Science News articles, delivered to your inbox Client key* E-mail* Go

The researchers used infrared light to set bifluoride ions vibrating and measured the hydrogen atoms’ response, revealing a series of energy levels at which the hydrogen atoms vibrated. For a typical hydrogen bond, the spacing between those energy levels would decrease as the atom climbed further up the energy ladder. But instead, the researchers found that the spacing increased. This behavior indicated that the hydrogen atom was shared between the two fluorine atoms equally, rather than being closely bound to one fluorine atom by a covalent bond and more loosely bound by a typical hydrogen bond to the other. In that arrangement, “the difference between the covalent and [hydrogen] bond is erased and is no longer meaningful,” says study coauthor Bogdan Dereka, a chemist also at the University of Chicago.

Computer calculations showed that this behavior is dependent on the distance between the two fluorine atoms. As the fluorine atoms move closer to each other, squeezing the hydrogen between them, the normal hydrogen bond becomes stronger, until all three atoms begin sharing electrons as in a covalent bond, forming a single link that the researchers call a hydrogen-mediated chemical bond. For fluorine atoms that are farther apart, the conventional description, with distinct covalent and hydrogen bonds, still applies.

The hydrogen-mediated chemical bond can’t be described as either a pure hydrogen bond or a pure covalent bond, the researchers conclude. “It’s really some hybrid of the two,” says chemist Mischa Bonn of the Max Planck Institute for Polymer Research in Mainz, Germany, who coauthored a perspective piece on the study, also published in Science.

Hydrogen bonds occur in a variety of substances, most famously in water. Without hydrogen bonds, water at room temperature would be a gas instead of a liquid. While most hydrogen bonds in water are weak, strong hydrogen bonds similar to the ones found in the bifluoride ions can form in water that contains excess hydrogen ions. Two water molecules can sandwich a hydrogen ion, creating what’s called a Zundel ion, in which the hydrogen ion is equally shared between the two water molecules. The new results echo the Zundel ion’s behavior, says chemist Erik Nibbering of the Max Born Institute for Nonlinear Optics and Short Pulse Spectroscopy in Berlin, who coauthored a 2017 paper in Science on the Zundel ion. “It all fits nicely.”

Strong hydrogen bonds are thought to play a role in transporting hydrogen ions, a process crucial for a variety of biological mechanisms including powering cells and for technologies such as fuel cells. So better understanding these bonds could shed light on a variety of effects.

And the new observation has implications for how scientists understand basic principles of chemistry. “It touches on our fundamental understanding of what a chemical bond is,” Bonn says.

That newfound understanding of chemical bonding also raises questions about what qualifies as a molecule. Atoms connected by covalent bonds are considered part of a single molecule, while those connected by hydrogen bonds can remain separate entities. So bonds in limbo between the two raise the question, “when do you go from two molecules to one molecule?” Tokmakoff says."	https://www.sciencenews.org/article/new-weird-hybrid-chemical-bond-hydrogen-covalent	"Chemistry students the world over are familiar with covalent bonds and hydrogen bonds. Now a study has revealed a strange variety of bond that acts like a hybrid of the two. Its properties raise questions about how chemical bonds are defined, chemists report in the Jan. 8 Science.
Hydrogen bonds are typically thought of as weak electrical attractions rather than true chemical bonds. Covalent bonds, on the other hand, are strong chemical bonds that hold together atoms within a molecule and result from electrons being shared among atoms. Now, researchers report that an unusually strong variety of hydrogen bond is in fact a hybrid, as it involves shared electrons, blurring the distinction between hydrogen and covalent bonds.
“Our understanding of chemical bonding, the way we teach it, is very much black and white,” says chemist Andrei Tokmakoff of the University of Chicago. The new study shows that “there’s actually a continuum.”
Tokmakoff and colleagues characterized the hybrid bond by observing groups of atoms called bifluoride ions, consisting of a single hydrogen atom sandwiched between a pair of fluorine atoms, in water. According to conventional wisdom, the hydrogen atom is bound to one fluorine by a covalent bond and to the other fluorine by a hydrogen bond.
Sign Up For the Latest from Science News Headlines and summaries of the latest Science News articles, delivered to your inbox Client key* E-mail* Go
The researchers used infrared light to set bifluoride ions vibrating and measured the hydrogen atoms’ response, revealing a series of energy levels at which the hydrogen atoms vibrated. For a typical hydrogen bond, the spacing between those energy levels would decrease as the atom climbed further up the energy ladder. But instead, the researchers found that the spacing increased. This behavior indicated that the hydrogen atom was shared between the two fluorine atoms equally, rather than being closely bound to one fluorine atom by a covalent bond and more loosely bound by a typical hydrogen bond to the other. In that arrangement, “the difference between the covalent and [hydrogen] bond is erased and is no longer meaningful,” says study coauthor Bogdan Dereka, a chemist also at the University of Chicago.
Computer calculations showed that this behavior is dependent on the distance between the two fluorine atoms. As the fluorine atoms move closer to each other, squeezing the hydrogen between them, the normal hydrogen bond becomes stronger, until all three atoms begin sharing electrons as in a covalent bond, forming a single link that the researchers call a hydrogen-mediated chemical bond. For fluorine atoms that are farther apart, the conventional description, with distinct covalent and hydrogen bonds, still applies.
The hydrogen-mediated chemical bond can’t be described as either a pure hydrogen bond or a pure covalent bond, the researchers conclude. “It’s really some hybrid of the two,” says chemist Mischa Bonn of the Max Planck Institute for Polymer Research in Mainz, Germany, who coauthored a perspective piece on the study, also published in Science.
Hydrogen bonds occur in a variety of substances, most famously in water. Without hydrogen bonds, water at room temperature would be a gas instead of a liquid. While most hydrogen bonds in water are weak, strong hydrogen bonds similar to the ones found in the bifluoride ions can form in water that contains excess hydrogen ions. Two water molecules can sandwich a hydrogen ion, creating what’s called a Zundel ion, in which the hydrogen ion is equally shared between the two water molecules. The new results echo the Zundel ion’s behavior, says chemist Erik Nibbering of the Max Born Institute for Nonlinear Optics and Short Pulse Spectroscopy in Berlin, who coauthored a 2017 paper in Science on the Zundel ion. “It all fits nicely.”
Strong hydrogen bonds are thought to play a role in transporting hydrogen ions, a process crucial for a variety of biological mechanisms including powering cells and for technologies such as fuel cells. So better understanding these bonds could shed light on a variety of effects.
And the new observation has implications for how scientists understand basic principles of chemistry. “It touches on our fundamental understanding of what a chemical bond is,” Bonn says.
That newfound understanding of chemical bonding also raises questions about what qualifies as a molecule. Atoms connected by covalent bonds are considered part of a single molecule, while those connected by hydrogen bonds can remain separate entities. So bonds in limbo between the two raise the question, “when do you go from two molecules to one molecule?” Tokmakoff says."	4689
chemistry	['Maria Temming', 'Jake Buehler', 'Charles Q. Choi', 'Tina Hesman Saey', 'Laura Sanders', 'Susan Milius', 'Carolyn Gramling', 'Jonathan Lambert', 'Emily Conover', 'Lisa Grossman']	2021-01-05 11:00:00-05:00	"Zinc-air batteries have a lot going for them. They’re lightweight, compact and made of more sustainable, less flammable materials than other batteries. But they’re usually not rechargeable.

A new battery design could change that. By tweaking the building materials, researchers created a prototype of a zinc-air battery that could be recharged hundreds of times. Such long-lasting devices, described in the Jan. 1 Science, could one day power electric cars or other electronics.

Zinc-air batteries are one of many potential next-generation batteries that could hold more energy while being cheaper and safer than existing devices (SN: 1/9/17). Every zinc-air battery cell contains two electrodes — a zinc anode and a porous cathode — separated by a liquid called an electrolyte. In standard zinc-air cells, the electrolyte is a high-pH substance, containing ingredients like potassium hydroxide. Oxygen from the air enters the cathode, where the gas reacts with water from the electrolyte to form hydroxide. Hydroxide formed at the cathode surface travels to the anode and reacts with zinc to release energy that powers other devices.

“The problem is, this reaction is not very reversible,” says Wei Sun, a materials scientist at the University of Münster in Germany. And that makes it hard to recharge the battery. The caustic electrolyte in conventional zinc-air batteries can also degrade the cathode and anode.

To solve those problems, Sun and colleagues built a zinc-air battery using a new electrolyte that contains water-repellant ions. Those ions stick to the cathode, preventing H 2 O from the electrolyte from reacting with incoming oxygen at the cathode surface. As a result, zinc ions from the anode can travel to the cathode and react directly with oxygen from the air. This relatively simple reaction is easy to run backward to recharge the battery.

What’s more, the new electrolyte doesn’t degrade the battery’s electrodes, which helps the battery last longer. In lab experiments, Sun and colleagues were able to drain and recharge a new zinc-air battery cell 320 times over 160 hours."	https://www.sciencenews.org/article/zinc-air-batteries-single-use-new-design-rechargeable	"Zinc-air batteries have a lot going for them. They’re lightweight, compact and made of more sustainable, less flammable materials than other batteries. But they’re usually not rechargeable.
A new battery design could change that. By tweaking the building materials, researchers created a prototype of a zinc-air battery that could be recharged hundreds of times. Such long-lasting devices, described in the Jan. 1 Science, could one day power electric cars or other electronics.
Zinc-air batteries are one of many potential next-generation batteries that could hold more energy while being cheaper and safer than existing devices (SN: 1/9/17). Every zinc-air battery cell contains two electrodes — a zinc anode and a porous cathode — separated by a liquid called an electrolyte. In standard zinc-air cells, the electrolyte is a high-pH substance, containing ingredients like potassium hydroxide. Oxygen from the air enters the cathode, where the gas reacts with water from the electrolyte to form hydroxide. Hydroxide formed at the cathode surface travels to the anode and reacts with zinc to release energy that powers other devices.
“The problem is, this reaction is not very reversible,” says Wei Sun, a materials scientist at the University of Münster in Germany. And that makes it hard to recharge the battery. The caustic electrolyte in conventional zinc-air batteries can also degrade the cathode and anode.
To solve those problems, Sun and colleagues built a zinc-air battery using a new electrolyte that contains water-repellant ions. Those ions stick to the cathode, preventing H 2 O from the electrolyte from reacting with incoming oxygen at the cathode surface. As a result, zinc ions from the anode can travel to the cathode and react directly with oxygen from the air. This relatively simple reaction is easy to run backward to recharge the battery.
What’s more, the new electrolyte doesn’t degrade the battery’s electrodes, which helps the battery last longer. In lab experiments, Sun and colleagues were able to drain and recharge a new zinc-air battery cell 320 times over 160 hours."	2100
chemistry	['Maria Temming', 'Jake Buehler', 'Charles Q. Choi', 'Tina Hesman Saey', 'Laura Sanders', 'Susan Milius', 'Carolyn Gramling', 'Jonathan Lambert', 'Emily Conover', 'Lisa Grossman']	2020-12-22 16:00:50-05:00	"Today, airplanes pump a lot of climate-warming carbon dioxide into the atmosphere. But someday, carbon dioxide sucked from the atmosphere could be used to power airplanes.

A new iron-based catalyst converts carbon dioxide into jet fuel, researchers report online December 22 in Nature Communications. Unlike cars, planes can’t carry batteries big enough to run on electricity from wind or solar power. But if CO 2 , rather than oil, were used to make jet fuel, that could reduce the air travel industry’s carbon footprint — which currently makes up 12 percent of all transportation-related CO 2 emissions.

Past attempts to convert carbon dioxide into fuel have relied on catalysts made of relatively expensive materials, like cobalt, and required multiple chemical processing steps. The new catalyst powder is made of inexpensive ingredients, including iron, and transforms CO 2 in a single step.

When placed in a reaction chamber with carbon dioxide and hydrogen gas, the catalyst helps carbon from the CO 2 molecules separate from oxygen and link up with hydrogen — forming the hydrocarbon molecules that make up jet fuel. The leftover oxygen atoms from the CO 2 join up with other hydrogen atoms to form water.

Tiancun Xiao, a chemist at the University of Oxford, and colleagues tested their new catalyst on carbon dioxide in a small reaction chamber set to 300° Celsius and pressurized to about 10 times the air pressure at sea level. Over 20 hours, the catalyst converted 38 percent of the carbon dioxide in the chamber into new chemical products. About 48 percent of those products were jet fuel hydrocarbons. Other by-products included similar petrochemicals, such as ethylene and propylene, which can be used to make plastics."	https://www.sciencenews.org/article/new-iron-based-catalyst-converts-carbon-dioxide-into-jet-fuel	"Today, airplanes pump a lot of climate-warming carbon dioxide into the atmosphere. But someday, carbon dioxide sucked from the atmosphere could be used to power airplanes.
A new iron-based catalyst converts carbon dioxide into jet fuel, researchers report online December 22 in Nature Communications. Unlike cars, planes can’t carry batteries big enough to run on electricity from wind or solar power. But if CO 2 , rather than oil, were used to make jet fuel, that could reduce the air travel industry’s carbon footprint — which currently makes up 12 percent of all transportation-related CO 2 emissions.
Past attempts to convert carbon dioxide into fuel have relied on catalysts made of relatively expensive materials, like cobalt, and required multiple chemical processing steps. The new catalyst powder is made of inexpensive ingredients, including iron, and transforms CO 2 in a single step.
When placed in a reaction chamber with carbon dioxide and hydrogen gas, the catalyst helps carbon from the CO 2 molecules separate from oxygen and link up with hydrogen — forming the hydrocarbon molecules that make up jet fuel. The leftover oxygen atoms from the CO 2 join up with other hydrogen atoms to form water.
Tiancun Xiao, a chemist at the University of Oxford, and colleagues tested their new catalyst on carbon dioxide in a small reaction chamber set to 300° Celsius and pressurized to about 10 times the air pressure at sea level. Over 20 hours, the catalyst converted 38 percent of the carbon dioxide in the chamber into new chemical products. About 48 percent of those products were jet fuel hydrocarbons. Other by-products included similar petrochemicals, such as ethylene and propylene, which can be used to make plastics."	1734
chemistry	['Erin Garcia De Jesús', 'Jake Buehler', 'Maria Temming', 'Charles Q. Choi', 'Tina Hesman Saey', 'Laura Sanders', 'Susan Milius', 'Carolyn Gramling', 'Jonathan Lambert', 'Emily Conover']	2020-11-10 14:00:00-05:00	"Just how hot is your chili pepper? A new chili-shaped device could quickly signal whether adding the pepper to a meal might set your mouth ablaze.

Called the Chilica-pod, the device detects capsaicin, a chemical compound that helps give peppers their sometimes painful kick. In general, the more capsaicin a pepper has, the hotter it tastes. The Chilica-pod is sensitive, capable of detecting extremely low levels of the fiery molecule, researchers report in the Oct. 23 ACS Applied Nano Materials.

The device could someday be used to test cooked meals or fresh peppers, says analytical chemist Warakorn Limbut of Prince of Songkla University in Hat Yai, Thailand. People with a capsaicin allergy could use the gadget to avoid the compound, or farmers could test harvested peppers to better indicate their spiciness, he says.

A portable, chili-shaped gadget determines chili peppers’ spice levels by measuring capsaicin — the fiery molecule that helps give peppers their kick. The “Chilica-pod” is then plugged into a smartphone, where an app analyzes the data. Adapted from ACS Applied Nano Materials 2020

A pepper’s relative spiciness typically is conveyed in Scoville heat units — an imperfect measurement determined by a panel of human taste testers. Other more precise methods for determining spiciness are time-intensive and involve expensive equipment, making the methods unsuitable for a quick answer.

Enter the portable, smartphone-compatible Chilica-pod. Built by Limbut and colleagues, the instrument’s sensor is composed of stacks of graphene sheets. When a drop of a chili pepper and ethanol solution is added to the sensor, the capsaicin from the pepper triggers the movement of electrons among the graphene atoms. The more capsaicin the solution has, the stronger the electrical current through the sheets.

The Chilica-pod registers that electrical activity and, once its “stem” is plugged into a smartphone, sends the information to an app for analysis. The device can detect capsaicin levels as low as 0.37 micromoles per liter of solution, equivalent to the amount in a pepper with no heat, one test showed.

Limbut’s team used the Chilica-pod to individually measure six dried chili peppers from a local market. The peppers’ capsaicin concentrations ranged from 7.5 to 90 micromoles per liter of solution, the team found. When translated to Scoville heat units, that range corresponds to the spice of peppers like serrano or cayenne — mild varieties compared to the blazing hot Carolina reaper, one of the world’s hottest peppers (SN: 4/9/18).

Paul Bosland, a plant geneticist and chili breeder at New Mexico State University in Las Cruces who wasn’t involved in the study, notes that capsaicin is just one of at least 24 related compounds that give peppers heat. “I would hope that [the device] could read them all,” he says."	https://www.sciencenews.org/article/portable-device-chili-pepper-heat-capsaicin	"Just how hot is your chili pepper? A new chili-shaped device could quickly signal whether adding the pepper to a meal might set your mouth ablaze.
Called the Chilica-pod, the device detects capsaicin, a chemical compound that helps give peppers their sometimes painful kick. In general, the more capsaicin a pepper has, the hotter it tastes. The Chilica-pod is sensitive, capable of detecting extremely low levels of the fiery molecule, researchers report in the Oct. 23 ACS Applied Nano Materials.
The device could someday be used to test cooked meals or fresh peppers, says analytical chemist Warakorn Limbut of Prince of Songkla University in Hat Yai, Thailand. People with a capsaicin allergy could use the gadget to avoid the compound, or farmers could test harvested peppers to better indicate their spiciness, he says.
A portable, chili-shaped gadget determines chili peppers’ spice levels by measuring capsaicin — the fiery molecule that helps give peppers their kick. The “Chilica-pod” is then plugged into a smartphone, where an app analyzes the data. Adapted from ACS Applied Nano Materials 2020
A pepper’s relative spiciness typically is conveyed in Scoville heat units — an imperfect measurement determined by a panel of human taste testers. Other more precise methods for determining spiciness are time-intensive and involve expensive equipment, making the methods unsuitable for a quick answer.
Enter the portable, smartphone-compatible Chilica-pod. Built by Limbut and colleagues, the instrument’s sensor is composed of stacks of graphene sheets. When a drop of a chili pepper and ethanol solution is added to the sensor, the capsaicin from the pepper triggers the movement of electrons among the graphene atoms. The more capsaicin the solution has, the stronger the electrical current through the sheets.
The Chilica-pod registers that electrical activity and, once its “stem” is plugged into a smartphone, sends the information to an app for analysis. The device can detect capsaicin levels as low as 0.37 micromoles per liter of solution, equivalent to the amount in a pepper with no heat, one test showed.
Limbut’s team used the Chilica-pod to individually measure six dried chili peppers from a local market. The peppers’ capsaicin concentrations ranged from 7.5 to 90 micromoles per liter of solution, the team found. When translated to Scoville heat units, that range corresponds to the spice of peppers like serrano or cayenne — mild varieties compared to the blazing hot Carolina reaper, one of the world’s hottest peppers (SN: 4/9/18).
Paul Bosland, a plant geneticist and chili breeder at New Mexico State University in Las Cruces who wasn’t involved in the study, notes that capsaicin is just one of at least 24 related compounds that give peppers heat. “I would hope that [the device] could read them all,” he says."	2844
chemistry	['Maria Temming', 'Jake Buehler', 'Charles Q. Choi', 'Tina Hesman Saey', 'Laura Sanders', 'Susan Milius', 'Carolyn Gramling', 'Jonathan Lambert', 'Emily Conover', 'Lisa Grossman']	2020-10-19 10:00:00-04:00	"A few minutes in the microwave made a common insecticide about 10 times more lethal to mosquitoes in lab experiments.

The toxin deltamethrin is used around the world in home sprays and bed nets to curb the spread of mosquito-borne diseases like malaria — which kills over 400,000 people each year, according to the World Health Organization. But “mosquitoes the world over are showing resistance to deltamethrin and [similar] compounds,” says Bart Kahr, a crystallographer at New York University who has helped develop a more potent form of deltamethrin by heating it.

This form of deltamethrin may stand a better chance of killing insecticide-resistant pests, Kahr and colleagues report online October 12 in the Proceedings of the National Academy of Sciences. Malaria has been essentially eradicated in the United States, but more effective pesticides could be a boon for regions like sub-Saharan Africa, where the disease is a major public health problem.

Kahr’s team increased the potency of commercial deltamethrin dust spray simply by melting a vial of it — either by heating it to 150° Celsius in an oil bath for five minutes or by popping it in a 700-watt microwave for the same amount of time. While the microscopic deltamethrin crystals in the original spray have a haphazard structure, which looks like a jumble of misaligned flakes, the melted deltamethrin crystals solidified into starburst shapes when they cooled to room temperature.

The deltamethrin crystals in typical insecticide spray (left microscope image) contain “lots of individual leaflets that are kind of oriented in a helter-skelter manner,” says crystallographer Bart Kahr of New York University. In a new version of the spray, deltamethrin crystals are shaped more like starbursts, with fibers growing out from a single point (right). Jingxiang Yang

Chemical bonds between deltamethrin molecules in the starburst-shaped crystals are not as strong as those in the original microcrystal structure. “The molecules are intrinsically less happy, or settled, in the arrangement,” Kahr says. So, when a mosquito lands on a dusting of starburst-shaped crystals, it should be easier for deltamethrin molecules to be absorbed into the insect’s body via its feet.

The researchers tested the more potent version of deltamethrin on lab-bred mosquitoes from two species: Anopheles quadrimaculatus, which can spread malaria, and Aedes aegypti, which can transmit other life-threatening diseases, such as Zika and dengue (SN: 1/8/19). Forty mosquitoes of each species were released into petri dishes coated in the original deltamethrin dust spray, and another 40 into a dish covered in the new form of the insecticide.

Sign Up For the Latest from Science News Headlines and summaries of the latest Science News articles, delivered to your inbox Client key* E-mail* Go

That altered version of deltamethrin knocked out about half of exposed A. quadrimaculatus mosquitoes within 24 minutes. In contrast, it took nearly five hours for the original spray to knock out half of exposed Anopheles — about 12 times as long. Likewise, it took only 21 minutes for the new spray to knock out half of exposed A. aegypti, while it took the original spray over three hours.

Although A. quadrimaculatus can carry the parasite that causes malaria, this mosquito species is native to North America, where the disease is not a major public health crisis. To ensure the new type of deltamethrin would be effective in the world’s malarial hot spots, “we need to do these experiments with species called gambiae and funestus, which are the African Anopheles mosquito species,” Kahr says, as well as the six major malaria-spreading Anopheles species in South Asia.

Heat treatment for deltamethrin sprays “might increase their toxicity, but there are several obvious experiments that we would need to do before we would even think about adding this to the production system,” says Janet Hemingway of the Liverpool School of Tropical Medicine in England, who studies mosquito insecticide resistance.

First, researchers need to test the new version of the insecticide against pesticide-resistant mosquitoes. Mosquito resistance to deltamethrin, along with other chemicals in the class of synthetic pesticides known as pyrethroids, is a growing problem (SN: 6/29/12). “My prediction … is that [the insects] would be highly resistant to both forms,” Hemingway says.

Researchers also need to ensure that the more toxic form of deltamethrin is safe for people to be around, says Hemingway, who was not involved in the study. “Bottom line — interesting observation, but one that is a good distance from something that could be implemented.”"	https://www.sciencenews.org/article/heat-deltamethrin-pesticide-resistant-mosquitoes-insecticide	"A few minutes in the microwave made a common insecticide about 10 times more lethal to mosquitoes in lab experiments.
The toxin deltamethrin is used around the world in home sprays and bed nets to curb the spread of mosquito-borne diseases like malaria — which kills over 400,000 people each year, according to the World Health Organization. But “mosquitoes the world over are showing resistance to deltamethrin and [similar] compounds,” says Bart Kahr, a crystallographer at New York University who has helped develop a more potent form of deltamethrin by heating it.
This form of deltamethrin may stand a better chance of killing insecticide-resistant pests, Kahr and colleagues report online October 12 in the Proceedings of the National Academy of Sciences. Malaria has been essentially eradicated in the United States, but more effective pesticides could be a boon for regions like sub-Saharan Africa, where the disease is a major public health problem.
Kahr’s team increased the potency of commercial deltamethrin dust spray simply by melting a vial of it — either by heating it to 150° Celsius in an oil bath for five minutes or by popping it in a 700-watt microwave for the same amount of time. While the microscopic deltamethrin crystals in the original spray have a haphazard structure, which looks like a jumble of misaligned flakes, the melted deltamethrin crystals solidified into starburst shapes when they cooled to room temperature.
The deltamethrin crystals in typical insecticide spray (left microscope image) contain “lots of individual leaflets that are kind of oriented in a helter-skelter manner,” says crystallographer Bart Kahr of New York University. In a new version of the spray, deltamethrin crystals are shaped more like starbursts, with fibers growing out from a single point (right). Jingxiang Yang
Chemical bonds between deltamethrin molecules in the starburst-shaped crystals are not as strong as those in the original microcrystal structure. “The molecules are intrinsically less happy, or settled, in the arrangement,” Kahr says. So, when a mosquito lands on a dusting of starburst-shaped crystals, it should be easier for deltamethrin molecules to be absorbed into the insect’s body via its feet.
The researchers tested the more potent version of deltamethrin on lab-bred mosquitoes from two species: Anopheles quadrimaculatus, which can spread malaria, and Aedes aegypti, which can transmit other life-threatening diseases, such as Zika and dengue (SN: 1/8/19). Forty mosquitoes of each species were released into petri dishes coated in the original deltamethrin dust spray, and another 40 into a dish covered in the new form of the insecticide.
Sign Up For the Latest from Science News Headlines and summaries of the latest Science News articles, delivered to your inbox Client key* E-mail* Go
That altered version of deltamethrin knocked out about half of exposed A. quadrimaculatus mosquitoes within 24 minutes. In contrast, it took nearly five hours for the original spray to knock out half of exposed Anopheles — about 12 times as long. Likewise, it took only 21 minutes for the new spray to knock out half of exposed A. aegypti, while it took the original spray over three hours.
Although A. quadrimaculatus can carry the parasite that causes malaria, this mosquito species is native to North America, where the disease is not a major public health crisis. To ensure the new type of deltamethrin would be effective in the world’s malarial hot spots, “we need to do these experiments with species called gambiae and funestus, which are the African Anopheles mosquito species,” Kahr says, as well as the six major malaria-spreading Anopheles species in South Asia.
Heat treatment for deltamethrin sprays “might increase their toxicity, but there are several obvious experiments that we would need to do before we would even think about adding this to the production system,” says Janet Hemingway of the Liverpool School of Tropical Medicine in England, who studies mosquito insecticide resistance.
First, researchers need to test the new version of the insecticide against pesticide-resistant mosquitoes. Mosquito resistance to deltamethrin, along with other chemicals in the class of synthetic pesticides known as pyrethroids, is a growing problem (SN: 6/29/12). “My prediction … is that [the insects] would be highly resistant to both forms,” Hemingway says.
Researchers also need to ensure that the more toxic form of deltamethrin is safe for people to be around, says Hemingway, who was not involved in the study. “Bottom line — interesting observation, but one that is a good distance from something that could be implemented.”"	4673
chemistry	['Carolyn Wilke', 'Jake Buehler', 'Maria Temming', 'Charles Q. Choi', 'Tina Hesman Saey', 'Laura Sanders', 'Susan Milius', 'Carolyn Gramling', 'Jonathan Lambert', 'Emily Conover']	2020-05-19 12:00:00-04:00	Here’s a clue to how this tube worm’s slime can glow blue for days Scientists start to untangle the chemistry behind the mysterious light	https://www.sciencenews.org/article/how-this-tube-worm-slime-goo-mucus-can-glow-blue	Here’s a clue to how this tube worm’s slime can glow blue for days Scientists start to untangle the chemistry behind the mysterious light	137
